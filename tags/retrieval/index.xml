<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Retrieval - Tag - Sumit&#39;s Diary</title>
        <link>https://blog.reachsumit.com/tags/retrieval/</link>
        <description>Retrieval - Tag - Sumit&#39;s Diary</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>hello@reachsumit.com (Sumit Kumar)</managingEditor>
            <webMaster>hello@reachsumit.com (Sumit Kumar)</webMaster><lastBuildDate>Sat, 27 Sep 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.reachsumit.com/tags/retrieval/" rel="self" type="application/rss+xml" /><item>
    <title>Probing LLMs&#39; Knowledge Boundary: Adaptive RAG, Part 3</title>
    <link>https://blog.reachsumit.com/posts/2025/09/probing-llms-knowledge-boundary/</link>
    <pubDate>Sat, 27 Sep 2025 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2025/09/probing-llms-knowledge-boundary/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2025/09/probing-llms-knowledge-boundary/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>This post introduces techniques that probe the LLM&rsquo;s internal confidence and knowledge boundaries. We explore prompt-based confidence detection, consistency-based uncertainty estimation, and internal state analysis approaches to determine when retrieval is truly necessary.]]></description>
</item><item>
    <title>Deciding When Not to Retrieve: Adaptive RAG, Part 2</title>
    <link>https://blog.reachsumit.com/posts/2025/09/deciding-when-not-to-retrieve/</link>
    <pubDate>Sun, 21 Sep 2025 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2025/09/deciding-when-not-to-retrieve/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2025/09/deciding-when-not-to-retrieve/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>Building on part 1&rsquo;s exploration of naive RAG&rsquo;s limitations, this post introduces adaptive retrieval frameworks and pre-generation retrieval decision-making methods that determine if retrieval is truly necessary.]]></description>
</item><item>
    <title>The Hidden Costs of Naive Retrieval: Adaptive RAG, Part 1</title>
    <link>https://blog.reachsumit.com/posts/2025/09/problems-with-naive-rag/</link>
    <pubDate>Mon, 01 Sep 2025 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2025/09/problems-with-naive-rag/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2025/09/problems-with-naive-rag/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>Retrieval-Augmented Generation (RAG) isn&rsquo;t a silver bullet. This post highlights the hidden costs associated with RAG and makes the case for a smarter, adaptive approach.]]></description>
</item><item>
    <title>Strategies for Effective and Efficient Text Ranking Using Large Language Models</title>
    <link>https://blog.reachsumit.com/posts/2023/12/towards-ranking-aware-llms/</link>
    <pubDate>Tue, 26 Dec 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/12/towards-ranking-aware-llms/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/12/towards-ranking-aware-llms/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>The previous article did a deep dive into the prompting-based pointwise, pairwise, and listwise techniques that directly use LLMs to perform reranking. In this article, we will take a closer look at some of the shortcomings of the prompting methods and explore the latest efforts to train ranking-aware LLMs. The article also describes several strategies to build effective and efficient LLM-based rerankers.]]></description>
</item><item>
    <title>Prompting-based Methods for Text Ranking Using Large Language Models</title>
    <link>https://blog.reachsumit.com/posts/2023/12/prompting-llm-for-ranking/</link>
    <pubDate>Wed, 20 Dec 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/12/prompting-llm-for-ranking/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/12/prompting-llm-for-ranking/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>Large Language Models (LLMs) have demonstrated impressive zero-shot performance on a wide variety of NLP tasks. Recently, there has been a growing interest in applying LLMs to zero-shot text ranking. This article describes a recent paradigm that uses prompting-based approaches to directly utilize LLMs as rerankers in a multi-stage ranking pipeline.]]></description>
</item><item>
    <title>Generative Retrieval for End-to-End Search Systems</title>
    <link>https://blog.reachsumit.com/posts/2023/09/generative-retrieval/</link>
    <pubDate>Wed, 06 Sep 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/09/generative-retrieval/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/09/generative-retrieval/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>Auto-regressive search engines emerge as a promising paradigm for next-gen information retrieval systems. This article introduces this generative retrieval and the various latest techniques that have been proposed to improve its effectiveness.]]></description>
</item><item>
    <title>Positive and Negative Sampling Strategies for Representation Learning in Semantic Search</title>
    <link>https://blog.reachsumit.com/posts/2023/03/pairing-for-representation/</link>
    <pubDate>Wed, 22 Mar 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/03/pairing-for-representation/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/03/pairing-for-representation/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>Defining positive and negative labels for a retrieval task in a search ranking system is a non-trivial problem. This article goes over various sampling strategies for creating negative and positive pairs for effective representation learning. It introduces the concept of mining hard examples, followed by various strategies to sample hard positive and hard negative pairs. The article includes a lot of tips and learnings based on heuristics and empirical results from a comprehensive set of research papers published across the industry and academia.]]></description>
</item><item>
    <title>Zero and Few Shot Text Retrieval and Ranking Using Large Language Models</title>
    <link>https://blog.reachsumit.com/posts/2023/03/llm-for-text-ranking/</link>
    <pubDate>Thu, 16 Mar 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/03/llm-for-text-ranking/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/03/llm-for-text-ranking/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>Large Language Models (LLMs), like GPT-x, PaLM, BLOOM, have shaken up the NLP domain and completely redefined the state-of-the-art for a variety of tasks. One reason for the popularity of these LLMs has been their out-of-the-box capability to produce excellent performance with none to little domain-specific labeled data. The information retrieval community is also witnessing a revolution due to LLMs. These large pre-trained models can understand task instructions specified in natural language and then perform well on tasks in a zero-shot or few-shot manner. In this article, I review this theme and some of the most prominent ideas proposed by researchers in the last few months to enable zero/few-shot learning in text retrieval and ranking applications like search ranking, question answering, fact verification, etc.]]></description>
</item></channel>
</rss>
