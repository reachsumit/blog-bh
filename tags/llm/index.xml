<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>LLM - Tag - Sumit&#39;s Diary</title>
        <link>https://blog.reachsumit.com/tags/llm/</link>
        <description>LLM - Tag - Sumit&#39;s Diary</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>sam.sumitkumar@gmail.com (Sumit Kumar)</managingEditor>
            <webMaster>sam.sumitkumar@gmail.com (Sumit Kumar)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Mon, 10 Apr 2023 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.reachsumit.com/tags/llm/" rel="self" type="application/rss+xml" /><item>
    <title>Zero and Few Shot Recommender Systems based on Large Language Models</title>
    <link>https://blog.reachsumit.com/posts/2023/04/llm-for-recsys/</link>
    <pubDate>Mon, 10 Apr 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/04/llm-for-recsys/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/04/llm-for-recsys/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div><p>Recent developments in Large Language Models (LLMs) have brought a significant paradigm shift in Natural Language Processing (NLP) domain. These pretrained language models encode an extensive amount of world knowledge, and they can be applied to a multitude of downstream NLP applications with zero or just a handful of demonstrations.</p>
<p>While existing recommender systems mainly focus on behavior data, large language models (LLMs) encode extensive world knowledge mined from large-scale web corpora. Hence these LLMs store knowledge that can complement the behavior data. For example, an LLM, like ChatGPT, can easily recommend buying turkey on Thanksgiving day, in a zero-shot manner, even without having click behavior data related to turkeys or Thanksgiving.</p>
<p>Many researchers have recently proposed different approaches to building recommender systems using LLMs. These methods convert different recommendation tasks into either language understanding or language generation templates. This article highlights the prominent work done on this theme.</p>]]></description>
</item><item>
    <title>Zero and Few Shot Text Retrieval and Ranking Using Large Language Models</title>
    <link>https://blog.reachsumit.com/posts/2023/03/llm-for-text-ranking/</link>
    <pubDate>Thu, 16 Mar 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/03/llm-for-text-ranking/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/03/llm-for-text-ranking/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>Large Language Models (LLMs), like GPT-x, PaLM, BLOOM, have shaken up the NLP domain and completely redefined the state-of-the-art for a variety of tasks. One reason for the popularity of these LLMs has been their out-of-the-box capability to produce excellent performance with none to little domain-specific labeled data. The information retrieval community is also witnessing a revolution due to LLMs. These large pre-trained models can understand task instructions specified in natural language and then perform well on tasks in a zero-shot or few-shot manner. In this article, I review this theme and some of the most prominent ideas proposed by researchers in the last few months to enable zero/few-shot learning in text retrieval and ranking applications like search ranking, question answering, fact verification, etc.]]></description>
</item></channel>
</rss>
