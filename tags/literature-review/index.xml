<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Literature Review - Tag - Sumit&#39;s Diary</title>
        <link>https://blog.reachsumit.com/tags/literature-review/</link>
        <description>Literature Review - Tag - Sumit&#39;s Diary</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>hello@reachsumit.com (Sumit Kumar)</managingEditor>
            <webMaster>hello@reachsumit.com (Sumit Kumar)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 11 Aug 2024 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.reachsumit.com/tags/literature-review/" rel="self" type="application/rss+xml" /><item>
    <title>Incorporating Ads into Large Language Models Outputs</title>
    <link>https://blog.reachsumit.com/posts/2024/08/ads-llm/</link>
    <pubDate>Sun, 11 Aug 2024 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2024/08/ads-llm/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2024/08/ads-llm/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>This article provides an introduction to online advertising systems and explores research work that incorporates ads into the LLM responses to user queries of commercial nature.]]></description>
</item><item>
    <title>The Evolution of Multi-task Learning Based Video Recommender Systems - Part 2</title>
    <link>https://blog.reachsumit.com/posts/2024/06/multi-task-video-recsys-p2/</link>
    <pubDate>Sat, 22 Jun 2024 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2024/06/multi-task-video-recsys-p2/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2024/06/multi-task-video-recsys-p2/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>This article continues the discussion on the evolution of multi-task learning-based large-scale recommender systems. We take a look at strategies from Kuaishou, Tencent, YouTube, Facebook, and Amazon Prime Video to disentangle input space and address systematic biases. The article ends with sharing several tips and learnings for professionals working in this domain.]]></description>
</item><item>
    <title>The Evolution of Multi-task Learning Based Video Recommender Systems - Part 1</title>
    <link>https://blog.reachsumit.com/posts/2024/06/multi-task-video-recsys-p1/</link>
    <pubDate>Sun, 16 Jun 2024 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2024/06/multi-task-video-recsys-p1/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2024/06/multi-task-video-recsys-p1/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>This article introduces the multi-task learning paradigm adopted by various large-scale video recommender systems. It introduces a general setup for such an MTL-based recommender. It highlights several associated challenges and describes solutions adopted by various state-of-the-art recommenders in the industry.]]></description>
</item><item>
    <title>An Introduction to Multi-Task Learning based Recommender Systems</title>
    <link>https://blog.reachsumit.com/posts/2024/01/multi-task-learning-recsys/</link>
    <pubDate>Fri, 26 Jan 2024 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2024/01/multi-task-learning-recsys/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2024/01/multi-task-learning-recsys/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>This article provides an introduction and literature review for multi-task learning based recommender systems. We learn how to discover task relations, design MTL architectures and overcome some of the associated challenges.]]></description>
</item><item>
    <title>A Guide to User Behavior Modeling</title>
    <link>https://blog.reachsumit.com/posts/2024/01/user-behavior-modeling-recsys/</link>
    <pubDate>Sun, 07 Jan 2024 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2024/01/user-behavior-modeling-recsys/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2024/01/user-behavior-modeling-recsys/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>Modeling users&rsquo; past historical interactions or behavior sequences is an essential task for domains like recommender systems, click-through rate prediction, targeted advertisement, and more. This article provides a comprehensive introduction to the user behavior modeling paradigm along with highlighting several relevant and recent research works.]]></description>
</item><item>
    <title>Strategies for Effective and Efficient Text Ranking Using Large Language Models</title>
    <link>https://blog.reachsumit.com/posts/2023/12/towards-ranking-aware-llms/</link>
    <pubDate>Tue, 26 Dec 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/12/towards-ranking-aware-llms/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/12/towards-ranking-aware-llms/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>The previous article did a deep dive into the prompting-based pointwise, pairwise, and listwise techniques that directly use LLMs to perform reranking. In this article, we will take a closer look at some of the shortcomings of the prompting methods and explore the latest efforts to train ranking-aware LLMs. The article also describes several strategies to build effective and efficient LLM-based rerankers.]]></description>
</item><item>
    <title>Prompting-based Methods for Text Ranking Using Large Language Models</title>
    <link>https://blog.reachsumit.com/posts/2023/12/prompting-llm-for-ranking/</link>
    <pubDate>Wed, 20 Dec 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/12/prompting-llm-for-ranking/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/12/prompting-llm-for-ranking/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>Large Language Models (LLMs) have demonstrated impressive zero-shot performance on a wide variety of NLP tasks. Recently, there has been a growing interest in applying LLMs to zero-shot text ranking. This article describes a recent paradigm that uses prompting-based approaches to directly utilize LLMs as rerankers in a multi-stage ranking pipeline.]]></description>
</item><item>
    <title>Generative Retrieval for End-to-End Search Systems</title>
    <link>https://blog.reachsumit.com/posts/2023/09/generative-retrieval/</link>
    <pubDate>Wed, 06 Sep 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/09/generative-retrieval/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/09/generative-retrieval/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>Auto-regressive search engines emerge as a promising paradigm for next-gen information retrieval systems. This article introduces this generative retrieval and the various latest techniques that have been proposed to improve its effectiveness.]]></description>
</item><item>
    <title>Representing Users and Items in Large Language Models based Recommender Systems</title>
    <link>https://blog.reachsumit.com/posts/2023/06/llms-for-recsys-entity-representation/</link>
    <pubDate>Sun, 18 Jun 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/06/llms-for-recsys-entity-representation/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/06/llms-for-recsys-entity-representation/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>Large Language Models (LLMs) have emerged as viable tools for various recommendation tasks. This article highlights various methods for incorporating users, items, and behavior data into the instructions for LLMs.]]></description>
</item><item>
    <title>Shallow Embedding Models for Heterogeneous Graphs</title>
    <link>https://blog.reachsumit.com/posts/2023/05/shallow-heterogeneous-graphs-rep/</link>
    <pubDate>Tue, 30 May 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/05/shallow-heterogeneous-graphs-rep/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/05/shallow-heterogeneous-graphs-rep/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div><p>In previous articles, I gave an <a href="https://blog.reachsumit.com/posts/2023/04/intro-to-graph-representation-learning/" rel="">introduction to graph representation learning</a> and highlighted several <a href="https://blog.reachsumit.com/posts/2023/05/shallow-homogeneous-graphs-rep/" rel="">shallow methods for learning homogeneous graph embeddings</a>. This article focuses on shallow representation learning methods for heterogeneous graphs.</p>
<p>While homogeneous networks have only one type of nodes and edges, heterogeneous networks contain different types of nodes or edges. So, a homogeneous network can also be considered as a special case of a heterogeneous network. Heterogeneous networks, also called heterogeneous information networks (HIN), are ubiquitous in real-world scenarios. For example, social media websites, like Facebook, contain a set of node types, such as users, posts, groups and, tags. By learning heterogeneous graph embeddings, we learn low-dimensional representations of the graph while preserving the heterogeneous structures and semantics for the downstream tasks (such as node classification, link prediction, etc.).</p>]]></description>
</item></channel>
</rss>
