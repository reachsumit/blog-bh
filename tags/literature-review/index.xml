<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>literature review - Tag - Sumit&#39;s Diary</title>
        <link>https://blog.reachsumit.com/tags/literature-review/</link>
        <description>literature review - Tag - Sumit&#39;s Diary</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>sam.sumitkumar@gmail.com (Sumit Kumar)</managingEditor>
            <webMaster>sam.sumitkumar@gmail.com (Sumit Kumar)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Thu, 16 Mar 2023 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.reachsumit.com/tags/literature-review/" rel="self" type="application/rss+xml" /><item>
    <title>Zero and Few Shot Text Retrieval and Ranking Using Large Language Models</title>
    <link>https://blog.reachsumit.com/posts/2023/03/llm-for-text-ranking/</link>
    <pubDate>Thu, 16 Mar 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/03/llm-for-text-ranking/</guid>
    <description><![CDATA[Large Language Models (LLMs), like GPT-x, PaLM, BLOOM, have shaken up the NLP domain and completely redefined the state-of-the-art for a variety of tasks. One reason for the popularity of these LLMs has been their out-of-the-box capability to produce excellent performance with none to little domain-specific labeled data. The information retrieval community is also witnessing a revolution due to LLMs. These large pre-trained models can understand task instructions specified in natural language and then perform well on tasks in a zero-shot or few-shot manner. In this article, I review this theme and some of the most prominent ideas proposed by researchers in the last few months to enable zero/few-shot learning in text retrieval and ranking applications like search ranking, question answering, fact verification, etc.]]></description>
</item><item>
    <title>Next Gen Recommender Systems: Real-time reranking on mobile devices</title>
    <link>https://blog.reachsumit.com/posts/2023/03/reranking-on-edge/</link>
    <pubDate>Thu, 09 Mar 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/03/reranking-on-edge/</guid>
    <description><![CDATA[A traditional cloud-to-edge recommender system can&rsquo;t respond to user engagement and interests in real time. This article introduces on-device inference and on-device learning paradigms that can capture rich user behavior and respond to users&rsquo; changing interests in real time. The article also goes through system design choices and implementation details of different industrial applications that have served recommendations to billions of users, such as Kuaishou&rsquo;s Short Video Recommendation on Mobile Devices, and Taobao&rsquo;s (Alibaba) on-device recommender systems.]]></description>
</item><item>
    <title>Two Tower Model Architecture: Current State and Promising Extensions</title>
    <link>https://blog.reachsumit.com/posts/2023/03/two-tower-model/</link>
    <pubDate>Sat, 04 Mar 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/03/two-tower-model/</guid>
    <description><![CDATA[Two-tower model is widely adopted in industrial-scale retrieval and ranking workflows across a broad range of application domains, such as content recommendations, advertisement systems, and search engines. It is also the current go-to state-of-the-art solution for pre-ranking tasks. This article explores the history and current state of the Two Tower models and also highlights potential improvements proposed in some of the recently published literature. The goal here is to help understand what makes the Two Tower model an appropriate choice for a bunch of applications, and how it can be potentially extended from its current state.]]></description>
</item><item>
    <title>Specialized Deep Learning Architectures for Time Series Forecasting</title>
    <link>https://blog.reachsumit.com/posts/2023/01/dl-for-forecasting/</link>
    <pubDate>Sat, 21 Jan 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/01/dl-for-forecasting/</guid>
    <description><![CDATA[Modern time series forecasting requires a model to learn from multiple related time series. These time series often number in thousands or millions. Traditional statistical models do not scale well to these settings because they learn individual series in isolation and do not share parameters across series. Various deep learning models have been proposed recently with different inductive biases to work effectively under these settings. This article explores some of the most popular advances in deep learning architectures for modern time series forecasting.]]></description>
</item><item>
    <title>Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting</title>
    <link>https://blog.reachsumit.com/posts/2022/12/stats-vs-ml-for-ts/</link>
    <pubDate>Tue, 20 Dec 2022 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2022/12/stats-vs-ml-for-ts/</guid>
    <description><![CDATA[Statistical methods have been used in the time series domain for multiple decades. But given the recent advances in Machine Learning and especially its sub-domain Deep Learning, are statistical methods still superior for forecasting? In this article, we will do a deep dive into literature and recent time series competitions to do a multifaceted comparison between Statistical, Machine Learning, and Deep Learning methods for time series forecasting.]]></description>
</item><item>
    <title>Recommender Systems for Modeling Feature Interactions under Sparse Settings</title>
    <link>https://blog.reachsumit.com/posts/2022/11/sparse-recsys/</link>
    <pubDate>Sun, 06 Nov 2022 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2022/11/sparse-recsys/</guid>
    <description><![CDATA[Many machine learning domains, such as recommender systems, targeted advertisement, search ranking, and text analysis contain highly sparse data because of the large categorical variable domains. This sparsity makes it hard for ML algorithms to model second-order and above feature interactions. In this article, I summarize the need for modeling feature interactions and introduce some of the most popular ML architectures designed for estimating interactions from sparse data.
IntroductionProblem with sparse inputsA variety of Information Retrieval and Data Mining tasks, such as Recommender Systems, Targeted Advertising, Search Ranking, etc.]]></description>
</item><item>
    <title>Collaborative Filtering based Recommender Systems for Implicit Feedback Data</title>
    <link>https://blog.reachsumit.com/posts/2022/09/explicit-implicit-cf/</link>
    <pubDate>Sun, 25 Sep 2022 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2022/09/explicit-implicit-cf/</guid>
    <description><![CDATA[This article explains what explicit and implicit feedback data means for recommender systems. We discuss their characteristics and peculiarities concerning collaborative filtering based algorithms. Then we go over one of the most popular collaborative filtering algorithms for implicit data and implement it in Python with an example dataset.]]></description>
</item><item>
    <title>Towards Empathetic Dialogue Systems</title>
    <link>https://blog.reachsumit.com/posts/2020/12/generating-empathetic-responses/</link>
    <pubDate>Mon, 07 Dec 2020 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2020/12/generating-empathetic-responses/</guid>
    <description><![CDATA[<p>Recognizing feelings in the conversation partner and replying empathetically is a trivial skill for humans. But how can we infuse empathy into responses generated by a conversational dialogue agent or any of the text generation algorithm in Natural Language Processing? In this article, I will describe what empathy means through the lens of various academic disciplines and then do an in-depth review of the prior and current state-of-the-art NLU systems that can simulate empathy.</p>]]></description>
</item></channel>
</rss>
