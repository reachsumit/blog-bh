<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>ranking - Tag - Sumit&#39;s Diary</title>
        <link>https://blog.reachsumit.com/tags/ranking/</link>
        <description>ranking - Tag - Sumit&#39;s Diary</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>hello@reachsumit.com (Sumit Kumar)</managingEditor>
            <webMaster>hello@reachsumit.com (Sumit Kumar)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Wed, 20 Dec 2023 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.reachsumit.com/tags/ranking/" rel="self" type="application/rss+xml" /><item>
    <title>Prompting-based Methods for Text Ranking Using Large Language Models</title>
    <link>https://blog.reachsumit.com/posts/2023/12/prompting-llm-for-ranking/</link>
    <pubDate>Wed, 20 Dec 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/12/prompting-llm-for-ranking/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/12/prompting-llm-for-ranking/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>Large Language Models (LLMs) have demonstrated impressive zero-shot performance on a wide variety of NLP tasks. Recently, there has been a growing interest in applying LLMs to zero-shot text ranking. This article describes a recent paradigm that uses prompting-based approaches to directly utilize LLMs as rerankers in a multi-stage ranking pipeline.]]></description>
</item><item>
    <title>Twitter&#39;s For You Recommendation Algorithm</title>
    <link>https://blog.reachsumit.com/posts/2023/04/the-twitter-ml-algo/</link>
    <pubDate>Tue, 04 Apr 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/04/the-twitter-ml-algo/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/04/the-twitter-ml-algo/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>Twitter has open-sourced a majority of its recommendation algorithm. It offers an exciting opportunity for researchers, industry practitioners, and RecSys enthusiasts to take a close look at how Twitter computes the recommended feed for the For You page. This article described Twitter&rsquo;s end-to-end recommender system along with relevant literature and code references.]]></description>
</item><item>
    <title>Positive and Negative Sampling Strategies for Representation Learning in Semantic Search</title>
    <link>https://blog.reachsumit.com/posts/2023/03/pairing-for-representation/</link>
    <pubDate>Wed, 22 Mar 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/03/pairing-for-representation/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/03/pairing-for-representation/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>Defining positive and negative labels for a retrieval task in a search ranking system is a non-trivial problem. This article goes over various sampling strategies for creating negative and positive pairs for effective representation learning. It introduces the concept of mining hard examples, followed by various strategies to sample hard positive and hard negative pairs. The article includes a lot of tips and learnings based on heuristics and empirical results from a comprehensive set of research papers published across the industry and academia.]]></description>
</item><item>
    <title>Zero and Few Shot Text Retrieval and Ranking Using Large Language Models</title>
    <link>https://blog.reachsumit.com/posts/2023/03/llm-for-text-ranking/</link>
    <pubDate>Thu, 16 Mar 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/03/llm-for-text-ranking/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/03/llm-for-text-ranking/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>Large Language Models (LLMs), like GPT-x, PaLM, BLOOM, have shaken up the NLP domain and completely redefined the state-of-the-art for a variety of tasks. One reason for the popularity of these LLMs has been their out-of-the-box capability to produce excellent performance with none to little domain-specific labeled data. The information retrieval community is also witnessing a revolution due to LLMs. These large pre-trained models can understand task instructions specified in natural language and then perform well on tasks in a zero-shot or few-shot manner. In this article, I review this theme and some of the most prominent ideas proposed by researchers in the last few months to enable zero/few-shot learning in text retrieval and ranking applications like search ranking, question answering, fact verification, etc.]]></description>
</item><item>
    <title>Next Gen Recommender Systems: Real-time reranking on mobile devices</title>
    <link>https://blog.reachsumit.com/posts/2023/03/reranking-on-edge/</link>
    <pubDate>Thu, 09 Mar 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/03/reranking-on-edge/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/03/reranking-on-edge/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>A traditional cloud-to-edge recommender system can&rsquo;t respond to user engagement and interests in real time. This article introduces on-device inference and on-device learning paradigms that can capture rich user behavior and respond to users&rsquo; changing interests in real time. The article also goes through system design choices and implementation details of different industrial applications that have served recommendations to billions of users, such as Kuaishou&rsquo;s Short Video Recommendation on Mobile Devices, and Taobao&rsquo;s (Alibaba) on-device recommender systems.]]></description>
</item><item>
    <title>Two Tower Model Architecture: Current State and Promising Extensions</title>
    <link>https://blog.reachsumit.com/posts/2023/03/two-tower-model/</link>
    <pubDate>Sat, 04 Mar 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/03/two-tower-model/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/03/two-tower-model/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>Two-tower model is widely adopted in industrial-scale retrieval and ranking workflows across a broad range of application domains, such as content recommendations, advertisement systems, and search engines. It is also the current go-to state-of-the-art solution for pre-ranking tasks. This article explores the history and current state of the Two Tower models and also highlights potential improvements proposed in some of the recently published literature. The goal here is to help understand what makes the Two Tower model an appropriate choice for a bunch of applications, and how it can be potentially extended from its current state.]]></description>
</item></channel>
</rss>
