<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>All Posts - Sumit&#39;s Diary</title>
        <link>https://blog.reachsumit.com/posts/</link>
        <description>All Posts | Sumit&#39;s Diary</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>hello@reachsumit.com (Sumit Kumar)</managingEditor>
            <webMaster>hello@reachsumit.com (Sumit Kumar)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 16 Jun 2024 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.reachsumit.com/posts/" rel="self" type="application/rss+xml" /><item>
    <title>The Evolution of Multi-task Learning Based Video Recommender Systems - Part 1</title>
    <link>https://blog.reachsumit.com/posts/2024/06/multi-task-video-recsys-p1/</link>
    <pubDate>Sun, 16 Jun 2024 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2024/06/multi-task-video-recsys-p1/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2024/06/multi-task-video-recsys-p1/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>This article introduces the multi-task learning paradigm adopted by various large-scale video recommender systems. It introduces a general setup for such an MTL-based recommender. It highlights several associated challenges and describes solutions adopted by various state-of-the-art recommenders in the industry.]]></description>
</item><item>
    <title>An Introduction to Multi-Task Learning based Recommender Systems</title>
    <link>https://blog.reachsumit.com/posts/2024/01/multi-task-learning-recsys/</link>
    <pubDate>Fri, 26 Jan 2024 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2024/01/multi-task-learning-recsys/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2024/01/multi-task-learning-recsys/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>This article provides an introduction and literature review for multi-task learning based recommender systems. We learn how to discover task relations, design MTL architectures and overcome some of the associated challenges.]]></description>
</item><item>
    <title>A Guide to User Behavior Modeling</title>
    <link>https://blog.reachsumit.com/posts/2024/01/user-behavior-modeling-recsys/</link>
    <pubDate>Sun, 07 Jan 2024 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2024/01/user-behavior-modeling-recsys/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2024/01/user-behavior-modeling-recsys/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>Modeling users&rsquo; past historical interactions or behavior sequences is an essential task for domains like recommender systems, click-through rate prediction, targeted advertisement, and more. This article provides a comprehensive introduction to the user behavior modeling paradigm along with highlighting several relevant and recent research works.]]></description>
</item><item>
    <title>Strategies for Effective and Efficient Text Ranking Using Large Language Models</title>
    <link>https://blog.reachsumit.com/posts/2023/12/towards-ranking-aware-llms/</link>
    <pubDate>Tue, 26 Dec 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/12/towards-ranking-aware-llms/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/12/towards-ranking-aware-llms/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>The previous article did a deep dive into the prompting-based pointwise, pairwise, and listwise techniques that directly use LLMs to perform reranking. In this article, we will take a closer look at some of the shortcomings of the prompting methods and explore the latest efforts to train ranking-aware LLMs. The article also describes several strategies to build effective and efficient LLM-based rerankers.]]></description>
</item><item>
    <title>Prompting-based Methods for Text Ranking Using Large Language Models</title>
    <link>https://blog.reachsumit.com/posts/2023/12/prompting-llm-for-ranking/</link>
    <pubDate>Wed, 20 Dec 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/12/prompting-llm-for-ranking/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/12/prompting-llm-for-ranking/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>Large Language Models (LLMs) have demonstrated impressive zero-shot performance on a wide variety of NLP tasks. Recently, there has been a growing interest in applying LLMs to zero-shot text ranking. This article describes a recent paradigm that uses prompting-based approaches to directly utilize LLMs as rerankers in a multi-stage ranking pipeline.]]></description>
</item><item>
    <title>Generative Retrieval for End-to-End Search Systems</title>
    <link>https://blog.reachsumit.com/posts/2023/09/generative-retrieval/</link>
    <pubDate>Wed, 06 Sep 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/09/generative-retrieval/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/09/generative-retrieval/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>Auto-regressive search engines emerge as a promising paradigm for next-gen information retrieval systems. This article introduces this generative retrieval and the various latest techniques that have been proposed to improve its effectiveness.]]></description>
</item><item>
    <title>Representing Users and Items in Large Language Models based Recommender Systems</title>
    <link>https://blog.reachsumit.com/posts/2023/06/llms-for-recsys-entity-representation/</link>
    <pubDate>Sun, 18 Jun 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/06/llms-for-recsys-entity-representation/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/06/llms-for-recsys-entity-representation/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>Large Language Models (LLMs) have emerged as viable tools for various recommendation tasks. This article highlights various methods for incorporating users, items, and behavior data into the instructions for LLMs.]]></description>
</item><item>
    <title>Shallow Embedding Models for Heterogeneous Graphs</title>
    <link>https://blog.reachsumit.com/posts/2023/05/shallow-heterogeneous-graphs-rep/</link>
    <pubDate>Tue, 30 May 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/05/shallow-heterogeneous-graphs-rep/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/05/shallow-heterogeneous-graphs-rep/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>In previous articles, I gave an introduction to graph representation learning and highlighted several shallow methods for learning homogeneous graph embeddings. This article focuses on shallow representation learning methods for heterogeneous graphs.
While homogeneous networks have only one type of nodes and edges, heterogeneous networks contain different types of nodes or edges. So, a homogeneous network can also be considered as a special case of a heterogeneous network. Heterogeneous networks, also called heterogeneous information networks (HIN), are ubiquitous in real-world scenarios.]]></description>
</item><item>
    <title>Tuning Large Language Models for Recommendation Tasks</title>
    <link>https://blog.reachsumit.com/posts/2023/05/tuning-llm-for-recsys/</link>
    <pubDate>Sun, 21 May 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/05/tuning-llm-for-recsys/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/05/tuning-llm-for-recsys/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>Instruction-tuning methods enable open-source Large Language Models (LLMs) usage for building highly effective recommender systems on private data. This article highlights the latest research work on this paradigm of using LLMs for recommendation tasks.]]></description>
</item><item>
    <title>ChatGPT-based Recommender Systems</title>
    <link>https://blog.reachsumit.com/posts/2023/05/chatgpt-for-recsys/</link>
    <pubDate>Mon, 15 May 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/05/chatgpt-for-recsys/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/05/chatgpt-for-recsys/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div><p>With its outstanding performance, ChatGPT has become a hot topic of discussion in the NLP community and beyond. This article delves into recent efforts to harness the power of ChatGPT for recommendation tasks.</p>]]></description>
</item></channel>
</rss>
