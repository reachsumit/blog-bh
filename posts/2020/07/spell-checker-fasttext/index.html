<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
		<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7470684047959463"
     crossorigin="anonymous"></script>
        <title>Building a spell-checker with FastText word embeddings - Sumit&#39;s Diary</title><meta name="Description" content="Word vector representations with subword information are great for NLP modeling. But can we make lexical corrections using a trained embeddings space? Can its accuracy be high enough to beat Peter Norvig&#39;s spell-corrector? Let&#39;s find out!"><meta property="og:title" content="Building a spell-checker with FastText word embeddings" />
<meta property="og:description" content="Word vector representations with subword information are great for NLP modeling. But can we make lexical corrections using a trained embeddings space? Can its accuracy be high enough to beat Peter Norvig&#39;s spell-corrector? Let&#39;s find out!" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.reachsumit.com/posts/2020/07/spell-checker-fasttext/" /><meta property="og:image" content="https://blog.reachsumit.com/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-07-18T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-07-18T00:00:00+00:00" /><meta property="og:site_name" content="My cool site" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://blog.reachsumit.com/logo.png"/>

<meta name="twitter:title" content="Building a spell-checker with FastText word embeddings"/>
<meta name="twitter:description" content="Word vector representations with subword information are great for NLP modeling. But can we make lexical corrections using a trained embeddings space? Can its accuracy be high enough to beat Peter Norvig&#39;s spell-corrector? Let&#39;s find out!"/>
<meta name="application-name" content="Sumit&#39;s Diary">
<meta name="apple-mobile-web-app-title" content="Sumit&#39;s Diary"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="icon" href="/img/avatar/favicon.ico"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://blog.reachsumit.com/posts/2020/07/spell-checker-fasttext/" /><link rel="prev" href="https://blog.reachsumit.com/posts/2020/07/skip-list/" /><link rel="next" href="https://blog.reachsumit.com/posts/2020/07/twitter-search-redesign/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Building a spell-checker with FastText word embeddings",
        "inLanguage": "en",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/blog.reachsumit.com\/posts\/2020\/07\/spell-checker-fasttext\/"
        },"image": ["https:\/\/blog.reachsumit.com\/images\/Apple-Devices-Preview.png"],"genre": "posts","keywords": "embeddings","wordcount":  2349 ,
        "url": "https:\/\/blog.reachsumit.com\/posts\/2020\/07\/spell-checker-fasttext\/","datePublished": "2020-07-18T00:00:00+00:00","dateModified": "2020-07-18T00:00:00+00:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "xxxx","logo": "https:\/\/blog.reachsumit.com\/images\/avatar.png"},"author": {
                "@type": "Person",
                "name": "Sumit Kumar"
            },"description": "Word vector representations with subword information are great for NLP modeling. But can we make lexical corrections using a trained embeddings space? Can its accuracy be high enough to beat Peter Norvig's spell-corrector? Let's find out!"
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Sumit&#39;s Diary"><span id="id-3" class="typeit"></span></a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><a class="menu-item" href="/about/"> About </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search this blog" id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Sumit&#39;s Diary"><span id="id-4" class="typeit"></span></a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search this blog" id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a class="menu-item" href="/about/" title="">About</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Building a spell-checker with FastText word embeddings</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://reachsumit.com" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>Sumit Kumar</a></span>&nbsp;<span class="post-category">included in <a href="/categories/nlp/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>NLP</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2020-07-18">2020-07-18</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;2349 words&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;12 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a>
      <ul>
        <li><a href="#what-are-word-embeddings">What are word embeddings?</a></li>
        <li><a href="#what-are-subword-embeddings">What are subword embeddings?</a></li>
        <li><a href="#what-is-fasttext">What is FastText?</a></li>
      </ul>
    </li>
    <li><a href="#the-task-spell-checking">The Task: Spell-Checking</a>
      <ul>
        <li><a href="#dataset--benchmark">Dataset &amp; Benchmark</a></li>
        <li><a href="#installing-fasttext">Installing FastText</a></li>
        <li><a href="#method-1-using-pre-trained-word-vectors">Method 1: Using Pre-trained Word Vectors</a></li>
        <li><a href="#method-2-trained-our-own-word-vectors">Method 2: Trained Our Own Word Vectors</a></li>
      </ul>
    </li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><p>Word vector representations with subword information are great for NLP modeling. But can we make lexical corrections using a trained embeddings space? Can its accuracy be high enough to beat Peter Norvig&rsquo;s spell-corrector? Let&rsquo;s find out!</p>
<h2 id="introduction">Introduction</h2>
<p>Let&rsquo;s first define some terminology.</p>
<h3 id="what-are-word-embeddings">What are word embeddings?</h3>
<p>Word Embedding techniques have been an important factor behind recent advancements and successes in the field of Natural Language Processing. Word Embeddings provide a way for Machine Learning modelers to represent textual information as the input to ML algorithms. Simply put, they are a hashmap where the key is a language word, and the corresponding value is a vector of real numbers fed to the models in place of that word.</p>
<p>There are different kinds of word embeddings available out there that vary in the way they learn and transform a word to a vector. The word vector representations can be as simple as a hot-encoded vector, or they can be more complex (and more successful) representations that are trained on large corpus, take context into account, break the words into subword representations etc</p>
<h3 id="what-are-subword-embeddings">What are subword embeddings?</h3>
<p>Suppose you have a deep learning NLP model, say a chatbot, running in production. Your customers are directly interacting with the ML model. The model is being fed the vector values, such that each word from the customer is queried against a hashmap and the value corresponding to the word is the input vector. All of the keys in your hashmap represents the vocabulary, i.e. all the words you know. Now, how would you handle a case where the customer uses a word that is not already present in your vocabulary?</p>
<p>There are many ways to solve this <strong>out-of-vocabulary (OOV) problem</strong>. One popular approach is to split the words into &ldquo;subword&rdquo; units, and use those subwords to learn your hashmap during model training stage. At the time of inference, you would again divide each incoming word into smaller subword units, find the word vector corresponding to each subword unit using the hashmap, then aggregate each subword vector to get the vector representation for the complete word.</p>
<p>For example, let&rsquo;s say we have the word <code>tiktok</code>, the corresponding subwords could be <code>tik</code>, <code>ikt</code>, <code>kto</code>, <code>tok</code>, <code>tikt</code>, <code>ikto</code>, <code>ktok</code> etc.  This is the character n-gram division for the input word, where n is the subword sequence length, and is fixed by the modeler.</p>
<div id="id-1"><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/img/posts/spell-checker-fasttext/subword.png"
        data-srcset="/img/posts/spell-checker-fasttext/subword.png, /img/posts/spell-checker-fasttext/subword.png 1.5x, /img/posts/spell-checker-fasttext/subword.png 2x"
        data-sizes="auto"
        alt="/img/posts/spell-checker-fasttext/subword.png"
        title="subword-example" /><br>
Figure: example subword representations</div>
<h3 id="what-is-fasttext">What is FastText?</h3>
<p><a href="https://fasttext.cc/" target="_blank" rel="noopener noreffer">FastText <i class="fa fa-link" aria-hidden="true"></i></a> is an <a href="https://github.com/facebookresearch/fastText" target="_blank" rel="noopener noreffer">open-source project</a> from Facebook Research. It is a library for fast text-representations and classifications. It is written in C++ and supports multiprocessing. It can be used to train unsupervised word vectors and supervised classification tasks. For learning word-vectors it supports both skipgram and continuous bag-of-words approaches.</p>
<p>FastText supports subword representations such that each word can be represented as a bag of character n-grams in addition to the word itself. Incorporating finer (subword level) information is pretty good for handling rare words. You can read more about the FastText approach in <a href="https://arxiv.org/abs/1607.04606" target="_blank" rel="noopener noreffer">their paper here <i class="fa fa-book" aria-hidden="true"></i></a>.</p>
<p>For an example, let&rsquo;s say you have a word &ldquo;superman&rdquo; in FastText trained word embeddings (&ldquo;hashmap&rdquo;). Let&rsquo;s assume the hyperparameters minimum and maximum length of ngram was set to <code>4</code>. Corresponding to this word, the hashmap would have the following keys:</p>
<p><strong>Original word:</strong> superman</p>
<div id="id-2"><table>
<thead>
<tr>
<th>n-gram size</th>
<th>subword</th>
</tr>
</thead>
<tbody>
<tr>
<td>4</td>
<td>&lt;sup</td>
</tr>
<tr>
<td>4</td>
<td>supe</td>
</tr>
<tr>
<td>4</td>
<td>uper</td>
</tr>
<tr>
<td>4</td>
<td>perm</td>
</tr>
<tr>
<td>4</td>
<td>erma</td>
</tr>
<tr>
<td>4</td>
<td>rman</td>
</tr>
<tr>
<td>4</td>
<td>man&gt;</td>
</tr>
</tbody>
</table>
</div>
<p>where &ldquo;&lt;&rdquo; and &ldquo;&gt;&rdquo; characters mark the start and end of a word respectively</p>
<h2 id="the-task-spell-checking">The Task: Spell-Checking</h2>
<p>In NLP, the learnt word embedding vectors not only have lexical representations for words, but the vector values also have semantically related positioning in the embedding space. We are going to try and build a spell-checker application based on FastText word vectors such that given a misspelled word, our task will be to find the word vector representation closest to the vector representation of that word in trained embedding space.</p>
<p>We will work based on this simple heuristic:</p>
<div class="details admonition example open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-list-ol fa-fw" aria-hidden="true"></i>heuristic<i class="details-icon fas fa-angle-right fa-fw" aria-hidden="true"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content"><p>IF word exists in the Vocabulary</p>
<p>â€‹        Do not change the word</p>
<p>ELSE</p>
<p>â€‹        Replace the word with the one closest to its sub-word representation</p>
</div>
        </div>
    </div>
<h3 id="dataset--benchmark">Dataset &amp; Benchmark</h3>
<p>For fun, let&rsquo;s build and evaluate our spell-checker on the same training and testing data as this classic article: &ldquo;<a href="https://norvig.com/spell-correct.html" target="_blank" rel="noopener noreffer">How to write a Spelling Corrector</a>&rdquo; by <a href="https://norvig.com/" target="_blank" rel="noopener noreffer">Peter Norvig</a> , Director of Research at Google.  In this article, Norvig build a simple spelling corrector based on basic probability theory. Let&rsquo;s see how does this FastText based approach hold up against it.</p>
<h3 id="installing-fasttext">Installing FastText</h3>
<p>Installation for FastText is straightforward. FastText can be used as a command line tool or via Python client. <a href="https://fasttext.cc/docs/en/support.html" target="_blank" rel="noopener noreffer">Click here</a> to access the latest installation instructions for both approaches.</p>
<h3 id="method-1-using-pre-trained-word-vectors">Method 1: Using Pre-trained Word Vectors</h3>
<p>FastText provides pretrained word vectors based on common-crawl and wikipedia datasets. The details and download instructions for the embeddings can be found <a href="https://fasttext.cc/docs/en/english-vectors.html" target="_blank" rel="noopener noreffer">here</a>. For a quick experiment, let&rsquo;s load the largest pretrained model available from FastText and use that to perform spelling-correction.</p>
<p>Download and unzip the trained vectors and binary model file.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">-&gt; wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip
</span></span><span class="line"><span class="cl">-&gt; unzip crawl-300d-2M-subword.zip
</span></span></code></pre></td></tr></table>
</div>
</div><p>There are two files inside the zip:</p>
<ul>
<li><code>crawl-300d-2M-subword.vec</code> : This file contains the number of words (2M) and the size of each word (vector dimensions; 300) in the first line. All of the following lines start with the word (or the subword) followed by the 300 real number values representing the learnt word vector.</li>
<li><code>crawl-300d-2M-subword.bin</code>: This binary file is the exported model trained on the Common-Crawl dataset.</li>
</ul>
<p>As mentioned <a href="https://norvig.com/spell-correct.html" target="_blank" rel="noopener noreffer">in his article</a>, Norvig used <a href="https://norvig.com/spell-testset1.txt" target="_blank" rel="noopener noreffer">spell-testset1.txt</a> and <a href="https://norvig.com/spell-testset2.txt" target="_blank" rel="noopener noreffer">spell-testset2.txt</a> as development and test set respectively, to evaluate the performance of his spelling-corrector. I&rsquo;m going to load the pretrained FastText model and make predictions based on the heurisitic defined above. I&rsquo;m also going to borrow some of the evaluation code from Norvig.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">io</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">fasttext</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">load_vectors</span><span class="p">(</span><span class="n">fname</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">fin</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span> <span class="n">newline</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">fin</span><span class="o">.</span><span class="n">readline</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    <span class="n">data</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">fin</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">tokens</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">data</span><span class="p">[</span><span class="n">tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="n">tokens</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">data</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">spelltest</span><span class="p">(</span><span class="n">tests</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">vocab</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;Run correction(wrong) on all (right, wrong) pairs; report results.&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="kn">import</span> <span class="nn">time</span>
</span></span><span class="line"><span class="cl">    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">clock</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">good</span><span class="p">,</span> <span class="n">unknown</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tests</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">right</span><span class="p">,</span> <span class="n">wrong</span> <span class="ow">in</span> <span class="n">tests</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">w</span> <span class="o">=</span> <span class="n">wrong</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;word: </span><span class="si">{}</span><span class="s1"> exists in the vocabulary. No correction required&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">w_old</span> <span class="o">=</span> <span class="n">w</span>
</span></span><span class="line"><span class="cl">            <span class="n">w</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_nearest_neighbors</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;found replacement: </span><span class="si">{}</span><span class="s2"> for word: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">w_old</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">good</span> <span class="o">+=</span> <span class="p">(</span><span class="n">w</span> <span class="o">==</span> <span class="n">right</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dt</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">clock</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{:.0%}</span><span class="s1"> of </span><span class="si">{}</span><span class="s1"> correct at </span><span class="si">{:.0f}</span><span class="s1"> words per second &#39;</span>
</span></span><span class="line"><span class="cl">          <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">good</span> <span class="o">/</span> <span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">n</span> <span class="o">/</span> <span class="n">dt</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">Testset</span><span class="p">(</span><span class="n">lines</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;Parse &#39;right: wrong1 wrong2&#39; lines into [(&#39;right&#39;, &#39;wrong1&#39;), (&#39;right&#39;, &#39;wrong2&#39;)] pairs.&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[(</span><span class="n">right</span><span class="p">,</span> <span class="n">wrong</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="p">(</span><span class="n">right</span><span class="p">,</span> <span class="n">wrongs</span><span class="p">)</span> <span class="ow">in</span> <span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;:&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">wrong</span> <span class="ow">in</span> <span class="n">wrongs</span><span class="o">.</span><span class="n">split</span><span class="p">()]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&#34;__main__&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span> <span class="o">=</span> <span class="n">fasttext</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&#34;crawl-300d-2M-subword.bin&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">vocab</span> <span class="o">=</span> <span class="n">load_vectors</span><span class="p">(</span><span class="s2">&#34;crawl-300d-2M-subword.vec&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">spelltest</span><span class="p">(</span><span class="n">Testset</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s1">&#39;spell-testset1.txt&#39;</span><span class="p">)),</span> <span class="n">model</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">spelltest</span><span class="p">(</span><span class="n">Testset</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s1">&#39;spell-testset2.txt&#39;</span><span class="p">)),</span> <span class="n">model</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>For the sake of brevity, I&rsquo;m reducing the output log to just the metrics trace.</p>
<div class="details admonition example open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-list-ol fa-fw" aria-hidden="true"></i>output<i class="details-icon fas fa-angle-right fa-fw" aria-hidden="true"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">&hellip;<br>
0% of 270 correct at 3 words per second.<br>
&hellip;<br>
0% of 400 correct at 4 words per second.</div>
        </div>
    </div>
<p>That didn&rsquo;t go well! ðŸ˜…  None of the corrections from the model were right. Let&rsquo;s dig into the results. Here are a few snapshots from the output log:</p>
<div class="details admonition example open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-list-ol fa-fw" aria-hidden="true"></i>output<i class="details-icon fas fa-angle-right fa-fw" aria-hidden="true"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">&hellip;<br>
word: sysem exists in the vocabulary. No correction required<br>
word: controled exists in the vocabulary. No correction required<br>
word: reffered exists in the vocabulary. No correction required<br>
word: irrelavent exists in the vocabulary. No correction required<br>
word: financialy exists in the vocabulary. No correction required<br>
word: whould exists in the vocabulary. No correction required<br>
&hellip;<br>
found replacement: reequipped for word: reequired<br>
found replacement: putput for word: oputput<br>
found replacement: catecholate for word: colate<br>
found replacement: yeahNow for word: yesars<br>
found replacement: detale for word: segemnt<br>
found replacement: &lt;li&gt;&lt;strong&gt;Style for word: earlyest<br>
&hellip;</div>
        </div>
    </div>
<p>Looks like there are a lot of garbage subwords in the pretrained vocabulary that directly matches our misspelled input. Also, the performed lexical corrections show that the model replaced misspelled input words with the closest semantic neighbor. However none of those neighbors are meaningful English words.</p>
<p>We can verify this by checking out the neighbors for random misspellings.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">model</span><span class="o">.</span><span class="n">get_nearest_neighbors</span><span class="p">(</span><span class="s2">&#34;incorrct&#34;</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">[(</span><span class="mf">0.68821</span><span class="p">,</span> <span class="s1">&#39;corrct&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.67809</span><span class="p">,</span> <span class="s1">&#39;corrctly&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.63196</span><span class="p">,</span> <span class="s1">&#39;altadena&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.58083</span><span class="p">,</span> <span class="s1">&#39;huntersville&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mf">0.579281</span><span class="p">,</span> <span class="s1">&#39;concorda&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.578464</span><span class="p">,</span> <span class="s1">&#39;bartlesville&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.576386</span><span class="p">,</span> <span class="s1">&#39;controlin&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.576119</span><span class="p">,</span> <span class="s1">&#39;variaton&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.574023</span><span class="p">,</span> <span class="s1">&#39;relavence&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.572117</span><span class="p">,</span> <span class="s1">&#39;concorrenza&#39;</span><span class="p">))]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>As you can see, misspelled words have misspelled semantic neighbors. And the top ones also seem syntactically related. This makes sense. But we were hoping that the subword approach would be able to prioritize the potentially correct lexical variants of the word, and not just it&rsquo;s semantic neighbors.</p>
<div class="details admonition failure open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-times-circle fa-fw" aria-hidden="true"></i>why do we see bad output?<i class="details-icon fas fa-angle-right fa-fw" aria-hidden="true"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">One reason for this behavior could be that the pretrained model was originally trained by FastText with a &gt;1 neighborhood window. <a href="https://fasttext.cc/docs/en/crawl-vectors.html" target="_blank" rel="noopener noreffer">This FastText documentation page</a> confirms the fact that the <code>wordNgrams</code> (<em>max length of word ngram</em>) was set to <code>5</code> during training. If we train our own word vectors, we could keep the <code>wordNgrams</code> hyperparameter to <code>1</code>, so that FastText trains with 0 neighbors (i.e. each word is considered a line on it&rsquo;s own).</div>
        </div>
    </div>
<p>The pre-trained vectors do not seem to be sufficient for this task. Let&rsquo;s try training our word embeddings and see what results we can get.</p>
<h3 id="method-2-trained-our-own-word-vectors">Method 2: Trained Our Own Word Vectors</h3>
<p>For a fair comparison with Norvig&rsquo;s spell-checker, let&rsquo;s use the same training data that he used (<a href="https://norvig.com/big.txt" target="_blank" rel="noopener noreffer">big.txt</a>).</p>
<div class="details admonition quote open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-quote-right fa-fw" aria-hidden="true"></i>From Norvig&#39;s article:<i class="details-icon fas fa-angle-right fa-fw" aria-hidden="true"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">&hellip; by counting the number of times each word appears in a text file of about a million words, <a href="https://norvig.com/big.txt" target="_blank" rel="noopener noreffer"><code>big.txt</code></a>. It is a concatenation of public domain book excerpts from <a href="http://www.gutenberg.org/wiki/Main_Page" target="_blank" rel="noopener noreffer">Project Gutenberg</a> and lists of most frequent words from <a href="http://en.wiktionary.org/wiki/Wiktionary:Frequency_lists" target="_blank" rel="noopener noreffer">Wiktionary</a> and the <a href="http://www.kilgarriff.co.uk/bnc-readme.html" target="_blank" rel="noopener noreffer">British National Corpus</a>.</div>
        </div>
    </div>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">-&gt; wc big.txt
</span></span><span class="line"><span class="cl"><span class="m">128457</span> <span class="m">1095695</span> <span class="m">6488666</span> big.txt
</span></span></code></pre></td></tr></table>
</div>
</div><p>The training data has around 128K lines, 1M words, 6.5M characters.</p>
<p>As discussed above, we should try with keeping the <code>wordNgrams</code> hyperparameter to <code>1</code>, and use the trained FastText model to perform spell-checking.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">io</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">fasttext</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">load_vectors</span><span class="p">(</span><span class="n">fname</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">fin</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span> <span class="n">newline</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">fin</span><span class="o">.</span><span class="n">readline</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    <span class="n">data</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">fin</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">tokens</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">data</span><span class="p">[</span><span class="n">tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="n">tokens</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">data</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">spelltest</span><span class="p">(</span><span class="n">tests</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;Run correction(wrong) on all (right, wrong) pairs; report results.&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="kn">import</span> <span class="nn">time</span>
</span></span><span class="line"><span class="cl">    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">clock</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">good</span><span class="p">,</span> <span class="n">unknown</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tests</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">right</span><span class="p">,</span> <span class="n">wrong</span> <span class="ow">in</span> <span class="n">tests</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">w_old</span> <span class="o">=</span> <span class="n">wrong</span>
</span></span><span class="line"><span class="cl">        <span class="n">w</span> <span class="o">=</span> <span class="n">wrong</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">words</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">pass</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">w</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_nearest_neighbors</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">good</span> <span class="o">+=</span> <span class="p">(</span><span class="n">w</span> <span class="o">==</span> <span class="n">right</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">w</span> <span class="o">==</span> <span class="n">right</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">w_old</span> <span class="o">!=</span> <span class="n">w</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Edited </span><span class="si">{}</span><span class="s2"> to </span><span class="si">{}</span><span class="s2">, but the correct word is: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w_old</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">right</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">dt</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">clock</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{:.0%}</span><span class="s1"> of </span><span class="si">{}</span><span class="s1"> correct at </span><span class="si">{:.0f}</span><span class="s1"> words per second &#39;</span>
</span></span><span class="line"><span class="cl">          <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">good</span> <span class="o">/</span> <span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">n</span> <span class="o">/</span> <span class="n">dt</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">Testset</span><span class="p">(</span><span class="n">lines</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;Parse &#39;right: wrong1 wrong2&#39; lines into [(&#39;right&#39;, &#39;wrong1&#39;), (&#39;right&#39;, &#39;wrong2&#39;)] pairs.&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[(</span><span class="n">right</span><span class="p">,</span> <span class="n">wrong</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="p">(</span><span class="n">right</span><span class="p">,</span> <span class="n">wrongs</span><span class="p">)</span> <span class="ow">in</span> <span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;:&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">wrong</span> <span class="ow">in</span> <span class="n">wrongs</span><span class="o">.</span><span class="n">split</span><span class="p">()]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&#34;__main__&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span> <span class="o">=</span> <span class="n">fasttext</span><span class="o">.</span><span class="n">train_unsupervised</span><span class="p">(</span><span class="s1">&#39;big.txt&#39;</span><span class="p">,</span> <span class="n">wordNgrams</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">minn</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">maxn</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">ws</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">neg</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">minCount</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bucket</span><span class="o">=</span><span class="mi">900000</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">spelltest</span><span class="p">(</span><span class="n">Testset</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s1">&#39;spell-testset1.txt&#39;</span><span class="p">)),</span> <span class="n">model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">spelltest</span><span class="p">(</span><span class="n">Testset</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s1">&#39;spell-testset2.txt&#39;</span><span class="p">)),</span> <span class="n">model</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>There are a couple of changes in this code. We are training the model with specific hyper-parameters, and the output traces will inform us what went wrong in our decisions. Note that <a href="https://github.com/facebookresearch/fastText/issues/776" target="_blank" rel="noopener noreffer">FastText doesn&rsquo;t provide an option to set seed</a> in the above implementation, so the results may vary by 1%-2% on every execution.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">-&gt; python fasttext_trained.py
</span></span><span class="line"><span class="cl">Read 1M words
</span></span><span class="line"><span class="cl">Number of words: <span class="m">81398</span>
</span></span><span class="line"><span class="cl">Number of labels: <span class="m">0</span>
</span></span><span class="line"><span class="cl">Progress: 100.0% words/sec/thread:	<span class="m">20348</span> lr: 0.000000 avg. loss 1.943424 ETA:	0h 0m 0s
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">73% of <span class="m">270</span> correct at <span class="m">46</span> words per second.
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">69% of <span class="m">400</span> correct at <span class="m">48</span> words per second.
</span></span></code></pre></td></tr></table>
</div>
</div><p>This is a big improvement! ðŸ˜„ We have almost the same accuracy as Norvig&rsquo;s spell-corrector.</p>
<p>Here are Norvig&rsquo;s results for comparison.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">75% of 270 correct at 41 words per second
</span></span><span class="line"><span class="cl">68% of 400 correct at 35 words per second
</span></span></code></pre></td></tr></table>
</div>
</div><p>Looking at some of the edits in the output traces, I can see that the model output isn&rsquo;t essentially incorrect, but the model is biased to certain edit-based operations.</p>
<div class="details admonition example open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-list-ol fa-fw" aria-hidden="true"></i>output<i class="details-icon fas fa-angle-right fa-fw" aria-hidden="true"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">&hellip;<br>
Edited reffered to referred, but the correct word is: refered<br>
Edited applogised to apologized, but the correct word is: apologised<br>
Edited speeking to seeking, but the correct word is: speaking<br>
Edited guidlines to guideline, but the correct word is: guidelines<br>
Edited throut to throughout, but the correct word is: through<br>
Edited nite to unite, but the correct word is: night<br>
Edited oranised to organised, but the correct word is: organized<br>
Edited thay to thy, but the correct word is: they<br>
Edited stoped to stooped, but the correct word is: stopped<br>
Edited upplied to supplied, but the correct word is: applied<br>
Edited goegraphicaly to geographical, but the correct word is: geographically<br>
&hellip;</div>
        </div>
    </div>
<p>As you can see our trained model is nicely producing dictionary-based words as output. It&rsquo;s likely that with contextual training approaches and evaluations, along with more training data, we can come up with an even better approaches that would understand context in a full sentence and produce the correct word as the spell-checked output.</p>
<p>Let&rsquo;s also compare the model with the pretrained model from method one.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">model</span><span class="o">.</span><span class="n">get_nearest_neighbors</span><span class="p">(</span><span class="s2">&#34;incorrct&#34;</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">[(</span><span class="mf">0.96987646</span><span class="p">,</span> <span class="s1">&#39;incorrect&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.937522232</span><span class="p">,</span> <span class="s1">&#39;correct&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.91315931</span><span class="p">,</span> <span class="s1">&#39;corrects&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.90868479</span><span class="p">,</span> <span class="s1">&#39;correcting&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mf">0.89917653</span><span class="p">,</span> <span class="s1">&#39;corrective&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.89813524</span><span class="p">,</span> <span class="s1">&#39;contractor&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.89806741</span><span class="p">,</span> <span class="s1">&#39;correction&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.896661</span><span class="p">,</span> <span class="s1">&#39;disconcert&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.894903957</span><span class="p">,</span> <span class="s1">&#39;contradict&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.8944035768</span><span class="p">,</span> <span class="s1">&#39;concurrent&#39;</span><span class="p">))]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>The trained model&rsquo;s outputs are much more sensible now and the topmost word is also the word that we are actually looking for.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Our model was able to match Peter Norvig&rsquo;s spell-corrector. ðŸ˜Š Looking at the failing cases, we realize that the model could potentially do even better with more training data and a contextual training and evaluation strategy.</p></div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2020-07-18</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="https://blog.reachsumit.com/posts/2020/07/spell-checker-fasttext/" data-title="Building a spell-checker with FastText word embeddings" data-via="_reachsumit" data-hashtags="embeddings"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="https://blog.reachsumit.com/posts/2020/07/spell-checker-fasttext/" data-hashtag="embeddings"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Linkedin" data-sharer="linkedin" data-url="https://blog.reachsumit.com/posts/2020/07/spell-checker-fasttext/"><i class="fab fa-linkedin fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on WhatsApp" data-sharer="whatsapp" data-url="https://blog.reachsumit.com/posts/2020/07/spell-checker-fasttext/" data-title="Building a spell-checker with FastText word embeddings" data-web><i class="fab fa-whatsapp fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Hacker News" data-sharer="hackernews" data-url="https://blog.reachsumit.com/posts/2020/07/spell-checker-fasttext/" data-title="Building a spell-checker with FastText word embeddings"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Reddit" data-sharer="reddit" data-url="https://blog.reachsumit.com/posts/2020/07/spell-checker-fasttext/"><i class="fab fa-reddit fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Line" data-sharer="line" data-url="https://blog.reachsumit.com/posts/2020/07/spell-checker-fasttext/" data-title="Building a spell-checker with FastText word embeddings"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.0.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Pocket" data-sharer="pocket" data-url="https://blog.reachsumit.com/posts/2020/07/spell-checker-fasttext/"><i class="fab fa-get-pocket fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on å¾®åš" data-sharer="weibo" data-url="https://blog.reachsumit.com/posts/2020/07/spell-checker-fasttext/" data-title="Building a spell-checker with FastText word embeddings"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Evernote" data-sharer="evernote" data-url="https://blog.reachsumit.com/posts/2020/07/spell-checker-fasttext/" data-title="Building a spell-checker with FastText word embeddings"><i class="fab fa-evernote fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Trello" data-sharer="trello" data-url="https://blog.reachsumit.com/posts/2020/07/spell-checker-fasttext/" data-title="Building a spell-checker with FastText word embeddings" data-description="Word vector representations with subword information are great for NLP modeling. But can we make lexical corrections using a trained embeddings space? Can its accuracy be high enough to beat Peter Norvig&#39;s spell-corrector? Let&#39;s find out!"><i class="fab fa-trello fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/embeddings/">embeddings</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/2020/07/skip-list/" class="prev" rel="prev" title="Skip List Data Structure - Explained!"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>Skip List Data Structure - Explained!</a>
            <a href="/posts/2020/07/twitter-search-redesign/" class="next" rel="next" title="How Twitter Reduced Search Indexing Latency to One Second">How Twitter Reduced Search Indexing Latency to One Second<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
<div id="comments"><div id="disqus_thread" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://disqus.com/?ref_noscript">Disqus</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2020 - 2022</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="https://reachsumit.com" target="_blank">Sumit Kumar</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css"><link rel="stylesheet" href="/css/cc8928.min.css"><script type="text/javascript" src="https://reachsumit-blog.disqus.com/embed.js" defer></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/algoliasearch@4.13.1/dist/algoliasearch-lite.umd.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.1/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/typeit@8.5.4/dist/index.umd.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":10},"comment":{},"data":{"id-3":"Sumit's Diary","id-4":"Sumit's Diary"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"LV11CUNTAX","algoliaIndex":"blog_reachsumit","algoliaSearchKey":"98d868016771f8a06b967e7eb3eaf63a","highlightTag":"em","maxResultLength":10,"noResultsFound":"No results found","snippetLength":30,"type":"algolia"},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"id-3":["id-3"],"id-4":["id-4"]},"duration":2700,"speed":100}};</script><script type="text/javascript" src="/js/theme.min.js"></script><script type="text/javascript">
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'UA-171612692-1');
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=UA-171612692-1" async></script></body>
</html>
