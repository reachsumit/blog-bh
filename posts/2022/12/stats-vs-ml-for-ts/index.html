<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
		<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7470684047959463"
     crossorigin="anonymous"></script>
        <title>Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting - Sumit&#39;s Diary</title><meta name="Description" content="Statistical methods have been used in the time series domain for multiple decades. But given the recent advances in Machine Learning and especially its sub-domain Deep Learning, are statistical methods still superior for forecasting? In this article, we will do a deep dive into literature and recent time series competitions to do a multifaceted comparison between Statistical, Machine Learning, and Deep Learning methods for time series forecasting."><meta property="og:title" content="Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting" />
<meta property="og:description" content="Statistical methods have been used in the time series domain for multiple decades. But given the recent advances in Machine Learning and especially its sub-domain Deep Learning, are statistical methods still superior for forecasting? In this article, we will do a deep dive into literature and recent time series competitions to do a multifaceted comparison between Statistical, Machine Learning, and Deep Learning methods for time series forecasting." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.reachsumit.com/posts/2022/12/stats-vs-ml-for-ts/" /><meta property="og:image" content="https://blog.reachsumit.com/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-12-20T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-12-20T00:00:00+00:00" /><meta property="og:site_name" content="My cool site" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://blog.reachsumit.com/logo.png"/>

<meta name="twitter:title" content="Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting"/>
<meta name="twitter:description" content="Statistical methods have been used in the time series domain for multiple decades. But given the recent advances in Machine Learning and especially its sub-domain Deep Learning, are statistical methods still superior for forecasting? In this article, we will do a deep dive into literature and recent time series competitions to do a multifaceted comparison between Statistical, Machine Learning, and Deep Learning methods for time series forecasting."/>
<meta name="application-name" content="Sumit&#39;s Diary">
<meta name="apple-mobile-web-app-title" content="Sumit&#39;s Diary"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="icon" href="/img/avatar/favicon.ico"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://blog.reachsumit.com/posts/2022/12/stats-vs-ml-for-ts/" /><link rel="prev" href="https://blog.reachsumit.com/posts/2022/11/sparse-recsys/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting",
        "inLanguage": "en",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/blog.reachsumit.com\/posts\/2022\/12\/stats-vs-ml-for-ts\/"
        },"image": ["https:\/\/blog.reachsumit.com\/images\/Apple-Devices-Preview.png"],"genre": "posts","keywords": "forecasting, literature review","wordcount":  3747 ,
        "url": "https:\/\/blog.reachsumit.com\/posts\/2022\/12\/stats-vs-ml-for-ts\/","datePublished": "2022-12-20T00:00:00+00:00","dateModified": "2022-12-20T00:00:00+00:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "xxxx","logo": "https:\/\/blog.reachsumit.com\/images\/avatar.png"},"author": {
                "@type": "Person",
                "name": "Sumit Kumar"
            },"description": "Statistical methods have been used in the time series domain for multiple decades. But given the recent advances in Machine Learning and especially its sub-domain Deep Learning, are statistical methods still superior for forecasting? In this article, we will do a deep dive into literature and recent time series competitions to do a multifaceted comparison between Statistical, Machine Learning, and Deep Learning methods for time series forecasting."
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('light' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'light' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Sumit&#39;s Diary"><span id="id-1" class="typeit"></span></a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><a class="menu-item" href="/about/"> About </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search this blog" id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Sumit&#39;s Diary"><span id="id-2" class="typeit"></span></a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search this blog" id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a class="menu-item" href="/about/" title="">About</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://reachsumit.com" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>Sumit Kumar</a></span>&nbsp;<span class="post-category">included in <a href="/categories/time-series/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Time Series</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2022-12-20">2022-12-20</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;3747 words&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;18 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#whats-the-difference-between-statistical-and-machine-learning-forecasting-models">What&rsquo;s the difference between Statistical and Machine Learning forecasting models?</a></li>
    <li><a href="#comparisons-based-on-the-forecasting-literature">Comparisons based on the forecasting literature</a>
      <ul>
        <li><a href="#statistical-and-machine-learning-forecasting-methods-concerns-and-ways-forward-by-makridakis-et-al">&ldquo;<em>Statistical and Machine Learning forecasting methods: Concerns and ways forward</em>&rdquo; by Makridakis et al.</a>
          <ul>
            <li><a href="#summary">Summary</a></li>
            <li><a href="#additional-notes">Additional Notes</a></li>
          </ul>
        </li>
        <li><a href="#statistical-machine-learning-and-deep-learning-forecasting-methods-comparisons-and-ways-forward-by-makridakis-et-al">&ldquo;<em>Statistical, machine learning and deep learning forecasting methods: Comparisons and ways forward</em>&rdquo; by Makridakis et al.</a>
          <ul>
            <li><a href="#summary-of-results-at-different-forecasting-horizons">Summary of results at different forecasting horizons</a></li>
            <li><a href="#extending-to-all-3003-time-series-of-the-m3-dataset">Extending to all 3,003 time series of the M3 dataset</a></li>
            <li><a href="#balancing-the-comparison-among-ensembles">Balancing the comparison among ensembles</a></li>
            <li><a href="#results-at-different-forecasting-horizons-using-all-of-the-m3-data">Results at different forecasting horizons using all of the M3 data</a></li>
            <li><a href="#comparison-using-the-mcb-method">Comparison using the MCB method</a></li>
            <li><a href="#summary-1">Summary</a></li>
            <li><a href="#additional-notes-1">Additional Notes</a></li>
          </ul>
        </li>
        <li><a href="#statistical-vs-deep-learning-forecasting-methods-by-statsforecast">&ldquo;<em>Statistical vs Deep Learning forecasting methods</em>&rdquo; by Statsforecast</a>
          <ul>
            <li><a href="#summary-2">Summary</a></li>
            <li><a href="#additional-notes-2">Additional Notes</a></li>
          </ul>
        </li>
        <li><a href="#are-there-any-unique-time-series-characteristics-for-which-some-methods-perform-better-than-others">Are there any unique time series characteristics for which some methods perform better than others?</a></li>
        <li><a href="#are-there-certain-methods-that-can-be-expected-to-work-reasonably-well-with-data-from-certain-data-domains">Are there certain methods that can be expected to work reasonably well with data from certain data domains?</a></li>
        <li><a href="#do-some-models-tend-to-work-better-with-high-dimensional-data">Do some models tend to work better with high-dimensional data?</a></li>
        <li><a href="#why-do-statistical-models-work-better-than-ml-models-for-small-datasets">Why do statistical models work better than ML models for small datasets?</a></li>
        <li><a href="#is-it-better-to-train-a-model-over-multiple-time-series-rather-than-training-it-per-series">Is it better to train a model over multiple time series rather than training it per series?</a></li>
      </ul>
    </li>
    <li><a href="#comparisons-based-on-forecasting-competitions">Comparisons based on forecasting competitions</a>
      <ul>
        <li><a href="#learnings-from-the-m3-competition">Learnings from the M3 competition</a>
          <ul>
            <li><a href="#additional-notes-3">Additional Notes</a></li>
          </ul>
        </li>
        <li><a href="#learnings-from-the-m4-competition">Learnings from the M4 competition</a></li>
        <li><a href="#learnings-from-the-m5-competition">Learnings from the M5 competition</a></li>
      </ul>
    </li>
    <li><a href="#conclusion">Conclusion</a>
      <ul>
        <li><a href="#takeaways">Takeaways</a></li>
      </ul>
    </li>
    <li><a href="#references">References</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><p>Over recent decades, Machine Learning (ML) and, its subdomain, Deep Learning (DL) based algorithms have achieved remarkable success in various areas, such as image processing, natural language understanding, and speech recognition. However, ML algorithms haven&rsquo;t quite had the same widely known and unquestionable superiority when it comes to forecasting applications in the time series domain. While ML algorithms have to overcome the challenges in modeling the non-i.i.d. nature of data inherent in the time series domain, statistical models excel in this setting and provide explicit means to model time series structural elements, such as trend and seasonality.</p>
<p>In this article, we will do a deep dive into literature and recent time series competitions to do a multifaceted comparison between Statistical, Machine Learning, and Deep Learning methods for time series forecasting.</p>
<p><strong>Note:</strong> This is a long-form article. If you need a TL;DR, feel free to skip to the last section named <a href="#takeaways" rel="">Takeaways</a>.</p>
<h2 id="whats-the-difference-between-statistical-and-machine-learning-forecasting-models">What&rsquo;s the difference between Statistical and Machine Learning forecasting models?</h2>
<p>Before continuing, we should define what &ldquo;statistical&rdquo; and &ldquo;machine learning&rdquo; models are in the context of forecasting. This distinction is not always trivial because a majority of machine learning algorithms are maximum likelihood estimators meaning that they are statistical in nature. Similarly, traditional statistical models, such as autoregressive models, can be specified as linear regression on lags of the series, and linear regression is mostly considered a machine learning algorithm. So to distinguish between the two, we will use the following definition proposed by various researchers <cite><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></cite><cite><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></cite>.</p>
<p>A statistical model prescribes the data-generating process, for example, an autoregressive (AR) model only looks at the relationship between lags of a series and its future values. Whereas a machine learning model learns this relationship thus being more generic, for example, a neural network creates its own features using non-linear combinations of its inputs. ML methods, in contrast to statistical ones, are also more data-hungry. While the completely non-linear and assumption-free estimation of ML forecasts (e.g., in terms of trend and seasonality) let the data speak for themselves, it also demands a large volume of data for effectively capturing the dynamics and interconnections of the data. Moreover, many statistical methods have developed excellent heuristics that allow even novices to create reasonable models with a few function calls, determining a good approach for ML modeling is more experimental<cite><sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></cite>.</p>
<h2 id="comparisons-based-on-the-forecasting-literature">Comparisons based on the forecasting literature</h2>
<h3 id="statistical-and-machine-learning-forecasting-methods-concerns-and-ways-forward-by-makridakis-et-al">&ldquo;<em>Statistical and Machine Learning forecasting methods: Concerns and ways forward</em>&rdquo; by Makridakis et al.</h3>
<p>One of the earliest and most popular empirical studies that compared Statistical and ML methods was done by Makridakis et al <cite><sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup></cite>. In their 2018 paper, they used a set of 1045 monthly (univariate) time series sampled from the dataset used in the M3 competition to evaluate performances of 8 traditional statistical (Seasonal Random Walk, SES, Holt &amp; Damped Exponential Smoothing, Comb: average of SES, Holt, and Damped, Theta, ARIMA, and ETS) and 10 ML methods (MLP, BNN, RBF, GRNN, KNN, CART, SVR, GP, RNN, and LSTM) on multi-step ahead forecasting task.</p>
<p>To generate multi-horizon forecasts from ML models, they considered three approaches: iterative, multi-node output, multi-network forecasting, and for preprocessing the input, they considered detrending, deseasonalizing, box-cox power transformation, a combination of the three, as well as directly using the original data. They also considered using mean, median, or mode over the outputs of multiple models trained with different hyperparameter initializations and reported the results from the most appropriate alternatives. The following figure shows the overall sMAPE (symmetric Mean Absolute Percentage Error) values averaged over 18 one-step-ahead forecasts for all the models they considered. Similar conclusions were drawn for the case of multi-step-ahead forecasts.</p>
<img src="/img/posts/2022/deep-learning-multivariate-forecast/stats_vs_ml_vs_dl.png" alt="Stats vs ML vs DL 2018">
<h4 id="summary">Summary</h4>
<p>As seen above, the six most accurate methods were statistical. Even the Naïve 2 (seasonal random walk) baseline was shown to be more accurate than half of the ML methods. Through additional experiments, they also showed that the ML models, specially LSTMs, overfitted the data during training but had a worse performance during the testing phase.</p>
<h4 id="additional-notes">Additional Notes</h4>
<ul>
<li>ML methods are computationally more demanding, and methods like deseasonalizing the data, utilizing simpler models, and using principled approaches to turning hyperparameters can help in reducing this cost.</li>
<li>Using a sliding window approach gives as much information as possible about future values and also reduces the uncertainty in forecasting.</li>
</ul>
<h3 id="statistical-machine-learning-and-deep-learning-forecasting-methods-comparisons-and-ways-forward-by-makridakis-et-al">&ldquo;<em>Statistical, machine learning and deep learning forecasting methods: Comparisons and ways forward</em>&rdquo; by Makridakis et al.</h3>
<p>In 2022, the authors of the above paper followed up with another study and added more modern deep learning networks in this comparison <cite><sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup></cite>. For statistical models, they used the top-2 performing algorithms from the earlier study: ETS and ARIMA, and an ensemble of the two: Ensemble-S. Similarly, they used top-2 performing ML algorithms: MLP and BNN. Next, they used four modern deep learning algorithms: DeepAR, Feed-forward, Transformer, WaveNet, and an ensemble of the four: Ensemble-DL. The following table shows the sMAPE and MASE accuracy metrics for all of these methods tested on the same 1045 M3 series data using the respective optimal hyperparameters found during experimentation. The computational time (CT), i.e., the time (measured in minutes) required by the methods for training and predicting, as well as the relative computational complexity (RCC) of the methods, i.e., the number of floating point operations required by the methods for inference when compared to the Naive 2, is also reported.</p>
<img src="/img/posts/2022/deep-learning-multivariate-forecast/second-study-on-m3.png" alt="Stats vs ML vs DL 2022">
<h4 id="summary-of-results-at-different-forecasting-horizons">Summary of results at different forecasting horizons</h4>
<p>Their experiment shows that ETS and ARIMA are still more accurate on average than the ML methods and many of the individual DL methods across all forecasting horizons (short: 1-6, medium: 7-12, long: 13-18). <u>Both ensembles are more accurate than any of the individual models</u>. Ensemble-DL performed 2-5% better than Ensemble-S overall, but Ensemble-S performed best on MASE on short-term forecasts. ML methods also performed better than DL methods and their ensemble on short-term forecast periods. Long-term forecasts tend to be less accurate than short-term ones for all methods.</p>
<p>Through a separate set of methods, the authors show that when models are optimized based on their one-step-ahead forecasting accuracy, their forecasts will be relatively more accurate for short-term forecasts compared to models that are optimized based on their multi-step-ahead forecasting accuracy, and vice versa. Typically, statistical forecasting methods are optimized in terms of parameters so that the one-step-ahead forecast error is minimized, for example, ETS and ARIMA are parameterized with the objective to minimize the in-sample mean squared error of their forecasts. Similarly, MLP and BNN are parameterized with the objective to produce accurate multi-step-ahead forecasts in a recursive fashion.</p>
<h4 id="extending-to-all-3003-time-series-of-the-m3-dataset">Extending to all 3,003 time series of the M3 dataset</h4>
<p>Further expanding their work, the authors considered all of the 3,003 time series of the M3 dataset for the next experiment. The following table shows the resulting sMAPE accuracy both per data frequency and total. The table also shows the reported results from a variety of forecasting methods originally submitted in the M3 competition.</p>
<img src="/img/posts/2022/deep-learning-multivariate-forecast/m3_study_and_comps_results.png" alt="M3 study results on all of M3 data">
<p><u>Ensemble-DL continues to be the most accurate forecasting approach overall</u>. DL ensemble was outperformed by a small margin by the LGT model (discussed later) on yearly data, and DeepAR performed better on the data labeled as &ldquo;Other&rdquo; (no frequency information available). The Ensemble-DL improvement over Ensemble-S decreases as the data frequency is increased. Ensemble-DL performance is very similar to the N-BEATS method with N-BEATS performing better than Ensemble-DL for the &ldquo;Other&rdquo; series.</p>
<h4 id="balancing-the-comparison-among-ensembles">Balancing the comparison among ensembles</h4>
<p>Note that in the above discussion, Ensemble-S only used 2 models while Ensemble-DL had 4. To make a fair comparison, the authors also evaluated the DL models in pairs and found that all of the pairwise DL models performed better than Ensemble-S from 0.4-6.1%.</p>
<img src="/img/posts/2022/deep-learning-multivariate-forecast/ml-vs-stats-2on2.png" alt="M3 results: pairwise ensemble comparison">
<p>Interestingly, ensembles that involve DeepAR, i.e., the most accurate individual DL model, consistently outperform the rest, while ensembles that involve WaveNet, i.e., the least accurate individual DL model, sometimes deteriorate forecasting performance. Also, the accuracy of Ensemble-DL is comparable to that of the best pairwise ensemble.</p>
<h4 id="results-at-different-forecasting-horizons-using-all-of-the-m3-data">Results at different forecasting horizons using all of the M3 data</h4>
<p>When examined on various forecasting horizons, with the exception of DeepAR, the individual DL models are less accurate than the best performing statistical one for short and medium horizons, with the differences becoming smaller however as the forecasting horizon increases. For example, Theta is more accurate than the Feed-Forward, Transformer, and WaveNet models for short and medium-term forecasts, and it it also better than Feed-Forward and Transformer models for long-term forecasts. Both Ensemble-S and Ensemble-DL provided more accurate results than those of their constituent methods. However, Ensemble-DL is at least as accurate or superior to Ensemble-S when the complete M3 data set is considered, across all horizons.</p>
<img src="/img/posts/2022/deep-learning-multivariate-forecast/m3_smape_on_horizons.png" alt="M3 results: different horizons">
<h4 id="comparison-using-the-mcb-method">Comparison using the MCB method</h4>
<p>When ranked using MCB (Multiple comparisons with the best)<cite><sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup></cite> method with 95% confidence intervals, we find that Ensemble-DL is significantly more accurate than the rest of the forecasting methods considered in this study, including the DL models it consists of. On a second level, Ensemble-S provides significantly more accurate forecasts than the contributing ETS and ARIMA models. Overall, the power of ensembling is once again confirmed. Finally, despite the dominance of Ensemble-DL, only DeepAR, among all individual DL methods, produces significantly more accurate forecasts than those of the top-performing statistical methods.</p>
<img src="/img/posts/2022/deep-learning-multivariate-forecast/mcb_output.png" alt="M3 results: MCB comparison">
<h4 id="summary-1">Summary</h4>
<ul>
<li>In this study, DL models produced more accurate forecasts (up to 6% improvements) than statistical and ML ones, especially for longer forecasting horizons.</li>
<li>Combinations of DL models perform better than most standard models, both statistical and ML, especially in the case of monthly series and long-term forecasts. However, these improvements come at the cost of significantly increased computational time.</li>
</ul>
<h4 id="additional-notes-1">Additional Notes</h4>
<ul>
<li>The authors recommend the utilization of the series index for data sets that contain a limited number of series and no additional series-specific information. By using the index as an additional feature, models are better poised to identify series-specific behaviors.</li>
<li>In 2019, the authors of the N-BEATS paper (Oreshkin et al.<cite><sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup></cite>) did a similar experiment that showed ensembles of multiple DL models achieved more accurate forecasts than standard statistical methods on the M3 competition data. However, the individual DL models used for constructing the ensembles were relatively inaccurate.</li>
</ul>
<h3 id="statistical-vs-deep-learning-forecasting-methods-by-statsforecast">&ldquo;<em>Statistical vs Deep Learning forecasting methods</em>&rdquo; by Statsforecast</h3>
<p>In the paper above, the authors concluded that &ldquo;We find that combinations of DL models perform better than most standard models, both statistical and ML, especially for the case of monthly series and long-term forecasts.&rdquo; However, a recent experiment conducted by the authors of the Statsforecast library challenged this deduction by using a more powerful statistical ensemble<cite><sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup></cite>. They combined four statistical models: AutoARIMA, ETS, CES, and DynamicOptimizedTheta (same as their 6th place winning entry in the M4 competition) and shared sMAPE results on the same M3 data.</p>
<img src="/img/posts/2022/deep-learning-multivariate-forecast/m3_smape_from_statsforecast.png" alt="M3 results: statsforecast smape">
<p>As shown above, their statistical ensemble is consistently better than the Transformer, Wavenet, and Feed-Forward models. The following table summarizes their sMAPE results along with associated costs.</p>
<img src="/img/posts/2022/deep-learning-multivariate-forecast/statsforecast_costs.png" alt="M3 results: statsforecast costs">
<p>Their statistical ensemble outperforms most individual deep-learning models with average SMAPE results similar to DeepAR but with computational savings of 99%.</p>
<h4 id="summary-2">Summary</h4>
<p>The authors conclude that &ldquo;deep-learning ensembles outperform statistical ensembles just by 0.36 points in SMAPE. However, the DL ensemble takes more than 14 days to run and costs around USD 11,000, while the statistical ensemble takes 6 minutes to run and costs $0.5c.&rdquo;.</p>
<h4 id="additional-notes-2">Additional Notes</h4>
<p>In another recent experiment, the authors of Statsforecast compared their statistical ensemble against Amazon Forecast <cite><sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup></cite>.  Amazon Forecast is an AutoML time-series forecasting service on AWS that includes algorithms ranging from commonly known statistical models like ARIMA to complex algorithms like CNN-QR and DeepAR+, CNN-QR, DeepAR+, Prophet, NPTS, ARIMA, and ETS. They showed that Amazon Forecast was 60% less accurate and 669 times more expensive than running their statistical alternative.</p>
<h3 id="are-there-any-unique-time-series-characteristics-for-which-some-methods-perform-better-than-others">Are there any unique time series characteristics for which some methods perform better than others?</h3>
<h3 id="are-there-certain-methods-that-can-be-expected-to-work-reasonably-well-with-data-from-certain-data-domains">Are there certain methods that can be expected to work reasonably well with data from certain data domains?</h3>
<p>We can define a time series in terms of its characteristics such as forecastability (aka spectral entropy), the strength of trend and seasonality, etc. Montero-Manso et al <cite><sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup></cite> utilized 42 such features in their runner-up model in the M4 competition. In Makridakis et al.<cite><sup id="fnref1:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup></cite> authors used 5 out 6 features recommended by Kang et al <cite><sup id="fnref1:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup></cite> for the M3 dataset. The features were: Spectral Entropy, Strength of Trend, Strength of Seasonality, Linearity, and Stability. The sMAPE results for the statistical and deep ensembles were then correlated with these features using a Multiple Linear Regression (MLR).</p>
<img src="/img/posts/2022/deep-learning-multivariate-forecast/feature_wise_comparison.png" alt="Featurewise comparison">
<p>We can see that the Ensemble-DL is generally more effective in handling noisy and trended series, in contrast to Ensemble-S which provides more accurate forecasts for seasonal data, as well as for series that are stable or linear. Similarly, other works show that some methods can be expected to perform better when the time series exhibits certain features like intermittency, lumpiness, or smoothness<cite><sup id="fnref1:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></cite>. Also, sparsity in data might make ML models less effective<cite><sup id="fnref1:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></cite>.</p>
<p>Additionally, research work done by Spiliotis et al.<cite><sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup></cite> shows that certain features are dominant in time series data from the domains in the M4 dataset. This information can provide relevant criteria for model selection. For example, since the following table shows that the financial data tend to be noisy, untrended, skewed, and not seasonal, the Theta method could potentially be utilized to boost the forecasting performance. Accordingly, ETS and ARIMA could be powerful solutions for extrapolating demographic data that are characterized by strong seasonality, trend, and linearity. Additionally, a study of M5 contest submissions shows that the ML methods have entered the mainstream of forecasting
applications, at least in the area of retail sales forecasting<cite><sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup></cite>.</p>
<img src="/img/posts/2022/deep-learning-multivariate-forecast/industry_wise_m4_features.png" alt="Industry-wise M4 features">
<h3 id="do-some-models-tend-to-work-better-with-high-dimensional-data">Do some models tend to work better with high-dimensional data?</h3>
<h3 id="why-do-statistical-models-work-better-than-ml-models-for-small-datasets">Why do statistical models work better than ML models for small datasets?</h3>
<h3 id="is-it-better-to-train-a-model-over-multiple-time-series-rather-than-training-it-per-series">Is it better to train a model over multiple time series rather than training it per series?</h3>
<p>The author of &ldquo;<em>Machine learning in M4: What makes a good unstructured model?</em>&rdquo; paper <cite><sup id="fnref2:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></cite> puts forward an excellent theory in order to explain why ML models tend to work better with high-dimensional data. Kang et al.<cite><sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup></cite> showed in their work that certain combinations of time series features may not be possible. Due to their structured nature, statistical models define the class of manifold before the data are observed, which allows the manifold to be defined by a smaller amount of data. However, this comes with a drawback that if the data do not match that class of manifold, the model can never be accurately fitted to it. On the other hand, ML models learn the manifold by observing the data. This creates a more flexible model that can fit more types of relationships but needs more data to define the space.</p>
<p>Another important observation is that Cross-Learning i.e. a single model learning across many related series simultaneously, as opposed to learning in a series-by-series fashion, can enhance forecasting performance. Modeling many series with similar properties is likely to generate a more densely populated, and thus better defined, manifold that is preferred over search spaces with data sparsity. Data normalization used by ML models (as opposed to decomposition-based preprocessing methods used by statistical methods) and rolling-origin cross-validation steps also help in densely populating the same manifold space.</p>
<h2 id="comparisons-based-on-forecasting-competitions">Comparisons based on forecasting competitions</h2>
<h3 id="learnings-from-the-m3-competition">Learnings from the M3 competition</h3>
<p>We learned a bit about the best performing models on M3 data in &ldquo;<em>Extending to all 3,003 time series of M3 dataset</em>&rdquo;. As
the study found, statistical models are better suited for short-term forecasts, while DL ones are better at capturing the long-term characteristics of the data. So it could be beneficial to combine statistical or ML models with DL ones depending on the time horizon of forecasting. The winning model of the M3 competition was built on this theme using a statistical algorithm (ETS) for preprocessing followed by an LSTM-based neural network <cite><sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup></cite>. This hybrid model performance produced the best results (as measured by sMAPE) in the M3 competition.</p>
<h4 id="additional-notes-3">Additional Notes</h4>
<p>The authors of this M3 winning paper used categorical variables, like error type, trend type (such as additive, multiplicative), etc. as additional information for improving forecasting accuracy.</p>
<h3 id="learnings-from-the-m4-competition">Learnings from the M4 competition</h3>
<p>The winning author of the M3 competition also authored the winning method of the M4 competition in a similar fashion. The contest-winning model was a truly hybrid forecasting approach that utilized both statistical and ML elements, using cross-learning. This approach used exponential smoothing mixed with a dilated LSTM network<cite><sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup></cite>. The ES formulas enable the method to capture the local components (like per-series seasonality), while LSTM allows it to learn non-linear trends (like NN parameters) and cross-learning. To account for seasonality, the author advocate for incorporating categorical seasonality components (one-hot vector of size 6 appended to time series derived features), and smoothing coefficients into model fitting to allow deseasonalizing of data in the absence of any calendar features like the day of the week or the month number.</p>
<p>Additionally, the second-best M4 method was also based on a cross-learning approach, utilizing an ML algorithm (XGBoost) for selecting the most appropriate weights for combining various statistical methods (Theta, ARIMA, TBATS, etc.) <cite><sup id="fnref1:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup></cite>.</p>
<h3 id="learnings-from-the-m5-competition">Learnings from the M5 competition</h3>
<p>Barker&rsquo;s study <cite><sup id="fnref3:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></cite> pointed out how learning to forecast using multiple time series simultaneously leads to narrower and more densely populated manifolds when the series have a commonality in their origin. The dataset of the M5 competition consisted of more than 30,000 series of daily sales at Walmart. So theoretically we would expect ML models trained in a cross-learning fashion to perform well on this dataset without having to specify additional features. Also, the statistical models that do assume interdependence between the series should perform well. It is worth noting that 48.4% of participating teams could beat the Naïve benchmark, 35.8% could beat the Seasonal Naïve, and only 7.5% of teams could beat the exponential smoothing-based benchmark.</p>
<p>LightGBM, a decision tree-based ML approach, was used by almost all of the top 50 M5 competitors <cite><sup id="fnref1:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup></cite> including the winner, second, fourth, and fifth-positioned teams. The winning submission used an equally weighted combination of various LightGBM models trained on data pooled per store, store-category, and store-department. Runnerup submission used the LightGBM model that was externally adjusted through multipliers according to the forecasts product by an N-BEATS model for the top five aggregation levels of the dataset.  The LightGBM models were trained using only some basic features of calendar effects and prices (past unit sales were not considered), and the N-BEATS model was based solely on historical unit sales.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The No-Free-Lunch theorem, proposed for supervised machine learning, tells us that there is never likely to be a single method that fits all situations. Similarly, there is no one time series forecasting method that will always perform best. But the literature suggests a lot of useful learnings that can help us in making more informed decisions when it comes to modeling for time series forecasting. As there are &ldquo;horses for courses&rdquo;, there must also be forecasting methods that are more tailored to some types of data and some aggregation levels<cite><sup id="fnref1:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup></cite>. The following section lists all of the important takeaways from this article.</p>
<h3 id="takeaways">Takeaways</h3>
<ul>
<li>ML methods need more data for better forecasting especially when the series being predicted is non-stationary, displaying seasonality and trend.</li>
<li>For many years, ML methods have been outperformed by simple, yet robust statistical approaches specifically for domains that involve shorter, non-stationary data, such as business forecasting.</li>
<li>DL methods can effectively balance learning across multiple forecasting horizons, sacrificing part of their short-term accuracy to ensure adequate performance in the long term.</li>
<li>Ensemble and Hybrid methods outperform most of the individual statistical and ML models.</li>
<li>Ensembles of some relatively simple statistical methods like ARIMA, ETS, and Theta, often provide similarly accurate or even more accurate short-term forecasts than the individual DL models and even their ensembles (on rare occasions), but inferior results when medium or long-term forecasts are considered.</li>
<li>DL ensembles will almost always give better results than Stat ensembles but at a much higher cost and not necessarily a big margin.</li>
<li>Although the ensembles of multiple DL models lead to more accurate results, depending on the particular characteristics of the series, the additional cost that has to be paid for using a DL model in order to improve forecasting accuracy to a small extent is extensive.</li>
<li>Some studies suggest that DL methods can lead to greater accuracy improvements, compared to their statistical counterparts, when tasked with forecasting yearly or quarterly series.</li>
<li>DL ensembles are generally more effective in handling noisy and trended series, in contrast to statistical ensembles that provide more accurate forecasts for seasonal data, as well as for series that are stable or linear. Although DL models are more robust to randomness, seasonality is more effectively modeled by statistical methods.</li>
</ul>
<h2 id="references">References</h2>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Barker, Jocelyn. (2019). Machine learning in M4: What makes a good unstructured model?. International Journal of Forecasting. 36. 10.1016/j.ijforecast.2019.06.001.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Spiliotis, Evangelos &amp; Makridakis, Spyros &amp; Semenoglou, Artemios-Anargyros &amp; Assimakopoulos, Vassilis. (2022). Comparison of statistical and machine learning methods for daily SKU demand forecasting. Operational Research. 22. 10.1007/s12351-020-00605-2.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Barker, Jocelyn. (2019). Machine learning in M4: What makes a good unstructured model?. International Journal of Forecasting. 36. 10.1016/j.ijforecast.2019.06.001.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref3:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Makridakis, Spyros &amp; Spiliotis, Evangelos &amp; Assimakopoulos, Vassilis. (2018). Statistical and Machine Learning forecasting methods: Concerns and ways forward. PLoS ONE. 13. 10.1371/journal.pone.0194889.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Makridakis, Spyros &amp; Spiliotis, Evangelos &amp; Assimakopoulos, Vassilis &amp; Semenoglou, Artemios-Anargyros &amp; Mulder, Gary &amp; Nikolopoulos, Konstantinos. (2022). Statistical, machine learning and deep learning forecasting methods: Comparisons and ways forward. Journal of the Operational Research Society. 1-20. 10.1080/01605682.2022.2118629.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Koning, Alex &amp; Franses, Philip &amp; Hibon, Michele &amp; Stekler, H.O.. (2005). The M3 competition: Statistical tests of the results. International Journal of Forecasting. 21. 397-409. 10.1016/j.ijforecast.2004.10.003.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Oreshkin, Boris &amp; Carpo, Dmitri &amp; Chapados, Nicolas &amp; Bengio, Yoshua. (2019). N-BEATS: Neural basis expansion analysis for interpretable time series forecasting.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p><a href="https://github.com/Nixtla/statsforecast/tree/main/experiments/m3" target="_blank" rel="noopener noreffer">https://github.com/Nixtla/statsforecast/tree/main/experiments/m3</a>&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p><a href="https://github.com/Nixtla/statsforecast/tree/main/experiments/amazon_forecast" target="_blank" rel="noopener noreffer">https://github.com/Nixtla/statsforecast/tree/main/experiments/amazon_forecast</a>&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Montero-Manso, Pablo &amp; Athanasopoulos, George &amp; Hyndman, Rob &amp; Talagala, Thiyanga. (2019). FFORMA: Feature-based forecast model averaging. International Journal of Forecasting. 36. 10.1016/j.ijforecast.2019.02.011.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Spiliotis, Evangelos &amp; Kouloumos, Andreas &amp; Assimakopoulos, Vassilis &amp; Makridakis, Spyros. (2018). Are forecasting competitions data representative of the reality?. International Journal of Forecasting. 10.1016/j.ijforecast.2018.12.007.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Makridakis, Spyros &amp; Spiliotis, Evangelos &amp; Assimakopoulos, Vassilis. (2020). The M5 Accuracy competition: Results, findings and conclusions.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Kang, Yanfei &amp; Hyndman, Rob &amp; Smith-Miles, Kate. (2017). Visualising forecasting algorithm performance using time series instance spaces. International Journal of Forecasting. 33. 345-358. 10.1016/j.ijforecast.2016.09.004.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Smyl, Slawek &amp; Kuber, Karthik. (2016). Data Preprocessing and Augmentation for Multiple Short Time Series Forecasting with Recurrent Neural Networks.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Smyl, Slawek. (2019). A hybrid method of exponential smoothing and recurrent neural networks for time series forecasting. International Journal of Forecasting. 36. 10.1016/j.ijforecast.2019.03.017.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2022-12-20</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="https://blog.reachsumit.com/posts/2022/12/stats-vs-ml-for-ts/" data-title="Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting" data-via="_reachsumit" data-hashtags="forecasting,literature review"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="https://blog.reachsumit.com/posts/2022/12/stats-vs-ml-for-ts/" data-hashtag="forecasting"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Linkedin" data-sharer="linkedin" data-url="https://blog.reachsumit.com/posts/2022/12/stats-vs-ml-for-ts/"><i class="fab fa-linkedin fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on WhatsApp" data-sharer="whatsapp" data-url="https://blog.reachsumit.com/posts/2022/12/stats-vs-ml-for-ts/" data-title="Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting" data-web><i class="fab fa-whatsapp fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Hacker News" data-sharer="hackernews" data-url="https://blog.reachsumit.com/posts/2022/12/stats-vs-ml-for-ts/" data-title="Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Reddit" data-sharer="reddit" data-url="https://blog.reachsumit.com/posts/2022/12/stats-vs-ml-for-ts/"><i class="fab fa-reddit fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Line" data-sharer="line" data-url="https://blog.reachsumit.com/posts/2022/12/stats-vs-ml-for-ts/" data-title="Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.0.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Pocket" data-sharer="pocket" data-url="https://blog.reachsumit.com/posts/2022/12/stats-vs-ml-for-ts/"><i class="fab fa-get-pocket fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on 微博" data-sharer="weibo" data-url="https://blog.reachsumit.com/posts/2022/12/stats-vs-ml-for-ts/" data-title="Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Evernote" data-sharer="evernote" data-url="https://blog.reachsumit.com/posts/2022/12/stats-vs-ml-for-ts/" data-title="Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting"><i class="fab fa-evernote fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Trello" data-sharer="trello" data-url="https://blog.reachsumit.com/posts/2022/12/stats-vs-ml-for-ts/" data-title="Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting" data-description="Statistical methods have been used in the time series domain for multiple decades. But given the recent advances in Machine Learning and especially its sub-domain Deep Learning, are statistical methods still superior for forecasting? In this article, we will do a deep dive into literature and recent time series competitions to do a multifaceted comparison between Statistical, Machine Learning, and Deep Learning methods for time series forecasting."><i class="fab fa-trello fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/forecasting/">forecasting</a>,&nbsp;<a href="/tags/literature-review/">literature review</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/2022/11/sparse-recsys/" class="prev" rel="prev" title="Recommender Systems for Modeling Feature Interactions under Sparse Settings"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>Recommender Systems for Modeling Feature Interactions under Sparse Settings</a></div>
</div>
<div id="comments"><div id="disqus_thread" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://disqus.com/?ref_noscript">Disqus</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2020 - 2022</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="https://reachsumit.com" target="_blank">Sumit Kumar</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css"><script type="text/javascript" src="https://reachsumit-blog.disqus.com/embed.js" defer></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/algoliasearch@4.13.1/dist/algoliasearch-lite.umd.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.1/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/typeit@8.5.4/dist/index.umd.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":10},"comment":{},"data":{"id-1":"Sumit's Diary","id-2":"Sumit's Diary"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"LV11CUNTAX","algoliaIndex":"blog_reachsumit","algoliaSearchKey":"98d868016771f8a06b967e7eb3eaf63a","highlightTag":"em","maxResultLength":10,"noResultsFound":"No results found","snippetLength":30,"type":"algolia"},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"id-1":["id-1"],"id-2":["id-2"]},"duration":2700,"speed":100}};</script><script type="text/javascript" src="/js/theme.min.js"></script><script type="text/javascript">
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'UA-171612692-1');
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=UA-171612692-1" async></script></body>
</html>
