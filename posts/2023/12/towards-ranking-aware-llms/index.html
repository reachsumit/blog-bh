

<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="noodp" />
    <title>Strategies for Effective and Efficient Text Ranking Using Large Language Models - Sumit&#39;s Diary</title><meta name="Description" content="Welcome to Sumit Kumar&#39;s Personal Blog!"><meta property="og:title" content="Strategies for Effective and Efficient Text Ranking Using Large Language Models" />
<meta property="og:description" content="The previous article did a deep dive into the prompting-based pointwise, pairwise, and listwise techniques that directly use LLMs to perform reranking. In this article, we will take a closer look at some of the shortcomings of the prompting methods and explore the latest efforts to train ranking-aware LLMs. The article also describes several strategies to build effective and efficient LLM-based rerankers." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.reachsumit.com/posts/2023/12/towards-ranking-aware-llms/" /><meta property="og:image" content="https://blog.reachsumit.com/posts/2023/12/towards-ranking-aware-llms/featured-image-preview.webp"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-26T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-12-26T00:00:00+00:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://blog.reachsumit.com/posts/2023/12/towards-ranking-aware-llms/featured-image-preview.webp"/>
<meta name="twitter:title" content="Strategies for Effective and Efficient Text Ranking Using Large Language Models"/>
<meta name="twitter:description" content="The previous article did a deep dive into the prompting-based pointwise, pairwise, and listwise techniques that directly use LLMs to perform reranking. In this article, we will take a closer look at some of the shortcomings of the prompting methods and explore the latest efforts to train ranking-aware LLMs. The article also describes several strategies to build effective and efficient LLM-based rerankers."/>
<meta name="application-name" content="Sumit&#39;s Diary">
<meta name="apple-mobile-web-app-title" content="Sumit&#39;s Diary">

<meta name="theme-color" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="icon" href="/img/avatar/favicon.ico"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="canonical" href="https://blog.reachsumit.com/posts/2023/12/towards-ranking-aware-llms/" /><link rel="prev" href="https://blog.reachsumit.com/posts/2023/12/prompting-llm-for-ranking/" /><link rel="next" href="https://blog.reachsumit.com/posts/2024/01/user-behavior-modeling-recsys/" />
<link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/normalize.css@8.0.1/normalize.min.css"><link rel="stylesheet" href="/css/color.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" as="style" onload="this.onload=null;this.rel='stylesheet'" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css"></noscript><link rel="preload" as="style" onload="this.onload=null;this.rel='stylesheet'" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Strategies for Effective and Efficient Text Ranking Using Large Language Models",
        "inLanguage": "en",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/blog.reachsumit.com\/posts\/2023\/12\/towards-ranking-aware-llms\/"
        },"image": ["https:\/\/blog.reachsumit.com\/images\/Apple-Devices-Preview.png"],"genre": "posts","keywords": "literature review, retrieval, ranking, LLM","wordcount":  4390 ,
        "url": "https:\/\/blog.reachsumit.com\/posts\/2023\/12\/towards-ranking-aware-llms\/","datePublished": "2023-12-26T00:00:00+00:00","dateModified": "2023-12-26T00:00:00+00:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "xxxx","logo": "https:\/\/blog.reachsumit.com\/images\/avatar.png"},"author": {
                "@type": "Person",
                "name": "Sumit Kumar"
            },"description": ""
    }
    </script><script src="//instant.page/5.2.0" defer type="module" integrity="sha384-jnZyxPjiipYXnSU0ygqeac2q7CVYMbh84q0uHVRRxEtvFPiQYbXWUorga2aqZJ0z"></script>
</head>

<body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">
        function setTheme(theme) {document.body.setAttribute('theme', theme); document.documentElement.style.setProperty('color-scheme', theme === 'light' ? 'light' : 'dark'); window.theme = theme;   window.isDark = window.theme !== 'light' }
        function saveTheme(theme) {window.localStorage && localStorage.setItem('theme', theme);}
        function getMeta(metaName) {const metas = document.getElementsByTagName('meta'); for (let i = 0; i < metas.length; i++) if (metas[i].getAttribute('name') === metaName) return metas[i]; return '';}
        if (window.localStorage && localStorage.getItem('theme')) {let theme = localStorage.getItem('theme');theme === 'light' || theme === 'dark' || theme === 'black' ? setTheme(theme) : (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches ? setTheme('dark') : setTheme('light')); } else { if ('light' === 'light' || 'light' === 'dark' || 'light' === 'black') setTheme('light'), saveTheme('light'); else saveTheme('auto'), window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches ? setTheme('dark') : setTheme('light');}
        let metaColors = {'light': '#f8f8f8','dark': '#252627','black': '#000000'}
        getMeta('theme-color').content = metaColors[document.body.getAttribute('theme')];
        window.switchThemeEventSet = new Set()
    </script>
    <div id="back-to-top"></div>
    <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Sumit&#39;s Diary"><span id="desktop-header-typeit" class="typeit"></span></a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><a class="menu-item" href="/about/"> About </a><a class="menu-item" href="https://reachsumit.com/#contact" rel="noopener noreferrer" target="_blank"> Contact </a><a class="menu-item" href="/newsletter/"> Newsletter(s) </a><a class="menu-item" href="/talks/"> Talks </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search this blog" id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-select" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw"></i>
                    <select class="color-theme-select" id="theme-select-desktop" title="Switch Theme">
                        <option value="light">Light</option>
                        <option value="dark">Dark</option>
                        <option value="black">Black</option>
                        <option value="auto">Auto</option>
                    </select>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Sumit&#39;s Diary"><span id="mobile-header-typeit" class="typeit"></span></a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search this blog" id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a class="menu-item" href="/about/" title="">About</a><a class="menu-item" href="https://reachsumit.com/#contact" title="" rel="noopener noreferrer" target="_blank">Contact</a><a class="menu-item" href="/newsletter/" title="">Newsletter(s)</a><a class="menu-item" href="/talks/" title="">Talks</a><a href="javascript:void(0);" class="menu-item theme-select" title="Switch Theme">
                <i class="fas fa-adjust fa-fw"></i>
                <select class="color-theme-select" id="theme-select-mobile" title="Switch Theme">
                    <option value="light">Light</option>
                    <option value="dark">Dark</option>
                    <option value="black">Black</option>
                    <option value="auto">Auto</option>
                </select>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
            <div class="container"><div class="toc" id="toc-auto">
        <h2 class="toc-title">Contents</h2>
        <div class="toc-content" id="toc-content-auto"><nav id="TableOfContents">
  <ul>
    <li><a href="#current-challenges-with-prompting-for-ranking">Current Challenges with Prompting for Ranking</a>
      <ul>
        <li><a href="#sensitivity-to-initial-ordering">Sensitivity to Initial Ordering</a></li>
        <li><a href="#potential-causes-for-sensitivity-to-order">Potential Causes for Sensitivity to Order</a></li>
        <li><a href="#mitigating-the-order-sensitivity-problem">Mitigating the Order Sensitivity Problem</a></li>
      </ul>
    </li>
    <li><a href="#towards-ranking-aware-llms">Towards Ranking-aware LLMs</a>
      <ul>
        <li><a href="#distilling-llms-for-reranking">Distilling LLMs for Reranking</a></li>
        <li><a href="#fine-tuning-llms-for-reranking">Fine-tuning LLMs for Reranking</a></li>
      </ul>
    </li>
    <li><a href="#other-useful-takeaways">Other Useful Takeaways</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav></div>
    </div><script>document.getElementsByTagName("main")[0].setAttribute("autoTOC", "true")</script><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Strategies for Effective and Efficient Text Ranking Using Large Language Models</h1><div class="post-meta">
            <div class="post-meta-line">
                <span class="post-author"><span class="author fas fa-user-circle fa-fw"></span><a href="https://reachsumit.com" title="Author" target="_blank" rel="noopener noreferrer author" class="author">Sumit Kumar</a>
                </span>&nbsp;<span class="post-category">included in </span>&nbsp;<span class="post-category">category <a href="/categories/information-retrieval/"><i class="far fa-folder fa-fw"></i>Information Retrieval</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2023-12-26">2023-12-26</time>&nbsp;<i class="far fa-edit fa-fw"></i>&nbsp;<time datetime="2023-12-26">2023-12-26</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;4390 words&nbsp;<i class="far fa-clock fa-fw"></i>&nbsp;21 minutes&nbsp;</div>
        </div><div class="featured-image"><img
        
        loading="eager"
        src="/posts/2023/12/towards-ranking-aware-llms/featured-image.webp"
        srcset="/posts/2023/12/towards-ranking-aware-llms/featured-image.webp, /posts/2023/12/towards-ranking-aware-llms/featured-image.webp 1.5x, /posts/2023/12/towards-ranking-aware-llms/featured-image.webp 2x"
        sizes="auto"
        alt="/posts/2023/12/towards-ranking-aware-llms/featured-image.webp"
        title="/posts/2023/12/towards-ranking-aware-llms/featured-image.webp" height="600"   width="1200" ></div><div class="details toc" id="toc-static"  kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#current-challenges-with-prompting-for-ranking">Current Challenges with Prompting for Ranking</a>
      <ul>
        <li><a href="#sensitivity-to-initial-ordering">Sensitivity to Initial Ordering</a></li>
        <li><a href="#potential-causes-for-sensitivity-to-order">Potential Causes for Sensitivity to Order</a></li>
        <li><a href="#mitigating-the-order-sensitivity-problem">Mitigating the Order Sensitivity Problem</a></li>
      </ul>
    </li>
    <li><a href="#towards-ranking-aware-llms">Towards Ranking-aware LLMs</a>
      <ul>
        <li><a href="#distilling-llms-for-reranking">Distilling LLMs for Reranking</a></li>
        <li><a href="#fine-tuning-llms-for-reranking">Fine-tuning LLMs for Reranking</a></li>
      </ul>
    </li>
    <li><a href="#other-useful-takeaways">Other Useful Takeaways</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><p>In the previous article (linked below), we did a deep dive into the prompting-based pointwise, pairwise, and listwise techniques that directly use LLMs to perform reranking. In this article, we will take a closer look at some of the shortcomings of the prompting methods and explore the latest efforts to train ranking-aware LLMs. The article also describes several strategies to build effective and efficient LLM-based rerankers.</p>

<div class="showcase-box column-2">
    <div class="showcase-image">
        <a href=/posts/2023/12/prompting-llm-for-ranking/><img
        
        loading="lazy"
        src="/posts/2023/12/prompting-llm-for-ranking/featured-image-preview.webp"
        srcset="/posts/2023/12/prompting-llm-for-ranking/featured-image-preview.webp, /posts/2023/12/prompting-llm-for-ranking/featured-image-preview.webp 1.5x, /posts/2023/12/prompting-llm-for-ranking/featured-image-preview.webp 2x"
        sizes="auto"
        alt="Prompting-based Methods for Text Ranking Using Large Language Models"
        title="Prompting-based Methods for Text Ranking Using Large Language Models" height="200"   width="400" ></a>
    </div>
    <h2 class="showcase-title">
        <a href=/posts/2023/12/prompting-llm-for-ranking/>Prompting-based Methods for Text Ranking Using Large Language Models</a>
    </h2>
    <p class="showcase-summary">
        This article reviews recent research direction that directly prompts LLMs to perform ranking using pointwise, pairwise, or listwise techniques.
    </p>
    <a class="showcase-link" href=/posts/2023/12/prompting-llm-for-ranking/>
        Read more...
    </a>
</div>

<div class="showcase-box column-2">
    <div class="showcase-image">
        <a href=/posts/2023/03/llm-for-text-ranking/><img
        
        loading="lazy"
        src="/posts/2023/03/llm-for-text-ranking/featured-image-preview.webp"
        srcset="/posts/2023/03/llm-for-text-ranking/featured-image-preview.webp, /posts/2023/03/llm-for-text-ranking/featured-image-preview.webp 1.5x, /posts/2023/03/llm-for-text-ranking/featured-image-preview.webp 2x"
        sizes="auto"
        alt="Zero and Few Shot Text Retrieval and Ranking Using Large Language Models"
        title="Zero and Few Shot Text Retrieval and Ranking Using Large Language Models" height="200"   width="400" ></a>
    </div>
    <h2 class="showcase-title">
        <a href=/posts/2023/03/llm-for-text-ranking/>Zero and Few Shot Text Retrieval and Ranking Using Large Language Models</a>
    </h2>
    <p class="showcase-summary">
        This article reviews some of the recent proposals from the research community to boost text retrieval and ranking tasks using LLMs.
    </p>
    <a class="showcase-link" href=/posts/2023/03/llm-for-text-ranking/>
        Read more...
    </a>
</div>
<h2 id="current-challenges-with-prompting-for-ranking" class="headerLink">
    <a href="#current-challenges-with-prompting-for-ranking" class="header-mark"></a>Current Challenges with Prompting for Ranking</h2><p>Empirically, several studies have found that current LLMs do not fully understand the ranking task, potentially due to the lack of ranking awareness during the pre-training and fine-tuning procedures. Making LLMs more ranking aware in a data-efficient manner while maintaining their generality for other tasks, is a challenging research direction<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<p>The previous article highlighted several issues with prompting-based approaches. Pointwise ranking strategies do not work for generation APIs (common with closed-source LLMs, like GPT-4). Pointwise ranking also requires the model to output calibrated scores so that they can be used for comparisons in sorting. This is difficult to achieve across promptings, and also unnecessary because ranking only requires relative ordering. The pairwise approach tends to perform best because LLMs do have a sense of pairwise relative comparisons<sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. But comparing all possible pairs can be computationally prohibitive. Listwise ranking tasks have shown to be the most difficult for LLMs, especially for smaller and even moderate-sized models<sup id="fnref2:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. Context size limits make it impossible to include all possible candidate documents in the prompt. Also, LLMs may only output a partial list of documents, may output the same document more than once, produce irrelevant outputs, or may even refuse to perform the ranking task.</p>
<h3 id="sensitivity-to-initial-ordering" class="headerLink">
    <a href="#sensitivity-to-initial-ordering" class="header-mark"></a>Sensitivity to Initial Ordering</h3><p>Various studies have shown that language models struggle to robustly access and use information in long input contexts for reranking tasks. Specifically, the performance of language models is significantly affected by the position of relevant information and the relative ordering of candidate documents in the input context.</p>
<ul>
<li>
<p>Liu et al.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> conducted an empirical investigation using multi-document question-answering to understand how well language models with large context windows (e.g., 4096, 32K, and even 100K tokens) use their input context. They found that the language model performance is highest when relevant information/document occurs at the very beginning (<em>primacy bias</em>), or end (<em>recency bias</em>) of the input context, and performance significantly degrades when the models must access and use information in the middle of their input context.
<figure><a class="lightgallery" href="/img/posts/2023/towards-ranking-aware-llms/lost_in_the_middle.png" title="&amp;ldquo;Lost in the middle&amp;rdquo; effect" data-thumbnail="/img/posts/2023/towards-ranking-aware-llms/lost_in_the_middle.png" data-sub-html="<h2>the effect of changing the position of the relevant document on multi-document question answering performance</h2><p>&amp;ldquo;Lost in the middle&amp;rdquo; effect</p>">
        <img
            
            loading="lazy"
            src="/img/posts/2023/towards-ranking-aware-llms/lost_in_the_middle.png"
            srcset="/img/posts/2023/towards-ranking-aware-llms/lost_in_the_middle.png, /img/posts/2023/towards-ranking-aware-llms/lost_in_the_middle.png 1.5x, /img/posts/2023/towards-ranking-aware-llms/lost_in_the_middle.png 2x"
            sizes="auto"
            alt="&amp;ldquo;Lost in the middle&amp;rdquo; effect">
    </a><figcaption class="image-caption">the effect of changing the position of the relevant document on multi-document question answering performance</figcaption>
    </figure></p>
</li>
<li>
<p>Similarly, Wang et al.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> looked into the LLMs-as-evaluator paradigm and found that LLMs suffer from positional bias, i.e. they prefer the response in the specific position. Their study showed that GPT-4 tended to prefer the item in the first position, while ChatGPT preferred the response in the second position. Also, swapping the slots of the two responses and querying LLM twice will most likely produce conflicting evaluation results. Consequently, the ranking quality can easily be hacked to make one model appear superior to others, for example, Vicuna-13B could beat ChatGPT on 66 over 80 tested queries.
<figure><a class="lightgallery" href="/img/posts/2023/towards-ranking-aware-llms/fair_eval.png" title="LLMs are not Fair Evaluators" data-thumbnail="/img/posts/2023/towards-ranking-aware-llms/fair_eval.png" data-sub-html="<h2>Merely swapping the presentation order of candidate resposnses could lead to overturned comparison results</h2><p>LLMs are not Fair Evaluators</p>">
        <img
            
            loading="lazy"
            src="/img/posts/2023/towards-ranking-aware-llms/fair_eval.png"
            srcset="/img/posts/2023/towards-ranking-aware-llms/fair_eval.png, /img/posts/2023/towards-ranking-aware-llms/fair_eval.png 1.5x, /img/posts/2023/towards-ranking-aware-llms/fair_eval.png 2x"
            sizes="auto"
            alt="LLMs are not Fair Evaluators">
    </a><figcaption class="image-caption">Merely swapping the presentation order of candidate resposnses could lead to overturned comparison results</figcaption>
    </figure></p>
<p>As shown below, when swapping the order of two responses, GPT-4 is more likely to produce conflicting results when the score gap between the two responses is smaller.</p>
<p><figure><a class="lightgallery" href="/img/posts/2023/towards-ranking-aware-llms/conflict_rate.png" title="Score Gap v/s Conflict Count" data-thumbnail="/img/posts/2023/towards-ranking-aware-llms/conflict_rate.png" data-sub-html="<h2>The conflict rate is negatively correlated with the score gap between the two responses.</h2><p>Score Gap v/s Conflict Count</p>">
        <img
            
            loading="lazy"
            src="/img/posts/2023/towards-ranking-aware-llms/conflict_rate.png"
            srcset="/img/posts/2023/towards-ranking-aware-llms/conflict_rate.png, /img/posts/2023/towards-ranking-aware-llms/conflict_rate.png 1.5x, /img/posts/2023/towards-ranking-aware-llms/conflict_rate.png 2x"
            sizes="auto"
            alt="Score Gap v/s Conflict Count">
    </a><figcaption class="image-caption">The conflict rate is negatively correlated with the score gap between the two responses.</figcaption>
    </figure></p>
</li>
<li>
<p>Lu et al.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> studied the effect of input ordering on <em>in-context learning</em> performance and found that the right sample order can make as much of a difference as the right template. The performance variation of different permutations was shown to be a big issue, especially for smaller models. While increasing the model size helped, it still didn&rsquo;t resolve the problem. Also, there was no common denominator between the performant sample orders and performant prompts were not transferable across models.</p>
</li>
<li>
<p>Tang et al.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> conducted a passage reranking task analysis and concluded that different positional biases exist in reranking LLMs, varying by model and dataset. For example, in their experiments, GPT-3.5 did not focus well on the items past the fifteenth.</p>
</li>
</ul>
<h3 id="potential-causes-for-sensitivity-to-order" class="headerLink">
    <a href="#potential-causes-for-sensitivity-to-order" class="header-mark"></a>Potential Causes for Sensitivity to Order</h3><p>Liu et al.<sup id="fnref1:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> conducted an empirical study to understand why the language models struggle to robustly access and use information in their input context. Their findings with respect to the evaluated aspects were as follows.</p>
<ul>
<li><strong>Model Architecture</strong>: The authors compared the commonly used decoder-only models with encoder-decoder models. When encoder-decoder models (<em>Flan-T5-XXL</em>, <em>Flan-UL2</em>) are evaluated on sequences that are shorter than their encoder&rsquo;s training time maximum length, they are relatively more robust to changes in the position of the relevant information in their input context. However, when these models are evaluated on sequences longer than those seen during their training (a feat possible due to relative positional embedding usage), their performance begins to degrade. The authors hypothesize that encoder-decoder models may make better use of their context windows because their bidirectional encoder allows processing each document in the context of future documents, potentially improving relative importance estimation between documents, whereas decoder-only models may only attend to prior tokens.</li>
<li><strong>Query-Aware Contextualization</strong>: When the query is placed after the documents in the input context, decoder-only models cannot attend to the query tokens when contextualizing documents, since the decoder-only models can only attend to prior tokens at each timestep. In their experiments, placing the query before the documents dramatically improves performance on a subset of retrieval tasks.</li>
<li><strong>Effect of Instruction Fine-Tuning</strong>: After their pre-training, instruction fine-tuned models undergo supervised fine-tuning on a dataset of instructions and responses. The task specification and/or instruction is commonly placed at the beginning of the input context in supervised instruction fine-tuning data, which might lead these instruction fine-tuned models to place more weight on the start of the input context.</li>
<li><strong>Model Size</strong>: The authors also found that the &ldquo;lost in the middle&rdquo; phenomenon only appeared in sufficiently large language models. For example, the 7B Llama-2 models were solely recency biased (similar to the recency bias found in non-instruction fine-tuned language models), while the 13B and 70B models exhibit the &ldquo;lost in the middle&rdquo; problem. Additionally, Llama-2 supervised fine-tuning and reinforcement learning from human feedback procedure slightly mitigates the position bias in smaller models.</li>
</ul>
<h3 id="mitigating-the-order-sensitivity-problem" class="headerLink">
    <a href="#mitigating-the-order-sensitivity-problem" class="header-mark"></a>Mitigating the Order Sensitivity Problem</h3><p>In this section, we take a look at some of the recent proposals to alleviate the impact on ranking effectiveness stemming from the sensitivity to the order of candidates in the input context.</p>
<ul>
<li>
<p>Wang et al.<sup id="fnref1:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> proposed a calibration framework to alleviate position bias and achieve a more reliable and fair evaluation result.
<figure><a class="lightgallery" href="/img/posts/2023/towards-ranking-aware-llms/fair_calibration_framework.png" title="Calibration framework with three calibration methods" data-thumbnail="/img/posts/2023/towards-ranking-aware-llms/fair_calibration_framework.png" data-sub-html="<h2>Calibration framework with three calibration methods</h2><p>Calibration framework with three calibration methods</p>">
        <img
            
            loading="lazy"
            src="/img/posts/2023/towards-ranking-aware-llms/fair_calibration_framework.png"
            srcset="/img/posts/2023/towards-ranking-aware-llms/fair_calibration_framework.png, /img/posts/2023/towards-ranking-aware-llms/fair_calibration_framework.png 1.5x, /img/posts/2023/towards-ranking-aware-llms/fair_calibration_framework.png 2x"
            sizes="auto"
            alt="Calibration framework with three calibration methods">
    </a><figcaption class="image-caption">Calibration framework with three calibration methods</figcaption>
    </figure></p>
<p>Their calibration framework employs three simple strategies:</p>
<ol>
<li><strong>Multiple Evidence Calibration (MEC)</strong>:  The authors design an evidence calibration (EC) evaluation template $T_{EC}(q, r_1, r_2)$ that requires the model to generate the explanations (&ldquo;evaluation evidence&rdquo;) first and then give the score. In this way, the score can be calibrated with the evaluation evidence. Next, they sample $k$ EC scores ${S_{r_1}^1,&hellip;,S_{r_1}^k}$ and ${S_{r_2}^{&lsquo;1},&hellip;,S_{r_2}^{&lsquo;k}}$  for responses $r_1$ and $r_2$, where $S_{r}$ and $S_{r}^{&rsquo;}$ denote the scores of the response $r$ at the first and second positions respectively.</li>
<li><strong>Balance Position Calibration (BPC)</strong>: An additional $k$ scores are calculated by swapping the two responses in each example, such as creating a query prompt $T_{EC}(q,r_2,r_1)$ along with the original query prompt $T_{EC}(q,r_1,r_2)$. The final calibrated scores of the two responses are the corresponding averages of the $2k$ scores.</li>
<li><strong>Human-in-the-Loop Calibration (HITLC)</strong>: The authors introduce an entropy-based metric, the Balanced Position Diversity Entropy (BPDE) score, to find examples requiring auxiliary human calibration based on the evaluation results of the MEC and BPC. This strategy essentially measures the difficulty of each example and seeks human assistance when needed.</li>
</ol>
</li>
<li>
<p>Tang et al.<sup id="fnref1:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> apply the shuffle-aggregate paradigm of the self-consistency framework to the decoding step for listwise-ranking LLMs to achieve permutation invariance. In their &ldquo;<em>Permutation Self-Consistency</em>&rdquo; strategy, the list of candidates is shuffled to curate a diverse set of rankings. Each of these output rankings has positional bias, but mistakes are expected to differ among the outputs because of the input order randomization. Then the central ranking closest in Kendall tau distance to all the sampled rankings is computed, which marginalizes out the association between individual list order and output rankings. The authors provide a theoretical proof of true convergence assuming there always exists some random pair of items that are correctly ranked among randomly ordered observations. This method incurs additional financial costs due to multiple LLM calls, however, the latency hit can be minimized by making these calls in parallel.
<figure><a class="lightgallery" href="/img/posts/2023/towards-ranking-aware-llms/found_in_the_middle.png" title="Permutation Self-Consistency Process" data-thumbnail="/img/posts/2023/towards-ranking-aware-llms/found_in_the_middle.png" data-sub-html="<h2>Permutation Self-Consistency Process</h2><p>Permutation Self-Consistency Process</p>">
        <img
            
            loading="lazy"
            src="/img/posts/2023/towards-ranking-aware-llms/found_in_the_middle.png"
            srcset="/img/posts/2023/towards-ranking-aware-llms/found_in_the_middle.png, /img/posts/2023/towards-ranking-aware-llms/found_in_the_middle.png 1.5x, /img/posts/2023/towards-ranking-aware-llms/found_in_the_middle.png 2x"
            sizes="auto"
            alt="Permutation Self-Consistency Process">
    </a><figcaption class="image-caption">Permutation Self-Consistency Process</figcaption>
    </figure></p>
</li>
<li>
<p>Lu et al.<sup id="fnref1:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> proposed a probing method to identify the performant order of candidates in the input context under the few-shot setting. First, they construct a probing set by considering every possible permutation of the candidate items as a set of candidates. Then they use an entropy-based probing metric to calculate the average prediction entropy per data point. The entropy score is then used to rank the prompt ordering by performance.</p>
</li>
</ul>
<h2 id="towards-ranking-aware-llms" class="headerLink">
    <a href="#towards-ranking-aware-llms" class="header-mark"></a>Towards Ranking-aware LLMs</h2><p>Approaches that leverage prompt learning for LLM-based reranking have demonstrated promising effectiveness. However, it is difficult for them to outperform baseline reranker trained and/or fine-tuned on benchmark datasets. Some of the recent prompting-based methods rely on giant, blackbox, commercial LLMs like GPT-4, which has concerns around transparency, reproducibility, and cost constraints. These methods also run into issues like sensitivity to input order and often heavily rely on multiple decoding passes, and intricate prompt engineering. They also do not exploit available human judgments, such as MS MARCO, and do not allow joint reranker-retriever optimization<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>.</p>
<p>Supervised ranking models based on PLMs demonstrate the current state-of-the-art performance. However, these approaches require a large amount of training data in the form of (<em>query</em>, <em>relevant document</em>) pairs for fine-tuning. Generally, these models can be classified based on their language model structure<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>:</p>
<ul>
<li><strong>Encoder-only</strong>: Models like <em>monoBERT</em> formulate the input as: <code>[CLS] query [SEP] document [SEP]</code>. The <code>[CLS]</code> representation generated by the model is fed into a linear layer to compute the relevance score.</li>
<li><strong>Encoder-Decoder</strong>: Models like <em>monoT5</em> and <em>RankT5</em> formulate their input as: <code>Query: query Document: document Relevant:</code> into the encoder. The probability of a &ldquo;True&rdquo; token, generated by the decoder, serves as the relevance score for a text pair.</li>
<li><strong>Decoder-only</strong>: Unidirectional attention-based models described in the next section, such as <em>RankLLaMA</em>, input a prompt containing the query and document pair and utilize the last token representation as the basis for text pair relevance.</li>
</ul>
<p>Finally, there is still a noteworthy disparity between the training objective of LLMs, which typically centers around next token prediction and the objective of evaluating query-document relevance. Hence, off-the-shelf LLMs do not fully understand ranking formulations. This section highlights some of the recent efforts towards developing ranking-aware LLMs.</p>
<h3 id="distilling-llms-for-reranking" class="headerLink">
    <a href="#distilling-llms-for-reranking" class="header-mark"></a>Distilling LLMs for Reranking</h3><ul>
<li>
<p>In <strong>RankGPT</strong>, Sun et al.<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup> distilled the ranking capabilities of ChatGPT (<em>gpt</em>-<em>3.5</em>-<em>turbo</em>) into a small specialized model using a permutation distillation scheme. The authors randomly sampled 10K queries from the MS MARCO training set, and each query was retrieved by BM25 with 20 candidate passages. The permutations predicted by ChatGPT are directly used as the target and are distilled into a student model (a cross-encoder model based on <em>DeBERTa-large</em> and a <em>LLaMA-7B</em> model was used) using a RankNet-based distillation objective. Their student model (435M) was able to monoT5 (3B) model. The authors claim that even with a small amount of ChatGPT-generated data, the specialized student model can outperform strong supervised systems, while also surpassing ChatGPT&rsquo;s reranking performance. The code to reproduce RankGPT results is available <a href="https://github.com/sunnweiwei/RankGPT" target="_blank" rel="noopener noreferrer">on GitHub</a>.</p>
<p><figure><a class="lightgallery" href="/img/posts/2023/towards-ranking-aware-llms/deberta_llama.png" title="Student models" data-thumbnail="/img/posts/2023/towards-ranking-aware-llms/deberta_llama.png" data-sub-html="<h2>Two types of specialized models used in the study</h2><p>Student models</p>">
        <img
            
            loading="lazy"
            src="/img/posts/2023/towards-ranking-aware-llms/deberta_llama.png"
            srcset="/img/posts/2023/towards-ranking-aware-llms/deberta_llama.png, /img/posts/2023/towards-ranking-aware-llms/deberta_llama.png 1.5x, /img/posts/2023/towards-ranking-aware-llms/deberta_llama.png 2x"
            sizes="auto"
            alt="Student models">
    </a><figcaption class="image-caption">Two types of specialized models used in the study</figcaption>
    </figure></p>
</li>
<li>
<p>The authors of RankGPT also published another instruction distillation method<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup> that distilled the prediction of pairwise ranking with computationally demanding instruction (teacher instruction) to the efficient pointwise method but with simpler instruction (student instruction). The authors argue that generally pointwise ranking is more efficient but it compromises effectiveness. On the other hand, both listwise and pairwise ranking methods suffer from efficiency issues. Listwise ranking incurs exponential time complexity of the Transformer with respect to the input length, while pairwise ranking involves $O(n^2)$ calls to LLMs when it pairs every document with every other document. Their distillation process led to an increase in efficiency between 10 and 100x and up to 40% enhanced performance for the open-sourced <em>FLAN-T5</em> LLM during the inference stage. The distilled <em>FLAN-T5-XL</em> model also surpassed <em>monoT5-3B</em> on evaluated benchmarks.</p>
<p><figure><a class="lightgallery" href="/img/posts/2023/towards-ranking-aware-llms/instruction_distillation.png" title="Instruction distillation approach" data-thumbnail="/img/posts/2023/towards-ranking-aware-llms/instruction_distillation.png" data-sub-html="<h2>Instruction distillation to distill the abilities harvested from complex instruction techniques into a model that is more efficient with simple instruction techniques</h2><p>Instruction distillation approach</p>">
        <img
            
            loading="lazy"
            src="/img/posts/2023/towards-ranking-aware-llms/instruction_distillation.png"
            srcset="/img/posts/2023/towards-ranking-aware-llms/instruction_distillation.png, /img/posts/2023/towards-ranking-aware-llms/instruction_distillation.png 1.5x, /img/posts/2023/towards-ranking-aware-llms/instruction_distillation.png 2x"
            sizes="auto"
            alt="Instruction distillation approach">
    </a><figcaption class="image-caption">Instruction distillation to distill the abilities harvested from complex instruction techniques into a model that is more efficient with simple instruction techniques</figcaption>
    </figure></p>
<p>The code to reproduce instruction distillation work is available <a href="https://github.com/sunnweiwei/RankGPT/tree/main/InstructDistill" target="_blank" rel="noopener noreferrer">on GitHub</a>.
<div class="details admonition quote">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-quote-right fa-fw"></i>Pointwise Ranking Prompt for InstructDistill<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">Question:  Given a query “{{query}}”, Is the following passage relevant to the query?<br>
Passage : {{passage}}<br>
If it is relevant answer Yes, else answer No.<br>
Answer:</div>
        </div>
    </div></p>
<div class="details admonition quote">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-quote-right fa-fw"></i>Pairwise Ranking Prompt for InstructDistill<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">Question:  Given a query “{{query}}”, which of the following two passages is more relevant to the query?<br>
passage A: {{passage_A}}<br>
passage B: {{passage_B}}<br>
Output the identifier of the more relevant passage. The answer must be passage A or passage B.<br>
Answer:</div>
        </div>
    </div>
</li>
<li>
<p>Pradeep et al.<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup> presented <strong>RankVicuna</strong>, an open-source LLM capable of performing zero-shot listwise reranking. The authors used the <em>Vicuna</em> model, instruction fine-tuned from Meta&rsquo;s <em>LLaMA-v2</em>, as the student model trained on ranked lists generated by RankGPT-3.5 serving as the teacher model. To generate the training data, the authors randomly sampled 100K queries from the MS MARCO v1 passage ranking training set and retrieved 20 candidates using BM25. These 20 candidates were passed into RankGPT-3.5 to generate teacher orderings that were then distilled down to the student model, RankVicuna. To increase the model&rsquo;s robustness to complex reordering tasks, the authors also included the shuffled input order along with the original BM25 input ordering. The effectiveness of RankVicuna (7B) is on par with RankGPT-3.5 (175B). RankVicuna codebase is accessible <a href="https://github.com/castorini/rank_llm" target="_blank" rel="noopener noreferrer">on GitHub</a>.</p>
<div class="details admonition quote">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-quote-right fa-fw"></i>Input Prompt for RankVicuna (prepended with Vicuna&#39;s system description)<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content"><p>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user’s questions.<br>
USER: I will provide you with {num} passages, each indicated by a numerical identifier []. Rank the passages based on their relevance to the search
query: {query}.</p>
<p>[1] {passage 1}<br>
[2] {passage 2}<br>
&hellip;<br>
[{num}] {passage {num}}</p>
<p>Search Query: {query}.</p>
<p>Rank the {num} passages above based on their relevance to the search query. All the passages should be included and listed using identifiers, in descending order of relevance. The output format should be [] &gt; [], e.g., [4] &gt; [2]. Only respond with the ranking results, do not say any word or explain.</p>
</div>
        </div>
    </div>
</li>
<li>
<p>The authors of RankVicuna also proposed a 7B parameter Zephyr$_{\beta}$-based, state-of-the-art open-sourced <strong>RankZephyr</strong> model for listwise zero-shot reranking<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>. Similar to RankVicuna, they first trained the RankZephyr model using teacher orderings ranked from RankGPT-3.5. Next, they sampled 5K queries from the original 100K sampled queries (due to cost constraints) and further trained the RankZephyr model leveraging RankGPT$_4$ as the teacher. The authors also address the fixed training window size limitation in RankVicuna by sampling a subset of passages ($\le 20$) and training the model with variable window sizes. RankZephyr improves over the pointwise RankLLaMA model, closes the effectiveness gap with RankGPT$_4$ and in some cases also surpasses the proprietary model.</p>
<div class="details admonition quote">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-quote-right fa-fw"></i>Input Prompt and sample generation for RankZephyr<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content"><p>&lt;|system|&gt;<br>
You are RankLLM, an intelligent assistant that can rank passages based on their relevancy to the query.<br>
&lt;|user|&gt;<br>
I will provide you with {num} passages, each indicated by a numerical identifier []. Rank the passages based on their relevance to the search query: {query}.</p>
<p>[1] {passage 1}<br>
[2] {passage 2}<br>
&hellip;<br>
[{num}] {passage {num}}</p>
<p>Search Query: {query}.</p>
<p>Rank the {num} passages above based on their relevance to the search query. All the passages should be included and listed using identifiers, in descending order of relevance. The output format should be [] &gt; [], e.g., [4] &gt; [2]. Only respond with the ranking results, do not say any word or explain.<br>
&lt;|assistant|&gt;</p>
<p>Model Generation: <em>[9] &gt; [4] &gt; [20] &gt; . . . &gt; [13]</em></p>
</div>
        </div>
    </div>
</li>
</ul>
<h3 id="fine-tuning-llms-for-reranking" class="headerLink">
    <a href="#fine-tuning-llms-for-reranking" class="header-mark"></a>Fine-tuning LLMs for Reranking</h3><ul>
<li>
<p>Ma et al.<sup id="fnref1:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> proposed a zero-shot multi-stage ranking pipeline composed of a dense retriever (<em>RepLLaMA</em>) and a point-wise reranker (<strong>RankLLaMA</strong>), both based on fine-tuning the latest LLaMA model using the MS MARCO datasets.</p>
<p><em>RepLLaMA</em> follows the bi-encoder dense retriever architecture but with the backbone model initialized with LLaMA. An end-of-sequence token <code>&lt;/s&gt;</code> is appended to the input query and document to form the input sequence to LLaMA, and the corresponding representation for this token is used as the representation of the input sequence, which can either be a query or a document. Both the models take the first 2048 tokens as input, which covered about 77% of the documents in the corpus used by the authors. Relevance is computed in terms of dot product and the model is optimized end-to-end using InfoNCE loss.</p>
<p><em>RankLLaMA</em> takes a query and a candidate document as input and generates a score that indicates the relevance of the document to the query. This model is also optimized by contrastive loss, and the hard negatives are sampled from the top-ranking results from the retriever. Being a pointwise reranker, RankLLaMA can rerank candidate passages in parallel. Both models demonstrate superior effectiveness in in-domain and zero-shot evaluations. Model checkpoints for this study are available <a href="https://huggingface.co/castorini" target="_blank" rel="noopener noreferrer">on HuggingFace</a>.</p>
</li>
<li>
<p>In <strong>RankingGPT</strong>, Zhang et al.<sup id="fnref1:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> proposed a supervised training strategy to improve LLM ranking capability. First, they construct a large-scale dataset of weakly supervised text pairs using web resources (using query-document pairs such as (<em>title</em>, <em>body</em>), (<em>title</em>, <em>abstract</em>), (<em>post</em>, <em>comment</em>), (<em>entity</em>, <em>description</em>), etc.) to continually pretrain the model and imbue a nuanced understanding of text relevance. The model is trained with a next (query) token prediction task with context provided by the documents. To further enhance the performance, the authors employ a supervised fine-tuning stage using the MS MARCO training set and contrastive learning. A ranking loss is used to help the model effectively discriminate between positive and negative instances.</p>
<p><figure><a class="lightgallery" href="/img/posts/2023/towards-ranking-aware-llms/ranking_gpt.png" title="RankingGPT: two-stage training paradigm" data-thumbnail="/img/posts/2023/towards-ranking-aware-llms/ranking_gpt.png" data-sub-html="<h2>RankingGPT: two-stage training paradigm</h2><p>RankingGPT: two-stage training paradigm</p>">
        <img
            
            loading="lazy"
            src="/img/posts/2023/towards-ranking-aware-llms/ranking_gpt.png"
            srcset="/img/posts/2023/towards-ranking-aware-llms/ranking_gpt.png, /img/posts/2023/towards-ranking-aware-llms/ranking_gpt.png 1.5x, /img/posts/2023/towards-ranking-aware-llms/ranking_gpt.png 2x"
            sizes="auto"
            alt="RankingGPT: two-stage training paradigm">
    </a><figcaption class="image-caption">RankingGPT: two-stage training paradigm</figcaption>
    </figure>
The authors experiment with LLMs of different types and sizes: BLOOM (560M-7B), LLaMA-7B, Qwen-7B, Baichua-7B, and show consistent improvements over the baseline versions of the models, and other models like MonoT5 and RankLLaMA. The authors will release the source code and fine-tuned model <a href="https://github.com/Alibaba-NLP/RankingGPT" target="_blank" rel="noopener noreferrer">on GitHub</a>.</p>
</li>
</ul>
<h2 id="other-useful-takeaways" class="headerLink">
    <a href="#other-useful-takeaways" class="header-mark"></a>Other Useful Takeaways</h2><ul>
<li><strong>Use Variable Sizes for Sliding Window</strong>: In RankZephyr, Pradeep et al.<sup id="fnref1:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup> showed that a listwise-ranking LLM trained with a fixed window size could fail to generalize to an arbitrary number of candidates within the maximum input token size. To address this shortcoming, they fine-tune RankZephyr with variable window sizes by randomly choosing a subset of passages ($\le 20$) to add to the input context.</li>
<li><strong>Relevance Can Be More Nuanced than Binary</strong>: Relevance generation reranking methods choose from binary relevance labels like &ldquo;Yes&rdquo; and &ldquo;No&rdquo; and use the prediction likelihood of these answers to derive the ranking score. But some documents may not be primarily intended to answer the query, but still contain helpful information. Zhuang et al.<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup> proposed incorporating intermediate fine-grained relevance labels, such as &ldquo;<em>Highly Relevant</em>&rdquo;, &ldquo;<em>Somewhat Relevant</em>&rdquo;, and &ldquo;<em>Not Relevant</em>&rdquo;, into the prompt to help LLM develop a more nuanced understanding of relevance. The intuition is that the intermediate relevance labels will help LLM distinguish partially relevant documents from fully relevant or fully irrelevant documents. To avoid using relevant labels with potentially ambiguous order, one can also employ a rating scale, such as a relevance scale from 0 to 4.
<figure><a class="lightgallery" href="/img/posts/2023/towards-ranking-aware-llms/prompting_strategies_relevance_generation.png" title="Different prompting strategies for relevance generation LLM rankers" data-thumbnail="/img/posts/2023/towards-ranking-aware-llms/prompting_strategies_relevance_generation.png" data-sub-html="<h2>Different prompting strategies for relevance generation LLM rankers</h2><p>Different prompting strategies for relevance generation LLM rankers</p>">
        <img
            
            loading="lazy"
            src="/img/posts/2023/towards-ranking-aware-llms/prompting_strategies_relevance_generation.png"
            srcset="/img/posts/2023/towards-ranking-aware-llms/prompting_strategies_relevance_generation.png, /img/posts/2023/towards-ranking-aware-llms/prompting_strategies_relevance_generation.png 1.5x, /img/posts/2023/towards-ranking-aware-llms/prompting_strategies_relevance_generation.png 2x"
            sizes="auto"
            alt="Different prompting strategies for relevance generation LLM rankers">
    </a><figcaption class="image-caption">Different prompting strategies for relevance generation LLM rankers</figcaption>
    </figure></li>
<li><strong>LLMs Fine-Tuned for Ranking Might Lose Their Generalization Ability</strong>: Zhang et al.<sup id="fnref2:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> conducted a perplexity analysis before and after fine-tuning LLMs for reranking. Their experiments showed that RankLLaMA&rsquo;s perplexity increased significantly, despite using fine-tuning based on LoRA with relatively small trainable parameters. This indicates that the model lost some of its generalization ability, which is the most valuable ability of LLMs.</li>
<li><strong>Select High-Performing Few-shot Demonstrations</strong>: Some researchers have shown that the few-shot reranking performance varies drastically depending on the demonstrations included in the prompt. Also, increasing the number of demonstrations does not necessarily help. Following this, they proposed methods to algorithmically select high-performing demonstrations to include in the prompt.
<ul>
<li>
<p>Li et al.<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup> proposed the <strong>LENS</strong> (fi<strong>l</strong>ter-th<strong>en</strong>-<strong>s</strong>earch) method to filter the dataset to obtain informative in-context examples individually. First, they filter the dataset using an &ldquo;informativeness score&rdquo;, and then use a diversity-guided example search method that iteratively refines and evaluates the selected examples to find the supporting examples that can fully depict the task.</p>
</li>
<li>
<p>Drozdov et al.<sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup> proposed a difficulty-based selection (<strong>DBS</strong>) method to find challenging, i.e. low likelihood demonstration to include in the prompt. The authors propose to estimate difficulty using demonstration query likelihood (DQL): $DQL(z) \propto \frac{1}{\lvert q^(z) \rvert} \log P(q^(z) \lvert d^(z))$, and then selecting the demonstrations with the lowest DQL.</p>
</li>
</ul>
</li>
<li><strong>Current IR Datasets Are Insufficient for Supervised Listwise Ranking</strong>: Zhang et al.<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup> found that the current IR training datasets, like MS MARCO, were constructed to train pointwise rerankers in a supervised fashion and yielded worse results than using data generated by BM25 when training pointwise rerankers. The performance of listwise rerankers increases linearly with training data ranking quality, and this relationship hasn&rsquo;t plateaued yet. The authors call for future work on building human-annotated datasets purpose-designed for listwise ranking.</li>
<li><strong>Data Contamination May Cause Performance Overestimation</strong>: LLMs are often trained with a large amount of data crawled from the internet, which makes it very hard to know whether data from a specific benchmark was used to train the LLM<sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>. Since some of the commonly used IR benchmarks were also gathered years ago, the existing LLMs may already possess the knowledge of these datasets, including the benchmark test sets. However, the reranking models are expected to possess the capability to comprehend, deduce, and rank knowledge that is inherently unknown to them. To address this, Sun et al.<sup id="fnref1:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup> constructed a test set, called <em>NovelEval</em>, that is continuously updated to ensure that the questions and passages have likely not appeared in the training data for LLMs. Both RankGPT and RankZephyr reported their performance on NovelEval.</li>
<li><strong>Impact of the First-Stage Retrieval</strong>: Several studies have confirmed that first-stage retrieval has a substantial impact on the overall effectiveness of the LLM reranker. Both the choice of the retrieval model and the number of candidates considered for reranking are crucial. For example, Pradeep et al.<sup id="fnref1:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup> <sup id="fnref2:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup> show that as the first-stage effectiveness increases, additional improvement from prompt-based reranker LLM decreases. Their reranker led to larger improvements when used with the top 100 BM25 candidates compared to candidates from SPLADE++ EnsembleDistil. Comparison between reranking the top 20 versus the top 100 candidates shows that processing a larger pool generally leads to larger improvements. The same study also shows that data augmentations, such as shuffling the input order of documents and permuting the generation orders provided by the teacher model led to more effective results.</li>
</ul>
<h2 id="conclusion" class="headerLink">
    <a href="#conclusion" class="header-mark"></a>Conclusion</h2><p>There has been a growing interest in leveraging the language understanding and reasoning capabilities of LLMs in the information retrieval domain. In the previous article, we learned the prompting-based techniques to exploit LLMs as text rerankers. In this article, we looked closer at associated challenges and some of the potential improvements that can be done to make these methods more ranking-aware. The article also described the latest research work that bridges the gap between relevance ranking and the training objectives of LLMs. The article ended with a few additional learnings and takeaways from these works.</p>
<h2 id="references" class="headerLink">
    <a href="#references" class="header-mark"></a>References</h2><div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Qin, Z., Jagerman, R., Hui, K., Zhuang, H., Wu, J., Shen, J., Liu, T., Liu, J., Metzler, D., Wang, X., &amp; Bendersky, M. (2023). Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting. <em>ArXiv</em>. /abs/2306.17563&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., &amp; Liang, P. (2023). Lost in the Middle: How Language Models Use Long Contexts. <em>ArXiv</em>. /abs/2307.03172&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Wang, P., Li, L., Chen, L., Cai, Z., Zhu, D., Lin, B., Cao, Y., Liu, Q., Liu, T., &amp; Sui, Z. (2023). Large Language Models are not Fair Evaluators. <em>ArXiv</em>. /abs/2305.17926&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Lu, Y., Bartolo, M., Moore, A., Riedel, S., &amp; Stenetorp, P. (2021). Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity. <em>ArXiv</em>. /abs/2104.08786&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Tang, R., Zhang, X., Ma, X., Lin, J., &amp; Ture, F. (2023). Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models. <em>ArXiv</em>. /abs/2310.07712&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Ma, X., Wang, L., Yang, N., Wei, F., &amp; Lin, J. (2023). Fine-Tuning LLaMA for Multi-Stage Text Retrieval. <em>ArXiv</em>. /abs/2310.08319&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Zhang, L., Zhang, Y., Long, D., Xie, P., Zhang, M., &amp; Zhang, M. (2023). RankingGPT: Empowering Large Language Models in Text Ranking with Progressive Enhancement. <em>ArXiv</em>. /abs/2311.16720&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Sun, W., Yan, L., Ma, X., Wang, S., Ren, P., Chen, Z., Yin, D., &amp; Ren, Z. (2023). Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents. <em>ArXiv</em>. /abs/2304.09542&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Sun, W., Chen, Z., Ma, X., Yan, L., Wang, S., Ren, P., Chen, Z., Yin, D., &amp; Ren, Z. (2023). Instruction Distillation Makes Large Language Models Efficient Zero-shot Rankers. <em>ArXiv</em>. /abs/2311.01555&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Pradeep, R., Sharifymoghaddam, S., &amp; Lin, J. (2023). RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models. <em>ArXiv</em>. /abs/2309.15088&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Pradeep, R., Sharifymoghaddam, S., &amp; Lin, J. (2023). RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a Breeze! <em>ArXiv</em>. /abs/2312.02724&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Zhuang, H., Qin, Z., Hui, K., Wu, J., Yan, L., Wang, X., &amp; Bendersky, M. (2023). Beyond Yes and No: Improving Zero-Shot LLM Rankers via Scoring Fine-Grained Relevance Labels. <em>ArXiv</em>. /abs/2310.14122&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Li, X., &amp; Qiu, X. (2023). Finding Support Examples for In-Context Learning. <em>ArXiv</em>. /abs/2302.13539&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Drozdov, A., Zhuang, H., Dai, Z., Qin, Z., Rahimi, R., Wang, X., Alon, D., Iyyer, M., McCallum, A., Metzler, D., &amp; Hui, K. (2023). PaRaDe: Passage Ranking using Demonstrations with Large Language Models. <em>ArXiv</em>. /abs/2310.14408&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Zhang, X., Hofstätter, S., Lewis, P., Tang, R., &amp; Lin, J. (2023). Rank-without-GPT: Building GPT-Independent Listwise Rerankers on Open-Source Large Language Models. <em>ArXiv</em>. /abs/2312.02969&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Sainz, O., Campos, J.A., García-Ferrero, I., Etxaniz, J., Lacalle, O.L., &amp; Agirre, E. (2023). NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark. <em>Conference on Empirical Methods in Natural Language Processing</em>.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
</div>
		
        


<h2>Related Content</h2>
<div class="related-container">
    <div class="related-item-container">
            <div class="related-image">
                <a href="/posts/2024/01/user-behavior-modeling-recsys/"><img
        
        loading="lazy"
        src="/posts/2024/01/user-behavior-modeling-recsys/featured-image-preview.webp"
        srcset="/posts/2024/01/user-behavior-modeling-recsys/featured-image-preview.webp, /posts/2024/01/user-behavior-modeling-recsys/featured-image-preview.webp 1.5x, /posts/2024/01/user-behavior-modeling-recsys/featured-image-preview.webp 2x"
        sizes="auto"
        alt="/posts/2024/01/user-behavior-modeling-recsys/featured-image-preview.webp"
        title="/posts/2024/01/user-behavior-modeling-recsys/featured-image-preview.webp" height="200"   width="400" ></a>
            </div><h2 class="related-title">
                <a href="/posts/2024/01/user-behavior-modeling-recsys/">A Guide to User Behavior Modeling</a>
            </h2>
        </div>
    <div class="related-item-container">
            <div class="related-image">
                <a href="/posts/2023/12/prompting-llm-for-ranking/"><img
        
        loading="lazy"
        src="/posts/2023/12/prompting-llm-for-ranking/featured-image-preview.webp"
        srcset="/posts/2023/12/prompting-llm-for-ranking/featured-image-preview.webp, /posts/2023/12/prompting-llm-for-ranking/featured-image-preview.webp 1.5x, /posts/2023/12/prompting-llm-for-ranking/featured-image-preview.webp 2x"
        sizes="auto"
        alt="/posts/2023/12/prompting-llm-for-ranking/featured-image-preview.webp"
        title="/posts/2023/12/prompting-llm-for-ranking/featured-image-preview.webp" height="200"   width="400" ></a>
            </div><h2 class="related-title">
                <a href="/posts/2023/12/prompting-llm-for-ranking/">Prompting-based Methods for Text Ranking Using Large Language Models</a>
            </h2>
        </div>
    <div class="related-item-container">
            <div class="related-image">
                <a href="/posts/2023/09/generative-retrieval/"><img
        
        loading="lazy"
        src="/posts/2023/09/generative-retrieval/featured-image-preview.webp"
        srcset="/posts/2023/09/generative-retrieval/featured-image-preview.webp, /posts/2023/09/generative-retrieval/featured-image-preview.webp 1.5x, /posts/2023/09/generative-retrieval/featured-image-preview.webp 2x"
        sizes="auto"
        alt="/posts/2023/09/generative-retrieval/featured-image-preview.webp"
        title="/posts/2023/09/generative-retrieval/featured-image-preview.webp" height="200"   width="400" ></a>
            </div><h2 class="related-title">
                <a href="/posts/2023/09/generative-retrieval/">Generative Retrieval for End-to-End Search Systems</a>
            </h2>
        </div>
    <div class="related-item-container">
            <div class="related-image">
                <a href="/posts/2023/06/llms-for-recsys-entity-representation/"><img
        
        loading="lazy"
        src="/posts/2023/06/llms-for-recsys-entity-representation/featured-image-preview.webp"
        srcset="/posts/2023/06/llms-for-recsys-entity-representation/featured-image-preview.webp, /posts/2023/06/llms-for-recsys-entity-representation/featured-image-preview.webp 1.5x, /posts/2023/06/llms-for-recsys-entity-representation/featured-image-preview.webp 2x"
        sizes="auto"
        alt="/posts/2023/06/llms-for-recsys-entity-representation/featured-image-preview.webp"
        title="/posts/2023/06/llms-for-recsys-entity-representation/featured-image-preview.webp" height="200"   width="400" ></a>
            </div><h2 class="related-title">
                <a href="/posts/2023/06/llms-for-recsys-entity-representation/">Representing Users and Items in Large Language Models based Recommender Systems</a>
            </h2>
        </div>
    <div class="related-item-container">
            <div class="related-image">
                <a href="/posts/2023/05/shallow-heterogeneous-graphs-rep/"><img
        
        loading="lazy"
        src="/posts/2023/05/shallow-heterogeneous-graphs-rep/featured-image-preview.webp"
        srcset="/posts/2023/05/shallow-heterogeneous-graphs-rep/featured-image-preview.webp, /posts/2023/05/shallow-heterogeneous-graphs-rep/featured-image-preview.webp 1.5x, /posts/2023/05/shallow-heterogeneous-graphs-rep/featured-image-preview.webp 2x"
        sizes="auto"
        alt="/posts/2023/05/shallow-heterogeneous-graphs-rep/featured-image-preview.webp"
        title="/posts/2023/05/shallow-heterogeneous-graphs-rep/featured-image-preview.webp" height="200"   width="400" ></a>
            </div><h2 class="related-title">
                <a href="/posts/2023/05/shallow-heterogeneous-graphs-rep/">Shallow Embedding Models for Heterogeneous Graphs</a>
            </h2>
        </div>
    

</div>


        <script src="https://f.convertkit.com/ckjs/ck.5.js"></script>
      <form action="https://app.convertkit.com/forms/4932644/subscriptions" class="seva-form formkit-form" method="post" data-sv-form="4932644" data-uid="e309c832a6" data-format="inline" data-version="5" data-options="{&quot;settings&quot;:{&quot;after_subscribe&quot;:{&quot;action&quot;:&quot;message&quot;,&quot;success_message&quot;:&quot;Success! Now check your email to confirm your subscription.&quot;,&quot;redirect_url&quot;:&quot;&quot;},&quot;analytics&quot;:{&quot;google&quot;:null,&quot;fathom&quot;:null,&quot;facebook&quot;:null,&quot;segment&quot;:null,&quot;pinterest&quot;:null,&quot;sparkloop&quot;:null,&quot;googletagmanager&quot;:null},&quot;modal&quot;:{&quot;trigger&quot;:&quot;timer&quot;,&quot;scroll_percentage&quot;:null,&quot;timer&quot;:5,&quot;devices&quot;:&quot;all&quot;,&quot;show_once_every&quot;:15},&quot;powered_by&quot;:{&quot;show&quot;:true,&quot;url&quot;:&quot;https://convertkit.com/features/forms?utm_campaign=poweredby&amp;utm_content=form&amp;utm_medium=referral&amp;utm_source=dynamic&quot;},&quot;recaptcha&quot;:{&quot;enabled&quot;:false},&quot;return_visitor&quot;:{&quot;action&quot;:&quot;show&quot;,&quot;custom_content&quot;:&quot;&quot;},&quot;slide_in&quot;:{&quot;display_in&quot;:&quot;bottom_right&quot;,&quot;trigger&quot;:&quot;timer&quot;,&quot;scroll_percentage&quot;:null,&quot;timer&quot;:5,&quot;devices&quot;:&quot;all&quot;,&quot;show_once_every&quot;:15},&quot;sticky_bar&quot;:{&quot;display_in&quot;:&quot;top&quot;,&quot;trigger&quot;:&quot;timer&quot;,&quot;scroll_percentage&quot;:null,&quot;timer&quot;:5,&quot;devices&quot;:&quot;all&quot;,&quot;show_once_every&quot;:15}},&quot;version&quot;:&quot;5&quot;}" min-width="400 500 600 700 800" style="background-color: rgb(249, 250, 251); border-radius: 4px;"><div class="formkit-background" style="opacity: 0.33;"></div><div data-style="minimal"><div class="formkit-header" data-element="header" style="color: rgb(77, 77, 77); font-size: 27px; font-weight: 700;"><h2>Be the First to Know</h2></div><div class="formkit-subheader" data-element="subheader" style="color: rgb(104, 104, 104); font-size: 18px;"><p>Subscribe to get notified when I write a new post.</p></div><ul class="formkit-alert formkit-alert-error" data-element="errors" data-group="alert"></ul><div data-element="fields" data-stacked="false" class="seva-fields formkit-fields"><div class="formkit-field"><input class="formkit-input" name="email_address" aria-label="Email Address" placeholder="Email Address" required="" type="email" style="color: rgb(0, 0, 0); border-color: rgb(227, 227, 227); border-radius: 4px; font-weight: 400;"></div><button data-element="submit" class="formkit-submit formkit-submit" style="color: rgb(255, 255, 255); background-color: rgb(22, 119, 190); border-radius: 4px; font-weight: 400;"><div class="formkit-spinner"><div></div><div></div><div></div></div><span class="">Subscribe</span></button></div><div class="formkit-guarantee" data-element="guarantee" style="color: rgb(77, 77, 77); font-size: 13px; font-weight: 400;"><p>We won't send you spam. Unsubscribe at any time.</p></div><div class="formkit-powered-by-convertkit-container"><a href="https://convertkit.com/features/forms?utm_campaign=poweredby&amp;utm_content=form&amp;utm_medium=referral&amp;utm_source=dynamic" data-element="powered-by" class="formkit-powered-by-convertkit" data-variant="dark" target="_blank" rel="nofollow">Built with ConvertKit</a></div></div><style>.formkit-form[data-uid="e309c832a6"] *{box-sizing:border-box;}.formkit-form[data-uid="e309c832a6"]{-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;}.formkit-form[data-uid="e309c832a6"] legend{border:none;font-size:inherit;margin-bottom:10px;padding:0;position:relative;display:table;}.formkit-form[data-uid="e309c832a6"] fieldset{border:0;padding:0.01em 0 0 0;margin:0;min-width:0;}.formkit-form[data-uid="e309c832a6"] body:not(:-moz-handler-blocked) fieldset{display:table-cell;}.formkit-form[data-uid="e309c832a6"] h1,.formkit-form[data-uid="e309c832a6"] h2,.formkit-form[data-uid="e309c832a6"] h3,.formkit-form[data-uid="e309c832a6"] h4,.formkit-form[data-uid="e309c832a6"] h5,.formkit-form[data-uid="e309c832a6"] h6{color:inherit;font-size:inherit;font-weight:inherit;}.formkit-form[data-uid="e309c832a6"] h2{font-size:1.5em;margin:1em 0;}.formkit-form[data-uid="e309c832a6"] h3{font-size:1.17em;margin:1em 0;}.formkit-form[data-uid="e309c832a6"] p{color:inherit;font-size:inherit;font-weight:inherit;}.formkit-form[data-uid="e309c832a6"] ol:not([template-default]),.formkit-form[data-uid="e309c832a6"] ul:not([template-default]),.formkit-form[data-uid="e309c832a6"] blockquote:not([template-default]){text-align:left;}.formkit-form[data-uid="e309c832a6"] p:not([template-default]),.formkit-form[data-uid="e309c832a6"] hr:not([template-default]),.formkit-form[data-uid="e309c832a6"] blockquote:not([template-default]),.formkit-form[data-uid="e309c832a6"] ol:not([template-default]),.formkit-form[data-uid="e309c832a6"] ul:not([template-default]){color:inherit;font-style:initial;}.formkit-form[data-uid="e309c832a6"] .ordered-list,.formkit-form[data-uid="e309c832a6"] .unordered-list{list-style-position:outside !important;padding-left:1em;}.formkit-form[data-uid="e309c832a6"] .list-item{padding-left:0;}.formkit-form[data-uid="e309c832a6"][data-format="modal"]{display:none;}.formkit-form[data-uid="e309c832a6"][data-format="slide in"]{display:none;}.formkit-form[data-uid="e309c832a6"][data-format="sticky bar"]{display:none;}.formkit-sticky-bar .formkit-form[data-uid="e309c832a6"][data-format="sticky bar"]{display:block;}.formkit-form[data-uid="e309c832a6"] .formkit-input,.formkit-form[data-uid="e309c832a6"] .formkit-select,.formkit-form[data-uid="e309c832a6"] .formkit-checkboxes{width:100%;}.formkit-form[data-uid="e309c832a6"] .formkit-button,.formkit-form[data-uid="e309c832a6"] .formkit-submit{border:0;border-radius:5px;color:#ffffff;cursor:pointer;display:inline-block;text-align:center;font-size:15px;font-weight:500;cursor:pointer;margin-bottom:15px;overflow:hidden;padding:0;position:relative;vertical-align:middle;}.formkit-form[data-uid="e309c832a6"] .formkit-button:hover,.formkit-form[data-uid="e309c832a6"] .formkit-submit:hover,.formkit-form[data-uid="e309c832a6"] .formkit-button:focus,.formkit-form[data-uid="e309c832a6"] .formkit-submit:focus{outline:none;}.formkit-form[data-uid="e309c832a6"] .formkit-button:hover > span,.formkit-form[data-uid="e309c832a6"] .formkit-submit:hover > span,.formkit-form[data-uid="e309c832a6"] .formkit-button:focus > span,.formkit-form[data-uid="e309c832a6"] .formkit-submit:focus > span{background-color:rgba(0,0,0,0.1);}.formkit-form[data-uid="e309c832a6"] .formkit-button > span,.formkit-form[data-uid="e309c832a6"] .formkit-submit > span{display:block;-webkit-transition:all 300ms ease-in-out;transition:all 300ms ease-in-out;padding:12px 24px;}.formkit-form[data-uid="e309c832a6"] .formkit-input{background:#ffffff;font-size:15px;padding:12px;border:1px solid #e3e3e3;-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;line-height:1.4;margin:0;-webkit-transition:border-color ease-out 300ms;transition:border-color ease-out 300ms;}.formkit-form[data-uid="e309c832a6"] .formkit-input:focus{outline:none;border-color:#1677be;-webkit-transition:border-color ease 300ms;transition:border-color ease 300ms;}.formkit-form[data-uid="e309c832a6"] .formkit-input::-webkit-input-placeholder{color:inherit;opacity:0.8;}.formkit-form[data-uid="e309c832a6"] .formkit-input::-moz-placeholder{color:inherit;opacity:0.8;}.formkit-form[data-uid="e309c832a6"] .formkit-input:-ms-input-placeholder{color:inherit;opacity:0.8;}.formkit-form[data-uid="e309c832a6"] .formkit-input::placeholder{color:inherit;opacity:0.8;}.formkit-form[data-uid="e309c832a6"] [data-group="dropdown"]{position:relative;display:inline-block;width:100%;}.formkit-form[data-uid="e309c832a6"] [data-group="dropdown"]::before{content:"";top:calc(50% - 2.5px);right:10px;position:absolute;pointer-events:none;border-color:#4f4f4f transparent transparent transparent;border-style:solid;border-width:6px 6px 0 6px;height:0;width:0;z-index:999;}.formkit-form[data-uid="e309c832a6"] [data-group="dropdown"] select{height:auto;width:100%;cursor:pointer;color:#333333;line-height:1.4;margin-bottom:0;padding:0 6px;-webkit-appearance:none;-moz-appearance:none;appearance:none;font-size:15px;padding:12px;padding-right:25px;border:1px solid #e3e3e3;background:#ffffff;}.formkit-form[data-uid="e309c832a6"] [data-group="dropdown"] select:focus{outline:none;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"]{text-align:left;margin:0;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"]{margin-bottom:10px;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] *{cursor:pointer;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"]:last-of-type{margin-bottom:0;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] input[type="checkbox"]{display:none;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] input[type="checkbox"] + label::after{content:none;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] input[type="checkbox"]:checked + label::after{border-color:#ffffff;content:"";}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] input[type="checkbox"]:checked + label::before{background:#10bf7a;border-color:#10bf7a;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] label{position:relative;display:inline-block;padding-left:28px;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] label::before,.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] label::after{position:absolute;content:"";display:inline-block;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] label::before{height:16px;width:16px;border:1px solid #e3e3e3;background:#ffffff;left:0px;top:3px;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] label::after{height:4px;width:8px;border-left:2px solid #4d4d4d;border-bottom:2px solid #4d4d4d;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);left:4px;top:8px;}.formkit-form[data-uid="e309c832a6"] .formkit-alert{background:#f9fafb;border:1px solid #e3e3e3;border-radius:5px;-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;list-style:none;margin:25px auto;padding:12px;text-align:center;width:100%;}.formkit-form[data-uid="e309c832a6"] .formkit-alert:empty{display:none;}.formkit-form[data-uid="e309c832a6"] .formkit-alert-success{background:#d3fbeb;border-color:#10bf7a;color:#0c905c;}.formkit-form[data-uid="e309c832a6"] .formkit-alert-error{background:#fde8e2;border-color:#f2643b;color:#ea4110;}.formkit-form[data-uid="e309c832a6"] .formkit-spinner{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:0px;width:0px;margin:0 auto;position:absolute;top:0;left:0;right:0;width:0px;overflow:hidden;text-align:center;-webkit-transition:all 300ms ease-in-out;transition:all 300ms ease-in-out;}.formkit-form[data-uid="e309c832a6"] .formkit-spinner > div{margin:auto;width:12px;height:12px;background-color:#fff;opacity:0.3;border-radius:100%;display:inline-block;-webkit-animation:formkit-bouncedelay-formkit-form-data-uid-e309c832a6- 1.4s infinite ease-in-out both;animation:formkit-bouncedelay-formkit-form-data-uid-e309c832a6- 1.4s infinite ease-in-out both;}.formkit-form[data-uid="e309c832a6"] .formkit-spinner > div:nth-child(1){-webkit-animation-delay:-0.32s;animation-delay:-0.32s;}.formkit-form[data-uid="e309c832a6"] .formkit-spinner > div:nth-child(2){-webkit-animation-delay:-0.16s;animation-delay:-0.16s;}.formkit-form[data-uid="e309c832a6"] .formkit-submit[data-active] .formkit-spinner{opacity:1;height:100%;width:50px;}.formkit-form[data-uid="e309c832a6"] .formkit-submit[data-active] .formkit-spinner ~ span{opacity:0;}.formkit-form[data-uid="e309c832a6"] .formkit-powered-by[data-active="false"]{opacity:0.35;}.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit-container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;width:100%;z-index:5;margin:10px 0;position:relative;}.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit-container[data-active="false"]{opacity:0.35;}.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:#ffffff;border:1px solid #dde2e7;border-radius:4px;color:#373f45;cursor:pointer;display:block;height:36px;margin:0 auto;opacity:0.95;padding:0;-webkit-text-decoration:none;text-decoration:none;text-indent:100%;-webkit-transition:ease-in-out all 200ms;transition:ease-in-out all 200ms;white-space:nowrap;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:190px;background-repeat:no-repeat;background-position:center;background-image:url("data:image/svg+xml;charset=utf8,%3Csvg width='162' height='20' viewBox='0 0 162 20' fill='none' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='M83.0561 15.2457C86.675 15.2457 89.4722 12.5154 89.4722 9.14749C89.4722 5.99211 86.8443 4.06563 85.1038 4.06563C82.6801 4.06563 80.7373 5.76407 80.4605 8.28551C80.4092 8.75244 80.0387 9.14403 79.5686 9.14069C78.7871 9.13509 77.6507 9.12841 76.9314 9.13092C76.6217 9.13199 76.3658 8.88106 76.381 8.57196C76.4895 6.38513 77.2218 4.3404 78.618 2.76974C80.1695 1.02445 82.4289 0 85.1038 0C89.5979 0 93.8406 4.07791 93.8406 9.14749C93.8406 14.7608 89.1832 19.3113 83.1517 19.3113C78.8502 19.3113 74.5179 16.5041 73.0053 12.5795C72.9999 12.565 72.9986 12.5492 73.0015 12.534C73.0218 12.4179 73.0617 12.3118 73.1011 12.2074C73.1583 12.0555 73.2143 11.907 73.2062 11.7359L73.18 11.1892C73.174 11.0569 73.2075 10.9258 73.2764 10.8127C73.3452 10.6995 73.4463 10.6094 73.5666 10.554L73.7852 10.4523C73.9077 10.3957 74.0148 10.3105 74.0976 10.204C74.1803 10.0974 74.2363 9.97252 74.2608 9.83983C74.3341 9.43894 74.6865 9.14749 75.0979 9.14749C75.7404 9.14749 76.299 9.57412 76.5088 10.1806C77.5188 13.1 79.1245 15.2457 83.0561 15.2457Z' fill='%23373F45'/%3E%3Cpath d='M155.758 6.91365C155.028 6.91365 154.804 6.47916 154.804 5.98857C154.804 5.46997 154.986 5.06348 155.758 5.06348C156.53 5.06348 156.712 5.46997 156.712 5.98857C156.712 6.47905 156.516 6.91365 155.758 6.91365ZM142.441 12.9304V9.32833L141.415 9.32323V8.90392C141.415 8.44719 141.786 8.07758 142.244 8.07986L142.441 8.08095V6.55306L144.082 6.09057V8.08073H145.569V8.50416C145.569 8.61242 145.548 8.71961 145.506 8.81961C145.465 8.91961 145.404 9.01047 145.328 9.08699C145.251 9.16351 145.16 9.2242 145.06 9.26559C144.96 9.30698 144.853 9.32826 144.745 9.32822H144.082V12.7201C144.082 13.2423 144.378 13.4256 144.76 13.4887C145.209 13.5629 145.583 13.888 145.583 14.343V14.9626C144.029 14.9626 142.441 14.8942 142.441 12.9304Z' fill='%23373F45'/%3E%3Cpath d='M110.058 7.92554C108.417 7.88344 106.396 8.92062 106.396 11.5137C106.396 14.0646 108.417 15.0738 110.058 15.0318C111.742 15.0738 113.748 14.0646 113.748 11.5137C113.748 8.92062 111.742 7.88344 110.058 7.92554ZM110.07 13.7586C108.878 13.7586 108.032 12.8905 108.032 11.461C108.032 10.1013 108.878 9.20569 110.071 9.20569C111.263 9.20569 112.101 10.0995 112.101 11.459C112.101 12.8887 111.263 13.7586 110.07 13.7586Z' fill='%23373F45'/%3E%3Cpath d='M118.06 7.94098C119.491 7.94098 120.978 8.33337 120.978 11.1366V14.893H120.063C119.608 14.893 119.238 14.524 119.238 14.0689V10.9965C119.238 9.66506 118.747 9.16047 117.891 9.16047C117.414 9.16047 116.797 9.52486 116.502 9.81915V14.069C116.502 14.1773 116.481 14.2845 116.44 14.3845C116.398 14.4845 116.337 14.5753 116.261 14.6519C116.184 14.7284 116.093 14.7891 115.993 14.8305C115.893 14.8719 115.786 14.8931 115.678 14.8931H114.847V8.10918H115.773C115.932 8.10914 116.087 8.16315 116.212 8.26242C116.337 8.36168 116.424 8.50033 116.46 8.65577C116.881 8.19328 117.428 7.94098 118.06 7.94098ZM122.854 8.09713C123.024 8.09708 123.19 8.1496 123.329 8.2475C123.468 8.34541 123.574 8.48391 123.631 8.64405L125.133 12.8486L126.635 8.64415C126.692 8.48402 126.798 8.34551 126.937 8.2476C127.076 8.1497 127.242 8.09718 127.412 8.09724H128.598L126.152 14.3567C126.091 14.5112 125.986 14.6439 125.849 14.7374C125.711 14.831 125.549 14.881 125.383 14.8809H124.333L121.668 8.09713H122.854Z' fill='%23373F45'/%3E%3Cpath d='M135.085 14.5514C134.566 14.7616 133.513 15.0416 132.418 15.0416C130.496 15.0416 129.024 13.9345 129.024 11.4396C129.024 9.19701 130.451 7.99792 132.191 7.99792C134.338 7.99792 135.254 9.4378 135.158 11.3979C135.139 11.8029 134.786 12.0983 134.38 12.0983H130.679C130.763 13.1916 131.562 13.7662 132.615 13.7662C133.028 13.7662 133.462 13.7452 133.983 13.6481C134.535 13.545 135.085 13.9375 135.085 14.4985V14.5514ZM133.673 10.949C133.785 9.87621 133.061 9.28752 132.191 9.28752C131.321 9.28752 130.734 9.93979 130.679 10.9489L133.673 10.949Z' fill='%23373F45'/%3E%3Cpath d='M137.345 8.11122C137.497 8.11118 137.645 8.16229 137.765 8.25635C137.884 8.35041 137.969 8.48197 138.005 8.62993C138.566 8.20932 139.268 7.94303 139.759 7.94303C139.801 7.94303 140.068 7.94303 140.489 7.99913V8.7265C140.489 9.11748 140.15 9.4147 139.759 9.4147C139.31 9.4147 138.651 9.5829 138.131 9.8773V14.8951H136.462V8.11112L137.345 8.11122ZM156.6 14.0508V8.09104H155.769C155.314 8.09104 154.944 8.45999 154.944 8.9151V14.8748H155.775C156.23 14.8748 156.6 14.5058 156.6 14.0508ZM158.857 12.9447V9.34254H157.749V8.91912C157.749 8.46401 158.118 8.09506 158.574 8.09506H158.857V6.56739L160.499 6.10479V8.09506H161.986V8.51848C161.986 8.97359 161.617 9.34254 161.161 9.34254H160.499V12.7345C160.499 13.2566 160.795 13.44 161.177 13.503C161.626 13.5774 162 13.9024 162 14.3574V14.977C160.446 14.977 158.857 14.9086 158.857 12.9447ZM98.1929 10.1124C98.2033 6.94046 100.598 5.16809 102.895 5.16809C104.171 5.16809 105.342 5.44285 106.304 6.12953L105.914 6.6631C105.654 7.02011 105.16 7.16194 104.749 6.99949C104.169 6.7702 103.622 6.7218 103.215 6.7218C101.335 6.7218 99.9169 7.92849 99.9068 10.1123C99.9169 12.2959 101.335 13.5201 103.215 13.5201C103.622 13.5201 104.169 13.4717 104.749 13.2424C105.16 13.0799 105.654 13.2046 105.914 13.5615L106.304 14.0952C105.342 14.7819 104.171 15.0566 102.895 15.0566C100.598 15.0566 98.2033 13.2842 98.1929 10.1124ZM147.619 5.21768C148.074 5.21768 148.444 5.58663 148.444 6.04174V9.81968L151.82 5.58131C151.897 5.47733 151.997 5.39282 152.112 5.3346C152.227 5.27638 152.355 5.24607 152.484 5.24611H153.984L150.166 10.0615L153.984 14.8749H152.484C152.355 14.8749 152.227 14.8446 152.112 14.7864C151.997 14.7281 151.897 14.6436 151.82 14.5397L148.444 10.3025V14.0508C148.444 14.5059 148.074 14.8749 147.619 14.8749H146.746V5.21768H147.619Z' fill='%23373F45'/%3E%3Cpath d='M0.773438 6.5752H2.68066C3.56543 6.5752 4.2041 6.7041 4.59668 6.96191C4.99219 7.21973 5.18994 7.62695 5.18994 8.18359C5.18994 8.55859 5.09326 8.87061 4.8999 9.11963C4.70654 9.36865 4.42822 9.52539 4.06494 9.58984V9.63379C4.51611 9.71875 4.84717 9.88721 5.05811 10.1392C5.27197 10.3882 5.37891 10.7266 5.37891 11.1543C5.37891 11.7314 5.17676 12.1841 4.77246 12.5122C4.37109 12.8374 3.81152 13 3.09375 13H0.773438V6.5752ZM1.82373 9.22949H2.83447C3.27393 9.22949 3.59473 9.16064 3.79688 9.02295C3.99902 8.88232 4.1001 8.64502 4.1001 8.31104C4.1001 8.00928 3.99023 7.79102 3.77051 7.65625C3.55371 7.52148 3.20801 7.4541 2.7334 7.4541H1.82373V9.22949ZM1.82373 10.082V12.1167H2.93994C3.37939 12.1167 3.71045 12.0332 3.93311 11.8662C4.15869 11.6963 4.27148 11.4297 4.27148 11.0664C4.27148 10.7324 4.15723 10.4849 3.92871 10.3237C3.7002 10.1626 3.35303 10.082 2.88721 10.082H1.82373Z' fill='%23373F45'/%3E%3Cpath d='M13.011 6.5752V10.7324C13.011 11.207 12.9084 11.623 12.7034 11.9805C12.5012 12.335 12.2068 12.6089 11.8201 12.8022C11.4363 12.9927 10.9763 13.0879 10.4402 13.0879C9.6433 13.0879 9.02368 12.877 8.5813 12.4551C8.13892 12.0332 7.91772 11.4531 7.91772 10.7148V6.5752H8.9724V10.6401C8.9724 11.1704 9.09546 11.5615 9.34155 11.8135C9.58765 12.0654 9.96557 12.1914 10.4753 12.1914C11.4656 12.1914 11.9607 11.6714 11.9607 10.6313V6.5752H13.011Z' fill='%23373F45'/%3E%3Cpath d='M15.9146 13V6.5752H16.9649V13H15.9146Z' fill='%23373F45'/%3E%3Cpath d='M19.9255 13V6.5752H20.9758V12.0991H23.696V13H19.9255Z' fill='%23373F45'/%3E%3Cpath d='M28.2828 13H27.2325V7.47607H25.3428V6.5752H30.1724V7.47607H28.2828V13Z' fill='%23373F45'/%3E%3Cpath d='M41.9472 13H40.8046L39.7148 9.16796C39.6679 9.00097 39.6093 8.76074 39.539 8.44727C39.4687 8.13086 39.4262 7.91113 39.4116 7.78809C39.3823 7.97559 39.3339 8.21875 39.2665 8.51758C39.2021 8.81641 39.1479 9.03905 39.1039 9.18554L38.0405 13H36.8979L36.0673 9.7832L35.2236 6.5752H36.2958L37.2143 10.3193C37.3578 10.9199 37.4604 11.4502 37.5219 11.9102C37.5541 11.6611 37.6025 11.3828 37.6669 11.0752C37.7314 10.7676 37.79 10.5186 37.8427 10.3281L38.8886 6.5752H39.9301L41.0024 10.3457C41.1049 10.6943 41.2133 11.2158 41.3276 11.9102C41.3715 11.4912 41.477 10.958 41.644 10.3105L42.558 6.5752H43.6215L41.9472 13Z' fill='%23373F45'/%3E%3Cpath d='M45.7957 13V6.5752H46.846V13H45.7957Z' fill='%23373F45'/%3E%3Cpath d='M52.0258 13H50.9755V7.47607H49.0859V6.5752H53.9155V7.47607H52.0258V13Z' fill='%23373F45'/%3E%3Cpath d='M61.2312 13H60.1765V10.104H57.2146V13H56.1643V6.5752H57.2146V9.20312H60.1765V6.5752H61.2312V13Z' fill='%23373F45'/%3E%3C/svg%3E");}.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit:hover,.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit:focus{background-color:#ffffff;-webkit-transform:scale(1.025) perspective(1px);-ms-transform:scale(1.025) perspective(1px);transform:scale(1.025) perspective(1px);opacity:1;}.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit[data-variant="dark"],.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit[data-variant="light"]{background-color:transparent;border-color:transparent;width:166px;}.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit[data-variant="light"]{color:#ffffff;background-image:url("data:image/svg+xml;charset=utf8,%3Csvg width='162' height='20' viewBox='0 0 162 20' fill='none' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='M83.0561 15.2457C86.675 15.2457 89.4722 12.5154 89.4722 9.14749C89.4722 5.99211 86.8443 4.06563 85.1038 4.06563C82.6801 4.06563 80.7373 5.76407 80.4605 8.28551C80.4092 8.75244 80.0387 9.14403 79.5686 9.14069C78.7871 9.13509 77.6507 9.12841 76.9314 9.13092C76.6217 9.13199 76.3658 8.88106 76.381 8.57196C76.4895 6.38513 77.2218 4.3404 78.618 2.76974C80.1695 1.02445 82.4289 0 85.1038 0C89.5979 0 93.8406 4.07791 93.8406 9.14749C93.8406 14.7608 89.1832 19.3113 83.1517 19.3113C78.8502 19.3113 74.5179 16.5041 73.0053 12.5795C72.9999 12.565 72.9986 12.5492 73.0015 12.534C73.0218 12.4179 73.0617 12.3118 73.1011 12.2074C73.1583 12.0555 73.2143 11.907 73.2062 11.7359L73.18 11.1892C73.174 11.0569 73.2075 10.9258 73.2764 10.8127C73.3452 10.6995 73.4463 10.6094 73.5666 10.554L73.7852 10.4523C73.9077 10.3957 74.0148 10.3105 74.0976 10.204C74.1803 10.0974 74.2363 9.97252 74.2608 9.83983C74.3341 9.43894 74.6865 9.14749 75.0979 9.14749C75.7404 9.14749 76.299 9.57412 76.5088 10.1806C77.5188 13.1 79.1245 15.2457 83.0561 15.2457Z' fill='white'/%3E%3Cpath d='M155.758 6.91365C155.028 6.91365 154.804 6.47916 154.804 5.98857C154.804 5.46997 154.986 5.06348 155.758 5.06348C156.53 5.06348 156.712 5.46997 156.712 5.98857C156.712 6.47905 156.516 6.91365 155.758 6.91365ZM142.441 12.9304V9.32833L141.415 9.32323V8.90392C141.415 8.44719 141.786 8.07758 142.244 8.07986L142.441 8.08095V6.55306L144.082 6.09057V8.08073H145.569V8.50416C145.569 8.61242 145.548 8.71961 145.506 8.81961C145.465 8.91961 145.404 9.01047 145.328 9.08699C145.251 9.16351 145.16 9.2242 145.06 9.26559C144.96 9.30698 144.853 9.32826 144.745 9.32822H144.082V12.7201C144.082 13.2423 144.378 13.4256 144.76 13.4887C145.209 13.5629 145.583 13.888 145.583 14.343V14.9626C144.029 14.9626 142.441 14.8942 142.441 12.9304Z' fill='white'/%3E%3Cpath d='M110.058 7.92554C108.417 7.88344 106.396 8.92062 106.396 11.5137C106.396 14.0646 108.417 15.0738 110.058 15.0318C111.742 15.0738 113.748 14.0646 113.748 11.5137C113.748 8.92062 111.742 7.88344 110.058 7.92554ZM110.07 13.7586C108.878 13.7586 108.032 12.8905 108.032 11.461C108.032 10.1013 108.878 9.20569 110.071 9.20569C111.263 9.20569 112.101 10.0995 112.101 11.459C112.101 12.8887 111.263 13.7586 110.07 13.7586Z' fill='white'/%3E%3Cpath d='M118.06 7.94098C119.491 7.94098 120.978 8.33337 120.978 11.1366V14.893H120.063C119.608 14.893 119.238 14.524 119.238 14.0689V10.9965C119.238 9.66506 118.747 9.16047 117.891 9.16047C117.414 9.16047 116.797 9.52486 116.502 9.81915V14.069C116.502 14.1773 116.481 14.2845 116.44 14.3845C116.398 14.4845 116.337 14.5753 116.261 14.6519C116.184 14.7284 116.093 14.7891 115.993 14.8305C115.893 14.8719 115.786 14.8931 115.678 14.8931H114.847V8.10918H115.773C115.932 8.10914 116.087 8.16315 116.212 8.26242C116.337 8.36168 116.424 8.50033 116.46 8.65577C116.881 8.19328 117.428 7.94098 118.06 7.94098ZM122.854 8.09713C123.024 8.09708 123.19 8.1496 123.329 8.2475C123.468 8.34541 123.574 8.48391 123.631 8.64405L125.133 12.8486L126.635 8.64415C126.692 8.48402 126.798 8.34551 126.937 8.2476C127.076 8.1497 127.242 8.09718 127.412 8.09724H128.598L126.152 14.3567C126.091 14.5112 125.986 14.6439 125.849 14.7374C125.711 14.831 125.549 14.881 125.383 14.8809H124.333L121.668 8.09713H122.854Z' fill='white'/%3E%3Cpath d='M135.085 14.5514C134.566 14.7616 133.513 15.0416 132.418 15.0416C130.496 15.0416 129.024 13.9345 129.024 11.4396C129.024 9.19701 130.451 7.99792 132.191 7.99792C134.338 7.99792 135.254 9.4378 135.158 11.3979C135.139 11.8029 134.786 12.0983 134.38 12.0983H130.679C130.763 13.1916 131.562 13.7662 132.615 13.7662C133.028 13.7662 133.462 13.7452 133.983 13.6481C134.535 13.545 135.085 13.9375 135.085 14.4985V14.5514ZM133.673 10.949C133.785 9.87621 133.061 9.28752 132.191 9.28752C131.321 9.28752 130.734 9.93979 130.679 10.9489L133.673 10.949Z' fill='white'/%3E%3Cpath d='M137.345 8.11122C137.497 8.11118 137.645 8.16229 137.765 8.25635C137.884 8.35041 137.969 8.48197 138.005 8.62993C138.566 8.20932 139.268 7.94303 139.759 7.94303C139.801 7.94303 140.068 7.94303 140.489 7.99913V8.7265C140.489 9.11748 140.15 9.4147 139.759 9.4147C139.31 9.4147 138.651 9.5829 138.131 9.8773V14.8951H136.462V8.11112L137.345 8.11122ZM156.6 14.0508V8.09104H155.769C155.314 8.09104 154.944 8.45999 154.944 8.9151V14.8748H155.775C156.23 14.8748 156.6 14.5058 156.6 14.0508ZM158.857 12.9447V9.34254H157.749V8.91912C157.749 8.46401 158.118 8.09506 158.574 8.09506H158.857V6.56739L160.499 6.10479V8.09506H161.986V8.51848C161.986 8.97359 161.617 9.34254 161.161 9.34254H160.499V12.7345C160.499 13.2566 160.795 13.44 161.177 13.503C161.626 13.5774 162 13.9024 162 14.3574V14.977C160.446 14.977 158.857 14.9086 158.857 12.9447ZM98.1929 10.1124C98.2033 6.94046 100.598 5.16809 102.895 5.16809C104.171 5.16809 105.342 5.44285 106.304 6.12953L105.914 6.6631C105.654 7.02011 105.16 7.16194 104.749 6.99949C104.169 6.7702 103.622 6.7218 103.215 6.7218C101.335 6.7218 99.9169 7.92849 99.9068 10.1123C99.9169 12.2959 101.335 13.5201 103.215 13.5201C103.622 13.5201 104.169 13.4717 104.749 13.2424C105.16 13.0799 105.654 13.2046 105.914 13.5615L106.304 14.0952C105.342 14.7819 104.171 15.0566 102.895 15.0566C100.598 15.0566 98.2033 13.2842 98.1929 10.1124ZM147.619 5.21768C148.074 5.21768 148.444 5.58663 148.444 6.04174V9.81968L151.82 5.58131C151.897 5.47733 151.997 5.39282 152.112 5.3346C152.227 5.27638 152.355 5.24607 152.484 5.24611H153.984L150.166 10.0615L153.984 14.8749H152.484C152.355 14.8749 152.227 14.8446 152.112 14.7864C151.997 14.7281 151.897 14.6436 151.82 14.5397L148.444 10.3025V14.0508C148.444 14.5059 148.074 14.8749 147.619 14.8749H146.746V5.21768H147.619Z' fill='white'/%3E%3Cpath d='M0.773438 6.5752H2.68066C3.56543 6.5752 4.2041 6.7041 4.59668 6.96191C4.99219 7.21973 5.18994 7.62695 5.18994 8.18359C5.18994 8.55859 5.09326 8.87061 4.8999 9.11963C4.70654 9.36865 4.42822 9.52539 4.06494 9.58984V9.63379C4.51611 9.71875 4.84717 9.88721 5.05811 10.1392C5.27197 10.3882 5.37891 10.7266 5.37891 11.1543C5.37891 11.7314 5.17676 12.1841 4.77246 12.5122C4.37109 12.8374 3.81152 13 3.09375 13H0.773438V6.5752ZM1.82373 9.22949H2.83447C3.27393 9.22949 3.59473 9.16064 3.79688 9.02295C3.99902 8.88232 4.1001 8.64502 4.1001 8.31104C4.1001 8.00928 3.99023 7.79102 3.77051 7.65625C3.55371 7.52148 3.20801 7.4541 2.7334 7.4541H1.82373V9.22949ZM1.82373 10.082V12.1167H2.93994C3.37939 12.1167 3.71045 12.0332 3.93311 11.8662C4.15869 11.6963 4.27148 11.4297 4.27148 11.0664C4.27148 10.7324 4.15723 10.4849 3.92871 10.3237C3.7002 10.1626 3.35303 10.082 2.88721 10.082H1.82373Z' fill='white'/%3E%3Cpath d='M13.011 6.5752V10.7324C13.011 11.207 12.9084 11.623 12.7034 11.9805C12.5012 12.335 12.2068 12.6089 11.8201 12.8022C11.4363 12.9927 10.9763 13.0879 10.4402 13.0879C9.6433 13.0879 9.02368 12.877 8.5813 12.4551C8.13892 12.0332 7.91772 11.4531 7.91772 10.7148V6.5752H8.9724V10.6401C8.9724 11.1704 9.09546 11.5615 9.34155 11.8135C9.58765 12.0654 9.96557 12.1914 10.4753 12.1914C11.4656 12.1914 11.9607 11.6714 11.9607 10.6313V6.5752H13.011Z' fill='white'/%3E%3Cpath d='M15.9146 13V6.5752H16.9649V13H15.9146Z' fill='white'/%3E%3Cpath d='M19.9255 13V6.5752H20.9758V12.0991H23.696V13H19.9255Z' fill='white'/%3E%3Cpath d='M28.2828 13H27.2325V7.47607H25.3428V6.5752H30.1724V7.47607H28.2828V13Z' fill='white'/%3E%3Cpath d='M41.9472 13H40.8046L39.7148 9.16796C39.6679 9.00097 39.6093 8.76074 39.539 8.44727C39.4687 8.13086 39.4262 7.91113 39.4116 7.78809C39.3823 7.97559 39.3339 8.21875 39.2665 8.51758C39.2021 8.81641 39.1479 9.03905 39.1039 9.18554L38.0405 13H36.8979L36.0673 9.7832L35.2236 6.5752H36.2958L37.2143 10.3193C37.3578 10.9199 37.4604 11.4502 37.5219 11.9102C37.5541 11.6611 37.6025 11.3828 37.6669 11.0752C37.7314 10.7676 37.79 10.5186 37.8427 10.3281L38.8886 6.5752H39.9301L41.0024 10.3457C41.1049 10.6943 41.2133 11.2158 41.3276 11.9102C41.3715 11.4912 41.477 10.958 41.644 10.3105L42.558 6.5752H43.6215L41.9472 13Z' fill='white'/%3E%3Cpath d='M45.7957 13V6.5752H46.846V13H45.7957Z' fill='white'/%3E%3Cpath d='M52.0258 13H50.9755V7.47607H49.0859V6.5752H53.9155V7.47607H52.0258V13Z' fill='white'/%3E%3Cpath d='M61.2312 13H60.1765V10.104H57.2146V13H56.1643V6.5752H57.2146V9.20312H60.1765V6.5752H61.2312V13Z' fill='white'/%3E%3C/svg%3E");}@-webkit-keyframes formkit-bouncedelay-formkit-form-data-uid-e309c832a6-{0%,80%,100%{-webkit-transform:scale(0);-ms-transform:scale(0);transform:scale(0);}40%{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}@keyframes formkit-bouncedelay-formkit-form-data-uid-e309c832a6-{0%,80%,100%{-webkit-transform:scale(0);-ms-transform:scale(0);transform:scale(0);}40%{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}.formkit-form[data-uid="e309c832a6"] blockquote{padding:10px 20px;margin:0 0 20px;border-left:5px solid #e1e1e1;}.formkit-form[data-uid="e309c832a6"] .seva-custom-content{padding:15px;font-size:16px;color:#fff;mix-blend-mode:difference;}.formkit-form[data-uid="e309c832a6"] .formkit-modal.guard{max-width:420px;width:100%;} .formkit-form[data-uid="e309c832a6"]{border:1px solid #e3e3e3;max-width:700px;position:relative;overflow:hidden;}.formkit-form[data-uid="e309c832a6"] .formkit-background{width:100%;height:100%;position:absolute;top:0;left:0;background-size:cover;background-position:center;opacity:0.3;}.formkit-form[data-uid="e309c832a6"] [data-style="minimal"]{padding:20px;width:100%;position:relative;}.formkit-form[data-uid="e309c832a6"] .formkit-header{margin:0 0 27px 0;text-align:center;}.formkit-form[data-uid="e309c832a6"] .formkit-subheader{margin:18px 0;text-align:center;}.formkit-form[data-uid="e309c832a6"] .formkit-guarantee{font-size:13px;margin:10px 0 15px 0;text-align:center;}.formkit-form[data-uid="e309c832a6"] .formkit-guarantee > p{margin:0;}.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit-container{margin-bottom:0;}.formkit-form[data-uid="e309c832a6"] .formkit-fields{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;margin:25px auto 0 auto;}.formkit-form[data-uid="e309c832a6"] .formkit-field{min-width:220px;}.formkit-form[data-uid="e309c832a6"] .formkit-field,.formkit-form[data-uid="e309c832a6"] .formkit-submit{margin:0 0 15px 0;-webkit-flex:1 0 100%;-ms-flex:1 0 100%;flex:1 0 100%;}.formkit-form[data-uid="e309c832a6"][min-width~="600"] [data-style="minimal"]{padding:40px;}.formkit-form[data-uid="e309c832a6"][min-width~="600"] .formkit-fields[data-stacked="false"]{margin-left:-5px;margin-right:-5px;}.formkit-form[data-uid="e309c832a6"][min-width~="600"] .formkit-fields[data-stacked="false"] .formkit-field,.formkit-form[data-uid="e309c832a6"][min-width~="600"] .formkit-fields[data-stacked="false"] .formkit-submit{margin:0 5px 15px 5px;}.formkit-form[data-uid="e309c832a6"][min-width~="600"] .formkit-fields[data-stacked="false"] .formkit-field{-webkit-flex:100 1 auto;-ms-flex:100 1 auto;flex:100 1 auto;}.formkit-form[data-uid="e309c832a6"][min-width~="600"] .formkit-fields[data-stacked="false"] .formkit-submit{-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;} </style></form>

		<div class="sponsor">
        <div class="sponsor-avatar"></div><p class="sponsor-bio"><em>Did you find this article helpful?</em></p><div class="sponsor-custom"><script type="text/javascript" src="https://cdnjs.buymeacoffee.com/1.0.0/button.prod.min.js" data-name="bmc-button" data-slug="reachsumit" data-color="#FFDD00" data-emoji=""  data-font="Cookie" data-text="Buy me a coffee" data-outline-color="#000000" data-font-color="#000000" data-coffee-color="#ffffff" ></script></div></div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2023-12-26</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share"><button title="Share on Twitter" data-sharer="twitter" data-url="https://blog.reachsumit.com/posts/2023/12/towards-ranking-aware-llms/" data-title="Strategies for Effective and Efficient Text Ranking Using Large Language Models" data-via="_reachsumit" data-hashtags="literature review,retrieval,ranking,LLM"><span class="fab fa-twitter fa-fw"></span></button><button title="Share on Facebook" data-sharer="facebook" data-url="https://blog.reachsumit.com/posts/2023/12/towards-ranking-aware-llms/" data-hashtag="literature review"><span class="fab fa-facebook-square fa-fw"></span></button><button title="Share on Linkedin" data-sharer="linkedin" data-url="https://blog.reachsumit.com/posts/2023/12/towards-ranking-aware-llms/"><span class="fab fa-linkedin fa-fw"></span></button><button title="Share on WhatsApp" data-sharer="whatsapp" data-url="https://blog.reachsumit.com/posts/2023/12/towards-ranking-aware-llms/" data-title="Strategies for Effective and Efficient Text Ranking Using Large Language Models" data-web><span class="fab fa-whatsapp fa-fw"></span></button><button title="Share on Hacker News" data-sharer="hackernews" data-url="https://blog.reachsumit.com/posts/2023/12/towards-ranking-aware-llms/" data-title="Strategies for Effective and Efficient Text Ranking Using Large Language Models"><span class="fab fa-hacker-news fa-fw"></span></button><button title="Share on Reddit" data-sharer="reddit" data-url="https://blog.reachsumit.com/posts/2023/12/towards-ranking-aware-llms/"><span class="fab fa-reddit fa-fw"></span></button><button title="Share on Line" data-sharer="line" data-url="https://blog.reachsumit.com/posts/2023/12/towards-ranking-aware-llms/" data-title="Strategies for Effective and Efficient Text Ranking Using Large Language Models"><span data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@v5.21.1/icons/line.svg"></span></button><button title="Share on Pocket" data-sharer="pocket" data-url="https://blog.reachsumit.com/posts/2023/12/towards-ranking-aware-llms/"><span class="fab fa-get-pocket fa-fw"></span></button><button title="Share on 微博" data-sharer="weibo" data-url="https://blog.reachsumit.com/posts/2023/12/towards-ranking-aware-llms/" data-title="Strategies for Effective and Efficient Text Ranking Using Large Language Models" data-image="featured-image.webp"><span class="fab fa-weibo fa-fw"></span></button><button title="Share on Evernote" data-sharer="evernote" data-url="https://blog.reachsumit.com/posts/2023/12/towards-ranking-aware-llms/" data-title="Strategies for Effective and Efficient Text Ranking Using Large Language Models"><span class="fab fa-evernote fa-fw"></span></button><button title="Share on Trello" data-sharer="trello" data-url="https://blog.reachsumit.com/posts/2023/12/towards-ranking-aware-llms/" data-title="Strategies for Effective and Efficient Text Ranking Using Large Language Models" data-description=""><span class="fab fa-trello fa-fw"></span></button></div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw"></i>&nbsp;<a href="/tags/literature-review/">literature review</a>,&nbsp;<a href="/tags/retrieval/">retrieval</a>,&nbsp;<a href="/tags/ranking/">ranking</a>,&nbsp;<a href="/tags/llm/">LLM</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/2023/12/prompting-llm-for-ranking/" class="prev" rel="prev" title="Prompting-based Methods for Text Ranking Using Large Language Models"><i class="fas fa-angle-left fa-fw"></i>Prompting-based Methods for Text Ranking Using Large Language Models</a>
            <a href="/posts/2024/01/user-behavior-modeling-recsys/" class="next" rel="next" title="A Guide to User Behavior Modeling">A Guide to User Behavior Modeling<i class="fas fa-angle-right fa-fw"></i></a></div>
</div>
<div id="comments"><div id="gitalk" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://github.com/gitalk/gitalk"></a>Gitalk</a>.
            </noscript></div></article></div>
        </main><footer class="footer">
        <div class="footer-container"><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2020 - 2024</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="https://reachsumit.com" target="_blank" rel="noopener noreferrer">Sumit Kumar</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
            <div class="footer-line"></div>
            <div class="footer-line">
            </div>
        </div></footer></div>

    <div id="fixed-buttons"><a href="#back-to-top" id="back-to-top-button" class="fixed-button" title="Back to Top">
            <i class="fas fa-arrow-up fa-fw"></i>
        </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
            <i class="fas fa-comment fa-fw"></i>
        </a>
    </div><div class="assets"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><link rel="preload" as="style" onload="this.onload=null;this.rel='stylesheet'" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/copy-tex.min.css">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/copy-tex.min.css"></noscript><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":10},"comment":{"gitalk":{"admin":["reachsumit"],"clientID":"e13962a172516867862a","clientSecret":"43e82fd70da96d006eec6c9bee0a861aaa13ee89","id":"2023-12-26T00:00:00Z","owner":"reachsumit","repo":"reachsumit-blog-gitalk","title":"Strategies for Effective and Efficient Text Ranking Using Large Language Models"}},"data":{"desktop-header-typeit":"Sumit's Diary","mobile-header-typeit":"Sumit's Diary"},"lightGallery":{"actualSize":false,"exThumbImage":"data-thumbnail","hideBarsDelay":2000,"selector":".lightgallery","speed":400,"thumbContHeight":80,"thumbWidth":80,"thumbnail":true},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"LV11CUNTAX","algoliaIndex":"blog_reachsumit","algoliaSearchKey":"98d868016771f8a06b967e7eb3eaf63a","highlightTag":"em","maxResultLength":10,"noResultsFound":"No results found","snippetLength":30,"type":"algolia"},"sharerjs":true,"table":{"sort":true},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"desktop-header-typeit":["desktop-header-typeit"],"mobile-header-typeit":["mobile-header-typeit"]},"duration":2700,"speed":100}};</script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/tablesort@5.3.0/src/tablesort.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/js/lightgallery.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lg-thumbnail.js@1.2.0/dist/lg-thumbnail.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lg-zoom.js@1.3.0/dist/lg-zoom.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.4.2/sharer.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/typeit@7.0.4/dist/typeit.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/copy-tex.min.js" defer></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script type="text/javascript" src="/js/katex.min.js" defer></script><script type="text/javascript" src="/js/theme.min.js" defer></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script><script type="text/javascript" src="/js/gitalk.min.js" defer></script><script type="text/javascript">
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'G-TGH87J92Z3');
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=G-TGH87J92Z3" async></script></div>
</body>

</html>