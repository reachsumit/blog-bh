<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="noodp" /><title> Teaching Models to Decide When to Retrieve: Adaptive RAG, Part 4 - Sumit&#39;s Diary</title><meta name="Description" content="Welcome to Sumit Kumar&#39;s Personal Blog!"><meta property="og:url" content="https://blog.reachsumit.com/posts/2025/10/learning-to-retrieve/">
  <meta property="og:site_name" content="Sumit&#39;s Diary">
  <meta property="og:title" content=" Teaching Models to Decide When to Retrieve: Adaptive RAG, Part 4">
  <meta property="og:description" content="This final post of the Adaptive RAG series explores methods that treat adaptive retrieval as a learned skill and explicitly teach models when to retrieve. We examine three paradigms in increasing order of sophistication.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-10-05T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-10-05T00:00:00+00:00">
    <meta property="article:tag" content="Retrieval">
    <meta property="article:tag" content="Rag">
    <meta property="article:tag" content="Adaptive-Rag">
    <meta property="og:image" content="https://blog.reachsumit.com/posts/2025/10/learning-to-retrieve/featured-image-preview.webp">
      <meta property="og:see_also" content="https://blog.reachsumit.com/posts/2025/09/problems-with-naive-rag/">
      <meta property="og:see_also" content="https://blog.reachsumit.com/posts/2025/09/deciding-when-not-to-retrieve/">
      <meta property="og:see_also" content="https://blog.reachsumit.com/posts/2025/09/probing-llms-knowledge-boundary/">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://blog.reachsumit.com/posts/2025/10/learning-to-retrieve/featured-image-preview.webp">
  <meta name="twitter:title" content=" Teaching Models to Decide When to Retrieve: Adaptive RAG, Part 4">
  <meta name="twitter:description" content="This final post of the Adaptive RAG series explores methods that treat adaptive retrieval as a learned skill and explicitly teach models when to retrieve. We examine three paradigms in increasing order of sophistication.">
      <meta name="twitter:site" content="@_reachsumit">
<meta name="application-name" content="Sumit&#39;s Diary">
<meta name="apple-mobile-web-app-title" content="Sumit&#39;s Diary">

<meta name="theme-color" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><meta name="twitter:creator" content="@_reachsumit" /><link rel="icon" href="/img/avatar/favicon.ico"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">

<link rel="canonical" href="https://blog.reachsumit.com/posts/2025/10/learning-to-retrieve/" /><link rel="prev" href="https://blog.reachsumit.com/posts/2025/09/probing-llms-knowledge-boundary/" />
<link rel="stylesheet" href="/css/main.min.css"><link rel="stylesheet" href="/css/style.min.css"><script type="application/ld+json">{"@context": "https://schema.org","@type": "BlogPosting",
        "headline": " Teaching Models to Decide When to Retrieve: Adaptive RAG, Part 4",
        "inLanguage": "en",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https://blog.reachsumit.com/posts/2025/10/learning-to-retrieve/"
        },"image": [{
                            "@type": "ImageObject",
                            "url": "https://blog.reachsumit.com/posts/2025/10/learning-to-retrieve/featured-image.webp",
                            "width":  1200 ,
                            "height":  600 
                        },{
                            "@type": "ImageObject",
                            "url": "https://blog.reachsumit.com/posts/2025/10/learning-to-retrieve/featured-image-preview.webp",
                            "width":  1000 ,
                            "height":  300 
                        }],"genre": "posts","keywords":["retrieval","rag","adaptive-rag"],"wordcount":  7573 ,
        "url": "https://blog.reachsumit.com/posts/2025/10/learning-to-retrieve/","datePublished": "2025-10-05T00:00:00+00:00","dateModified": "2025-10-05T00:00:00+00:00","publisher": {
            "@type": "Organization",
            "name": "xxxx","logo": "https://blog.reachsumit.com/images/avatar.png"},"author": {
                "@type": "Person",
                "name": "Sumit Kumar",
                "url": "https://reachsumit.com"
            },"description": ""
    }</script></head>


<body data-instant-intensity="viewport" class="tw-flex tw-min-h-screen tw-flex-col"><script>
    function setTheme(theme) {
      document.body.setAttribute('theme', theme); 
      document.documentElement.className = theme;
      document.documentElement.style.setProperty('color-scheme', theme === 'light' ? 'light' : 'dark');
      if (theme === 'light') {
        document.documentElement.classList.remove('tw-dark')
      } else {
        document.documentElement.classList.add('tw-dark')
      }
      window.theme = theme;   
      window.isDark = window.theme !== 'light' 
    }
    function saveTheme(theme) {window.localStorage && localStorage.setItem('theme', theme);}
    function getMeta(metaName) {const metas = document.getElementsByTagName('meta'); for (let i = 0; i < metas.length; i++) if (metas[i].getAttribute('name') === metaName) return metas[i]; return '';}
    if (window.localStorage && localStorage.getItem('theme')) {
        let theme = localStorage.getItem('theme');
        if (theme === 'light' || theme === 'dark') {
        setTheme(theme);
        } else {
            if ((window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
                setTheme('dark');
            } else {
                setTheme('light');
            }
        }
      } else { 
        if ('light' === 'light' || 'light' === 'dark') 
            setTheme('light'), saveTheme('light'); 
        else saveTheme('auto'), window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches ? setTheme('dark') : setTheme('light');
    }
    let metaColors = {'light': '#f8f8f8','dark': '#161b22'}
    getMeta('theme-color').content = metaColors[document.body.getAttribute('theme')];
    window.switchThemeEventSet = new Set()
</script><div id="back-to-top"></div>
    <div id="mask"></div><header class="desktop print:!tw-hidden" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Sumit&#39;s Diary"><span id="desktop-header-typeit" class="typeit"></span></a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item"
                    href="/posts/" > Posts </a><a class="menu-item"
                    href="/tags/" > Tags </a><a class="menu-item"
                    href="/categories/" > Categories </a><a class="menu-item"
                    href="/about/" > About </a><a class="menu-item"
                    href="/series/" > Series </a><a class="menu-item"
                    href="/newsletter/" > Newsletter(s) </a><a class="menu-item"
                    href="/talks/" > Talks </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                    <input type="text"
                        placeholder="Search this blog"
                        id="search-input-desktop">
                    <button class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                        <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
                    </button>
                    <button class="search-button search-clear" id="search-clear-desktop" title="Clear">
                        <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm121.6 313.1c4.7 4.7 4.7 12.3 0 17L338 377.6c-4.7 4.7-12.3 4.7-17 0L256 312l-65.1 65.6c-4.7 4.7-12.3 4.7-17 0L134.4 338c-4.7-4.7-4.7-12.3 0-17l65.6-65-65.6-65.1c-4.7-4.7-4.7-12.3 0-17l39.6-39.6c4.7-4.7 12.3-4.7 17 0l65 65.7 65.1-65.6c4.7-4.7 12.3-4.7 17 0l39.6 39.6c4.7 4.7 4.7 12.3 0 17L312 256l65.6 65.1z"/></svg>
                    </button>
                    <span class="search-button search-loading tw-animate-spin" id="search-loading-desktop">
                        <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M304 48c0 26.51-21.49 48-48 48s-48-21.49-48-48 21.49-48 48-48 48 21.49 48 48zm-48 368c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zm208-208c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zM96 256c0-26.51-21.49-48-48-48S0 229.49 0 256s21.49 48 48 48 48-21.49 48-48zm12.922 99.078c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48c0-26.509-21.491-48-48-48zm294.156 0c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48c0-26.509-21.49-48-48-48zM108.922 60.922c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.491-48-48-48z"/></svg>
                    </span>
                </span><button class="menu-item theme-select" aria-label="Switch Theme">
                    <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M8 256c0 136.966 111.033 248 248 248s248-111.034 248-248S392.966 8 256 8 8 119.033 8 256zm248 184V72c101.705 0 184 82.311 184 184 0 101.705-82.311 184-184 184z"/></svg>
                    <select class="color-theme-select" id="theme-select-desktop" aria-label="Switch Theme">
                        <option value="light">Light</option>
                        <option value="dark">Dark</option>
                        <option value="auto">Auto</option>
                    </select>
                </button></div>
        </div>
    </div>
</header><header class="mobile print:!tw-hidden" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Sumit&#39;s Diary"><span id="mobile-header-typeit" class="typeit"></span></a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                <div class="search mobile" id="search-mobile">
                    <input type="text"
                        placeholder="Search this blog"
                        id="search-input-mobile">
                    <button class="search-button search-toggle tw-h-10" id="search-toggle-mobile" title="Search">
                        <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
                    </button>
                    <button class="search-button search-clear tw-h-fit" id="search-clear-mobile" title="Clear">
                        <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm121.6 313.1c4.7 4.7 4.7 12.3 0 17L338 377.6c-4.7 4.7-12.3 4.7-17 0L256 312l-65.1 65.6c-4.7 4.7-12.3 4.7-17 0L134.4 338c-4.7-4.7-4.7-12.3 0-17l65.6-65-65.6-65.1c-4.7-4.7-4.7-12.3 0-17l39.6-39.6c4.7-4.7 12.3-4.7 17 0l65 65.7 65.1-65.6c4.7-4.7 12.3-4.7 17 0l39.6 39.6c4.7 4.7 4.7 12.3 0 17L312 256l65.6 65.1z"/></svg>
                    </button>
                    <span class="search-button search-loading tw-animate-spin" id="search-loading-mobile">
                        <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M304 48c0 26.51-21.49 48-48 48s-48-21.49-48-48 21.49-48 48-48 48 21.49 48 48zm-48 368c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zm208-208c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zM96 256c0-26.51-21.49-48-48-48S0 229.49 0 256s21.49 48 48 48 48-21.49 48-48zm12.922 99.078c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48c0-26.509-21.491-48-48-48zm294.156 0c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48c0-26.509-21.49-48-48-48zM108.922 60.922c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.491-48-48-48z"/></svg>
                    </span>
                </div>
                <button class="search-cancel" id="search-cancel-mobile">
                    Cancel
                </button>
            </div><a class="menu-item" href="/posts/" title="" >Posts</a><a class="menu-item" href="/tags/" title="" >Tags</a><a class="menu-item" href="/categories/" title="" >Categories</a><a class="menu-item" href="/about/" title="" >About</a><a class="menu-item" href="/series/" title="" >Series</a><a class="menu-item" href="/newsletter/" title="" >Newsletter(s)</a><a class="menu-item" href="/talks/" title="" >Talks</a><button class="menu-item theme-select tw-w-full" aria-label="Switch Theme">
                <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M8 256c0 136.966 111.033 248 248 248s248-111.034 248-248S392.966 8 256 8 8 119.033 8 256zm248 184V72c101.705 0 184 82.311 184 184 0 101.705-82.311 184-184 184z"/></svg>
                <select class="color-theme-select" id="theme-select-mobile" aria-label="Switch Theme">
                    <option value="light">Light</option>
                    <option value="dark">Dark</option>
                    <option value="auto">Auto</option>
                </select>
            </button></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div><main class="tw-mx-4 tw-flex-1"><div class="toc print:!tw-hidden" id="toc-auto">
        <h2 class="toc-title">Contents</h2>
        <div class="toc-content" id="toc-content-auto"><nav id="TableOfContents">
  <ul>
    <li><a href="#paradigm-1-training-separate-gatekeeper-models">Paradigm 1: Training Separate &ldquo;Gatekeeper&rdquo; Models</a>
      <ul>
        <li><a href="#using-a-query-complexity-classifier">Using a Query Complexity Classifier</a></li>
        <li><a href="#using-multiple-classifiers-to-determine-the-need-for-retrieval">Using Multiple Classifiers to Determine the Need for Retrieval</a></li>
        <li><a href="#using-multiple-classifiers-to-filter-the-retrieved-context">Using Multiple Classifiers to Filter the Retrieved Context</a></li>
        <li><a href="#using-an-encoder-based-classifier">Using an Encoder-Based Classifier</a></li>
        <li><a href="#training-a-lightweight-knowledge-boundary-classifier">Training a Lightweight Knowledge Boundary Classifier</a></li>
        <li><a href="#combining-proactive-and-reactive-classifier-strategies">Combining Proactive and Reactive Classifier Strategies</a></li>
        <li><a href="#a-bandit-approach-to-extend-since-class-supervision-methods">A Bandit Approach to Extend Since-Class Supervision Methods</a></li>
        <li><a href="#use-a-smaller-model-from-the-same-family-to-predict-the-knowledge-gaps">Use a Smaller Model From the Same Family to Predict the Knowledge Gaps</a></li>
      </ul>
    </li>
    <li><a href="#paradigm-2-teaching-the-main-llm">Paradigm 2: Teaching the Main LLM</a>
      <ul>
        <li><a href="#train-language-model-to-introspect-its-output-and-request-retrieval-when-required">Train Language Model to Introspect Its Output and Request Retrieval When Required</a></li>
        <li><a href="#simplifying-self-rag">Simplifying SELF-RAG</a></li>
        <li><a href="#multi-turn-conversational-qa-extension-for-self-rag">Multi-Turn Conversational QA Extension for Self-RAG</a></li>
        <li><a href="#multimodal-extension-for-self-rag-framework">Multimodal Extension for SELF-RAG Framework</a></li>
        <li><a href="#fine-tuning-the-llm-to-predict-information-needs">Fine-Tuning the LLM to Predict Information Needs</a></li>
        <li><a href="#parameter-efficient-fine-tuning-for-the-llm-to-decide-when-to-retrieve">Parameter-Efficient Fine-Tuning for the LLM to Decide When to Retrieve</a></li>
      </ul>
    </li>
    <li><a href="#paradigm-3-teaching-reasoning-about-retrieval">Paradigm 3: Teaching Reasoning About Retrieval</a>
      <ul>
        <li><a href="#teaching-llms-to-leverage-reasoning-for-retrieval-as-a-dialogue">Teaching LLMs to Leverage Reasoning for Retrieval as a Dialogue</a></li>
        <li><a href="#calibrate-llms-to-make-atomic-decisions">Calibrate LLMs to Make Atomic Decisions</a></li>
        <li><a href="#incorporate-knowledge-verbalization-when-retrieval-is-skipped">Incorporate Knowledge Verbalization When Retrieval Is Skipped</a></li>
      </ul>
    </li>
    <li><a href="#summary">Summary</a></li>
    <li><a href="#series-conclusion-the-path-from-naive-to-autonomous-rag">Series Conclusion: The Path From Naive to Autonomous RAG</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav></div>
    </div><dialog id="toc-dialog" class="tw-max-w-full tw-w-full tw-max-h-full tw-h-full tw-ml-16">
        <div class="toc tw-mx-4 tw-max-w-full">
            <h2 class="tw-mx-0 tw-my-6 tw-uppercase tw-text-2xl">Contents</h2>
            <div class="toc-content"><nav id="TableOfContents">
  <ul>
    <li><a href="#paradigm-1-training-separate-gatekeeper-models">Paradigm 1: Training Separate &ldquo;Gatekeeper&rdquo; Models</a>
      <ul>
        <li><a href="#using-a-query-complexity-classifier">Using a Query Complexity Classifier</a></li>
        <li><a href="#using-multiple-classifiers-to-determine-the-need-for-retrieval">Using Multiple Classifiers to Determine the Need for Retrieval</a></li>
        <li><a href="#using-multiple-classifiers-to-filter-the-retrieved-context">Using Multiple Classifiers to Filter the Retrieved Context</a></li>
        <li><a href="#using-an-encoder-based-classifier">Using an Encoder-Based Classifier</a></li>
        <li><a href="#training-a-lightweight-knowledge-boundary-classifier">Training a Lightweight Knowledge Boundary Classifier</a></li>
        <li><a href="#combining-proactive-and-reactive-classifier-strategies">Combining Proactive and Reactive Classifier Strategies</a></li>
        <li><a href="#a-bandit-approach-to-extend-since-class-supervision-methods">A Bandit Approach to Extend Since-Class Supervision Methods</a></li>
        <li><a href="#use-a-smaller-model-from-the-same-family-to-predict-the-knowledge-gaps">Use a Smaller Model From the Same Family to Predict the Knowledge Gaps</a></li>
      </ul>
    </li>
    <li><a href="#paradigm-2-teaching-the-main-llm">Paradigm 2: Teaching the Main LLM</a>
      <ul>
        <li><a href="#train-language-model-to-introspect-its-output-and-request-retrieval-when-required">Train Language Model to Introspect Its Output and Request Retrieval When Required</a></li>
        <li><a href="#simplifying-self-rag">Simplifying SELF-RAG</a></li>
        <li><a href="#multi-turn-conversational-qa-extension-for-self-rag">Multi-Turn Conversational QA Extension for Self-RAG</a></li>
        <li><a href="#multimodal-extension-for-self-rag-framework">Multimodal Extension for SELF-RAG Framework</a></li>
        <li><a href="#fine-tuning-the-llm-to-predict-information-needs">Fine-Tuning the LLM to Predict Information Needs</a></li>
        <li><a href="#parameter-efficient-fine-tuning-for-the-llm-to-decide-when-to-retrieve">Parameter-Efficient Fine-Tuning for the LLM to Decide When to Retrieve</a></li>
      </ul>
    </li>
    <li><a href="#paradigm-3-teaching-reasoning-about-retrieval">Paradigm 3: Teaching Reasoning About Retrieval</a>
      <ul>
        <li><a href="#teaching-llms-to-leverage-reasoning-for-retrieval-as-a-dialogue">Teaching LLMs to Leverage Reasoning for Retrieval as a Dialogue</a></li>
        <li><a href="#calibrate-llms-to-make-atomic-decisions">Calibrate LLMs to Make Atomic Decisions</a></li>
        <li><a href="#incorporate-knowledge-verbalization-when-retrieval-is-skipped">Incorporate Knowledge Verbalization When Retrieval Is Skipped</a></li>
      </ul>
    </li>
    <li><a href="#summary">Summary</a></li>
    <li><a href="#series-conclusion-the-path-from-naive-to-autonomous-rag">Series Conclusion: The Path From Naive to Autonomous RAG</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav></div>
        </div>
    </dialog><script>document.getElementsByTagName("main")[0].setAttribute("autoTOC", "true")</script><article class="page single print:!tw-w-full print:!tw-max-w-none print:!tw-m-0 print:!tw-p-0"><h1 class="single-title" data-pagefind-meta="date:2025-10-05" data-pagefind-body> Teaching Models to Decide When to Retrieve: Adaptive RAG, Part 4</h1><div class="post-meta">
            <div class="post-meta-line">
                <span class="post-author"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm0 96c48.6 0 88 39.4 88 88s-39.4 88-88 88-88-39.4-88-88 39.4-88 88-88zm0 344c-58.7 0-111.3-26.6-146.5-68.2 18.8-35.4 55.6-59.8 98.5-59.8 2.4 0 4.8.4 7.1 1.1 13 4.2 26.6 6.9 40.9 6.9 14.3 0 28-2.7 40.9-6.9 2.3-.7 4.7-1.1 7.1-1.1 42.9 0 79.7 24.4 98.5 59.8C359.3 421.4 306.7 448 248 448z"/></svg><a href="https://reachsumit.com" title="Author" target="_blank" rel="noopener noreferrer author" class="author">Sumit Kumar</a>
                </span>&nbsp;<span class="post-category">included in </span>&nbsp;<span class="post-category">category <a href="/categories/information-retrieval/"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M464 128H272l-54.63-54.63c-6-6-14.14-9.37-22.63-9.37H48C21.49 64 0 85.49 0 112v288c0 26.51 21.49 48 48 48h416c26.51 0 48-21.49 48-48V176c0-26.51-21.49-48-48-48zm0 272H48V112h140.12l54.63 54.63c6 6 14.14 9.37 22.63 9.37H464v224z"/></svg>Information Retrieval</a></span>&nbsp;<span class="post-category">and</span>&nbsp;<span class="post-series">series <a href="/series/selective-retrieval/"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M464 32H48C21.49 32 0 53.49 0 80v352c0 26.51 21.49 48 48 48h416c26.51 0 48-21.49 48-48V80c0-26.51-21.49-48-48-48zm-6 400H54a6 6 0 0 1-6-6V86a6 6 0 0 1 6-6h404a6 6 0 0 1 6 6v340a6 6 0 0 1-6 6zm-42-92v24c0 6.627-5.373 12-12 12H204c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h200c6.627 0 12 5.373 12 12zm0-96v24c0 6.627-5.373 12-12 12H204c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h200c6.627 0 12 5.373 12 12zm0-96v24c0 6.627-5.373 12-12 12H204c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h200c6.627 0 12 5.373 12 12zm-252 12c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36zm0 96c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36zm0 96c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36z"/></svg>Adaptive RAG (Selective Retrieval)</a></span></div>
            <div class="post-meta-line"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M148 288h-40c-6.6 0-12-5.4-12-12v-40c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v40c0 6.6-5.4 12-12 12zm108-12v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm-96 96v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm-96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm192 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm96-260v352c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V112c0-26.5 21.5-48 48-48h48V12c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v52h128V12c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v52h48c26.5 0 48 21.5 48 48zm-48 346V160H48v298c0 3.3 2.7 6 6 6h340c3.3 0 6-2.7 6-6z"/></svg>&nbsp;<time datetime="2025-10-05">2025-10-05</time>&nbsp;<svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M402.3 344.9l32-32c5-5 13.7-1.5 13.7 5.7V464c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V112c0-26.5 21.5-48 48-48h273.5c7.1 0 10.7 8.6 5.7 13.7l-32 32c-1.5 1.5-3.5 2.3-5.7 2.3H48v352h352V350.5c0-2.1.8-4.1 2.3-5.6zm156.6-201.8L296.3 405.7l-90.4 10c-26.2 2.9-48.5-19.2-45.6-45.6l10-90.4L432.9 17.1c22.9-22.9 59.9-22.9 82.7 0l43.2 43.2c22.9 22.9 22.9 60 .1 82.8zM460.1 174L402 115.9 216.2 301.8l-7.3 65.3 65.3-7.3L460.1 174zm64.8-79.7l-43.2-43.2c-4.1-4.1-10.8-4.1-14.8 0L436 82l58.1 58.1 30.9-30.9c4-4.2 4-10.8-.1-14.9z"/></svg>&nbsp;<time datetime="2025-10-05">2025-10-05</time>&nbsp;<svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3 0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9 0l60.1 60.1c18.8 18.7 18.8 49.1 0 67.9zM284.2 99.8L21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3 0-17l-111-111c-4.8-4.7-12.4-4.7-17.1 0zM124.1 339.9c-5.5-5.5-5.5-14.3 0-19.8l154-154c5.5-5.5 14.3-5.5 19.8 0s5.5 14.3 0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8 0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"/></svg>&nbsp;7573 words&nbsp;
                    <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm0 448c-110.5 0-200-89.5-200-200S145.5 56 256 56s200 89.5 200 200-89.5 200-200 200zm61.8-104.4l-84.9-61.7c-3.1-2.3-4.9-5.9-4.9-9.7V116c0-6.6 5.4-12 12-12h32c6.6 0 12 5.4 12 12v141.7l66.8 48.6c5.4 3.9 6.5 11.4 2.6 16.8L334.6 349c-3.9 5.3-11.4 6.5-16.8 2.6z"/></svg>&nbsp;34 minutes&nbsp;</div>
        </div><div class="featured-image"><img  loading="eager" src='/posts/2025/10/learning-to-retrieve/featured-image.webp'    height="600" width="1200"></div><div class="details series-nav open">
                                <div class="details-summary series-title">
                                    <span>Series - Adaptive RAG (Selective Retrieval)</span>
                                    <span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
                                </div>
                                <div class="details-content series-content">
                                    <nav>
                                        <ul>
                                                    <li><a href="/posts/2025/09/problems-with-naive-rag/">The Hidden Costs of Naive Retrieval: Adaptive RAG, Part 1</a></li>
                                                    <li><a href="/posts/2025/09/deciding-when-not-to-retrieve/">Deciding When Not to Retrieve: Adaptive RAG, Part 2</a></li>
                                                    <li><a href="/posts/2025/09/probing-llms-knowledge-boundary/">Probing LLMs&#39; Knowledge Boundary: Adaptive RAG, Part 3</a></li><li><span class="active"> Teaching Models to Decide When to Retrieve: Adaptive RAG, Part 4</span></li></ul>
                                    </nav>
                                </div>
                            </div><div class="details toc print:!tw-block" id="toc-static"  kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#paradigm-1-training-separate-gatekeeper-models">Paradigm 1: Training Separate &ldquo;Gatekeeper&rdquo; Models</a>
      <ul>
        <li><a href="#using-a-query-complexity-classifier">Using a Query Complexity Classifier</a></li>
        <li><a href="#using-multiple-classifiers-to-determine-the-need-for-retrieval">Using Multiple Classifiers to Determine the Need for Retrieval</a></li>
        <li><a href="#using-multiple-classifiers-to-filter-the-retrieved-context">Using Multiple Classifiers to Filter the Retrieved Context</a></li>
        <li><a href="#using-an-encoder-based-classifier">Using an Encoder-Based Classifier</a></li>
        <li><a href="#training-a-lightweight-knowledge-boundary-classifier">Training a Lightweight Knowledge Boundary Classifier</a></li>
        <li><a href="#combining-proactive-and-reactive-classifier-strategies">Combining Proactive and Reactive Classifier Strategies</a></li>
        <li><a href="#a-bandit-approach-to-extend-since-class-supervision-methods">A Bandit Approach to Extend Since-Class Supervision Methods</a></li>
        <li><a href="#use-a-smaller-model-from-the-same-family-to-predict-the-knowledge-gaps">Use a Smaller Model From the Same Family to Predict the Knowledge Gaps</a></li>
      </ul>
    </li>
    <li><a href="#paradigm-2-teaching-the-main-llm">Paradigm 2: Teaching the Main LLM</a>
      <ul>
        <li><a href="#train-language-model-to-introspect-its-output-and-request-retrieval-when-required">Train Language Model to Introspect Its Output and Request Retrieval When Required</a></li>
        <li><a href="#simplifying-self-rag">Simplifying SELF-RAG</a></li>
        <li><a href="#multi-turn-conversational-qa-extension-for-self-rag">Multi-Turn Conversational QA Extension for Self-RAG</a></li>
        <li><a href="#multimodal-extension-for-self-rag-framework">Multimodal Extension for SELF-RAG Framework</a></li>
        <li><a href="#fine-tuning-the-llm-to-predict-information-needs">Fine-Tuning the LLM to Predict Information Needs</a></li>
        <li><a href="#parameter-efficient-fine-tuning-for-the-llm-to-decide-when-to-retrieve">Parameter-Efficient Fine-Tuning for the LLM to Decide When to Retrieve</a></li>
      </ul>
    </li>
    <li><a href="#paradigm-3-teaching-reasoning-about-retrieval">Paradigm 3: Teaching Reasoning About Retrieval</a>
      <ul>
        <li><a href="#teaching-llms-to-leverage-reasoning-for-retrieval-as-a-dialogue">Teaching LLMs to Leverage Reasoning for Retrieval as a Dialogue</a></li>
        <li><a href="#calibrate-llms-to-make-atomic-decisions">Calibrate LLMs to Make Atomic Decisions</a></li>
        <li><a href="#incorporate-knowledge-verbalization-when-retrieval-is-skipped">Incorporate Knowledge Verbalization When Retrieval Is Skipped</a></li>
      </ul>
    </li>
    <li><a href="#summary">Summary</a></li>
    <li><a href="#series-conclusion-the-path-from-naive-to-autonomous-rag">Series Conclusion: The Path From Naive to Autonomous RAG</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content" data-pagefind-body><p>In the previous posts of this series, we established that naive RAG is costly and often harmful. We then explored lightweight methods that decide whether to retrieve based on query characteristics alone, followed by more nuanced approaches that probe the LLM&rsquo;s confidence by analyzing its outputs and internal states. But all these approaches work with models as-is, either by analyzing inputs or observing output to infer when retrieval is needed.</p>
<p>This final post takes a fundamentally different approach and treats adaptive <strong>retrieval as a learned skill</strong>. Instead of passively observing the model, we actively train it to participate in the retrieval decision. We will examine three paradigms in increasing order of sophistication:</p>
<ol>
<li><strong>Training Separate &ldquo;Gatekeeper&rdquo; Models</strong>: Training lightweight models that act as intelligent routers, deciding whether to invoke retrieval before passing queries to the main LLM.</li>
<li><strong>Teaching the Main LLM</strong>: Fine-tuning approaches that give LLMs the ability to recognize their knowledge gaps and signal when they need external information.</li>
<li><strong>Teaching Reasoning About Retrieval</strong>: Advanced methods that train LLMs to engage in multi-step reasoning about what they know, what they need, and how to gather missing information iteratively.</li>
</ol>
<p>As you can guess, each of these paradigms represents a different point in the spectrum of training complexity, computational overhead, and decision-making sophistication.</p>
<h2 id="paradigm-1-training-separate-gatekeeper-models" class="headerLink">
    <a href="#paradigm-1-training-separate-gatekeeper-models" class="header-mark"></a>Paradigm 1: Training Separate &ldquo;Gatekeeper&rdquo; Models</h2><p>The first approach is to treat retrieval decision-making as a classification task handled by a separate, lightweight model. Rather than modifying the main LLM, these approaches train a specialized &ldquo;gatekeeper&rdquo; model that takes the user query, and based on its training, decides whether to pass it directly to the LLM for a generation-only response or to route it through a full RAG pipeline.</p>
<p>This separation of concerns offers <strong>several practical advantages</strong>. The gatekeeper model can be trained independently, updated frequently, and optimized specifically for the decision task without touching the main LLM&rsquo;s weights. This approach is also computationally efficient at inference time, as the gatekeeper is much smaller than the main LLM. This paradigm also allows the usage of proprietary closed-source LLMs while still implementing adaptive retrieval through the gatekeeping classifier model.</p>
<h3 id="using-a-query-complexity-classifier" class="headerLink">
    <a href="#using-a-query-complexity-classifier" class="header-mark"></a>Using a Query Complexity Classifier</h3><p>Jeong et al.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> argue that while applying a computationally expensive multi-step retrieval process to answer a simple question like &ldquo;What is the capital of France?&rdquo; is wasteful and slow, the reverse is also true, i.e., trying to answer a complex, multi-hop query with a simple single-step retrieval, or no retrieval at all, will likely produce an inaccurate answer. The authors propose training a small language model (such as T5-Large) that acts as a &ldquo;smart router&rdquo; to predict the complexity level of the incoming query, and dynamically adjust the operational strategy for the RAG pipeline. To train the classifier, they propose a two-part automatic labeling strategy, in which the labels are first generated by simply observing which strategy correctly answers a query, and the remaining unlabeled queries are either assigned the label B or C, based on whether the query originated from a single-hop or multi-hop dataset, respectively.</p>
<p><figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/query_classification_based_adaptive_rag.png" title="Adaptive strategy selection" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/query_classification_based_adaptive_rag.png" data-sub-html="<h2>Selecting the most suitable strategy based on the complexity of given queries</h2><p>Adaptive strategy selection</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/query_classification_based_adaptive_rag.png'   alt="Adaptive strategy selection"  ></a><figcaption class="image-caption">Selecting the most suitable strategy based on the complexity of given queries</figcaption>
</figure></p>
<p>The classifier assigns one of the three pre-defined complexity levels to the query: &ldquo;A&rdquo;, &ldquo;B&rdquo;, or &ldquo;C&rdquo;. The label &ldquo;A&rdquo; indicates that the query is straightforward and answerable by LLM&rsquo;s parametric memory, &ldquo;B&rdquo; indicates moderate complexity where a single-step RAG is activated, and &ldquo;C&rdquo; indicates a complex question requiring a multi-step iterative RAG process. The code for this paper is available on <a href="https://github.com/starsuzi/Adaptive-RAG" target="_blank" rel="noopener noreferrer">GitHub</a>.</p>
<h3 id="using-multiple-classifiers-to-determine-the-need-for-retrieval" class="headerLink">
    <a href="#using-multiple-classifiers-to-determine-the-need-for-retrieval" class="header-mark"></a>Using Multiple Classifiers to Determine the Need for Retrieval</h3><p>In <strong>UAR</strong> (<strong>U</strong>nified <strong>A</strong>ctive <strong>R</strong>etrieval), Cheng et al.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> hypothesize that relying on a single criterion, like either intent awareness, knowledge awareness, time awareness or, self-awareness, for adaptive RAG is limiting in real-world scenarios. These criteria are defined as follows:</p>
<ul>
<li><strong>Intent Aware</strong>: Does the user explicitly want the model to use retrieval?</li>
<li><strong>Knowledge-Aware</strong>: Does the question require factual knowledge to answer?</li>
<li><strong>Time-Sensitive Aware</strong>: Is the answer likely to change over time?</li>
<li><strong>Self-Aware</strong>: Does the LLM have the necessary knowledge stored internally?</li>
</ul>
<p><figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/uar_instruction_examples.png" title="Different types of user instructions" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/uar_instruction_examples.png" data-sub-html="<h2>Different types of user instructions which cannot be handled by a single criteria</h2><p>Different types of user instructions</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/uar_instruction_examples.png'   alt="Different types of user instructions"  ></a><figcaption class="image-caption">Different types of user instructions which cannot be handled by a single criteria</figcaption>
</figure></p>
<p>The authors propose a multi-faceted decision-making process that considers all four distinct orthogonal criteria to determine if retrieval is necessary. For each criterion, the authors train a lightweight, plug-and-play binary classifier on the last layer&rsquo;s hidden state of a fixed LLM to judge whether the input requires retrieval. The input for each classifier is the hidden state of the last token from the user&rsquo;s instructions.</p>
<p>At inference time, the UAR-Criteria framework specifies priority for multiple retrieval criteria and unifies them into a single decision tree, as shown below.</p>
<p><figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/uar_framework.png" title="Overview of the UAR framework" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/uar_framework.png" data-sub-html="<h2>Overview of the UAR framework</h2><p>Overview of the UAR framework</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/uar_framework.png'   alt="Overview of the UAR framework"  ></a><figcaption class="image-caption">Overview of the UAR framework</figcaption>
</figure></p>
<ul>
<li>First, the intent-aware classifier checks if the user has an explicit retrieval intent. If yes, retrieval is triggered immediately. Otherwise, the knowledge-aware classifier determines if the query requires factual knowledge. If not, retrieval is skipped.</li>
<li>If the query is deemed knowledge-intensive, the time-aware classifier checks if the answer is time-sensitive. If yes, retrieval is triggered.</li>
<li>Finally, if the query is knowledge-sensitive but not time-sensitive, the self-aware classifier decides if the model has the relevant internal knowledge to answer the question, or if retrieval should be triggered.</li>
</ul>
<p>The code, data, and models for UAR are available on <a href="https://github.com/xiami2019/UAR" target="_blank" rel="noopener noreferrer">GitHub</a>.</p>
<h3 id="using-multiple-classifiers-to-filter-the-retrieved-context" class="headerLink">
    <a href="#using-multiple-classifiers-to-filter-the-retrieved-context" class="header-mark"></a>Using Multiple Classifiers to Filter the Retrieved Context</h3><p>Zeng et al.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> propose a multi-stage filtering approach that uses four task-specific binary classifiers, each trained on one of the four distinct knowledge-checking tasks:</p>
<ol>
<li><strong>Internal Knowledge Checking</strong>: Does the LLM have the knowledge to answer a query without any help?</li>
<li><strong>Uninformed Helpfulness Checking</strong>: Is a retrieved document helpful when the LLM lacks internal knowledge about the query?</li>
<li><strong>Informed Helpfulness Checking</strong>: Is a retrieved document helpful even when the LLM already knows the answer?</li>
<li><strong>Contradiction Checking</strong>: Does the retrieved information contradict the LLM&rsquo;s internal knowledge?</li>
</ol>
<p>The authors used the RetrievalQA dataset for the first task, subsets of the Natural Questions dataset for the two helpfulness tasks, and a subset of the ConflictQA dataset for the final task. The positive and negative examples for each task are fed into a base LLM (e.g., Mistral-7B-Instruct). The authors hypothesize that LLMs&rsquo; representations exhibit distinct patterns when they encounter high-level concepts, such as helpful vs unhelpful prompts. Based on this, the internal representation from the LLM&rsquo;s final layer for the last input token is used to train classifiers with respect to the four tasks.</p>
<p>While the other methods discussed in this post focus on the initial retrieval decision, this paper used the trained classifier after retrieval but before generation, to filter out all the unhelpful or misleading documents. During inference, the system retrieves an initial set of documents for the given user query. In parallel, the query is sent to the Internal Knowledge Classifier to determine if the query is &ldquo;Known&rdquo; or &ldquo;Unknown&rdquo;. Then each of the retrieved documents goes through the Helpfulness Checking Classifier. If the query was classified as &ldquo;Known&rdquo; by the first classifier, the documents also go through the Contradiction Checking Classifier. Finally, any document classified as &ldquo;unhelpful&rdquo; or &ldquo;contradictory&rdquo; is discarded before the final answer is generated.</p>
<h3 id="using-an-encoder-based-classifier" class="headerLink">
    <a href="#using-an-encoder-based-classifier" class="header-mark"></a>Using an Encoder-Based Classifier</h3><p>In <strong>RAGate-MHA</strong> (<strong>RAGate</strong>-<strong>M</strong>ulti-<strong>H</strong>ead <strong>A</strong>ttention), Wang et al.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> propose training a multi-head attention neural classifier model (based on the Transformer architecture&rsquo;s encoder part) specifically for the task of deciding when to retrieve.</p>
<p><figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/raggate_mha.png" title="The RAGate-MHA framework" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/raggate_mha.png" data-sub-html="<h2>The RAGate-MHA framework</h2><p>The RAGate-MHA framework</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/raggate_mha.png'   alt="The RAGate-MHA framework"  ></a><figcaption class="image-caption">The RAGate-MHA framework</figcaption>
</figure></p>
<p>For training the classifier, the authors used the KETOD dataset, which includes human labels as ground truth, indicating the turns where knowledge augmentation is needed. The main input to the classifier is the conversational context (dialogue history), but the authors additionally experimented with the following 3 setups.</p>
<ul>
<li><strong>Context Only</strong>: Use only the conversation history as input for queries, keys, and values in the attention mechanism.</li>
<li><strong>Context + Knowledge</strong>: Use conversation history combined (concatenated) with the retrieved knowledge snippet.</li>
<li><strong>Context x Knowledge</strong>: Use conversation context as the query and the retrieved knowledge as the keys and the values. This setup is configured to specifically model the interaction between dialogue and the document.</li>
</ul>
<p>Through experimentation, they found that the &lsquo;Context Only&rsquo; version provided the best overall performance. The model was particularly effective at capturing human retrieval patterns, learning to augment more at the beginning of conversations, and for information-heavy domains. After processing the input, the model passes its final representation through a linear layer and a softmax function to produce a binary output that is used to make the decision on whether to augment the response with external knowledge.</p>
<h3 id="training-a-lightweight-knowledge-boundary-classifier" class="headerLink">
    <a href="#training-a-lightweight-knowledge-boundary-classifier" class="header-mark"></a>Training a Lightweight Knowledge Boundary Classifier</h3><p>In <strong>KBM</strong> (<strong>K</strong>nowledge <strong>B</strong>oundary <strong>M</strong>odel), Zhang et al.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> argue that a decision to retrieve made by a powerful model like GPT-5 may not be appropriate for a smaller model like Llama-2 7B, as their inherent &ldquo;knowledge boundaries&rdquo; are very different. By systematically measuring a model&rsquo;s confidence and certainty on various questions, we can generate a dataset that maps its unique knowledge boundary. This dataset can then be used to train a specialized, light-weight model whose only job is to decide whether the main LLM needs knowledge-augmentation.</p>
<p><figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/kbm_framework.png" title="The KBM workflow" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/kbm_framework.png" data-sub-html="<h2>The KBM workflow</h2><p>The KBM workflow</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/kbm_framework.png'   alt="The KBM workflow"  ></a><figcaption class="image-caption">The KBM workflow</figcaption>
</figure></p>
<p>During training, for a large set of questions, their framework generates &ldquo;soft labels&rdquo; by repeatedly querying the target LLM (e.g, Qwen2-7B) and calculating the following two metrics.</p>
<ul>
<li><strong>Mastery (or Confidence)</strong>: For questions with known correct (gold) answers, we sample 30 responses from the LLM. The mastery score is simply the percentage of correct answers in that sample.</li>
<li><strong>Certainty</strong>: For questions without any gold answer, we measure the consistency of the 30 sample responses. This can be done by calculating the entropy of the different answer phrases. A low entropy score means that the model is highly consistent and certain, even if it&rsquo;s confidently wrong.</li>
</ul>
<p>The authors set a predefined threshold on these 2 scores to automatically label every question as &ldquo;Known&rdquo; or &ldquo;Unknown&rdquo; for that specific LLM. This dataset is then used to fine-tune a separate model (the KBM model). During inference, the user query is first sent to the KBM. If the KBM predicts the query is &ldquo;Known&rdquo;, the query is directed to the main LLM to produce the answer; otherwise, if KBM predicts the query to be &ldquo;Unknown&rdquo;, a RAG pipeline is activated instead.</p>
<h3 id="combining-proactive-and-reactive-classifier-strategies" class="headerLink">
    <a href="#combining-proactive-and-reactive-classifier-strategies" class="header-mark"></a>Combining Proactive and Reactive Classifier Strategies</h3><p>In <strong>DioR</strong>, Guo et al.<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> critique ARAG methods that rely on simplistic rules, like triggering retrieval when an output token&rsquo;s generation probability falls below a threshold. They argue that low token probability doesn&rsquo;t necessarily mean the final output will be a hallucination, and these triggers are often reactive, acting only after a hallucination has started. To solve this, they propose a framework called <strong>Adaptive Cognition Detection</strong> that uses a dual classifier approach that can act proactively before generation and reactively during generation.</p>
<p><figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/dior_framework.png" title="DioR&rsquo;s Adaptive Cognition Detection" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/dior_framework.png" data-sub-html="<h2>DioR&#39;s Adaptive Cognition Detection</h2><p>DioR&rsquo;s Adaptive Cognition Detection</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/dior_framework.png'   alt="DioR&rsquo;s Adaptive Cognition Detection"  ></a><figcaption class="image-caption">DioR's Adaptive Cognition Detection</figcaption>
</figure></p>
<ul>
<li>
<p><strong>Early Detection Classifier</strong>: The authors hypothesize that when a model is confident, its attention is sharply focused on a few key terms in the query. But when it&rsquo;s uncertain, the attention is more dispersed. They use a metric called Attribution Entropy to compute how the LLM &lsquo;attends&rsquo; to the user&rsquo;s initial question. A low entropy score suggests focused attention and high confidence, while a high entropy score suggests dispersed attention and a higher chance of hallucination. An RNN-based classifier is trained to predict the hallucination label on questions from Wikipedia based on the question&rsquo;s attribution entropy.</p>
</li>
<li>
<p><strong>Real-time Detection Classifier</strong>: If a query passes the initial check, a second classifier is used to monitor the LLM&rsquo;s output on-the-fly. This MLP-based classifier is trained to spot hallucinations as they appear during the generation process. The authors hypothesize that entity-level errors, such as incorrect names, dates, or places, are the most common source of hallucinations. Thus, to create training data for this classifier, the authors take passages from Wikipedia, truncate them, and ask LLM to continue the text. The entities in the generated text are then compared to the entities in the original, ground-truth text. If the entities are semantically different, the training example is labeled as a hallucination.</p>
</li>
</ul>
<p>During inference, a question is first fed to the early detection classifier. If it predicts a lack of confidence, the retrieval is triggered immediately; otherwise, the LLM generates the answer token-by-token. During this generation, the real-time detection classifier assesses each token. It pauses generation and triggers retrieval if a token is classified as being part of a likely hallucination.</p>
<h3 id="a-bandit-approach-to-extend-since-class-supervision-methods" class="headerLink">
    <a href="#a-bandit-approach-to-extend-since-class-supervision-methods" class="header-mark"></a>A Bandit Approach to Extend Since-Class Supervision Methods</h3><p>In <strong>MBA-RAG</strong> (<strong>M</strong>ulti-arm <strong>Ba</strong>ndit <strong>RAG</strong>), Tang et al.<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> argue that the prior classifier-based methods, such as the one proposed by Jeong et al.<sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, rely on &lsquo;single-class supervision&rsquo;. These methods assume there&rsquo;s only one retrieval strategy that&rsquo;s optimal for a given query, typically one with the lowest computational cost. This assumption could be inaccurate, as it penalizes other strategies that might also yield a correct answer that could also be more comprehensive. It also prevents the model from learning more nuanced tradeoffs between different approaches. Instead, the authors reframe the retrieval strategy selection as a game of exploration and exploitation using a classic reinforcement learning concept, i.e., the multi-arm bandit algorithm.</p>
<p>The MBA-RAG framework treats each retrieval strategy as a distinct &ldquo;arm&rdquo; of a slot machine:</p>
<ul>
<li><strong>Arm1</strong>: Direct Answer (no retrieval)</li>
<li><strong>Arm2</strong>: Retrieve Once</li>
<li><strong>Arm3</strong>: Iterative Retrieval</li>
</ul>
<p>The goal is to train an agent that learns to &ldquo;pull&rdquo; the arm that will yield the highest reward for a given query. The model is supervised only by the feedback (reward) of the arm it pulls, without penalizing the strategies that were not selected, allowing it to explore different strategies and learning a more comprehensive selection policy. To guide this process, the authors propose a fine-grained reward function that balances accuracy with computational efficiency, to penalize strategies that are correct but unnecessarily costly.</p>
<p><figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/mba_rag.png" title="the MBA-RAG framework" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/mba_rag.png" data-sub-html="<h2>left: relying on an inaccurate heuristic approach to assign queries of different complexities to a single generation process, right: the MBA-RAG framework</h2><p>the MBA-RAG framework</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/mba_rag.png'   alt="the MBA-RAG framework"  ></a><figcaption class="image-caption">left: relying on an inaccurate heuristic approach to assign queries of different complexities to a single generation process, right: the MBA-RAG framework</figcaption>
</figure></p>
<p>First, a lightweight pre-trained language model, DistilBERT, is used as a query encoder. It takes the user&rsquo;s query and outputs a prediction of the expected reward for each arm. To balance what it already knows (exploitation) and trying new things (exploration), an epsilon-greedy strategy is used, such that the arm with the highest predicted reward is selected with probability $1-\epsilon$, while a random arm is selected with probability $\epsilon$.</p>
<p>After the model selects an arm $a$ and the corresponding RAG strategy is executed, the model receives a reward $r_a$ calculated by a cost-aware function:
$$r_a = \mathcal{A}(y,\hat{y}_{a})  -\lambda C(a)$$</p>
<p>Where $\mathcal{A}(y,\hat{y}_{a})$ is an accuracy metric, such as an exact match, $C(a)$ is the computation cost of the strategy (measured by the number of retrieval steps), and $\lambda$ is a hyperparameter that balances accuracy vs efficiency. The query encoder is then trained to minimize the squared error between its predicted reward and the actual received reward. Over time, the model learns to accurately predict which retrieval strategy will yield the best balance of performance and cost for a given query&rsquo;s complexity. The authors note the challenges in scalability and adaptability when entirely new types of queries are encountered. The code for MBA-RAG is available on <a href="https://github.com/FUTUREEEEEE/MBA" target="_blank" rel="noopener noreferrer">GitHub</a>.</p>
<h3 id="use-a-smaller-model-from-the-same-family-to-predict-the-knowledge-gaps" class="headerLink">
    <a href="#use-a-smaller-model-from-the-same-family-to-predict-the-knowledge-gaps" class="header-mark"></a>Use a Smaller Model From the Same Family to Predict the Knowledge Gaps</h3><p>In SlimPLM (<strong>Slim</strong> <strong>P</strong>roxy <strong>L</strong>anguage <strong>M</strong>odel), Tan et al.<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup> hypothesize that large and small decoder-only models, which often share similar Transformer architectures and are trained on overlapping public datasets, can reach a consensus on what they know and what they don&rsquo;t know. Their preliminary experiments show that the knowledge gap between a large model like Llama2-70B and a small one like Llama2-7B is most apparent on difficult questions they both struggle with. And if the small model can correctly answer a question, the large model is very likely to be able to correctly answer it as well. Hence, the authors propose using a proxy model with fewer parameters to predict the knowledge gaps of a large one.</p>
<p>First, the small proxy model is given the user question as input to generate a preliminary &ldquo;heuristic answer&rdquo;. The heuristic answer and the original question are then fed into a light-weight, fine-tuned &ldquo;Retrieval Necessity Judgement&rdquo; (RJ) model, like Llama2-7B. The RJ model outputs a simple binary decision.</p>
<div class="details admonition quote">
    <div class="details-summary admonition-title">
        <span class="icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"/></svg></span>Instructions for fine-tuning the RJ model<span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
    </div>
    <div class="details-content">
        <div class="admonition-content"><p>Input:</p>
<p>You are a helpful assistant. Your task is to parse user input into structured formats and accomplish the task according to the heuristic answer.</p>
<p>Heuristic answer: {Heuristic Answer}
Question: {user question}
Retrieval Necessity Judgment Output:
Output:</p>
<p>Known (True / False)</p>
</div></div></div>
<p>If the output is &ldquo;Known&rdquo;, the system assumes that the LLM can answer from its internal knowledge. Whereas if the output is &ldquo;Unknown&rdquo;, the system triggers the retrieval-augmentation workflow.</p>
<p><figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/slimplm_overview.png" title="Overview of SlimPLM" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/slimplm_overview.png" data-sub-html="<h2>Overview of SlimPLM</h2><p>Overview of SlimPLM</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/slimplm_overview.png'   alt="Overview of SlimPLM"  ></a><figcaption class="image-caption">Overview of SlimPLM</figcaption>
</figure></p>
<p>When retrieval is triggered, a query rewriting model decomposes the heuristic answer into a set of distinct individual claims and generates a specific search query for each claim. The RJ model is then reused to filter out claim-query pairs that LLM is likely to already know. Retrieval is only performed for the queries that pass the filter, and the resulting documents are then passed with the original question to the LLM for generating the final answer. The code and dataset for SlimPLM are available on <a href="https://github.com/plageon/SlimPlm" target="_blank" rel="noopener noreferrer">GitHub</a>.</p>
<hr>
<p>Gatekeeper-based approaches are excellent at operational simplicity. They are easy to deploy and can be retrained without affecting the main LLM. However, this paradigm has <strong>several inherent limitations</strong>. These methods require managing a separate training pipeline and dataset for the gatekeeper and incur a small additional latency during inference. The gatekeeper can not adapt its judgement based on the LLM&rsquo;s generation process, as it makes its decision before seeing how the LLM would respond. There&rsquo;s also a risk of &ldquo;knowledge gap&rdquo; between the gatekeeper and the main LLM when the classifier may not perfectly align with the LLM&rsquo;s true knowledge boundaries, leading to suboptimal routing decisions.</p>
<p>To overcome some of these challenges, the next section introduces methods that teach LLM itself to recognize and communicate its knowledge boundaries.</p>
<h2 id="paradigm-2-teaching-the-main-llm" class="headerLink">
    <a href="#paradigm-2-teaching-the-main-llm" class="header-mark"></a>Paradigm 2: Teaching the Main LLM</h2><p>Instead of using an external model, we can teach the main LLM to be self-aware about its own knowledge limitations and communicate the need for retrieval. This paradigm involves fine-tuning the LLM on specialized datasets, teaching it to generate special tokens, specific phrases, or internal signals that trigger the retrieval process.</p>
<p>These methods make the LLM become self-aware learning, not just to answer questions, but also to recognize when it shouldn&rsquo;t answer without help. This paradigm offers <strong>several advantages</strong> in decision quality. The retrieval decision is better aligned with the LLM&rsquo;s internal states, as the model itself is the one making the decision. A single model handles both the retrieval decision and the subsequent generation, eliminating the coordination overhead of the gatekeeper-based methods. The use of Parameter-Efficient Fine-Tuning (PEFT) techniques also helps in making this approach computationally feasible.</p>
<h3 id="train-language-model-to-introspect-its-output-and-request-retrieval-when-required" class="headerLink">
    <a href="#train-language-model-to-introspect-its-output-and-request-retrieval-when-required" class="header-mark"></a>Train Language Model to Introspect Its Output and Request Retrieval When Required</h3><p>In <strong>Self</strong>-Reflective <strong>R</strong>etrieval-<strong>A</strong>ugmented <strong>G</strong>eneration (<strong>SELF-RAG</strong>), Asai et al.<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup> introduce a framework that trains an arbitrary LM in an end-to-end manner to adaptively retrieve passages on demand. SELF-RAG trains the model to generate intermittent special tokens, called reflection tokens, alongside the regular text. These tokens act as an internal monologue, allowing the model to critique its own output and hence control the generation process. The following figure shows the two types of reflection tokens (Retrieve and Critique) that the model learns to generate to indicate the need for retrieval and its generation quality, respectively.</p>
<p><figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/self_rag_special_tokens.png" title="SELF-RAG&rsquo;s special tokens" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/self_rag_special_tokens.png" data-sub-html="<h2>Four types of reflection tokens used in SELF-RAG</h2><p>SELF-RAG&rsquo;s special tokens</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/self_rag_special_tokens.png'   alt="SELF-RAG&rsquo;s special tokens"  ></a><figcaption class="image-caption">Four types of reflection tokens used in SELF-RAG</figcaption>
</figure></p>
<ul>
<li>Training Process: First, a proprietary LLM like GPT-4 is prompted to generate the appropriate reflection tokens for various examples. Then, inspired by reward models used in reinforcement learning, a separate &ldquo;critic&rdquo; model is trained by fine-tuning a pre-trained LM (like Llama-2) on this dataset, where reflection tokens are the labels. The goal here is to effectively distill the judgment capabilities of the larger model to a critic model, which is then used to annotate a large, diverse dataset of instructions and responses. For each piece of text, the critic inserts the appropriate <code>[Retrieval]</code>, <code>[ISREL]</code>, <code>[ISSUP]</code>, and <code>[ISUSE]</code> tokens to create a rich annotated corpus.</li>
</ul>
<p><figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/self_rag_training_example.png" title="SELF-RAG training examples" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/self_rag_training_example.png" data-sub-html="<h2>SELF-RAG training examples. The left example requires no retrieval, while the one on right requires retrieval.</h2><p>SELF-RAG training examples</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/self_rag_training_example.png'   alt="SELF-RAG training examples"  ></a><figcaption class="image-caption">SELF-RAG training examples. The left example requires no retrieval, while the one on right requires retrieval.</figcaption>
</figure></p>
<p>Finally, a generator LM (the end-product of the SELF-RAG model) is trained on the annotated corpus with a standard next-token prediction objective. This model predicts both the response text and the interleaved reflection tokens.</p>
<p><figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/self_rag_overview.png" title="Overview of SELF-RAG" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/self_rag_overview.png" data-sub-html="<h2>Overview of SELF-RAG</h2><p>Overview of SELF-RAG</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/self_rag_overview.png'   alt="Overview of SELF-RAG"  ></a><figcaption class="image-caption">Overview of SELF-RAG</figcaption>
</figure></p>
<ul>
<li>Inference Process: Given a prompt, the model first generates a <code>[Retrieval]</code> reflection token to decide if fetching external information would be helpful. A tunable probability threshold can be used here to optimize the decision to invoke retrieval. No retrieval is performed if the model generates <code>[Retrieval=No]</code>, and if it generates <code>[Retrieval=Yes]</code>, a retriever is used to fetch external documents. The model processes retrieved documents in parallel and generates a candidate response for each one. Then it critiques its own output by producing <code>[ISREL]</code> token to evaluate the relevance of the retrieved passage and <code>[ISSUP]</code> to evaluate if its own generation is supported by the passage. Finally, the best response is selected based on the critique tokens, and the process is repeated for the next segment. This reflection step also enables the model to discard generated segments that are based on irrelevant or contradicting passages.</li>
</ul>
<p>Some recent studies have argued that SELF-RAG&rsquo;s effectiveness is tied too closely to the base LM&rsquo;s ability to generate accurate self-reflection tokens <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup> <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>, and the model just learn to &ldquo;mechanically&rdquo; predict these tokens without developing any deeper reasoning capabilities <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. Also, this method is primarily designed to differentiate between knowledge-intensive and non-knowledge-intensive instructions, which may be less optimal for tasks like multi-turn conversational dialogue <sup id="fnref1:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. The code and trained models for SELF-RAG are available on <a href="https://selfrag.github.io/" target="_blank" rel="noopener noreferrer">Github</a>.</p>
<h3 id="simplifying-self-rag" class="headerLink">
    <a href="#simplifying-self-rag" class="header-mark"></a>Simplifying SELF-RAG</h3><p>In the previous section, we explored how Self-RAG trains a model to have a sophisticated internal monologue to retrieve, generate, and critique its own output segment-by-segment using a variety of special &ldquo;reflection tokens&rdquo;. Labruna et al.<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup> propose a framework, called <strong>ADAPT-LLM</strong>, to simplify this method by focusing on teaching an LLM a single, fundamental skill of recognizing its own knowledge gaps. Instead of relying on a separate critic model to generate labels, this method uses the base LLM&rsquo;s own performance to bootstrap the training corpus. The authors create a self-correcting feedback loop where if the model answers correctly from its parametric memory, it&rsquo;s taught to do so again. In case of incorrect answers, two different training instances are created. The first teaches the model to only emit a special <code>&lt;RET&gt;</code> token, which signals the need for retrieval, while the second teaches it to correctly answer the question after being provided with the question and the corresponding context passage.</p>
<p><figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/adapt_llm.png" title="The inference process of ADAPT-LLM" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/adapt_llm.png" data-sub-html="<h2>The inference process of ADAPT-LLM</h2><p>The inference process of ADAPT-LLM</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/adapt_llm.png'   alt="The inference process of ADAPT-LLM"  ></a><figcaption class="image-caption">The inference process of ADAPT-LLM</figcaption>
</figure></p>
<p>The base LLM is then fine-tuned on this newly constructed dataset. During inference, the model either generates the final answer or the <code>&lt;RET&gt;</code> token indicating the need for retrieval. The code for ADAPT-LLM is available on <a href="https://github.com/tLabruna/Adapt-LLM" target="_blank" rel="noopener noreferrer">GitHub</a>.</p>
<h3 id="multi-turn-conversational-qa-extension-for-self-rag" class="headerLink">
    <a href="#multi-turn-conversational-qa-extension-for-self-rag" class="header-mark"></a>Multi-Turn Conversational QA Extension for Self-RAG</h3><p>Roy et al.<sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup> proposed an extension to SELF-RAG for multi-turn conversational question-answering where context is often built over multiple turns. The authors redesign the prior framework to include a conversational history instead of a single-turn question.</p>
<p><figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/self_multi_rag.png" title="An example" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/self_multi_rag.png" data-sub-html="<h2>Example: the decision to retrieve or not might depend on the conversational context rather than only the last turn</h2><p>An example</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/self_multi_rag.png'   alt="An example"  ></a><figcaption class="image-caption">Example: the decision to retrieve or not might depend on the conversational context rather than only the last turn</figcaption>
</figure></p>
<p>They extend the binary decision to retrieve (<code>[Retrieve]</code>), or not retrieve (<code>[No Retrieve]</code>) by teaching LLM to generate a new special token <code>[Continue to Use Evidence]</code> to indicate that the answer can be found in the conversation history or within the previously retrieved passages.</p>
<h3 id="multimodal-extension-for-self-rag-framework" class="headerLink">
    <a href="#multimodal-extension-for-self-rag-framework" class="header-mark"></a>Multimodal Extension for SELF-RAG Framework</h3><p>In <strong>ReflectiVA</strong> (<strong>Reflecti</strong>ve LLa<strong>VA</strong>), Cocchi et al.<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup> adapt the Self-RAG framework to a knowledge-based Visual Question-Answering (VQA) task using multimodal LLMs (MLLMs).  The authors teach an MLLM to distinguish between a purely visual question (like, &ldquo;what color is the cat in this image?&rdquo;) that can be directly answered by an MLLM, versus ones that require external knowledge (&ldquo;what company manufactured this truck?&rdquo;). Similar to Self-RAG (although a bit less granular), ReflectiVA introduces four reflective tokens to the model&rsquo;s vocabulary:</p>
<ul>
<li><code>&lt;RET&gt;</code> and <code>&lt;NORET&gt;</code> to indicate whether retrieval is required or not, to answer the question about the image</li>
<li><code>&lt;REL&gt;</code> and <code>&lt;NOREL&gt;</code> to assess if a retrieved passage is relevant or not relevant to the image-question pair</li>
</ul>
<p><figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/reflectiVA_training_process.png" title="Training approach for ReflectiVA" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/reflectiVA_training_process.png" data-sub-html="<h2>Training approach for ReflectiVA</h2><p>Training approach for ReflectiVA</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/reflectiVA_training_process.png'   alt="Training approach for ReflectiVA"  ></a><figcaption class="image-caption">Training approach for ReflectiVA</figcaption>
</figure></p>
<p>Like Self-RAG&rsquo;s critic model, ReflectiVA trains a preliminary &ldquo;in-article reflective model&rdquo; to generate a high-quality training set for the final model. This model learns to distinguish between relevant and non-relevant text passages coming from the same source article. To create training data for this critic model, the authors used GPT-4 to generate positive (i.e., containing the answer) and negative labels, given an image, a question, and all of the passages from the ground-truth Wikipedia article. This critic model is then used to build a much larger dataset for training the final model. Passages are labeled <code>&lt;REL&gt;</code> and <code>&lt;NOREL&gt;</code> depending on whether the critic model finds them relevant to the given image-question pair. The authors add data from standard conversational and instruction-following datasets with <code>&lt;NORET&gt;</code> label and samples from knowledge-intensive VQA datasets with <code>&lt;RET&gt;</code> label. Finally, the ReflectiVA model is trained on this rich, annotated corpus of images, questions, and passages to learn to generate both the correct answer and the appropriate reflective tokens.</p>
<p>During inference, given an image and a question, the model is first asked to generate a single token, with its choices restricted to either <code>&lt;RET&gt;</code> or <code>NORET</code>. If the model outputs <code>&lt;RET&gt;</code>, it proceeds to generate the final answer using only the image-question pair and its internal memory. If the model outputs <code>&lt;NORET&gt;</code> instead, a retrieval process is activated. Before generating the final answer, the model judges the relevance of each retrieved passage by generating <code>&lt;REL&gt;</code> or <code>&lt;NOREL&gt;</code> tokens. The model then generates the final answer, but it is conditioned only on the retrieved passage marked as relevant. The source code and trained models for this paper are available on <a href="https://github.com/aimagelab/ReflectiVA" target="_blank" rel="noopener noreferrer">GitHub</a>.</p>
<h3 id="fine-tuning-the-llm-to-predict-information-needs" class="headerLink">
    <a href="#fine-tuning-the-llm-to-predict-information-needs" class="header-mark"></a>Fine-Tuning the LLM to Predict Information Needs</h3><p>Djean et al.<sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup> argue that the LLM can also be efficiently trained to determine if an answer is already in its parametric memory. Interestingly, they find that this capability is significantly enhanced by allowing the model to generate the first few tokens of its response before making the decision. This partial answer text seemingly provides a powerful clue about the model&rsquo;s internal state, making the decision to retrieve more reliable.</p>
<p>The process begins by creating training data using a separate LLM model (SOLAR-10.7B-Instruct-v1.0) to act as an &ldquo;evaluator&rdquo; or a &ldquo;judge&rdquo;. First, the main LLM (like Mistral-7B or Llama-3.1-8B) generates answers for a large set of questions from a labeled dataset (the paper uses the NQ dataset). Then the judge model evaluates these generated answers by comparing them against the ground-truth dataset. The judge&rsquo;s output is converted into a binary &ldquo;Yes&rdquo; or &ldquo;No&rdquo; label, creating a set of &ldquo;silver labels&rdquo; for training.</p>
<p>Next, the main LLM is fine-tuned on the dataset generated via the distilled &ldquo;LLM-as-a-Judge&rdquo; model. For models 7B or larger, this fine-tuning is done using adapters.</p>
<p>During inference, this fine-tuned model receives the user&rsquo;s question. The authors use a &ldquo;peek&rdquo; technique. Given the user&rsquo;s question, like &ldquo;Who directed the movie Jaws?&rdquo;, the LLM first runs in standard, non-RAG mode to generate the first 32 tokens of a potential answer, such as &ldquo;Jaws was directed by Steven &hellip;&rdquo;. The original query and this partial answer are then concatenated and fed to the same fine-tuned LLM. Now, the model&rsquo;s task is to perform the classification. The softmax function is then applied to the logits of &ldquo;Yes&rdquo; and &ldquo;No&rdquo; tokens, and the resulting probability for the &ldquo;Yes&rdquo; token is used as the IK (&ldquo;I Know&rdquo;) score. This score value is compared against a predefined threshold to decide whether to trigger retrieval-augmentation.</p>
<h3 id="parameter-efficient-fine-tuning-for-the-llm-to-decide-when-to-retrieve" class="headerLink">
    <a href="#parameter-efficient-fine-tuning-for-the-llm-to-decide-when-to-retrieve" class="header-mark"></a>Parameter-Efficient Fine-Tuning for the LLM to Decide When to Retrieve</h3><p>In <strong>RAGate-PEFT</strong>, Wang et al.<sup id="fnref1:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> propose using <strong>P</strong>arameter-<strong>E</strong>fficient <strong>F</strong>ine-<strong>T</strong>uning (PEFT) approach, specifically QLoRA, to adapt an LLM for determining the need for retrieval during conversations.</p>
<p><figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/raggate_peft.png" title="RAGate-PEFT" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/raggate_peft.png" data-sub-html="<h2>The RAGate-PEFT framework</h2><p>RAGate-PEFT</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/raggate_peft.png'   alt="RAGate-PEFT"  ></a><figcaption class="image-caption">The RAGate-PEFT framework</figcaption>
</figure></p>
<p>The authors source training data from the KETOD dataset, which already contains human labels indicating which specific turn in a conversation would benefit from being augmented with external knowledge. These human judgements serve as the ground truth label for the model&rsquo;s training. They structured the data into instruction-input-output triplets.</p>
<ul>
<li><strong>Instruction</strong>: This is simply a command describing the task (e.g., &ldquo;Analyze this conversation and decide if you need external knowledge&rdquo;)</li>
<li><strong>Input</strong>: Other than the conversational context (dialogue history), the authors experimented with several combinations to find the most effective one. They tested adding the system response, i.e., ground truth response for that turn (if available), synthetic response, named entities recognized within the incoming response, the actual text snippet retrieved from an external source, and a description of the external source (e.g., &ldquo;WikiHow website&rdquo;). The experiments showed that simply providing the conversational context along with a synthetic response (generated by a language model as a stand-in) and its named entities led to the best results.</li>
<li><strong>Output</strong>: The ground truth label (&ldquo;True&rdquo; or &ldquo;False&rdquo;) from the KETOD dataset.</li>
</ul>
<p>The model&rsquo;s predicted output (&ldquo;True&rdquo;, or &ldquo;False&rdquo;) directly controls whether the retrieval step is triggered or skipped for any given turn in the conversation.</p>
<hr>
<p>Some <strong>disadvantages</strong> of this paradigm include the fact that it requires modifying the LLM itself, which has practical challenges. Fine-tuning large models can be computationally expensive and can lead to catastrophic forgetting, where the model&rsquo;s general capabilities degrade after specialized tuning. The training data creation process can be complex, often requiring powerful teacher models or sophisticated labeling strategies to generate high-quality examples. It also requires access to model weights, making it incompatible with closed-source APIs.</p>
<p>In this section, we looked at ways to teach an LLM to make binary, &ldquo;retrieve&rdquo;, or &ldquo;don&rsquo;t retrieve&rdquo; decisions. However, the models don&rsquo;t explicitly plan multi-step information gathering or reason about what specific knowledge is missing. In the next section, we will look at a group of methods that not only recognize knowledge gaps but actively reason about how to fill them.</p>
<h2 id="paradigm-3-teaching-reasoning-about-retrieval" class="headerLink">
    <a href="#paradigm-3-teaching-reasoning-about-retrieval" class="header-mark"></a>Paradigm 3: Teaching Reasoning About Retrieval</h2><p>The final and most sophisticated paradigm teaches LLMs to articulate what information is missing, plan multi-step retrieval strategies, and iteratively refine their understanding as they gather knowledge. The LLM becomes an autonomous agent that decomposes complex queries, engages the retrieval system in a dialogue, evaluates retrieval information, and decides when it has gathered enough evidence to formulate the final answer.</p>
<p>Reasoning-based approaches offer <strong>several advantages</strong> due to their ability to handle complex, multi-hop questions that are difficult to answer with a single retrieval. They can recover from irrelevant retrievals by reformulating queries, demonstrating planning capabilities. The explicit reasoning traces also allow for interpretability.</p>
<h3 id="teaching-llms-to-leverage-reasoning-for-retrieval-as-a-dialogue" class="headerLink">
    <a href="#teaching-llms-to-leverage-reasoning-for-retrieval-as-a-dialogue" class="header-mark"></a>Teaching LLMs to Leverage Reasoning for Retrieval as a Dialogue</h3><p>In <strong>Auto-RAG</strong>, Yu et al.<sup id="fnref1:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup> propose a framework to train an LLM to become an autonomous agent that itself decides both when and what to retrieve. For complex, multi-hop questions (see example in the next image), a single search is often not enough to gather sufficient knowledge. The authors argue that existing adaptive RAG methods overlook the LLM&rsquo;s latent potential to use its own reasoning and decision-making abilities to plan its own retrieval strategy. Their Auto-RAG method uses the LLM&rsquo;s reasoning ability to engage in a multi-turn dialogue with a retriever, systematically planning retrieval and refining queries until it has enough information to answer the user&rsquo;s question.
<figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/autorag_example.png" title="Auto-RAG example" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/autorag_example.png" data-sub-html="<h2>An example of how Auto-RAG addresses complex multi-hop questions</h2><p>Auto-RAG example</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/autorag_example.png'   alt="Auto-RAG example"  ></a><figcaption class="image-caption">An example of how Auto-RAG addresses complex multi-hop questions</figcaption>
</figure></p>
<p>The authors first create a specialized instruction-tuning dataset built starting from question-answer pairs from existing datasets like NaturalQuestions and 2WikiMultihopQA. Each example in the training data is a complete trace of a successful multi-step retrieval process. For this data creation, a powerful teacher LLM (Llama-3-8B-Instruct) guided by few-shot prompts is used to generate reasoning steps, while another model (Qwen1.5-32B-Chat) is used to create diverse rewritten queries. This process follows a Chain-of-Thought paradigm with three types of reasoning:</p>
<ol>
<li><strong>Retrieval Planning</strong>: Given the user question and the current context, LLM identifies what knowledge is needed, assesses retrieved documents, and decides if another search is required.</li>
<li><strong>Information Extraction</strong>: LLM summarizes relevant information from the retrieved documents, effectively filtering out noise.</li>
<li><strong>Answer Inference</strong>: Once all the necessary information is collected, the LLM reasons through the facts to formulate the final answer.</li>
</ol>
<p>The model first generates an initial reasoning $R_0$ given just the question. For multi-hop datasets, it iteratively generates multiple candidate queries and retrieves documents for each. The authors only keep the queries that contain sub-answers. At each step, the model generates new reasoning $R_t$ given the question, all previous retrievals, and current documents. Pre-defined trigger words like &ldquo;However&rdquo;, &ldquo;no information&rdquo;, and &ldquo;find&rdquo; are used to detect if more retrieval is needed. A final filtering is applied such that only the training examples where the final answer matches the ground truth are kept.</p>
<div class="details admonition quote">
    <div class="details-summary admonition-title">
        <span class="icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"/></svg></span>Training data format<span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
    </div>
    <div class="details-content">
        <div class="admonition-content"><p>Turn 0:Input = Question $X$</p>
<p> Output = Reasoning $R_0$ + Query $Q_1$</p>
<p>Turn $t$:Input = Retrieved Documents $D_t$</p>
<p>Output = $R_t$ + Next Query $Q_{t+1}$ or Final Answer $A$</p>
</div></div></div>
<p>An LLM (Llama-3-8B-Instruct) is then fine-tuned on this dataset using a standard supervised learning objective. This training process teaches the model to autonomously generate the reasoning and actions needed for the iterative retrieval.</p>
<p>During inference, the fine-tuned model takes the user&rsquo;s question and generates initial reasoning. Based on that reasoning, it decides whether to output a query for the retriever or to directly generate the final answer. If retrieval is used, it assesses the retrieved documents, generates further reasoning, and decides whether to continue the retrieval cycle or generate the final answer. The code for AutoRAG is available on <a href="https://github.com/ictnlp/Auto-RAG" target="_blank" rel="noopener noreferrer">GitHub</a>.</p>
<h3 id="calibrate-llms-to-make-atomic-decisions" class="headerLink">
    <a href="#calibrate-llms-to-make-atomic-decisions" class="header-mark"></a>Calibrate LLMs to Make Atomic Decisions</h3><p>Guan et al.<sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup> argue that while iterative ARAG methods like Auto-RAG teach a model to engage in a multi-turn dialogue with a retriever, they can struggle with ineffective task decompositions or fall into continuous retrieval loops when no relevant information is found. These redundant retrieval attempts can introduce noise, ultimately degrading the quality of the final response. The challenge is to teach a model to break down a complex query into a coherent, step-by-step reasoning process and make the retrieval decision for each step.</p>
<p>To address this, the authors propose a framework called <strong>DeepRAG</strong> that models the entire retrieval-augmented reasoning process as a Markov Decision Process (MDP). This approach enables an LLM to iteratively decompose a query into atomic subqueries and make a dynamic &lsquo;atomic decision&rsquo; at each step about whether to retrieve external knowledge or rely on its own parametric memory.</p>
<p><figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/deeprag_framework.png" title="An overview of DeepRAG" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/deeprag_framework.png" data-sub-html="<h2>An overview of the DeepRAG framework</h2><p>An overview of DeepRAG</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/deeprag_framework.png'   alt="An overview of DeepRAG"  ></a><figcaption class="image-caption">An overview of the DeepRAG framework</figcaption>
</figure></p>
<p><strong>Data Synthesis</strong></p>
<p>To create the training dataset, DeepRAG starts with a supervised QA dataset (like HotpotQA) and generates its specialized data via a two-stage synthesis process built on Binary Tree Search.</p>
<ol>
<li>
<p><strong>Creating Optimal Paths for Imitation Learning</strong>: For any given question, the framework explores all reasoning paths. At each step, a subquery is generated, creating two branches:</p>
<ul>
<li><strong>Parametric Path</strong>: The model attempts to answer the subquery using only its internal knowledge.</li>
<li><strong>Retrieval Path</strong>: The model retrieves external documents to answer the subquery.</li>
</ul>
<p>A priority queue is used to manage this search. This queue is prioritized by the number of retrieval steps, ensuring that it always explores the path with the fewest retrievals first. The first path that reaches the correct final answer is therefore selected as the &lsquo;optimal reasoning process&rsquo;, and the corresponding sequence of subqueries, atomic decisions, intermediate answers, and the final answer becomes one training data point. The question is discarded if no path leads to a correct answer.</p>
</li>
<li>
<p><strong>Creating Preference Data for Calibration</strong>: After the model is trained on the data created in the first stage, a second dataset is created to fine-tune the model&rsquo;s decision-making ability. For a given question, the stage 1 model finds the optimal path (one with minimal retrieval) again. A &lsquo;preference pair&rsquo; is created for each subquery along this path. This pair consists of two possible outcomes: using retrieval and using parametric memory. The outcome that was also a part of the optimal path is labeled as the preferred choice. This dataset is intended to teach the model which action was better for a specific subquery given in a given context.</p>
</li>
</ol>
<p><strong>Model Training</strong></p>
<p>The training process uses the synthesized data to fine-tune the LLM in two stages.</p>
<ol>
<li><strong>Imitation Learning</strong>: The synthesized optimal paths dataset is used to teach the model to mimic the optimal reasoning process. It follows a standard next-token prediction objective with a masked loss function. The text of the retrieved document is masked out to prevent the model from learning to simply copy noisy or irrelevant text. This improves the model&rsquo;s ability to learn the high-level reasoning structure rather than superficial textual patterns.</li>
<li><strong>Chain of Calibration</strong>: The model from the previous stage is further fine-tuned using the preference data. This stage follows a Chain of Calibration objective. For each subquery, an objective function encourages the model to assign a higher probability to the &lsquo;winning&rsquo; response (either the parametric answer or the retrieved answer) from the preference pair. This step directly trains the model to make accurate judgements about its own knowledge boundary and whether it needs to retrieve information at each step.</li>
</ol>
<p>The code for DeepRAG is available on <a href="https://github.com/gxy-gxy/DeepRAG" target="_blank" rel="noopener noreferrer">GitHub</a>.</p>
<h3 id="incorporate-knowledge-verbalization-when-retrieval-is-skipped" class="headerLink">
    <a href="#incorporate-knowledge-verbalization-when-retrieval-is-skipped" class="header-mark"></a>Incorporate Knowledge Verbalization When Retrieval Is Skipped</h3><p>Wu et al.<sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup> argue that existing Adaptive RAG (ARAG) methods underutilize the LLM&rsquo;s inherent knowledge. Most of these methods make a binary choice: retrieve from an external source or the model&rsquo;s internal knowledge. However, when retrieval is skipped, they directly use the LLM to generate the final answer. The authors recommend that the model first verbalize its own knowledge before generating a final response. Their framework, <strong>SR-RAG</strong> (<strong>S</strong>elf-<strong>R</strong>outing <strong>RAG</strong>), fine-tunes the LLM to act as a smart router that can dynamically choose between retrieving from an external corpus or verbalizing its own parametric knowledge. It is a generalization of ARAG methods that treats the LLM itself as a first-class knowledge source.</p>
<p>The hypothesis here is that the knowledge verbalization expands the LLM&rsquo;s capacity to answer without retrieval, especially for complex queries where naive retrieval methods may return irrelevant results and a compute-intensive retrieval may return noisy context. Knowledge verbalization helps characterize the LLM&rsquo;s capabilities, allowing for more accurate retrieval decisions and better final answers.</p>
<p>For creating training data, the framework follows both potential knowledge sources for a given question. It retrieves context chunks from the external source (e.g., Wikipedia) and uses the GenRead technique in parallel to prompt the LLM to generate the correct answer. The source (either <code>&lt;Wiki&gt;</code> or <code>&lt;Self&gt;</code>) that provides the majority of the top-scoring snippets is designated as the preferred source for the training example.</p>
<div class="details admonition quote">
    <div class="details-summary admonition-title">
        <span class="icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"/></svg></span>GenRead Prompt for Knowledge Verbalization<span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
    </div>
    <div class="details-content">
        <div class="admonition-content">Generate a background document from Wikipedia to help answer the following question. Directly start with the document content and do not generate a URL.
Question: {question}
Background document:</div></div></div>
<p><figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/sr_rag_comparison.png" title="Traditional Selective RAG vs SR-RAG" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/sr_rag_comparison.png" data-sub-html="<h2>Traditional Selective RAG compared to SR-RAG</h2><p>Traditional Selective RAG vs SR-RAG</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/sr_rag_comparison.png'   alt="Traditional Selective RAG vs SR-RAG"  ></a><figcaption class="image-caption">Traditional Selective RAG compared to SR-RAG</figcaption>
</figure></p>
<p>The LLM is then fine-tuned on this data using a multi-task objective that simultaneously optimizes for three skills (with a separate loss function for each):</p>
<ul>
<li><strong>Source Selection</strong>: Learning to predict the special token for the preferred source (<code>&lt;Wiki&gt;</code> or <code>&lt;Self&gt;</code>) right after the user&rsquo;s query.</li>
<li><strong>Knowledge Verbalization</strong>: If the preferred source is <code>&lt;Self&gt;</code>, the model learns to generate the most helpful knowledge verbalization it produced during the data generation step.</li>
<li><strong>Answer Generation</strong>: Learning to generate the final answer conditioned on the knowledge from the preferred source.</li>
</ul>
<p>The training prompt format is shown below. Here <code>&lt;s&gt;</code> is the special token for source (<code>&lt;Wiki&gt;</code>, <code>&lt;Self&gt;</code>, etc.), <code>{Knowledge}</code> is the verbalized or retrieved text, <code>&lt;EOQ&gt;</code> and <code>&lt;EOK&gt;</code> are the special tokens for marking the end of query and knowledge, respectively.</p>
<div class="details admonition quote">
    <div class="details-summary admonition-title">
        <span class="icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"/></svg></span>SR-RAG Training Prompt<span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
    </div>
    <div class="details-content">
        <div class="admonition-content"><p>Question: {question} Background knowledge: <EOQ></p>
<p>&lt;s&gt; {knowledge} <EOK> Answer: {answer}</p>
</div></div></div>
<p>The authors also propose using Direct Preference Optimization (DPO) to teach the model to prefer its most helpful verbalization over its least helpful ones, refining its own internal knowledge generation capability.</p>
<p><figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/sr_rag_overview.png" title="An overview of SR-RAG" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/sr_rag_overview.png" data-sub-html="<h2>An overview of SR-RAG</h2><p>An overview of SR-RAG</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/sr_rag_overview.png'   alt="An overview of SR-RAG"  ></a><figcaption class="image-caption">An overview of SR-RAG</figcaption>
</figure></p>
<p>During inference, the authors propose using a Nearest Neighbor-Enhanced Source Selection mechanism instead of just picking the source with the highest probability. This method creates a &ldquo;policy datastore&rdquo; ahead of time. This datastore maps the hidden representation of the <code>&lt;EOQ&gt;</code> token for a given question to its preferred knowledge source. When a new query comes in, the model finds the k nearest neighbors from the datastore based on the similarity of their <code>&lt;EOQ&gt;</code> hidden states. The final source selected is based on a threshold applied to the product of the model&rsquo;s predicted probability and the distribution from the nearest neighbors. This inference approach makes the retrieval decision more robust to domain shifts and shifts in LLM abilities due to fine-tuning.</p>
<hr>
<p>The sophistication of the reasoning-based paradigm comes at <strong>substantial costs</strong>. Training these models requires carefully crafted, comprehensive datasets with annotated reasoning chains, which are expensive to create. The model needs to not just learn when to retrieve but also query formulation, information extraction, and reasoning strategies, thereby risking catastrophic forgetting. Inference is also significantly more expensive, as the model engages in multi-turn interactions with the retrieval system, generating intermediate reasoning steps and processing multiple retrieved documents.</p>
<h2 id="summary" class="headerLink">
    <a href="#summary" class="header-mark"></a>Summary</h2><p>This final piece of the Adaptive RAG series covered three paradigms for training-based approaches to adaptive retrieval. We saw how <strong>Gatekeeper models</strong> offer operational simplicity and modularity but make decisions with limited information. If you are using closed-sourced LLMs or need a quick solution, gatekeeper models provide a practical starting point. <strong>Fine-tuned LLMs</strong> leverage the model&rsquo;s internal knowledge for more accurate decisions, but require expensive training and model access. If you have resources to fine-tune and want higher decision quality, this approach has great potential. While <strong>Reasoning-based approaches</strong> handle complex queries through multi-step planning, they also demand substantial training setup and inference costs. If you are tackling complex, multi-hop queries and can afford the investment, these approaches represent the current frontier of adaptive retrieval.</p>
<h2 id="series-conclusion-the-path-from-naive-to-autonomous-rag" class="headerLink">
    <a href="#series-conclusion-the-path-from-naive-to-autonomous-rag" class="header-mark"></a>Series Conclusion: The Path From Naive to Autonomous RAG</h2><pre class="mermaid">graph TD;
    A{Using Closed-Source API?} -->|Yes| B{Latency Critical?}
    A -->|No| C{Have Training Resources?}
    
    B -->|Yes| D[Prompt Engineering<br/>or External Features]
    B -->|No| E[Consistency Sampling<br/>or Lightweight Classifier]
    
    C -->|No| F[Token/Internal State<br/>Probing]
    C -->|Yes| G{Query Complexity?}
    
    G -->|Simple/Single-hop| H[Fine-tune LLM<br/>with binary decision]
    G -->|Complex/Multi-hop| I[Reasoning-Based<br/>Iterative Retrieval]
    
    style D fill:#90EE90
    style E fill:#FFD700
    style F fill:#87CEEB
    style H fill:#FFA07A
    style I fill:#FF6347

</pre>
<p>In this series, we went from identifying the fundamental flaws in naive RAG to exploring progressively more intelligent solutions. Indiscriminate retrieval is costly and often counterproductive, while adaptive retrieval methods help in building reliable and trustworthy systems. We started with simple heuristics, moved to probing model confidence, and have now concluded with methods that empower the model with learned retrieval skills. The choice of which paradigm to adopt depends on the trade-off between implementation complexity, computational cost, and the required depth of reasoning for a given application. There is a clear evolution moving from static, one-size-fits-all towards dynamic, context-aware, autonomous systems. As the research continues and LLMs grow in capability and scale, the line between the &ldquo;retriever&rdquo; and &ldquo;generator&rdquo; will likely blur.</p>
<h2 id="references" class="headerLink">
    <a href="#references" class="header-mark"></a>References</h2><div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Jeong, S., Baek, J., Cho, S., Hwang, S. J., &amp; Park, J. C. (2024). Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity. <em>ArXiv</em>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Cheng, Q., Li, X., Li, S., Zhu, Q., Yin, Z., Shao, Y., Li, L., Sun, T., Yan, H., &amp; Qiu, X. (2024). Unified Active Retrieval for Retrieval Augmented Generation. <em>ArXiv</em>.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Zeng, S., Zhang, J., Li, B., Lin, Y., Zheng, T., Everaert, D., Lu, H., Liu, H., Liu, H., Xing, Y., Cheng, M. X., &amp; Tang, J. (2024). Towards Knowledge Checking in Retrieval-augmented Generation: A Representation Perspective. <em>ArXiv</em>.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Wang, X., Sen, P., Li, R., &amp; Yilmaz, E. (2024). Adaptive Retrieval-Augmented Generation for Conversational Systems. <em>ArXiv</em>.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Zhang, Z., Wang, X., Jiang, Y., Chen, Z., Mu, F., Hu, M., Xie, P., &amp; Huang, F. (2024). Exploring Knowledge Boundaries in Large Language Models for Retrieval Judgment. <em>ArXiv</em>.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Guo, H., Zhu, J., Di, S., Shi, W., Chen, Z., &amp; Xu, J. (2025). DioR: Adaptive Cognitive Detection and Contextual Retrieval Optimization for Dynamic Retrieval-Augmented Generation. <em>ArXiv</em>.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Tang, X., Gao, Q., Li, J., Du, N., Li, Q., &amp; Xie, S. (2024). MBA-RAG: A Bandit Approach for Adaptive Retrieval-Augmented Generation through Question Complexity. <em>ArXiv</em>.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Tan, J., Dou, Z., Zhu, Y., Guo, P., Fang, K., &amp; Wen, J. (2024). Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs. <em>ArXiv</em>.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Asai, A., Wu, Z., Wang, Y., Sil, A., &amp; Hajishirzi, H. (2023). Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. <em>ArXiv</em>.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Ding, H., Pang, L., Wei, Z., Shen, H., &amp; Cheng, X. (2024). Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models. <em>ArXiv</em>.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Wang, R., Zhao, Q., Yan, Y., Zha, D., Chen, Y., Yu, S., Liu, Z., Wang, Y., Wang, S., Han, X., Liu, Z., &amp; Sun, M. (2024). DeepNote: Note-Centric Deep Retrieval-Augmented Generation. <em>ArXiv</em>.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Yu, T., Zhang, S., &amp; Feng, Y. (2024). Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models. <em>ArXiv</em>.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Labruna, T., Campos, J. A., &amp; Azkune, G. (2024). When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively. <em>ArXiv</em>.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Roy, N., Ribeiro, L. F., Blloshmi, R., &amp; Small, K. (2024). Learning When to Retrieve, What to Rewrite, and How to Respond in Conversational QA. <em>ArXiv</em>.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Cocchi, F., Moratelli, N., Cornia, M., Baraldi, L., &amp; Cucchiara, R. (2024). Augmenting Multimodal LLMs with Self-Reflective Tokens for Knowledge-based Visual Question Answering. <em>ArXiv</em>.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>Djean, H. (2024). Let your LLM generate a few tokens and you will reduce the need for retrieval. <em>ArXiv</em>.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Guan, X., Zeng, J., Meng, F., Xin, C., Lu, Y., Lin, H., Han, X., Sun, L., &amp; Zhou, J. (2025). DeepRAG: Thinking to Retrieve Step by Step for Large Language Models. <em>ArXiv</em>.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>Wu, D., Gu, J., Chang, K., &amp; Peng, N. (2025). Self-Routing RAG: Binding Selective Retrieval with Knowledge Verbalization. <em>ArXiv</em>.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
</div>
		
        


<h2>Related Content</h2>
<div class="related-container">
    <div class="related-item-container">
            <div class="related-image">
                <a href="/posts/2025/09/probing-llms-knowledge-boundary/"><img   src='/posts/2025/09/probing-llms-knowledge-boundary/featured-image-preview.webp'    height="200" width="400"></a>
            </div><h2 class="related-title">
                <a href="/posts/2025/09/probing-llms-knowledge-boundary/">Probing LLMs&#39; Knowledge Boundary: Adaptive RAG, Part 3</a>
            </h2>
        </div>
    <div class="related-item-container">
            <div class="related-image">
                <a href="/posts/2025/09/deciding-when-not-to-retrieve/"><img   src='/posts/2025/09/deciding-when-not-to-retrieve/featured-image-preview.webp'    height="200" width="400"></a>
            </div><h2 class="related-title">
                <a href="/posts/2025/09/deciding-when-not-to-retrieve/">Deciding When Not to Retrieve: Adaptive RAG, Part 2</a>
            </h2>
        </div>
    <div class="related-item-container">
            <div class="related-image">
                <a href="/posts/2025/09/problems-with-naive-rag/"><img   src='/posts/2025/09/problems-with-naive-rag/featured-image-preview.webp'    height="200" width="400"></a>
            </div><h2 class="related-title">
                <a href="/posts/2025/09/problems-with-naive-rag/">The Hidden Costs of Naive Retrieval: Adaptive RAG, Part 1</a>
            </h2>
        </div>
    <div class="related-item-container">
            <div class="related-image">
                <a href="/posts/2023/12/towards-ranking-aware-llms/"><img   src='/posts/2023/12/towards-ranking-aware-llms/featured-image-preview.webp'    height="200" width="400"></a>
            </div><h2 class="related-title">
                <a href="/posts/2023/12/towards-ranking-aware-llms/">Strategies for Effective and Efficient Text Ranking Using Large Language Models</a>
            </h2>
        </div>
    <div class="related-item-container">
            <div class="related-image">
                <a href="/posts/2023/12/prompting-llm-for-ranking/"><img   src='/posts/2023/12/prompting-llm-for-ranking/featured-image-preview.webp'    height="200" width="400"></a>
            </div><h2 class="related-title">
                <a href="/posts/2023/12/prompting-llm-for-ranking/">Prompting-based Methods for Text Ranking Using Large Language Models</a>
            </h2>
        </div>
    

</div>


        <script src="https://f.convertkit.com/ckjs/ck.5.js"></script>
      <form action="https://app.convertkit.com/forms/4932644/subscriptions" class="seva-form formkit-form" method="post" data-sv-form="4932644" data-uid="e309c832a6" data-format="inline" data-version="5" data-options="{&quot;settings&quot;:{&quot;after_subscribe&quot;:{&quot;action&quot;:&quot;message&quot;,&quot;success_message&quot;:&quot;Success! Now check your email to confirm your subscription.&quot;,&quot;redirect_url&quot;:&quot;&quot;},&quot;analytics&quot;:{&quot;google&quot;:null,&quot;fathom&quot;:null,&quot;facebook&quot;:null,&quot;segment&quot;:null,&quot;pinterest&quot;:null,&quot;sparkloop&quot;:null,&quot;googletagmanager&quot;:null},&quot;modal&quot;:{&quot;trigger&quot;:&quot;timer&quot;,&quot;scroll_percentage&quot;:null,&quot;timer&quot;:5,&quot;devices&quot;:&quot;all&quot;,&quot;show_once_every&quot;:15},&quot;powered_by&quot;:{&quot;show&quot;:true,&quot;url&quot;:&quot;https://convertkit.com/features/forms?utm_campaign=poweredby&amp;utm_content=form&amp;utm_medium=referral&amp;utm_source=dynamic&quot;},&quot;recaptcha&quot;:{&quot;enabled&quot;:false},&quot;return_visitor&quot;:{&quot;action&quot;:&quot;show&quot;,&quot;custom_content&quot;:&quot;&quot;},&quot;slide_in&quot;:{&quot;display_in&quot;:&quot;bottom_right&quot;,&quot;trigger&quot;:&quot;timer&quot;,&quot;scroll_percentage&quot;:null,&quot;timer&quot;:5,&quot;devices&quot;:&quot;all&quot;,&quot;show_once_every&quot;:15},&quot;sticky_bar&quot;:{&quot;display_in&quot;:&quot;top&quot;,&quot;trigger&quot;:&quot;timer&quot;,&quot;scroll_percentage&quot;:null,&quot;timer&quot;:5,&quot;devices&quot;:&quot;all&quot;,&quot;show_once_every&quot;:15}},&quot;version&quot;:&quot;5&quot;}" min-width="400 500 600 700 800" style="background-color: rgb(249, 250, 251); border-radius: 4px;"><div class="formkit-background" style="opacity: 0.33;"></div><div data-style="minimal"><div class="formkit-header" data-element="header" style="color: rgb(77, 77, 77); font-size: 27px; font-weight: 700;"><h2>Be the First to Know</h2></div><div class="formkit-subheader" data-element="subheader" style="color: rgb(104, 104, 104); font-size: 18px;"><p>Subscribe to get notified when I write a new post.</p></div><ul class="formkit-alert formkit-alert-error" data-element="errors" data-group="alert"></ul><div data-element="fields" data-stacked="false" class="seva-fields formkit-fields"><div class="formkit-field"><input class="formkit-input" name="email_address" aria-label="Email Address" placeholder="Email Address" required="" type="email" style="color: rgb(0, 0, 0); border-color: rgb(227, 227, 227); border-radius: 4px; font-weight: 400;"></div><button data-element="submit" class="formkit-submit formkit-submit" style="color: rgb(255, 255, 255); background-color: rgb(22, 119, 190); border-radius: 4px; font-weight: 400;"><div class="formkit-spinner"><div></div><div></div><div></div></div><span class="">Subscribe</span></button></div><div class="formkit-guarantee" data-element="guarantee" style="color: rgb(77, 77, 77); font-size: 13px; font-weight: 400;"><p>We won't send you spam. Unsubscribe at any time.</p></div><div class="formkit-powered-by-convertkit-container"><a href="https://convertkit.com/features/forms?utm_campaign=poweredby&amp;utm_content=form&amp;utm_medium=referral&amp;utm_source=dynamic" data-element="powered-by" class="formkit-powered-by-convertkit" data-variant="dark" target="_blank" rel="nofollow">Built with ConvertKit</a></div></div><style>.formkit-form[data-uid="e309c832a6"] *{box-sizing:border-box;}.formkit-form[data-uid="e309c832a6"]{-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;}.formkit-form[data-uid="e309c832a6"] legend{border:none;font-size:inherit;margin-bottom:10px;padding:0;position:relative;display:table;}.formkit-form[data-uid="e309c832a6"] fieldset{border:0;padding:0.01em 0 0 0;margin:0;min-width:0;}.formkit-form[data-uid="e309c832a6"] body:not(:-moz-handler-blocked) fieldset{display:table-cell;}.formkit-form[data-uid="e309c832a6"] h1,.formkit-form[data-uid="e309c832a6"] h2,.formkit-form[data-uid="e309c832a6"] h3,.formkit-form[data-uid="e309c832a6"] h4,.formkit-form[data-uid="e309c832a6"] h5,.formkit-form[data-uid="e309c832a6"] h6{color:inherit;font-size:inherit;font-weight:inherit;}.formkit-form[data-uid="e309c832a6"] h2{font-size:1.5em;margin:1em 0;}.formkit-form[data-uid="e309c832a6"] h3{font-size:1.17em;margin:1em 0;}.formkit-form[data-uid="e309c832a6"] p{color:inherit;font-size:inherit;font-weight:inherit;}.formkit-form[data-uid="e309c832a6"] ol:not([template-default]),.formkit-form[data-uid="e309c832a6"] ul:not([template-default]),.formkit-form[data-uid="e309c832a6"] blockquote:not([template-default]){text-align:left;}.formkit-form[data-uid="e309c832a6"] p:not([template-default]),.formkit-form[data-uid="e309c832a6"] hr:not([template-default]),.formkit-form[data-uid="e309c832a6"] blockquote:not([template-default]),.formkit-form[data-uid="e309c832a6"] ol:not([template-default]),.formkit-form[data-uid="e309c832a6"] ul:not([template-default]){color:inherit;font-style:initial;}.formkit-form[data-uid="e309c832a6"] .ordered-list,.formkit-form[data-uid="e309c832a6"] .unordered-list{list-style-position:outside !important;padding-left:1em;}.formkit-form[data-uid="e309c832a6"] .list-item{padding-left:0;}.formkit-form[data-uid="e309c832a6"][data-format="modal"]{display:none;}.formkit-form[data-uid="e309c832a6"][data-format="slide in"]{display:none;}.formkit-form[data-uid="e309c832a6"][data-format="sticky bar"]{display:none;}.formkit-sticky-bar .formkit-form[data-uid="e309c832a6"][data-format="sticky bar"]{display:block;}.formkit-form[data-uid="e309c832a6"] .formkit-input,.formkit-form[data-uid="e309c832a6"] .formkit-select,.formkit-form[data-uid="e309c832a6"] .formkit-checkboxes{width:100%;}.formkit-form[data-uid="e309c832a6"] .formkit-button,.formkit-form[data-uid="e309c832a6"] .formkit-submit{border:0;border-radius:5px;color:#ffffff;cursor:pointer;display:inline-block;text-align:center;font-size:15px;font-weight:500;cursor:pointer;margin-bottom:15px;overflow:hidden;padding:0;position:relative;vertical-align:middle;}.formkit-form[data-uid="e309c832a6"] .formkit-button:hover,.formkit-form[data-uid="e309c832a6"] .formkit-submit:hover,.formkit-form[data-uid="e309c832a6"] .formkit-button:focus,.formkit-form[data-uid="e309c832a6"] .formkit-submit:focus{outline:none;}.formkit-form[data-uid="e309c832a6"] .formkit-button:hover > span,.formkit-form[data-uid="e309c832a6"] .formkit-submit:hover > span,.formkit-form[data-uid="e309c832a6"] .formkit-button:focus > span,.formkit-form[data-uid="e309c832a6"] .formkit-submit:focus > span{background-color:rgba(0,0,0,0.1);}.formkit-form[data-uid="e309c832a6"] .formkit-button > span,.formkit-form[data-uid="e309c832a6"] .formkit-submit > span{display:block;-webkit-transition:all 300ms ease-in-out;transition:all 300ms ease-in-out;padding:12px 24px;}.formkit-form[data-uid="e309c832a6"] .formkit-input{background:#ffffff;font-size:15px;padding:12px;border:1px solid #e3e3e3;-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;line-height:1.4;margin:0;-webkit-transition:border-color ease-out 300ms;transition:border-color ease-out 300ms;}.formkit-form[data-uid="e309c832a6"] .formkit-input:focus{outline:none;border-color:#1677be;-webkit-transition:border-color ease 300ms;transition:border-color ease 300ms;}.formkit-form[data-uid="e309c832a6"] .formkit-input::-webkit-input-placeholder{color:inherit;opacity:0.8;}.formkit-form[data-uid="e309c832a6"] .formkit-input::-moz-placeholder{color:inherit;opacity:0.8;}.formkit-form[data-uid="e309c832a6"] .formkit-input:-ms-input-placeholder{color:inherit;opacity:0.8;}.formkit-form[data-uid="e309c832a6"] .formkit-input::placeholder{color:inherit;opacity:0.8;}.formkit-form[data-uid="e309c832a6"] [data-group="dropdown"]{position:relative;display:inline-block;width:100%;}.formkit-form[data-uid="e309c832a6"] [data-group="dropdown"]::before{content:"";top:calc(50% - 2.5px);right:10px;position:absolute;pointer-events:none;border-color:#4f4f4f transparent transparent transparent;border-style:solid;border-width:6px 6px 0 6px;height:0;width:0;z-index:999;}.formkit-form[data-uid="e309c832a6"] [data-group="dropdown"] select{height:auto;width:100%;cursor:pointer;color:#333333;line-height:1.4;margin-bottom:0;padding:0 6px;-webkit-appearance:none;-moz-appearance:none;appearance:none;font-size:15px;padding:12px;padding-right:25px;border:1px solid #e3e3e3;background:#ffffff;}.formkit-form[data-uid="e309c832a6"] [data-group="dropdown"] select:focus{outline:none;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"]{text-align:left;margin:0;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"]{margin-bottom:10px;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] *{cursor:pointer;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"]:last-of-type{margin-bottom:0;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] input[type="checkbox"]{display:none;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] input[type="checkbox"] + label::after{content:none;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] input[type="checkbox"]:checked + label::after{border-color:#ffffff;content:"";}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] input[type="checkbox"]:checked + label::before{background:#10bf7a;border-color:#10bf7a;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] label{position:relative;display:inline-block;padding-left:28px;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] label::before,.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] label::after{position:absolute;content:"";display:inline-block;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] label::before{height:16px;width:16px;border:1px solid #e3e3e3;background:#ffffff;left:0px;top:3px;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] label::after{height:4px;width:8px;border-left:2px solid #4d4d4d;border-bottom:2px solid #4d4d4d;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);left:4px;top:8px;}.formkit-form[data-uid="e309c832a6"] .formkit-alert{background:#f9fafb;border:1px solid #e3e3e3;border-radius:5px;-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;list-style:none;margin:25px auto;padding:12px;text-align:center;width:100%;}.formkit-form[data-uid="e309c832a6"] .formkit-alert:empty{display:none;}.formkit-form[data-uid="e309c832a6"] .formkit-alert-success{background:#d3fbeb;border-color:#10bf7a;color:#0c905c;}.formkit-form[data-uid="e309c832a6"] .formkit-alert-error{background:#fde8e2;border-color:#f2643b;color:#ea4110;}.formkit-form[data-uid="e309c832a6"] .formkit-spinner{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:0px;width:0px;margin:0 auto;position:absolute;top:0;left:0;right:0;width:0px;overflow:hidden;text-align:center;-webkit-transition:all 300ms ease-in-out;transition:all 300ms ease-in-out;}.formkit-form[data-uid="e309c832a6"] .formkit-spinner > div{margin:auto;width:12px;height:12px;background-color:#fff;opacity:0.3;border-radius:100%;display:inline-block;-webkit-animation:formkit-bouncedelay-formkit-form-data-uid-e309c832a6- 1.4s infinite ease-in-out both;animation:formkit-bouncedelay-formkit-form-data-uid-e309c832a6- 1.4s infinite ease-in-out both;}.formkit-form[data-uid="e309c832a6"] .formkit-spinner > div:nth-child(1){-webkit-animation-delay:-0.32s;animation-delay:-0.32s;}.formkit-form[data-uid="e309c832a6"] .formkit-spinner > div:nth-child(2){-webkit-animation-delay:-0.16s;animation-delay:-0.16s;}.formkit-form[data-uid="e309c832a6"] .formkit-submit[data-active] .formkit-spinner{opacity:1;height:100%;width:50px;}.formkit-form[data-uid="e309c832a6"] .formkit-submit[data-active] .formkit-spinner ~ span{opacity:0;}.formkit-form[data-uid="e309c832a6"] .formkit-powered-by[data-active="false"]{opacity:0.35;}.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit-container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;width:100%;z-index:5;margin:10px 0;position:relative;}.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit-container[data-active="false"]{opacity:0.35;}.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:#ffffff;border:1px solid #dde2e7;border-radius:4px;color:#373f45;cursor:pointer;display:block;height:36px;margin:0 auto;opacity:0.95;padding:0;-webkit-text-decoration:none;text-decoration:none;text-indent:100%;-webkit-transition:ease-in-out all 200ms;transition:ease-in-out all 200ms;white-space:nowrap;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:190px;background-repeat:no-repeat;background-position:center;background-image:url("data:image/svg+xml;charset=utf8,%3Csvg width='162' height='20' viewBox='0 0 162 20' fill='none' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='M83.0561 15.2457C86.675 15.2457 89.4722 12.5154 89.4722 9.14749C89.4722 5.99211 86.8443 4.06563 85.1038 4.06563C82.6801 4.06563 80.7373 5.76407 80.4605 8.28551C80.4092 8.75244 80.0387 9.14403 79.5686 9.14069C78.7871 9.13509 77.6507 9.12841 76.9314 9.13092C76.6217 9.13199 76.3658 8.88106 76.381 8.57196C76.4895 6.38513 77.2218 4.3404 78.618 2.76974C80.1695 1.02445 82.4289 0 85.1038 0C89.5979 0 93.8406 4.07791 93.8406 9.14749C93.8406 14.7608 89.1832 19.3113 83.1517 19.3113C78.8502 19.3113 74.5179 16.5041 73.0053 12.5795C72.9999 12.565 72.9986 12.5492 73.0015 12.534C73.0218 12.4179 73.0617 12.3118 73.1011 12.2074C73.1583 12.0555 73.2143 11.907 73.2062 11.7359L73.18 11.1892C73.174 11.0569 73.2075 10.9258 73.2764 10.8127C73.3452 10.6995 73.4463 10.6094 73.5666 10.554L73.7852 10.4523C73.9077 10.3957 74.0148 10.3105 74.0976 10.204C74.1803 10.0974 74.2363 9.97252 74.2608 9.83983C74.3341 9.43894 74.6865 9.14749 75.0979 9.14749C75.7404 9.14749 76.299 9.57412 76.5088 10.1806C77.5188 13.1 79.1245 15.2457 83.0561 15.2457Z' fill='%23373F45'/%3E%3Cpath d='M155.758 6.91365C155.028 6.91365 154.804 6.47916 154.804 5.98857C154.804 5.46997 154.986 5.06348 155.758 5.06348C156.53 5.06348 156.712 5.46997 156.712 5.98857C156.712 6.47905 156.516 6.91365 155.758 6.91365ZM142.441 12.9304V9.32833L141.415 9.32323V8.90392C141.415 8.44719 141.786 8.07758 142.244 8.07986L142.441 8.08095V6.55306L144.082 6.09057V8.08073H145.569V8.50416C145.569 8.61242 145.548 8.71961 145.506 8.81961C145.465 8.91961 145.404 9.01047 145.328 9.08699C145.251 9.16351 145.16 9.2242 145.06 9.26559C144.96 9.30698 144.853 9.32826 144.745 9.32822H144.082V12.7201C144.082 13.2423 144.378 13.4256 144.76 13.4887C145.209 13.5629 145.583 13.888 145.583 14.343V14.9626C144.029 14.9626 142.441 14.8942 142.441 12.9304Z' fill='%23373F45'/%3E%3Cpath d='M110.058 7.92554C108.417 7.88344 106.396 8.92062 106.396 11.5137C106.396 14.0646 108.417 15.0738 110.058 15.0318C111.742 15.0738 113.748 14.0646 113.748 11.5137C113.748 8.92062 111.742 7.88344 110.058 7.92554ZM110.07 13.7586C108.878 13.7586 108.032 12.8905 108.032 11.461C108.032 10.1013 108.878 9.20569 110.071 9.20569C111.263 9.20569 112.101 10.0995 112.101 11.459C112.101 12.8887 111.263 13.7586 110.07 13.7586Z' fill='%23373F45'/%3E%3Cpath d='M118.06 7.94098C119.491 7.94098 120.978 8.33337 120.978 11.1366V14.893H120.063C119.608 14.893 119.238 14.524 119.238 14.0689V10.9965C119.238 9.66506 118.747 9.16047 117.891 9.16047C117.414 9.16047 116.797 9.52486 116.502 9.81915V14.069C116.502 14.1773 116.481 14.2845 116.44 14.3845C116.398 14.4845 116.337 14.5753 116.261 14.6519C116.184 14.7284 116.093 14.7891 115.993 14.8305C115.893 14.8719 115.786 14.8931 115.678 14.8931H114.847V8.10918H115.773C115.932 8.10914 116.087 8.16315 116.212 8.26242C116.337 8.36168 116.424 8.50033 116.46 8.65577C116.881 8.19328 117.428 7.94098 118.06 7.94098ZM122.854 8.09713C123.024 8.09708 123.19 8.1496 123.329 8.2475C123.468 8.34541 123.574 8.48391 123.631 8.64405L125.133 12.8486L126.635 8.64415C126.692 8.48402 126.798 8.34551 126.937 8.2476C127.076 8.1497 127.242 8.09718 127.412 8.09724H128.598L126.152 14.3567C126.091 14.5112 125.986 14.6439 125.849 14.7374C125.711 14.831 125.549 14.881 125.383 14.8809H124.333L121.668 8.09713H122.854Z' fill='%23373F45'/%3E%3Cpath d='M135.085 14.5514C134.566 14.7616 133.513 15.0416 132.418 15.0416C130.496 15.0416 129.024 13.9345 129.024 11.4396C129.024 9.19701 130.451 7.99792 132.191 7.99792C134.338 7.99792 135.254 9.4378 135.158 11.3979C135.139 11.8029 134.786 12.0983 134.38 12.0983H130.679C130.763 13.1916 131.562 13.7662 132.615 13.7662C133.028 13.7662 133.462 13.7452 133.983 13.6481C134.535 13.545 135.085 13.9375 135.085 14.4985V14.5514ZM133.673 10.949C133.785 9.87621 133.061 9.28752 132.191 9.28752C131.321 9.28752 130.734 9.93979 130.679 10.9489L133.673 10.949Z' fill='%23373F45'/%3E%3Cpath d='M137.345 8.11122C137.497 8.11118 137.645 8.16229 137.765 8.25635C137.884 8.35041 137.969 8.48197 138.005 8.62993C138.566 8.20932 139.268 7.94303 139.759 7.94303C139.801 7.94303 140.068 7.94303 140.489 7.99913V8.7265C140.489 9.11748 140.15 9.4147 139.759 9.4147C139.31 9.4147 138.651 9.5829 138.131 9.8773V14.8951H136.462V8.11112L137.345 8.11122ZM156.6 14.0508V8.09104H155.769C155.314 8.09104 154.944 8.45999 154.944 8.9151V14.8748H155.775C156.23 14.8748 156.6 14.5058 156.6 14.0508ZM158.857 12.9447V9.34254H157.749V8.91912C157.749 8.46401 158.118 8.09506 158.574 8.09506H158.857V6.56739L160.499 6.10479V8.09506H161.986V8.51848C161.986 8.97359 161.617 9.34254 161.161 9.34254H160.499V12.7345C160.499 13.2566 160.795 13.44 161.177 13.503C161.626 13.5774 162 13.9024 162 14.3574V14.977C160.446 14.977 158.857 14.9086 158.857 12.9447ZM98.1929 10.1124C98.2033 6.94046 100.598 5.16809 102.895 5.16809C104.171 5.16809 105.342 5.44285 106.304 6.12953L105.914 6.6631C105.654 7.02011 105.16 7.16194 104.749 6.99949C104.169 6.7702 103.622 6.7218 103.215 6.7218C101.335 6.7218 99.9169 7.92849 99.9068 10.1123C99.9169 12.2959 101.335 13.5201 103.215 13.5201C103.622 13.5201 104.169 13.4717 104.749 13.2424C105.16 13.0799 105.654 13.2046 105.914 13.5615L106.304 14.0952C105.342 14.7819 104.171 15.0566 102.895 15.0566C100.598 15.0566 98.2033 13.2842 98.1929 10.1124ZM147.619 5.21768C148.074 5.21768 148.444 5.58663 148.444 6.04174V9.81968L151.82 5.58131C151.897 5.47733 151.997 5.39282 152.112 5.3346C152.227 5.27638 152.355 5.24607 152.484 5.24611H153.984L150.166 10.0615L153.984 14.8749H152.484C152.355 14.8749 152.227 14.8446 152.112 14.7864C151.997 14.7281 151.897 14.6436 151.82 14.5397L148.444 10.3025V14.0508C148.444 14.5059 148.074 14.8749 147.619 14.8749H146.746V5.21768H147.619Z' fill='%23373F45'/%3E%3Cpath d='M0.773438 6.5752H2.68066C3.56543 6.5752 4.2041 6.7041 4.59668 6.96191C4.99219 7.21973 5.18994 7.62695 5.18994 8.18359C5.18994 8.55859 5.09326 8.87061 4.8999 9.11963C4.70654 9.36865 4.42822 9.52539 4.06494 9.58984V9.63379C4.51611 9.71875 4.84717 9.88721 5.05811 10.1392C5.27197 10.3882 5.37891 10.7266 5.37891 11.1543C5.37891 11.7314 5.17676 12.1841 4.77246 12.5122C4.37109 12.8374 3.81152 13 3.09375 13H0.773438V6.5752ZM1.82373 9.22949H2.83447C3.27393 9.22949 3.59473 9.16064 3.79688 9.02295C3.99902 8.88232 4.1001 8.64502 4.1001 8.31104C4.1001 8.00928 3.99023 7.79102 3.77051 7.65625C3.55371 7.52148 3.20801 7.4541 2.7334 7.4541H1.82373V9.22949ZM1.82373 10.082V12.1167H2.93994C3.37939 12.1167 3.71045 12.0332 3.93311 11.8662C4.15869 11.6963 4.27148 11.4297 4.27148 11.0664C4.27148 10.7324 4.15723 10.4849 3.92871 10.3237C3.7002 10.1626 3.35303 10.082 2.88721 10.082H1.82373Z' fill='%23373F45'/%3E%3Cpath d='M13.011 6.5752V10.7324C13.011 11.207 12.9084 11.623 12.7034 11.9805C12.5012 12.335 12.2068 12.6089 11.8201 12.8022C11.4363 12.9927 10.9763 13.0879 10.4402 13.0879C9.6433 13.0879 9.02368 12.877 8.5813 12.4551C8.13892 12.0332 7.91772 11.4531 7.91772 10.7148V6.5752H8.9724V10.6401C8.9724 11.1704 9.09546 11.5615 9.34155 11.8135C9.58765 12.0654 9.96557 12.1914 10.4753 12.1914C11.4656 12.1914 11.9607 11.6714 11.9607 10.6313V6.5752H13.011Z' fill='%23373F45'/%3E%3Cpath d='M15.9146 13V6.5752H16.9649V13H15.9146Z' fill='%23373F45'/%3E%3Cpath d='M19.9255 13V6.5752H20.9758V12.0991H23.696V13H19.9255Z' fill='%23373F45'/%3E%3Cpath d='M28.2828 13H27.2325V7.47607H25.3428V6.5752H30.1724V7.47607H28.2828V13Z' fill='%23373F45'/%3E%3Cpath d='M41.9472 13H40.8046L39.7148 9.16796C39.6679 9.00097 39.6093 8.76074 39.539 8.44727C39.4687 8.13086 39.4262 7.91113 39.4116 7.78809C39.3823 7.97559 39.3339 8.21875 39.2665 8.51758C39.2021 8.81641 39.1479 9.03905 39.1039 9.18554L38.0405 13H36.8979L36.0673 9.7832L35.2236 6.5752H36.2958L37.2143 10.3193C37.3578 10.9199 37.4604 11.4502 37.5219 11.9102C37.5541 11.6611 37.6025 11.3828 37.6669 11.0752C37.7314 10.7676 37.79 10.5186 37.8427 10.3281L38.8886 6.5752H39.9301L41.0024 10.3457C41.1049 10.6943 41.2133 11.2158 41.3276 11.9102C41.3715 11.4912 41.477 10.958 41.644 10.3105L42.558 6.5752H43.6215L41.9472 13Z' fill='%23373F45'/%3E%3Cpath d='M45.7957 13V6.5752H46.846V13H45.7957Z' fill='%23373F45'/%3E%3Cpath d='M52.0258 13H50.9755V7.47607H49.0859V6.5752H53.9155V7.47607H52.0258V13Z' fill='%23373F45'/%3E%3Cpath d='M61.2312 13H60.1765V10.104H57.2146V13H56.1643V6.5752H57.2146V9.20312H60.1765V6.5752H61.2312V13Z' fill='%23373F45'/%3E%3C/svg%3E");}.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit:hover,.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit:focus{background-color:#ffffff;-webkit-transform:scale(1.025) perspective(1px);-ms-transform:scale(1.025) perspective(1px);transform:scale(1.025) perspective(1px);opacity:1;}.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit[data-variant="dark"],.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit[data-variant="light"]{background-color:transparent;border-color:transparent;width:166px;}.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit[data-variant="light"]{color:#ffffff;background-image:url("data:image/svg+xml;charset=utf8,%3Csvg width='162' height='20' viewBox='0 0 162 20' fill='none' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='M83.0561 15.2457C86.675 15.2457 89.4722 12.5154 89.4722 9.14749C89.4722 5.99211 86.8443 4.06563 85.1038 4.06563C82.6801 4.06563 80.7373 5.76407 80.4605 8.28551C80.4092 8.75244 80.0387 9.14403 79.5686 9.14069C78.7871 9.13509 77.6507 9.12841 76.9314 9.13092C76.6217 9.13199 76.3658 8.88106 76.381 8.57196C76.4895 6.38513 77.2218 4.3404 78.618 2.76974C80.1695 1.02445 82.4289 0 85.1038 0C89.5979 0 93.8406 4.07791 93.8406 9.14749C93.8406 14.7608 89.1832 19.3113 83.1517 19.3113C78.8502 19.3113 74.5179 16.5041 73.0053 12.5795C72.9999 12.565 72.9986 12.5492 73.0015 12.534C73.0218 12.4179 73.0617 12.3118 73.1011 12.2074C73.1583 12.0555 73.2143 11.907 73.2062 11.7359L73.18 11.1892C73.174 11.0569 73.2075 10.9258 73.2764 10.8127C73.3452 10.6995 73.4463 10.6094 73.5666 10.554L73.7852 10.4523C73.9077 10.3957 74.0148 10.3105 74.0976 10.204C74.1803 10.0974 74.2363 9.97252 74.2608 9.83983C74.3341 9.43894 74.6865 9.14749 75.0979 9.14749C75.7404 9.14749 76.299 9.57412 76.5088 10.1806C77.5188 13.1 79.1245 15.2457 83.0561 15.2457Z' fill='white'/%3E%3Cpath d='M155.758 6.91365C155.028 6.91365 154.804 6.47916 154.804 5.98857C154.804 5.46997 154.986 5.06348 155.758 5.06348C156.53 5.06348 156.712 5.46997 156.712 5.98857C156.712 6.47905 156.516 6.91365 155.758 6.91365ZM142.441 12.9304V9.32833L141.415 9.32323V8.90392C141.415 8.44719 141.786 8.07758 142.244 8.07986L142.441 8.08095V6.55306L144.082 6.09057V8.08073H145.569V8.50416C145.569 8.61242 145.548 8.71961 145.506 8.81961C145.465 8.91961 145.404 9.01047 145.328 9.08699C145.251 9.16351 145.16 9.2242 145.06 9.26559C144.96 9.30698 144.853 9.32826 144.745 9.32822H144.082V12.7201C144.082 13.2423 144.378 13.4256 144.76 13.4887C145.209 13.5629 145.583 13.888 145.583 14.343V14.9626C144.029 14.9626 142.441 14.8942 142.441 12.9304Z' fill='white'/%3E%3Cpath d='M110.058 7.92554C108.417 7.88344 106.396 8.92062 106.396 11.5137C106.396 14.0646 108.417 15.0738 110.058 15.0318C111.742 15.0738 113.748 14.0646 113.748 11.5137C113.748 8.92062 111.742 7.88344 110.058 7.92554ZM110.07 13.7586C108.878 13.7586 108.032 12.8905 108.032 11.461C108.032 10.1013 108.878 9.20569 110.071 9.20569C111.263 9.20569 112.101 10.0995 112.101 11.459C112.101 12.8887 111.263 13.7586 110.07 13.7586Z' fill='white'/%3E%3Cpath d='M118.06 7.94098C119.491 7.94098 120.978 8.33337 120.978 11.1366V14.893H120.063C119.608 14.893 119.238 14.524 119.238 14.0689V10.9965C119.238 9.66506 118.747 9.16047 117.891 9.16047C117.414 9.16047 116.797 9.52486 116.502 9.81915V14.069C116.502 14.1773 116.481 14.2845 116.44 14.3845C116.398 14.4845 116.337 14.5753 116.261 14.6519C116.184 14.7284 116.093 14.7891 115.993 14.8305C115.893 14.8719 115.786 14.8931 115.678 14.8931H114.847V8.10918H115.773C115.932 8.10914 116.087 8.16315 116.212 8.26242C116.337 8.36168 116.424 8.50033 116.46 8.65577C116.881 8.19328 117.428 7.94098 118.06 7.94098ZM122.854 8.09713C123.024 8.09708 123.19 8.1496 123.329 8.2475C123.468 8.34541 123.574 8.48391 123.631 8.64405L125.133 12.8486L126.635 8.64415C126.692 8.48402 126.798 8.34551 126.937 8.2476C127.076 8.1497 127.242 8.09718 127.412 8.09724H128.598L126.152 14.3567C126.091 14.5112 125.986 14.6439 125.849 14.7374C125.711 14.831 125.549 14.881 125.383 14.8809H124.333L121.668 8.09713H122.854Z' fill='white'/%3E%3Cpath d='M135.085 14.5514C134.566 14.7616 133.513 15.0416 132.418 15.0416C130.496 15.0416 129.024 13.9345 129.024 11.4396C129.024 9.19701 130.451 7.99792 132.191 7.99792C134.338 7.99792 135.254 9.4378 135.158 11.3979C135.139 11.8029 134.786 12.0983 134.38 12.0983H130.679C130.763 13.1916 131.562 13.7662 132.615 13.7662C133.028 13.7662 133.462 13.7452 133.983 13.6481C134.535 13.545 135.085 13.9375 135.085 14.4985V14.5514ZM133.673 10.949C133.785 9.87621 133.061 9.28752 132.191 9.28752C131.321 9.28752 130.734 9.93979 130.679 10.9489L133.673 10.949Z' fill='white'/%3E%3Cpath d='M137.345 8.11122C137.497 8.11118 137.645 8.16229 137.765 8.25635C137.884 8.35041 137.969 8.48197 138.005 8.62993C138.566 8.20932 139.268 7.94303 139.759 7.94303C139.801 7.94303 140.068 7.94303 140.489 7.99913V8.7265C140.489 9.11748 140.15 9.4147 139.759 9.4147C139.31 9.4147 138.651 9.5829 138.131 9.8773V14.8951H136.462V8.11112L137.345 8.11122ZM156.6 14.0508V8.09104H155.769C155.314 8.09104 154.944 8.45999 154.944 8.9151V14.8748H155.775C156.23 14.8748 156.6 14.5058 156.6 14.0508ZM158.857 12.9447V9.34254H157.749V8.91912C157.749 8.46401 158.118 8.09506 158.574 8.09506H158.857V6.56739L160.499 6.10479V8.09506H161.986V8.51848C161.986 8.97359 161.617 9.34254 161.161 9.34254H160.499V12.7345C160.499 13.2566 160.795 13.44 161.177 13.503C161.626 13.5774 162 13.9024 162 14.3574V14.977C160.446 14.977 158.857 14.9086 158.857 12.9447ZM98.1929 10.1124C98.2033 6.94046 100.598 5.16809 102.895 5.16809C104.171 5.16809 105.342 5.44285 106.304 6.12953L105.914 6.6631C105.654 7.02011 105.16 7.16194 104.749 6.99949C104.169 6.7702 103.622 6.7218 103.215 6.7218C101.335 6.7218 99.9169 7.92849 99.9068 10.1123C99.9169 12.2959 101.335 13.5201 103.215 13.5201C103.622 13.5201 104.169 13.4717 104.749 13.2424C105.16 13.0799 105.654 13.2046 105.914 13.5615L106.304 14.0952C105.342 14.7819 104.171 15.0566 102.895 15.0566C100.598 15.0566 98.2033 13.2842 98.1929 10.1124ZM147.619 5.21768C148.074 5.21768 148.444 5.58663 148.444 6.04174V9.81968L151.82 5.58131C151.897 5.47733 151.997 5.39282 152.112 5.3346C152.227 5.27638 152.355 5.24607 152.484 5.24611H153.984L150.166 10.0615L153.984 14.8749H152.484C152.355 14.8749 152.227 14.8446 152.112 14.7864C151.997 14.7281 151.897 14.6436 151.82 14.5397L148.444 10.3025V14.0508C148.444 14.5059 148.074 14.8749 147.619 14.8749H146.746V5.21768H147.619Z' fill='white'/%3E%3Cpath d='M0.773438 6.5752H2.68066C3.56543 6.5752 4.2041 6.7041 4.59668 6.96191C4.99219 7.21973 5.18994 7.62695 5.18994 8.18359C5.18994 8.55859 5.09326 8.87061 4.8999 9.11963C4.70654 9.36865 4.42822 9.52539 4.06494 9.58984V9.63379C4.51611 9.71875 4.84717 9.88721 5.05811 10.1392C5.27197 10.3882 5.37891 10.7266 5.37891 11.1543C5.37891 11.7314 5.17676 12.1841 4.77246 12.5122C4.37109 12.8374 3.81152 13 3.09375 13H0.773438V6.5752ZM1.82373 9.22949H2.83447C3.27393 9.22949 3.59473 9.16064 3.79688 9.02295C3.99902 8.88232 4.1001 8.64502 4.1001 8.31104C4.1001 8.00928 3.99023 7.79102 3.77051 7.65625C3.55371 7.52148 3.20801 7.4541 2.7334 7.4541H1.82373V9.22949ZM1.82373 10.082V12.1167H2.93994C3.37939 12.1167 3.71045 12.0332 3.93311 11.8662C4.15869 11.6963 4.27148 11.4297 4.27148 11.0664C4.27148 10.7324 4.15723 10.4849 3.92871 10.3237C3.7002 10.1626 3.35303 10.082 2.88721 10.082H1.82373Z' fill='white'/%3E%3Cpath d='M13.011 6.5752V10.7324C13.011 11.207 12.9084 11.623 12.7034 11.9805C12.5012 12.335 12.2068 12.6089 11.8201 12.8022C11.4363 12.9927 10.9763 13.0879 10.4402 13.0879C9.6433 13.0879 9.02368 12.877 8.5813 12.4551C8.13892 12.0332 7.91772 11.4531 7.91772 10.7148V6.5752H8.9724V10.6401C8.9724 11.1704 9.09546 11.5615 9.34155 11.8135C9.58765 12.0654 9.96557 12.1914 10.4753 12.1914C11.4656 12.1914 11.9607 11.6714 11.9607 10.6313V6.5752H13.011Z' fill='white'/%3E%3Cpath d='M15.9146 13V6.5752H16.9649V13H15.9146Z' fill='white'/%3E%3Cpath d='M19.9255 13V6.5752H20.9758V12.0991H23.696V13H19.9255Z' fill='white'/%3E%3Cpath d='M28.2828 13H27.2325V7.47607H25.3428V6.5752H30.1724V7.47607H28.2828V13Z' fill='white'/%3E%3Cpath d='M41.9472 13H40.8046L39.7148 9.16796C39.6679 9.00097 39.6093 8.76074 39.539 8.44727C39.4687 8.13086 39.4262 7.91113 39.4116 7.78809C39.3823 7.97559 39.3339 8.21875 39.2665 8.51758C39.2021 8.81641 39.1479 9.03905 39.1039 9.18554L38.0405 13H36.8979L36.0673 9.7832L35.2236 6.5752H36.2958L37.2143 10.3193C37.3578 10.9199 37.4604 11.4502 37.5219 11.9102C37.5541 11.6611 37.6025 11.3828 37.6669 11.0752C37.7314 10.7676 37.79 10.5186 37.8427 10.3281L38.8886 6.5752H39.9301L41.0024 10.3457C41.1049 10.6943 41.2133 11.2158 41.3276 11.9102C41.3715 11.4912 41.477 10.958 41.644 10.3105L42.558 6.5752H43.6215L41.9472 13Z' fill='white'/%3E%3Cpath d='M45.7957 13V6.5752H46.846V13H45.7957Z' fill='white'/%3E%3Cpath d='M52.0258 13H50.9755V7.47607H49.0859V6.5752H53.9155V7.47607H52.0258V13Z' fill='white'/%3E%3Cpath d='M61.2312 13H60.1765V10.104H57.2146V13H56.1643V6.5752H57.2146V9.20312H60.1765V6.5752H61.2312V13Z' fill='white'/%3E%3C/svg%3E");}@-webkit-keyframes formkit-bouncedelay-formkit-form-data-uid-e309c832a6-{0%,80%,100%{-webkit-transform:scale(0);-ms-transform:scale(0);transform:scale(0);}40%{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}@keyframes formkit-bouncedelay-formkit-form-data-uid-e309c832a6-{0%,80%,100%{-webkit-transform:scale(0);-ms-transform:scale(0);transform:scale(0);}40%{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}.formkit-form[data-uid="e309c832a6"] blockquote{padding:10px 20px;margin:0 0 20px;border-left:5px solid #e1e1e1;}.formkit-form[data-uid="e309c832a6"] .seva-custom-content{padding:15px;font-size:16px;color:#fff;mix-blend-mode:difference;}.formkit-form[data-uid="e309c832a6"] .formkit-modal.guard{max-width:420px;width:100%;} .formkit-form[data-uid="e309c832a6"]{border:1px solid #e3e3e3;max-width:700px;position:relative;overflow:hidden;}.formkit-form[data-uid="e309c832a6"] .formkit-background{width:100%;height:100%;position:absolute;top:0;left:0;background-size:cover;background-position:center;opacity:0.3;}.formkit-form[data-uid="e309c832a6"] [data-style="minimal"]{padding:20px;width:100%;position:relative;}.formkit-form[data-uid="e309c832a6"] .formkit-header{margin:0 0 27px 0;text-align:center;}.formkit-form[data-uid="e309c832a6"] .formkit-subheader{margin:18px 0;text-align:center;}.formkit-form[data-uid="e309c832a6"] .formkit-guarantee{font-size:13px;margin:10px 0 15px 0;text-align:center;}.formkit-form[data-uid="e309c832a6"] .formkit-guarantee > p{margin:0;}.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit-container{margin-bottom:0;}.formkit-form[data-uid="e309c832a6"] .formkit-fields{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;margin:25px auto 0 auto;}.formkit-form[data-uid="e309c832a6"] .formkit-field{min-width:220px;}.formkit-form[data-uid="e309c832a6"] .formkit-field,.formkit-form[data-uid="e309c832a6"] .formkit-submit{margin:0 0 15px 0;-webkit-flex:1 0 100%;-ms-flex:1 0 100%;flex:1 0 100%;}.formkit-form[data-uid="e309c832a6"][min-width~="600"] [data-style="minimal"]{padding:40px;}.formkit-form[data-uid="e309c832a6"][min-width~="600"] .formkit-fields[data-stacked="false"]{margin-left:-5px;margin-right:-5px;}.formkit-form[data-uid="e309c832a6"][min-width~="600"] .formkit-fields[data-stacked="false"] .formkit-field,.formkit-form[data-uid="e309c832a6"][min-width~="600"] .formkit-fields[data-stacked="false"] .formkit-submit{margin:0 5px 15px 5px;}.formkit-form[data-uid="e309c832a6"][min-width~="600"] .formkit-fields[data-stacked="false"] .formkit-field{-webkit-flex:100 1 auto;-ms-flex:100 1 auto;flex:100 1 auto;}.formkit-form[data-uid="e309c832a6"][min-width~="600"] .formkit-fields[data-stacked="false"] .formkit-submit{-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;} </style></form>

		<div class="sponsor print:tw-hidden">
        <div class="sponsor-avatar"></div><p class="sponsor-bio"><em>Did you find this article helpful?</em></p><div class="sponsor-custom"><script type="text/javascript" src="https://cdnjs.buymeacoffee.com/1.0.0/button.prod.min.js" data-name="bmc-button" data-slug="reachsumit" data-color="#FFDD00" data-emoji=""  data-font="Cookie" data-text="Buy me a coffee" data-outline-color="#000000" data-font-color="#000000" data-coffee-color="#ffffff" ></script></div></div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2025-10-05</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line print:!tw-hidden">
            <div class="post-info-md"></div>
            <div class="post-info-share"><button title="Share on Facebook" data-sharer="facebook" data-url="https://blog.reachsumit.com/posts/2025/10/learning-to-retrieve/" data-hashtag="retrieval"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14 0 55.52 4.84 55.52 4.84v61h-31.28c-30.8 0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"/></svg></button><button title="Share on Linkedin" data-sharer="linkedin" data-url="https://blog.reachsumit.com/posts/2025/10/learning-to-retrieve/"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg></button><button title="Share on WhatsApp" data-sharer="whatsapp" data-url="https://blog.reachsumit.com/posts/2025/10/learning-to-retrieve/" data-title=" Teaching Models to Decide When to Retrieve: Adaptive RAG, Part 4" data-web><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M380.9 97.1C339 55.1 283.2 32 223.9 32c-122.4 0-222 99.6-222 222 0 39.1 10.2 77.3 29.6 111L0 480l117.7-30.9c32.4 17.7 68.9 27 106.1 27h.1c122.3 0 224.1-99.6 224.1-222 0-59.3-25.2-115-67.1-157zm-157 341.6c-33.2 0-65.7-8.9-94-25.7l-6.7-4-69.8 18.3L72 359.2l-4.4-7c-18.5-29.4-28.2-63.3-28.2-98.2 0-101.7 82.8-184.5 184.6-184.5 49.3 0 95.6 19.2 130.4 54.1 34.8 34.9 56.2 81.2 56.1 130.5 0 101.8-84.9 184.6-186.6 184.6zm101.2-138.2c-5.5-2.8-32.8-16.2-37.9-18-5.1-1.9-8.8-2.8-12.5 2.8-3.7 5.6-14.3 18-17.6 21.8-3.2 3.7-6.5 4.2-12 1.4-32.6-16.3-54-29.1-75.5-66-5.7-9.8 5.7-9.1 16.3-30.3 1.8-3.7.9-6.9-.5-9.7-1.4-2.8-12.5-30.1-17.1-41.2-4.5-10.8-9.1-9.3-12.5-9.5-3.2-.2-6.9-.2-10.6-.2-3.7 0-9.7 1.4-14.8 6.9-5.1 5.6-19.4 19-19.4 46.3 0 27.3 19.9 53.7 22.6 57.4 2.8 3.7 39.1 59.7 94.8 83.8 35.2 15.2 49 16.5 66.6 13.9 10.7-1.6 32.8-13.4 37.4-26.4 4.6-13 4.6-24.1 3.2-26.4-1.3-2.5-5-3.9-10.5-6.6z"/></svg></button><button title="Share on Hacker News" data-sharer="hackernews" data-url="https://blog.reachsumit.com/posts/2025/10/learning-to-retrieve/" data-title=" Teaching Models to Decide When to Retrieve: Adaptive RAG, Part 4"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M0 32v448h448V32H0zm21.2 197.2H21c.1-.1.2-.3.3-.4 0 .1 0 .3-.1.4zm218 53.9V384h-31.4V281.3L128 128h37.3c52.5 98.3 49.2 101.2 59.3 125.6 12.3-27 5.8-24.4 60.6-125.6H320l-80.8 155.1z"/></svg></button><button title="Share on Reddit" data-sharer="reddit" data-url="https://blog.reachsumit.com/posts/2025/10/learning-to-retrieve/"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M201.5 305.5c-13.8 0-24.9-11.1-24.9-24.6 0-13.8 11.1-24.9 24.9-24.9 13.6 0 24.6 11.1 24.6 24.9 0 13.6-11.1 24.6-24.6 24.6zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-132.3-41.2c-9.4 0-17.7 3.9-23.8 10-22.4-15.5-52.6-25.5-86.1-26.6l17.4-78.3 55.4 12.5c0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.3 24.9-24.9s-11.1-24.9-24.9-24.9c-9.7 0-18 5.8-22.1 13.8l-61.2-13.6c-3-.8-6.1 1.4-6.9 4.4l-19.1 86.4c-33.2 1.4-63.1 11.3-85.5 26.8-6.1-6.4-14.7-10.2-24.1-10.2-34.9 0-46.3 46.9-14.4 62.8-1.1 5-1.7 10.2-1.7 15.5 0 52.6 59.2 95.2 132 95.2 73.1 0 132.3-42.6 132.3-95.2 0-5.3-.6-10.8-1.9-15.8 31.3-16 19.8-62.5-14.9-62.5zM302.8 331c-18.2 18.2-76.1 17.9-93.6 0-2.2-2.2-6.1-2.2-8.3 0-2.5 2.5-2.5 6.4 0 8.6 22.8 22.8 87.3 22.8 110.2 0 2.5-2.2 2.5-6.1 0-8.6-2.2-2.2-6.1-2.2-8.3 0zm7.7-75c-13.6 0-24.6 11.1-24.6 24.9 0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.1 24.9-24.6 0-13.8-11-24.9-24.9-24.9z"/></svg></button><button title="Share on Line" data-sharer="line" data-url="https://blog.reachsumit.com/posts/2025/10/learning-to-retrieve/" data-title=" Teaching Models to Decide When to Retrieve: Adaptive RAG, Part 4"><svg class="icon" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>LINE</title><path d="M19.365 9.863c.349 0 .63.285.63.631 0 .345-.281.63-.63.63H17.61v1.125h1.755c.349 0 .63.283.63.63 0 .344-.281.629-.63.629h-2.386c-.345 0-.627-.285-.627-.629V8.108c0-.345.282-.63.63-.63h2.386c.346 0 .627.285.627.63 0 .349-.281.63-.63.63H17.61v1.125h1.755zm-3.855 3.016c0 .27-.174.51-.432.596-.064.021-.133.031-.199.031-.211 0-.391-.09-.51-.25l-2.443-3.317v2.94c0 .344-.279.629-.631.629-.346 0-.626-.285-.626-.629V8.108c0-.27.173-.51.43-.595.06-.023.136-.033.194-.033.195 0 .375.104.495.254l2.462 3.33V8.108c0-.345.282-.63.63-.63.345 0 .63.285.63.63v4.771zm-5.741 0c0 .344-.282.629-.631.629-.345 0-.627-.285-.627-.629V8.108c0-.345.282-.63.63-.63.346 0 .628.285.628.63v4.771zm-2.466.629H4.917c-.345 0-.63-.285-.63-.629V8.108c0-.345.285-.63.63-.63.348 0 .63.285.63.63v4.141h1.756c.348 0 .629.283.629.63 0 .344-.282.629-.629.629M24 10.314C24 4.943 18.615.572 12 .572S0 4.943 0 10.314c0 4.811 4.27 8.842 10.035 9.608.391.082.923.258 1.058.59.12.301.079.766.038 1.08l-.164 1.02c-.045.301-.24 1.186 1.049.645 1.291-.539 6.916-4.078 9.436-6.975C23.176 14.393 24 12.458 24 10.314"/></svg></button><button title="Share on Pocket" data-sharer="pocket" data-url="https://blog.reachsumit.com/posts/2025/10/learning-to-retrieve/"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M407.6 64h-367C18.5 64 0 82.5 0 104.6v135.2C0 364.5 99.7 464 224.2 464c124 0 223.8-99.5 223.8-224.2V104.6c0-22.4-17.7-40.6-40.4-40.6zm-162 268.5c-12.4 11.8-31.4 11.1-42.4 0C89.5 223.6 88.3 227.4 88.3 209.3c0-16.9 13.8-30.7 30.7-30.7 17 0 16.1 3.8 105.2 89.3 90.6-86.9 88.6-89.3 105.5-89.3 16.9 0 30.7 13.8 30.7 30.7 0 17.8-2.9 15.7-114.8 123.2z"/></svg></button><button title="Share on " data-sharer="weibo" data-url="https://blog.reachsumit.com/posts/2025/10/learning-to-retrieve/" data-title=" Teaching Models to Decide When to Retrieve: Adaptive RAG, Part 4" data-image="featured-image.webp"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M407 177.6c7.6-24-13.4-46.8-37.4-41.7-22 4.8-28.8-28.1-7.1-32.8 50.1-10.9 92.3 37.1 76.5 84.8-6.8 21.2-38.8 10.8-32-10.3zM214.8 446.7C108.5 446.7 0 395.3 0 310.4c0-44.3 28-95.4 76.3-143.7C176 67 279.5 65.8 249.9 161c-4 13.1 12.3 5.7 12.3 6 79.5-33.6 140.5-16.8 114 51.4-3.7 9.4 1.1 10.9 8.3 13.1 135.7 42.3 34.8 215.2-169.7 215.2zm143.7-146.3c-5.4-55.7-78.5-94-163.4-85.7-84.8 8.6-148.8 60.3-143.4 116s78.5 94 163.4 85.7c84.8-8.6 148.8-60.3 143.4-116zM347.9 35.1c-25.9 5.6-16.8 43.7 8.3 38.3 72.3-15.2 134.8 52.8 111.7 124-7.4 24.2 29.1 37 37.4 12 31.9-99.8-55.1-195.9-157.4-174.3zm-78.5 311c-17.1 38.8-66.8 60-109.1 46.3-40.8-13.1-58-53.4-40.3-89.7 17.7-35.4 63.1-55.4 103.4-45.1 42 10.8 63.1 50.2 46 88.5zm-86.3-30c-12.9-5.4-30 .3-38 12.9-8.3 12.9-4.3 28 8.6 34 13.1 6 30.8.3 39.1-12.9 8-13.1 3.7-28.3-9.7-34zm32.6-13.4c-5.1-1.7-11.4.6-14.3 5.4-2.9 5.1-1.4 10.6 3.7 12.9 5.1 2 11.7-.3 14.6-5.4 2.8-5.2 1.1-10.9-4-12.9z"/></svg></button><button title="Share on Evernote" data-sharer="evernote" data-url="https://blog.reachsumit.com/posts/2025/10/learning-to-retrieve/" data-title=" Teaching Models to Decide When to Retrieve: Adaptive RAG, Part 4"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M120.82 132.21c1.6 22.31-17.55 21.59-21.61 21.59-68.93 0-73.64-1-83.58 3.34-.56.22-.74 0-.37-.37L123.79 46.45c.38-.37.6-.22.38.37-4.35 9.99-3.35 15.09-3.35 85.39zm79 308c-14.68-37.08 13-76.93 52.52-76.62 17.49 0 22.6 23.21 7.95 31.42-6.19 3.3-24.95 1.74-25.14 19.2-.05 17.09 19.67 25 31.2 24.89A45.64 45.64 0 0 0 312 393.45v-.08c0-11.63-7.79-47.22-47.54-55.34-7.72-1.54-65-6.35-68.35-50.52-3.74 16.93-17.4 63.49-43.11 69.09-8.74 1.94-69.68 7.64-112.92-36.77 0 0-18.57-15.23-28.23-57.95-3.38-15.75-9.28-39.7-11.14-62 0-18 11.14-30.45 25.07-32.2 81 0 90 2.32 101-7.8 9.82-9.24 7.8-15.5 7.8-102.78 1-8.3 7.79-30.81 53.41-24.14 6 .86 31.91 4.18 37.48 30.64l64.26 11.15c20.43 3.71 70.94 7 80.6 57.94 22.66 121.09 8.91 238.46 7.8 238.46C362.15 485.53 267.06 480 267.06 480c-18.95-.23-54.25-9.4-67.27-39.83zm80.94-204.84c-1 1.92-2.2 6 .85 7 14.09 4.93 39.75 6.84 45.88 5.53 3.11-.25 3.05-4.43 2.48-6.65-3.53-21.85-40.83-26.5-49.24-5.92z"/></svg></button><button title="Share on Trello" data-sharer="trello" data-url="https://blog.reachsumit.com/posts/2025/10/learning-to-retrieve/" data-title=" Teaching Models to Decide When to Retrieve: Adaptive RAG, Part 4" data-description=""><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M392.3 32H56.1C25.1 32 0 57.1 0 88c-.1 0 0-4 0 336 0 30.9 25.1 56 56 56h336.2c30.8-.2 55.7-25.2 55.7-56V88c.1-30.8-24.8-55.8-55.6-56zM197 371.3c-.2 14.7-12.1 26.6-26.9 26.6H87.4c-14.8.1-26.9-11.8-27-26.6V117.1c0-14.8 12-26.9 26.9-26.9h82.9c14.8 0 26.9 12 26.9 26.9v254.2zm193.1-112c0 14.8-12 26.9-26.9 26.9h-81c-14.8 0-26.9-12-26.9-26.9V117.2c0-14.8 12-26.9 26.8-26.9h81.1c14.8 0 26.9 12 26.9 26.9v142.1z"/></svg></button></div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M497.941 225.941L286.059 14.059A48 48 0 0 0 252.118 0H48C21.49 0 0 21.49 0 48v204.118a48 48 0 0 0 14.059 33.941l211.882 211.882c18.744 18.745 49.136 18.746 67.882 0l204.118-204.118c18.745-18.745 18.745-49.137 0-67.882zM112 160c-26.51 0-48-21.49-48-48s21.49-48 48-48 48 21.49 48 48-21.49 48-48 48zm513.941 133.823L421.823 497.941c-18.745 18.745-49.137 18.745-67.882 0l-.36-.36L527.64 323.522c16.999-16.999 26.36-39.6 26.36-63.64s-9.362-46.641-26.36-63.64L331.397 0h48.721a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882z"/></svg>&nbsp;<a href="/tags/retrieval/">Retrieval</a>,&nbsp;<a href="/tags/rag/">Rag</a>,&nbsp;<a href="/tags/adaptive-rag/">Adaptive-Rag</a></section>
        <section class="print:!tw-hidden">
            <span><button class="tw-text-fgColor-link-muted hover:tw-text-fgColor-link-muted-hover" onclick="window.history.back();">Back</button></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav print:tw-hidden"><a href="/posts/2025/09/probing-llms-knowledge-boundary/" class="prev" rel="prev" title="Probing LLMs&#39; Knowledge Boundary: Adaptive RAG, Part 3"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M31.7 239l136-136c9.4-9.4 24.6-9.4 33.9 0l22.6 22.6c9.4 9.4 9.4 24.6 0 33.9L127.9 256l96.4 96.4c9.4 9.4 9.4 24.6 0 33.9L201.7 409c-9.4 9.4-24.6 9.4-33.9 0l-136-136c-9.5-9.4-9.5-24.6-.1-34z"/></svg>Probing LLMs&#39; Knowledge Boundary: Adaptive RAG, Part 3</a></div>
</div>
<div id="comments" class="print:!tw-hidden tw-pt-32 tw-pb-8"><div id="gitalk" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://github.com/gitalk/gitalk"></a>Gitalk</a>.
            </noscript></div></article></main><footer class="footer">
        <div class="footer-container"><div class="footer-line"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M256 8C119.033 8 8 119.033 8 256s111.033 248 248 248 248-111.033 248-248S392.967 8 256 8zm0 448c-110.532 0-200-89.451-200-200 0-110.531 89.451-200 200-200 110.532 0 200 89.451 200 200 0 110.532-89.451 200-200 200zm107.351-101.064c-9.614 9.712-45.53 41.396-104.065 41.396-82.43 0-140.484-61.425-140.484-141.567 0-79.152 60.275-139.401 139.762-139.401 55.531 0 88.738 26.62 97.593 34.779a11.965 11.965 0 0 1 1.936 15.322l-18.155 28.113c-3.841 5.95-11.966 7.282-17.499 2.921-8.595-6.776-31.814-22.538-61.708-22.538-48.303 0-77.916 35.33-77.916 80.082 0 41.589 26.888 83.692 78.277 83.692 32.657 0 56.843-19.039 65.726-27.225 5.27-4.857 13.596-4.039 17.82 1.738l19.865 27.17a11.947 11.947 0 0 1-1.152 15.518z"/></svg>2020 - 2025<span class="author">&nbsp;<a href="https://reachsumit.com" target="_blank" rel="noopener noreferrer">Sumit Kumar</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
            <div class="footer-line"></div>
            <div class="footer-line">
            </div>
        </div></footer><div class="print:!tw-hidden tw-flex tw-flex-col tw-fixed tw-right-4 tw-bottom-4 tw-gap-2"><a href="#back-to-top" id="back-to-top-button" class="tw-transition-opacity tw-opacity-0 tw-block tw-bg-bgColor-secondary tw-rounded-full" style="padding: 0.6rem; line-height: 1.3rem; font-size: 1rem;" title="Back to Top">
      <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M34.9 289.5l-22.2-22.2c-9.4-9.4-9.4-24.6 0-33.9L207 39c9.4-9.4 24.6-9.4 33.9 0l194.3 194.3c9.4 9.4 9.4 24.6 0 33.9L413 289.4c-9.5 9.5-25 9.3-34.3-.4L264 168.6V456c0 13.3-10.7 24-24 24h-32c-13.3 0-24-10.7-24-24V168.6L69.2 289.1c-9.3 9.8-24.8 10-34.3.4z"/></svg>
  </a>

  <button id="toc-drawer-button" class="tw-block tw-bg-bgColor-secondary tw-rounded-full md:tw-hidden" style="padding: 0.6rem; line-height: 1.3rem; font-size: 1rem;">
      <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"/></svg>
  </button><a href="#comments" id="view-comments" class="tw-block tw-bg-bgColor-secondary tw-rounded-full" style="padding: 0.6rem; line-height: 1.3rem; font-size: 1rem;" title="View Comments">
      <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M256 32C114.6 32 0 125.1 0 240c0 49.6 21.4 95 57 130.7C44.5 421.1 2.7 466 2.2 466.5c-2.2 2.3-2.8 5.7-1.5 8.7S4.8 480 8 480c66.3 0 116-31.8 140.6-51.4 32.7 12.3 69 19.4 107.4 19.4 141.4 0 256-93.1 256-208S397.4 32 256 32z"/></svg>
  </a></div>
<script type="module">
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
    mermaid.initialize({ startOnLoad: true, theme: (window.theme === 'dark' ? 'dark' : 'default') });
</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.css"><link rel="preload" as="style" onload="this.onload=null;this.rel='stylesheet'" href="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/css/lightgallery.min.css">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/css/lightgallery.min.css"></noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><link rel="preload" as="style" onload="this.onload=null;this.rel='stylesheet'" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/copy-tex.min.css">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/copy-tex.min.css"></noscript>
<script>window.config={"algoliasearch.min.js":"https://cdn.jsdelivr.net/npm/algoliasearch@4.11.0/dist/algoliasearch.umd.min.js","autocomplete.min.js":"https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.0/dist/autocomplete.min.js","comment":{"gitalk":{"admin":["reachsumit"],"clientID":"e13962a172516867862a","clientSecret":"43e82fd70da96d006eec6c9bee0a861aaa13ee89","id":"2025-10-05T00:00:00Z","owner":"reachsumit","repo":"reachsumit-blog-gitalk","title":" Teaching Models to Decide When to Retrieve: Adaptive RAG, Part 4"}},"data":{"desktop-header-typeit":"Sumit's Diary","mobile-header-typeit":"Sumit's Diary"},"lightGallery":{"actualSize":false,"exThumbImage":"data-thumbnail","hideBarsDelay":2000,"selector":".lightgallery","speed":400,"thumbContHeight":80,"thumbWidth":80,"thumbnail":true},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"LV11CUNTAX","algoliaIndex":"blog_reachsumit","algoliaSearchKey":"98d868016771f8a06b967e7eb3eaf63a","highlightTag":"em","maxResultLength":10,"noResultsFound":"No results found","snippetLength":30,"type":"algolia"},"sharerjs":true,"table":{"sort":true},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"desktop-header-typeit":["desktop-header-typeit"],"mobile-header-typeit":["mobile-header-typeit"]},"duration":2700,"speed":100}};</script><script
    src="https://cdn.jsdelivr.net/npm/tablesort@5.3.0/src/tablesort.min.js"
    
  ></script><script
    src="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/js/lightgallery.min.js"
    
      defer
    
  ></script><script
    src="https://cdn.jsdelivr.net/npm/lg-thumbnail.js@1.2.0/dist/lg-thumbnail.min.js"
    
      defer
    
  ></script><script
    src="https://cdn.jsdelivr.net/npm/lg-zoom.js@1.3.0/dist/lg-zoom.min.js"
    
      defer
    
  ></script><script
    src="https://cdn.jsdelivr.net/npm/sharer.js@0.4.2/sharer.min.js"
    
  ></script><script
    src="https://cdn.jsdelivr.net/npm/typeit@7.0.4/dist/typeit.min.js"
    
  ></script><script
    src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
    
      defer
    
  ></script><script
    src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
    
      defer
    
  ></script><script
    src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/copy-tex.min.js"
    
      defer
    
  ></script><script
    src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js"
    
      defer
    
  ></script><script
    src="/js/katex.min.js"
    
      defer
    
  ></script><script
    src="/js/theme.min.js"
    
      defer
    
  ></script><script
    src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"
    
  ></script><script
    src="/js/gitalk.min.js"
    
      defer
    
  ></script><script>
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'G-TGH87J92Z3');
        </script><script
    src="https://www.googletagmanager.com/gtag/js?id=G-TGH87J92Z3"
    async
  ></script>

<script type="speculationrules">
  {
    "prerender": [
      {
        "where": { "href_matches": "/*" },
        "eagerness": "moderate"
      }
    ]
  }
</script>
</body>

</html>
