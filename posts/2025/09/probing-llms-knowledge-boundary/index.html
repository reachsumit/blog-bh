<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="noodp" /><title>Probing LLMs&#39; Knowledge Boundary: Adaptive RAG, Part 3 - Sumit&#39;s Diary</title><meta name="Description" content="Welcome to Sumit Kumar&#39;s Personal Blog!"><meta property="og:url" content="https://blog.reachsumit.com/posts/2025/09/probing-llms-knowledge-boundary/">
  <meta property="og:site_name" content="Sumit&#39;s Diary">
  <meta property="og:title" content="Probing LLMs&#39; Knowledge Boundary: Adaptive RAG, Part 3">
  <meta property="og:description" content="This post introduces techniques that probe the LLM’s internal confidence and knowledge boundaries. We explore prompt-based confidence detection, consistency-based uncertainty estimation, and internal state analysis approaches to determine when retrieval is truly necessary.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-09-27T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-09-27T00:00:00+00:00">
    <meta property="article:tag" content="Retrieval">
    <meta property="article:tag" content="Rag">
    <meta property="article:tag" content="Adaptive-Rag">
    <meta property="og:image" content="https://blog.reachsumit.com/posts/2025/09/probing-llms-knowledge-boundary/featured-image-preview.webp">
      <meta property="og:see_also" content="https://blog.reachsumit.com/posts/2025/09/deciding-when-not-to-retrieve/">
      <meta property="og:see_also" content="https://blog.reachsumit.com/posts/2025/09/problems-with-naive-rag/">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://blog.reachsumit.com/posts/2025/09/probing-llms-knowledge-boundary/featured-image-preview.webp">
  <meta name="twitter:title" content="Probing LLMs&#39; Knowledge Boundary: Adaptive RAG, Part 3">
  <meta name="twitter:description" content="This post introduces techniques that probe the LLM’s internal confidence and knowledge boundaries. We explore prompt-based confidence detection, consistency-based uncertainty estimation, and internal state analysis approaches to determine when retrieval is truly necessary.">
      <meta name="twitter:site" content="@_reachsumit">
<meta name="application-name" content="Sumit&#39;s Diary">
<meta name="apple-mobile-web-app-title" content="Sumit&#39;s Diary">

<meta name="theme-color" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><meta name="twitter:creator" content="@_reachsumit" /><link rel="icon" href="/img/avatar/favicon.ico"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">

<link rel="canonical" href="https://blog.reachsumit.com/posts/2025/09/probing-llms-knowledge-boundary/" /><link rel="prev" href="https://blog.reachsumit.com/posts/2025/09/deciding-when-not-to-retrieve/" />
<link rel="stylesheet" href="/css/main.min.css"><link rel="stylesheet" href="/css/style.min.css"><script type="application/ld+json">{"@context": "https://schema.org","@type": "BlogPosting",
        "headline": "Probing LLMs' Knowledge Boundary: Adaptive RAG, Part 3",
        "inLanguage": "en",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https://blog.reachsumit.com/posts/2025/09/probing-llms-knowledge-boundary/"
        },"image": [{
                            "@type": "ImageObject",
                            "url": "https://blog.reachsumit.com/posts/2025/09/probing-llms-knowledge-boundary/featured-image.webp",
                            "width":  1200 ,
                            "height":  600 
                        },{
                            "@type": "ImageObject",
                            "url": "https://blog.reachsumit.com/posts/2025/09/probing-llms-knowledge-boundary/featured-image-preview.webp",
                            "width":  1000 ,
                            "height":  300 
                        }],"genre": "posts","keywords":["retrieval","rag","adaptive-rag"],"wordcount":  6071 ,
        "url": "https://blog.reachsumit.com/posts/2025/09/probing-llms-knowledge-boundary/","datePublished": "2025-09-27T00:00:00+00:00","dateModified": "2025-09-27T00:00:00+00:00","publisher": {
            "@type": "Organization",
            "name": "xxxx","logo": "https://blog.reachsumit.com/images/avatar.png"},"author": {
                "@type": "Person",
                "name": "Sumit Kumar",
                "url": "https://reachsumit.com"
            },"description": ""
    }</script></head>


<body data-instant-intensity="viewport" class="tw-flex tw-min-h-screen tw-flex-col"><script>
    function setTheme(theme) {
      document.body.setAttribute('theme', theme); 
      document.documentElement.className = theme;
      document.documentElement.style.setProperty('color-scheme', theme === 'light' ? 'light' : 'dark');
      if (theme === 'light') {
        document.documentElement.classList.remove('tw-dark')
      } else {
        document.documentElement.classList.add('tw-dark')
      }
      window.theme = theme;   
      window.isDark = window.theme !== 'light' 
    }
    function saveTheme(theme) {window.localStorage && localStorage.setItem('theme', theme);}
    function getMeta(metaName) {const metas = document.getElementsByTagName('meta'); for (let i = 0; i < metas.length; i++) if (metas[i].getAttribute('name') === metaName) return metas[i]; return '';}
    if (window.localStorage && localStorage.getItem('theme')) {
        let theme = localStorage.getItem('theme');
        if (theme === 'light' || theme === 'dark') {
        setTheme(theme);
        } else {
            if ((window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
                setTheme('dark');
            } else {
                setTheme('light');
            }
        }
      } else { 
        if ('light' === 'light' || 'light' === 'dark') 
            setTheme('light'), saveTheme('light'); 
        else saveTheme('auto'), window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches ? setTheme('dark') : setTheme('light');
    }
    let metaColors = {'light': '#f8f8f8','dark': '#161b22'}
    getMeta('theme-color').content = metaColors[document.body.getAttribute('theme')];
    window.switchThemeEventSet = new Set()
</script><div id="back-to-top"></div>
    <div id="mask"></div><header class="desktop print:!tw-hidden" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Sumit&#39;s Diary"><span id="desktop-header-typeit" class="typeit"></span></a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item"
                    href="/posts/" > Posts </a><a class="menu-item"
                    href="/tags/" > Tags </a><a class="menu-item"
                    href="/categories/" > Categories </a><a class="menu-item"
                    href="/about/" > About </a><a class="menu-item"
                    href="/series/" > Series </a><a class="menu-item"
                    href="/newsletter/" > Newsletter(s) </a><a class="menu-item"
                    href="/talks/" > Talks </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                    <input type="text"
                        placeholder="Search this blog"
                        id="search-input-desktop">
                    <button class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                        <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
                    </button>
                    <button class="search-button search-clear" id="search-clear-desktop" title="Clear">
                        <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm121.6 313.1c4.7 4.7 4.7 12.3 0 17L338 377.6c-4.7 4.7-12.3 4.7-17 0L256 312l-65.1 65.6c-4.7 4.7-12.3 4.7-17 0L134.4 338c-4.7-4.7-4.7-12.3 0-17l65.6-65-65.6-65.1c-4.7-4.7-4.7-12.3 0-17l39.6-39.6c4.7-4.7 12.3-4.7 17 0l65 65.7 65.1-65.6c4.7-4.7 12.3-4.7 17 0l39.6 39.6c4.7 4.7 4.7 12.3 0 17L312 256l65.6 65.1z"/></svg>
                    </button>
                    <span class="search-button search-loading tw-animate-spin" id="search-loading-desktop">
                        <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M304 48c0 26.51-21.49 48-48 48s-48-21.49-48-48 21.49-48 48-48 48 21.49 48 48zm-48 368c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zm208-208c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zM96 256c0-26.51-21.49-48-48-48S0 229.49 0 256s21.49 48 48 48 48-21.49 48-48zm12.922 99.078c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48c0-26.509-21.491-48-48-48zm294.156 0c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48c0-26.509-21.49-48-48-48zM108.922 60.922c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.491-48-48-48z"/></svg>
                    </span>
                </span><button class="menu-item theme-select" aria-label="Switch Theme">
                    <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M8 256c0 136.966 111.033 248 248 248s248-111.034 248-248S392.966 8 256 8 8 119.033 8 256zm248 184V72c101.705 0 184 82.311 184 184 0 101.705-82.311 184-184 184z"/></svg>
                    <select class="color-theme-select" id="theme-select-desktop" aria-label="Switch Theme">
                        <option value="light">Light</option>
                        <option value="dark">Dark</option>
                        <option value="auto">Auto</option>
                    </select>
                </button></div>
        </div>
    </div>
</header><header class="mobile print:!tw-hidden" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Sumit&#39;s Diary"><span id="mobile-header-typeit" class="typeit"></span></a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                <div class="search mobile" id="search-mobile">
                    <input type="text"
                        placeholder="Search this blog"
                        id="search-input-mobile">
                    <button class="search-button search-toggle tw-h-10" id="search-toggle-mobile" title="Search">
                        <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
                    </button>
                    <button class="search-button search-clear tw-h-fit" id="search-clear-mobile" title="Clear">
                        <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm121.6 313.1c4.7 4.7 4.7 12.3 0 17L338 377.6c-4.7 4.7-12.3 4.7-17 0L256 312l-65.1 65.6c-4.7 4.7-12.3 4.7-17 0L134.4 338c-4.7-4.7-4.7-12.3 0-17l65.6-65-65.6-65.1c-4.7-4.7-4.7-12.3 0-17l39.6-39.6c4.7-4.7 12.3-4.7 17 0l65 65.7 65.1-65.6c4.7-4.7 12.3-4.7 17 0l39.6 39.6c4.7 4.7 4.7 12.3 0 17L312 256l65.6 65.1z"/></svg>
                    </button>
                    <span class="search-button search-loading tw-animate-spin" id="search-loading-mobile">
                        <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M304 48c0 26.51-21.49 48-48 48s-48-21.49-48-48 21.49-48 48-48 48 21.49 48 48zm-48 368c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zm208-208c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zM96 256c0-26.51-21.49-48-48-48S0 229.49 0 256s21.49 48 48 48 48-21.49 48-48zm12.922 99.078c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48c0-26.509-21.491-48-48-48zm294.156 0c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48c0-26.509-21.49-48-48-48zM108.922 60.922c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.491-48-48-48z"/></svg>
                    </span>
                </div>
                <button class="search-cancel" id="search-cancel-mobile">
                    Cancel
                </button>
            </div><a class="menu-item" href="/posts/" title="" >Posts</a><a class="menu-item" href="/tags/" title="" >Tags</a><a class="menu-item" href="/categories/" title="" >Categories</a><a class="menu-item" href="/about/" title="" >About</a><a class="menu-item" href="/series/" title="" >Series</a><a class="menu-item" href="/newsletter/" title="" >Newsletter(s)</a><a class="menu-item" href="/talks/" title="" >Talks</a><button class="menu-item theme-select tw-w-full" aria-label="Switch Theme">
                <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M8 256c0 136.966 111.033 248 248 248s248-111.034 248-248S392.966 8 256 8 8 119.033 8 256zm248 184V72c101.705 0 184 82.311 184 184 0 101.705-82.311 184-184 184z"/></svg>
                <select class="color-theme-select" id="theme-select-mobile" aria-label="Switch Theme">
                    <option value="light">Light</option>
                    <option value="dark">Dark</option>
                    <option value="auto">Auto</option>
                </select>
            </button></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div><main class="tw-mx-4 tw-flex-1"><div class="toc print:!tw-hidden" id="toc-auto">
        <h2 class="toc-title">Contents</h2>
        <div class="toc-content" id="toc-content-auto"><nav id="TableOfContents">
  <ul>
    <li><a href="#a-framework-for-assessing-llms-knowledge-boundary">A Framework for Assessing LLM&rsquo;s Knowledge Boundary</a></li>
    <li><a href="#asking-the-model-prompt-based-adaptive-rag">Asking the Model: Prompt-based Adaptive RAG</a>
      <ul>
        <li><a href="#using-prompts-to-urge-llms-to-be-prudent">Using Prompts to Urge LLMs to Be Prudent</a></li>
        <li><a href="#using-icl-for-time-awareness-and-long-tail-knowledge">Using ICL for Time-Awareness and Long-Tail Knowledge</a></li>
        <li><a href="#using-prompts-as-knowledge-gates">Using Prompts as Knowledge Gates</a></li>
        <li><a href="#using-llms-confidence-to-triage-complex-questions">Using LLM&rsquo;s Confidence to Triage Complex Questions</a></li>
        <li><a href="#trade-offs-and-limitations">Trade-offs and Limitations</a></li>
      </ul>
    </li>
    <li><a href="#observing-behavior-consistency-based-adaptive-rag">Observing Behavior: Consistency-based Adaptive RAG</a>
      <ul>
        <li><a href="#retrieve-when-generated-answers-have-high-entropy">Retrieve When Generated Answers Have High Entropy</a></li>
        <li><a href="#using-a-semantic-measure-of-entropy">Using a Semantic Measure of Entropy</a></li>
        <li><a href="#gauging-model-uncertainty-with-graph-metrics">Gauging Model Uncertainty With Graph Metrics</a></li>
        <li><a href="#cross-examine-models-consistency-on-perturbed-questions">Cross-examine Model&rsquo;s Consistency on Perturbed Questions</a></li>
        <li><a href="#dual-generation-as-a-retrieval-gate">Dual Generation as a Retrieval Gate</a></li>
        <li><a href="#trade-offs-and-limitations-1">Trade-offs and Limitations</a></li>
      </ul>
    </li>
    <li><a href="#probing-the-models-mind-internal-state-based-adaptive-rag">Probing the Model&rsquo;s Mind: Internal State-based Adaptive RAG</a>
      <ul>
        <li><a href="#retrieve-when-llm-generates-low-confidence-tokens">Retrieve When LLM Generates Low Confidence Tokens</a></li>
        <li><a href="#improving-flares-understanding-of-llms-information-needs-during-generation">Improving FLARE&rsquo;s Understanding of LLM&rsquo;s Information Needs During Generation</a></li>
        <li><a href="#detect-entity-level-hallucinations">Detect Entity-level Hallucinations</a></li>
        <li><a href="#deciding-when-to-retrieve-by-steering-for-honesty">Deciding When to Retrieve by Steering for Honesty</a></li>
        <li><a href="#looking-for-signs-of-uncertainty-in-llms-internal-states">Looking for Signs of Uncertainty in LLMs&rsquo; Internal States</a></li>
        <li><a href="#using-llm-confidence-shift-as-a-retrieval-gate">Using LLM Confidence Shift as a Retrieval Gate</a></li>
        <li><a href="#trade-offs-and-limitations-2">Trade-offs and Limitations</a></li>
      </ul>
    </li>
    <li><a href="#summary">Summary</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav></div>
    </div><dialog id="toc-dialog" class="tw-max-w-full tw-w-full tw-max-h-full tw-h-full tw-ml-16">
        <div class="toc tw-mx-4 tw-max-w-full">
            <h2 class="tw-mx-0 tw-my-6 tw-uppercase tw-text-2xl">Contents</h2>
            <div class="toc-content"><nav id="TableOfContents">
  <ul>
    <li><a href="#a-framework-for-assessing-llms-knowledge-boundary">A Framework for Assessing LLM&rsquo;s Knowledge Boundary</a></li>
    <li><a href="#asking-the-model-prompt-based-adaptive-rag">Asking the Model: Prompt-based Adaptive RAG</a>
      <ul>
        <li><a href="#using-prompts-to-urge-llms-to-be-prudent">Using Prompts to Urge LLMs to Be Prudent</a></li>
        <li><a href="#using-icl-for-time-awareness-and-long-tail-knowledge">Using ICL for Time-Awareness and Long-Tail Knowledge</a></li>
        <li><a href="#using-prompts-as-knowledge-gates">Using Prompts as Knowledge Gates</a></li>
        <li><a href="#using-llms-confidence-to-triage-complex-questions">Using LLM&rsquo;s Confidence to Triage Complex Questions</a></li>
        <li><a href="#trade-offs-and-limitations">Trade-offs and Limitations</a></li>
      </ul>
    </li>
    <li><a href="#observing-behavior-consistency-based-adaptive-rag">Observing Behavior: Consistency-based Adaptive RAG</a>
      <ul>
        <li><a href="#retrieve-when-generated-answers-have-high-entropy">Retrieve When Generated Answers Have High Entropy</a></li>
        <li><a href="#using-a-semantic-measure-of-entropy">Using a Semantic Measure of Entropy</a></li>
        <li><a href="#gauging-model-uncertainty-with-graph-metrics">Gauging Model Uncertainty With Graph Metrics</a></li>
        <li><a href="#cross-examine-models-consistency-on-perturbed-questions">Cross-examine Model&rsquo;s Consistency on Perturbed Questions</a></li>
        <li><a href="#dual-generation-as-a-retrieval-gate">Dual Generation as a Retrieval Gate</a></li>
        <li><a href="#trade-offs-and-limitations-1">Trade-offs and Limitations</a></li>
      </ul>
    </li>
    <li><a href="#probing-the-models-mind-internal-state-based-adaptive-rag">Probing the Model&rsquo;s Mind: Internal State-based Adaptive RAG</a>
      <ul>
        <li><a href="#retrieve-when-llm-generates-low-confidence-tokens">Retrieve When LLM Generates Low Confidence Tokens</a></li>
        <li><a href="#improving-flares-understanding-of-llms-information-needs-during-generation">Improving FLARE&rsquo;s Understanding of LLM&rsquo;s Information Needs During Generation</a></li>
        <li><a href="#detect-entity-level-hallucinations">Detect Entity-level Hallucinations</a></li>
        <li><a href="#deciding-when-to-retrieve-by-steering-for-honesty">Deciding When to Retrieve by Steering for Honesty</a></li>
        <li><a href="#looking-for-signs-of-uncertainty-in-llms-internal-states">Looking for Signs of Uncertainty in LLMs&rsquo; Internal States</a></li>
        <li><a href="#using-llm-confidence-shift-as-a-retrieval-gate">Using LLM Confidence Shift as a Retrieval Gate</a></li>
        <li><a href="#trade-offs-and-limitations-2">Trade-offs and Limitations</a></li>
      </ul>
    </li>
    <li><a href="#summary">Summary</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav></div>
        </div>
    </dialog><script>document.getElementsByTagName("main")[0].setAttribute("autoTOC", "true")</script><article class="page single print:!tw-w-full print:!tw-max-w-none print:!tw-m-0 print:!tw-p-0"><h1 class="single-title" data-pagefind-meta="date:2025-09-27" data-pagefind-body>Probing LLMs&#39; Knowledge Boundary: Adaptive RAG, Part 3</h1><div class="post-meta">
            <div class="post-meta-line">
                <span class="post-author"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm0 96c48.6 0 88 39.4 88 88s-39.4 88-88 88-88-39.4-88-88 39.4-88 88-88zm0 344c-58.7 0-111.3-26.6-146.5-68.2 18.8-35.4 55.6-59.8 98.5-59.8 2.4 0 4.8.4 7.1 1.1 13 4.2 26.6 6.9 40.9 6.9 14.3 0 28-2.7 40.9-6.9 2.3-.7 4.7-1.1 7.1-1.1 42.9 0 79.7 24.4 98.5 59.8C359.3 421.4 306.7 448 248 448z"/></svg><a href="https://reachsumit.com" title="Author" target="_blank" rel="noopener noreferrer author" class="author">Sumit Kumar</a>
                </span>&nbsp;<span class="post-category">included in </span>&nbsp;<span class="post-category">category <a href="/categories/information-retrieval/"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M464 128H272l-54.63-54.63c-6-6-14.14-9.37-22.63-9.37H48C21.49 64 0 85.49 0 112v288c0 26.51 21.49 48 48 48h416c26.51 0 48-21.49 48-48V176c0-26.51-21.49-48-48-48zm0 272H48V112h140.12l54.63 54.63c6 6 14.14 9.37 22.63 9.37H464v224z"/></svg>Information Retrieval</a></span>&nbsp;<span class="post-category">and</span>&nbsp;<span class="post-series">series <a href="/series/selective-retrieval/"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M464 32H48C21.49 32 0 53.49 0 80v352c0 26.51 21.49 48 48 48h416c26.51 0 48-21.49 48-48V80c0-26.51-21.49-48-48-48zm-6 400H54a6 6 0 0 1-6-6V86a6 6 0 0 1 6-6h404a6 6 0 0 1 6 6v340a6 6 0 0 1-6 6zm-42-92v24c0 6.627-5.373 12-12 12H204c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h200c6.627 0 12 5.373 12 12zm0-96v24c0 6.627-5.373 12-12 12H204c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h200c6.627 0 12 5.373 12 12zm0-96v24c0 6.627-5.373 12-12 12H204c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h200c6.627 0 12 5.373 12 12zm-252 12c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36zm0 96c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36zm0 96c0 19.882-16.118 36-36 36s-36-16.118-36-36 16.118-36 36-36 36 16.118 36 36z"/></svg>Adaptive RAG (Selective Retrieval)</a></span></div>
            <div class="post-meta-line"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M148 288h-40c-6.6 0-12-5.4-12-12v-40c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v40c0 6.6-5.4 12-12 12zm108-12v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm-96 96v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm-96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm192 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm96-260v352c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V112c0-26.5 21.5-48 48-48h48V12c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v52h128V12c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v52h48c26.5 0 48 21.5 48 48zm-48 346V160H48v298c0 3.3 2.7 6 6 6h340c3.3 0 6-2.7 6-6z"/></svg>&nbsp;<time datetime="2025-09-27">2025-09-27</time>&nbsp;<svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M402.3 344.9l32-32c5-5 13.7-1.5 13.7 5.7V464c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V112c0-26.5 21.5-48 48-48h273.5c7.1 0 10.7 8.6 5.7 13.7l-32 32c-1.5 1.5-3.5 2.3-5.7 2.3H48v352h352V350.5c0-2.1.8-4.1 2.3-5.6zm156.6-201.8L296.3 405.7l-90.4 10c-26.2 2.9-48.5-19.2-45.6-45.6l10-90.4L432.9 17.1c22.9-22.9 59.9-22.9 82.7 0l43.2 43.2c22.9 22.9 22.9 60 .1 82.8zM460.1 174L402 115.9 216.2 301.8l-7.3 65.3 65.3-7.3L460.1 174zm64.8-79.7l-43.2-43.2c-4.1-4.1-10.8-4.1-14.8 0L436 82l58.1 58.1 30.9-30.9c4-4.2 4-10.8-.1-14.9z"/></svg>&nbsp;<time datetime="2025-09-27">2025-09-27</time>&nbsp;<svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3 0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9 0l60.1 60.1c18.8 18.7 18.8 49.1 0 67.9zM284.2 99.8L21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3 0-17l-111-111c-4.8-4.7-12.4-4.7-17.1 0zM124.1 339.9c-5.5-5.5-5.5-14.3 0-19.8l154-154c5.5-5.5 14.3-5.5 19.8 0s5.5 14.3 0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8 0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"/></svg>&nbsp;6071 words&nbsp;
                    <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm0 448c-110.5 0-200-89.5-200-200S145.5 56 256 56s200 89.5 200 200-89.5 200-200 200zm61.8-104.4l-84.9-61.7c-3.1-2.3-4.9-5.9-4.9-9.7V116c0-6.6 5.4-12 12-12h32c6.6 0 12 5.4 12 12v141.7l66.8 48.6c5.4 3.9 6.5 11.4 2.6 16.8L334.6 349c-3.9 5.3-11.4 6.5-16.8 2.6z"/></svg>&nbsp;27 minutes&nbsp;</div>
        </div><div class="featured-image"><img  loading="eager" src='/posts/2025/09/probing-llms-knowledge-boundary/featured-image.webp'    height="600" width="1200"></div><div class="details series-nav open">
                                <div class="details-summary series-title">
                                    <span>Series - Adaptive RAG (Selective Retrieval)</span>
                                    <span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
                                </div>
                                <div class="details-content series-content">
                                    <nav>
                                        <ul><li><span class="active">Probing LLMs&#39; Knowledge Boundary: Adaptive RAG, Part 3</span></li>
                                                    <li><a href="/posts/2025/09/deciding-when-not-to-retrieve/">Deciding When Not to Retrieve: Adaptive RAG, Part 2</a></li>
                                                    <li><a href="/posts/2025/09/problems-with-naive-rag/">The Hidden Costs of Naive Retrieval: Adaptive RAG, Part 1</a></li></ul>
                                    </nav>
                                </div>
                            </div><div class="details toc print:!tw-block" id="toc-static"  kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#a-framework-for-assessing-llms-knowledge-boundary">A Framework for Assessing LLM&rsquo;s Knowledge Boundary</a></li>
    <li><a href="#asking-the-model-prompt-based-adaptive-rag">Asking the Model: Prompt-based Adaptive RAG</a>
      <ul>
        <li><a href="#using-prompts-to-urge-llms-to-be-prudent">Using Prompts to Urge LLMs to Be Prudent</a></li>
        <li><a href="#using-icl-for-time-awareness-and-long-tail-knowledge">Using ICL for Time-Awareness and Long-Tail Knowledge</a></li>
        <li><a href="#using-prompts-as-knowledge-gates">Using Prompts as Knowledge Gates</a></li>
        <li><a href="#using-llms-confidence-to-triage-complex-questions">Using LLM&rsquo;s Confidence to Triage Complex Questions</a></li>
        <li><a href="#trade-offs-and-limitations">Trade-offs and Limitations</a></li>
      </ul>
    </li>
    <li><a href="#observing-behavior-consistency-based-adaptive-rag">Observing Behavior: Consistency-based Adaptive RAG</a>
      <ul>
        <li><a href="#retrieve-when-generated-answers-have-high-entropy">Retrieve When Generated Answers Have High Entropy</a></li>
        <li><a href="#using-a-semantic-measure-of-entropy">Using a Semantic Measure of Entropy</a></li>
        <li><a href="#gauging-model-uncertainty-with-graph-metrics">Gauging Model Uncertainty With Graph Metrics</a></li>
        <li><a href="#cross-examine-models-consistency-on-perturbed-questions">Cross-examine Model&rsquo;s Consistency on Perturbed Questions</a></li>
        <li><a href="#dual-generation-as-a-retrieval-gate">Dual Generation as a Retrieval Gate</a></li>
        <li><a href="#trade-offs-and-limitations-1">Trade-offs and Limitations</a></li>
      </ul>
    </li>
    <li><a href="#probing-the-models-mind-internal-state-based-adaptive-rag">Probing the Model&rsquo;s Mind: Internal State-based Adaptive RAG</a>
      <ul>
        <li><a href="#retrieve-when-llm-generates-low-confidence-tokens">Retrieve When LLM Generates Low Confidence Tokens</a></li>
        <li><a href="#improving-flares-understanding-of-llms-information-needs-during-generation">Improving FLARE&rsquo;s Understanding of LLM&rsquo;s Information Needs During Generation</a></li>
        <li><a href="#detect-entity-level-hallucinations">Detect Entity-level Hallucinations</a></li>
        <li><a href="#deciding-when-to-retrieve-by-steering-for-honesty">Deciding When to Retrieve by Steering for Honesty</a></li>
        <li><a href="#looking-for-signs-of-uncertainty-in-llms-internal-states">Looking for Signs of Uncertainty in LLMs&rsquo; Internal States</a></li>
        <li><a href="#using-llm-confidence-shift-as-a-retrieval-gate">Using LLM Confidence Shift as a Retrieval Gate</a></li>
        <li><a href="#trade-offs-and-limitations-2">Trade-offs and Limitations</a></li>
      </ul>
    </li>
    <li><a href="#summary">Summary</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content" data-pagefind-body><p>In part 1 of this Adaptive RAG series, we established that the naive &ldquo;retrieve-then-read&rdquo; RAG&rsquo;s indiscriminate retrieval strategy can be inefficient, degrade quality, and even harm the final output. Part 2 introduced Selective Retrieval as a solution, along with a three-dimensional evaluation framework, and explored lightweight, pre-generation strategies that decide whether to retrieve based purely on the input query&rsquo;s characteristics, without requiring any LLM generation.</p>
<p>While the pre-generation methods from part 2 are computationally efficient, they often rely on surface-level features and lack the nuances to account for the LLM&rsquo;s own knowledge and confidence. A model may already know the answer, even if the query seems complex or obscure. In this post, we move from analyzing the query to analyzing the model. We will explore methods that directly assess the LLM&rsquo;s confidence and knowledge gaps. These methods probe the LLM&rsquo;s initial outputs or internal states to get a direct signal of its uncertainty, allowing for a more nuanced and accurate retrieval decision.</p>
<h2 id="a-framework-for-assessing-llms-knowledge-boundary" class="headerLink">
    <a href="#a-framework-for-assessing-llms-knowledge-boundary" class="header-mark"></a>A Framework for Assessing LLM&rsquo;s Knowledge Boundary</h2><p>When given questions beyond their knowledge scope, LLMs often generate plausible yet inaccurate responses (&ldquo;hallucinations&rdquo;). RAG methods help in such scenarios by leveraging external knowledge sources to expand the answerable question boundary. But how can we determine if the retrieved knowledge truly enhances the LLM&rsquo;s ability to answer? Knowledge Boundary Estimation refers to the set of methods that investigate the extent to which an LLM can reliably answer a given question based on its internal knowledge. Current research work in knowledge boundary awareness can be categorized into the following three fundamental confidence estimation approaches:</p>
<ol>
<li><strong>Prompt-based Confidence Detection</strong>: Using engineered prompts to elicit a direct, verbalized confidence score from the model.</li>
<li><strong>Sampling-based Confidence Estimation</strong>: Generating multiple different responses (via query paraphrasing, output variations, etc.) to a single query and measuring their consistency to infer the model&rsquo;s certainty.</li>
<li><strong>Internal State-based Confidence Estimation</strong>: Analyzing the LLM&rsquo;s internal hidden states and generation probabilities as a more direct and fine-grained signal to detect knowledge gaps.</li>
</ol>
<p>Several Adaptive RAG methods utilize these knowledge boundary estimation methods to identify when a query falls inside or outside the model&rsquo;s inherent knowledge scope, thereby assessing whether external information retrieval is necessary and sufficient to provide a correct answer. Let&rsquo;s examine how each approach tackles the challenge of determining when an LLM needs external assistance.</p>
<div class="details admonition tip open">
    <div class="details-summary admonition-title">
        <span class="icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 352 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M96.06 454.35c.01 6.29 1.87 12.45 5.36 17.69l17.09 25.69a31.99 31.99 0 0 0 26.64 14.28h61.71a31.99 31.99 0 0 0 26.64-14.28l17.09-25.69a31.989 31.989 0 0 0 5.36-17.69l.04-38.35H96.01l.05 38.35zM0 176c0 44.37 16.45 84.85 43.56 115.78 16.52 18.85 42.36 58.23 52.21 91.45.04.26.07.52.11.78h160.24c.04-.26.07-.51.11-.78 9.85-33.22 35.69-72.6 52.21-91.45C335.55 260.85 352 220.37 352 176 352 78.61 272.91-.3 175.45 0 73.44.31 0 82.97 0 176zm176-80c-44.11 0-80 35.89-80 80 0 8.84-7.16 16-16 16s-16-7.16-16-16c0-61.76 50.24-112 112-112 8.84 0 16 7.16 16 16s-7.16 16-16 16z"/></svg></span>A note on LLMs&#39; perception of their knowledge boundaries<span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
    </div>
    <div class="details-content">
        <div class="admonition-content"><p>A straightforward idea for ARAG is to only conduct retrieval when language models are uncertain about a question. For safety critical domains, like healthcare, LLMs should acknowledge when they cannot answer a factual question, instead of providing a specious answer. However, several studies have found LLMs to have difficulty in perceiving their factual knowledge boundaries, i.e. knowing what they know and what they don&rsquo;t know, often displaying overconfidence [^1].</p>
<p>In general, these studies either utilize model logits, or follow non-logits methods, such as asking closed-source LLMs to express uncertainty about their own answer in natural language. According to their findings, the QA performance and confidence of LLMs may not always align, even with more powerful LLMs [^1]. In this post, we will take a look at several adaptive RAG methods, including ones that quantitively measure and enhance LLMs&rsquo; perception of their knowledge boundary.</p>
</div></div></div>
<h2 id="asking-the-model-prompt-based-adaptive-rag" class="headerLink">
    <a href="#asking-the-model-prompt-based-adaptive-rag" class="header-mark"></a>Asking the Model: Prompt-based Adaptive RAG</h2><p>In this section, we will take a look at the set of Adaptive RAG methods that utilize prompt-based confidence detection methods to make the retrieval decision. These methods rely on LLMs&rsquo; self-assessment as they let LLMs judge their own knowledge.</p>
<h3 id="using-prompts-to-urge-llms-to-be-prudent" class="headerLink">
    <a href="#using-prompts-to-urge-llms-to-be-prudent" class="header-mark"></a>Using Prompts to Urge LLMs to Be Prudent</h3><p>Ni et al.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> used three metrics: overconfidence, conservativeness, and alignment, to match LLMs&rsquo; expression of confidence with the correctness of their responses. They found that overconfidence, not excessive caution, was the primary reason for LLMs&rsquo; poor perception of their knowledge boundaries. Their experiments also established a negative correlation between a model&rsquo;s expressed certainty and its reliance on external information, i.e., the more certain a model is, the less it will leverage provided documents. This led to the hypothesis that if LLMs can be made to express their uncertainty more reliably, it can serve as an effective, low-cost trigger to decide when to do retrieval-augmentation.</p>
<p>To mitigate overconfidence, the authors proposed two prompt-based strategies.</p>
<ul>
<li><strong>Urging Prudence:</strong> This approach uses prompts designed to make LLMs more cautious about their claims of certainty. The most effective strategy was the <code>Punish</code> method, which adds a simple warning in the prompt: &ldquo;You will be punished if the answer is not right, but you say certain&rdquo;. This was shown to reduce overconfidence without making the model overly conservative.
<div class="details admonition quote">
    <div class="details-summary admonition-title">
        <span class="icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"/></svg></span>&#34;Punish&#34; Prompt Template<span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
    </div>
    <div class="details-content">
        <div class="admonition-content"><pre><code>Answer the following question based on your internal knowledge with one or a few words.
If you are sure the answer is accurate and correct, please say &quot;certain&quot; after the answer.
If you are not confident with the answer, please say &quot;uncertain&quot;.
**You will be punished if the answer is not right, but you say &quot;certain&quot;.**

Q:

A:
</code></pre>
</div></div></div></li>
<li><strong>Enhancing QA Performance</strong>: This second group of prompts aims to improve the model&rsquo;s accuracy, which in turn calibrates its confidence. The most successful method was <code>Explain</code>, which instructed the model to not only provide an answer but also to &ldquo;explain why you give this answer&rdquo;. This simple addition has been shown to improve accuracy as well as the model&rsquo;s perception of its knowledge boundary.
<div class="details admonition quote">
    <div class="details-summary admonition-title">
        <span class="icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"/></svg></span>&#34;Explain&#34; Prompt Template<span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
    </div>
    <div class="details-content">
        <div class="admonition-content"><pre><code>Answer the following question based on your internal knowledge with one or a few words.
**and explain why you give this answer**. If you are sure the answer is accurate and correct,
please say &quot;certain&quot; after the answer. If you are not confident with the answer, please say &quot;uncertain&quot;.

Q:

A:
</code></pre>
</div></div></div>
Adaptive RAG can be enabled when the model believes that it cannot answer the question based on these approaches. The paper showed that a hybrid <code>Punish+Explain</code> method was particularly effective, achieving comparable or even better performance than static (always-on) RAG with significantly fewer retrieval calls. The code for this project is available on <a href="https://github.com/ShiyuNee/When-to-Retrieve" target="_blank" rel="noopener noreferrer">GitHub</a>.</li>
</ul>
<h3 id="using-icl-for-time-awareness-and-long-tail-knowledge" class="headerLink">
    <a href="#using-icl-for-time-awareness-and-long-tail-knowledge" class="header-mark"></a>Using ICL for Time-Awareness and Long-Tail Knowledge</h3><p>In <strong>T</strong>ime-<strong>A</strong>ware <strong>A</strong>daptive <strong>RE</strong>trieval (TA-ARE), Zhang et al.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> argue that questions regarding recent events or obscure long-tail generally require retrieval. The authors use an in-context learning approach to make the LLM more aware of its knowledge boundary without requiring new training or complex calibration. TA-ARE simply adds a <code>Today is {current_date()}</code> instruction to the prompt to help the model identify questions that involve knowledge falling outside of its static pretraining data.
<div class="details admonition quote">
    <div class="details-summary admonition-title">
        <span class="icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"/></svg></span>TA-RAE Prompt Template<span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
    </div>
    <div class="details-content">
        <div class="admonition-content">Today is {datetime.today()}. Given a question, determine whether you need to retrieve external resources, such as real-time search engines, Wikipedia, or databases, to answer the question correctly. Only answer &ldquo;[Yes]&rdquo; or &ldquo;[No]&rdquo;.<br>
Here are some examples:<br>
{demonstration examples}<br>
Question: {question}<br>
Answer:</div></div></div></p>
<p>Additionally, to improve performance on obscure or long-tail topics, the authors propose a prompt design that includes 4 demonstrations of when retrieval is and is not needed. For 2 examples that require retrieval, they dynamically select the top-2 questions from a test set that are semantically closest to the user&rsquo;s current question and were previously answered incorrectly by the model. The remaining 2 examples, where retrieval is not required, are manually created. The dataset and code for TA-ARE are available on <a href="https://github.com/hyintell/RetrievalQA" target="_blank" rel="noopener noreferrer">GitHub</a>.</p>
<h3 id="using-prompts-as-knowledge-gates" class="headerLink">
    <a href="#using-prompts-as-knowledge-gates" class="header-mark"></a>Using Prompts as Knowledge Gates</h3><p>In RAGate-Prompt, Wang et al.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> propose using prompting as a binary knowledge gate mechanism, in conversational systems, to determine when to search for external information. The authors explore two types of prompts: zero-shot and in-context learning. Zero-shot prompts describe the task that uses the conversational context, and optionally, the retrieved knowledge to generate binary feedback.</p>
<p><div class="details admonition quote">
    <div class="details-summary admonition-title">
        <span class="icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"/></svg></span>RAGate-Prompt Zero-shot<span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
    </div>
    <div class="details-content">
        <div class="admonition-content"><p>Below is an instruction that describes a task. Please respond with ‘True’ or ‘False’ only that appropriately completes the request.</p>
<p>### Instruction: Analyze the conversational context so far. Generate an appropriate response. Consider the involved entities. Estimate if augmenting the response with external knowledge is helpful with an output of ‘True’ or ‘False’ only.<br>
### Input: [Conversation Context Input]</p>
<p>### Response:</p>
</div></div></div>
For in-context learning prompts, they simply augment the prompts with illustrative examples.
<div class="details admonition quote">
    <div class="details-summary admonition-title">
        <span class="icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"/></svg></span>RAGate-Prompt In-context Learning<span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
    </div>
    <div class="details-content">
        <div class="admonition-content">Below is an instruction that describes a task. Please respond with ‘True’ or ‘False’ only that appropriately completes the request.<br>
### Instruction: Analyze the conversational context so far. Generate an appropriate response. Consider the involved entities. Estimate if augmenting the response with external knowledge is helpful with an output of ‘True’ or ‘False’ only.<br>
### Example 1: USER: &hellip; SYSTEM: &hellip;<br>
### Response: True<br>
### Example 2: USER: &hellip; SYSTEM: &hellip;<br>
### Response: False<br>
### Input: [Conversation Context Input]<br>
### Response:</div></div></div></p>
<p>The &lsquo;True&rsquo; or &lsquo;False&rsquo; output is then used as a trigger that decides whether knowledge-augmentation will be helpful. However, the study found that these prompting methods were significantly outperformed by more specialized, fine-tuned approaches.</p>
<h3 id="using-llms-confidence-to-triage-complex-questions" class="headerLink">
    <a href="#using-llms-confidence-to-triage-complex-questions" class="header-mark"></a>Using LLM&rsquo;s Confidence to Triage Complex Questions</h3><p>In Self-DC (Self Divide-and-Conquer), Wang et al.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> argue that the real-world questions are often &ldquo;compositional&rdquo;, which contain multiple sub-questions. For example, &ldquo;Is the President of the United States in 2025 the same individual serving as the President in 2018?&rdquo; is a compositional question, in which one part may be known to an LLM while another may be unknown. A naive strategy that classifies all questions into a simple binary of known or unknown may lead to redundant retrievals and harm accuracy. So the authors suggest a more nuanced approach.</p>
<p>For any given question, their framework first gets a confidence score from the LLM. This score can be verbalized (LLM is asked to state its confidence number from 0 to 100, which is then normalized to [0, 1]) or probability-based (average token probability of the generated answer). Self-DC then uses mean ($\alpha$) and standard deviation ($\beta$) derived from LLM&rsquo;s confidence-score distribution to classify the question as &ldquo;Unknown&rdquo; ($\text{score}\le\alpha-\beta)$, &ldquo;Known&rdquo; ($\text{score}\ge\alpha+\beta)$, or &ldquo;Uncertain&rdquo; ($\text{score}$ between $\alpha-\beta$ and $\alpha+\beta$).</p>
<p><figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/self_dc_framework.png" title="Overview of Self-DC" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/self_dc_framework.png" data-sub-html="<h2>Overview of Self-DC</h2><p>Overview of Self-DC</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/self_dc_framework.png'   alt="Overview of Self-DC"  ></a><figcaption class="image-caption">Overview of Self-DC</figcaption>
</figure></p>
<p>The &ldquo;Known&rdquo; questions are answered via LLM&rsquo;s internal memory, while RAG is triggered for &ldquo;Unknown&rdquo; queries. The framework breaks down uncertain questions into several simpler sub-questions and recursively applies the Self-DC process to each sub-question until each sub-question is answered via generation or retrieval. All answers are combined to form the final response to the original query.</p>
<h3 id="trade-offs-and-limitations" class="headerLink">
    <a href="#trade-offs-and-limitations" class="header-mark"></a>Trade-offs and Limitations</h3><p>Prompt-based ARAG methods are generally easiest to implement, are training-free, cost-effective, and require no special tooling. They also work with closed-source models as they only rely on text input and output. However, their effectiveness depends entirely on LLM being well-calibrated. Models can often be &ldquo;confidently wrong&rdquo;, making their verbalized confidence unreliable. They are also highly sensitive to specific wording of the prompt, requiring careful engineering.</p>
<h2 id="observing-behavior-consistency-based-adaptive-rag" class="headerLink">
    <a href="#observing-behavior-consistency-based-adaptive-rag" class="header-mark"></a>Observing Behavior: Consistency-based Adaptive RAG</h2><p>Next, we look at the Adaptive RAG methods that measure uncertainty through the consistency across multiple samples.</p>
<h3 id="retrieve-when-generated-answers-have-high-entropy" class="headerLink">
    <a href="#retrieve-when-generated-answers-have-high-entropy" class="header-mark"></a>Retrieve When Generated Answers Have High Entropy</h3><p>In Web-augmented LLM (UNIWEB), Li et al.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> hypothesized that LLMs can self-evaluate the confidence level of their generation results. They utilize a self-evaluation mechanism to determine whether it is necessary to access additional information for knowledge-intensive tasks like fact checking, slot filling, ODQA, etc. Instead of a neural network-based retriever over a static knowledge source, UNIWEB uses a commercial search engine (like Google Search) via API to obtain high-quality results from the web, only when LLM is not confident in its predictions. Hypothetically, when LLM is confident about its output given a specific input, sampling the output many times would result in an output distribution with small entropy. For a given input question, the model is asked to generate an answer 200 times, and the entropy (or &ldquo;uncertainty&rdquo;) among the answers is calculated using the following formula.</p>
<p>$$H(\hat{\mathcal{Y}}|\mathcal{X}) = \mathbb{E}_{\hat{y} \sim G}[-\log P(\hat{y}|\mathcal{X})]$$</p>
<p>where $\mathcal{X}$ is the input question, $\hat{\mathcal{Y}}$ is the distribution of possible answers, $\mathbb{E}_{\hat{y} \sim G}$ is the expected value over all possible answers $\hat{y}$ that the generator model $G$ could produce, and $\log P(\hat{y}|\mathcal{X})$ is the uncertainty score. Based on a chosen threshold, top-$k$ HTML pages are retrieved for the query if retrieval is deemed necessary, and the 5 passages that are most similar to the query (cosine similarity) are selected to form the final passage. Documents on the web can have harmful content and inconsistent quality, which may steer the LLMs towards unexpected outputs. To address this, the authors also propose a continual knowledge learning task.</p>
<h3 id="using-a-semantic-measure-of-entropy" class="headerLink">
    <a href="#using-a-semantic-measure-of-entropy" class="header-mark"></a>Using a Semantic Measure of Entropy</h3><p>In SUGAR (Semantic Uncertainty Guided Adaptive Retrieval), Zubkova et al.<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> argue that the standard predictive entropy is a flawed metric for deciding when to retrieve in RAG systems because it fails to account for linguistic variance. An LLM might be highly certain about a fact but uncertain about how to phrase it. For example, the answers &ldquo;Shakespeare wrote Romeo and Juliet&rdquo; and &ldquo;Romeo and Juliet was written by Shakespeare&rdquo; are semantically identical but lexically different. If we calculate token-by-token entropy, a model might appear highly uncertain simply because these different but semantically identical forms are competing for probability mass. This can result in a misleading uncertainty score, causing the system to retrieve information when it doesn&rsquo;t actually need it.</p>
<p>To solve this, the authors propose a new metric, &lsquo;Semantic Entropy&rsquo;, which measures uncertainty over the semantic meaning of potential answers, instead of focusing on tokens. During inference, for a given user query, the LLM is prompted to generate multiple (around 10) diverse potential answers using high-temperature sampling. A bidirectional entailment model is then used to cluster these candidate answers into groups, based on their meaning. The system then calculates the entropy over the probability distribution of these semantic clusters, rather than the individual sequences. This entropy scores is calculated as: $SE(x) \approx -|C|^{-1} \sum_{i=1}^{|C|} \log p(C_i|x)$, where $SE(x)$ is the semantic entropy, $C$ is the set of semantic clusters, and $x$ is the given input query.</p>
<p><figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/sugar_framework.png" title="SUGAR&rsquo;s usage of semantic entropy to measure the model confidence" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/sugar_framework.png" data-sub-html="<h2>SUGAR&#39;s usage of semantic entropy to measure the model confidence</h2><p>SUGAR&rsquo;s usage of semantic entropy to measure the model confidence</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/sugar_framework.png'   alt="SUGAR&rsquo;s usage of semantic entropy to measure the model confidence"  ></a><figcaption class="image-caption">SUGAR's usage of semantic entropy to measure the model confidence</figcaption>
</figure></p>
<p>Finally, the entropy score is compared against predefined thresholds to determine the model&rsquo;s level of uncertainty and perform one of the following three actions:</p>
<ul>
<li><strong>Low entropy</strong>: This indicates the model is confident. The answer is generated directly from the LLM&rsquo;s parametric memory without retrieval.</li>
<li><strong>Medium entropy</strong>: This shows the model is somewhat uncertain. The system performs one round of retrieval to augment the context.</li>
<li><strong>High entropy</strong>: This indicates the model is highly uncertain or confused. The system performs multiple rounds of retrieval to provide more comprehensive context.
As it is necessary to compute the semantic entropy, the inference time for SUGAR takes considerably longer than other ARAG methods for single-hop questions.</li>
</ul>
<h3 id="gauging-model-uncertainty-with-graph-metrics" class="headerLink">
    <a href="#gauging-model-uncertainty-with-graph-metrics" class="header-mark"></a>Gauging Model Uncertainty With Graph Metrics</h3><p>In Dhole et al.<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>, researchers build upon the prior ARAG methods and explore which Uncertainty Detection (UD) metrics are more effective to gauge uncertainty for dynamically triggering retrieval. The authors specifically focus on a family of black-box, sequence-level UD metrics and test their viability as a trigger within long-form, multi-hop QA.</p>
<p>They adapted the FLARE framework from Jiang et al.<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>, where given a query, the system first generates a temporary future sentence without retrieval. However, instead of checking token probabilities, a sequence-level uncertainty score is calculated over this temporary sentence. If this score exceeds a pre-defined threshold, the system concludes that the model lacks knowledge. It then prompts the model to generate a specific subquery to find the missing information, use that subquery to retrieve relevant documents, and regenerates the sentence given the new context.</p>
<p>For computing uncertainty, the authors generated multiple candidate responses and evaluated each of the following 5 UD metrics that compute pairwise similarity scores among these answers.</p>
<ul>
<li><strong>Semantic Sets</strong>: Based on prior research work, this metric clusters the generated responses into groups based on semantic equivalence using an NLI classifier. The number of resulting clusters serves as the uncertainty score.</li>
<li><strong>Eigen Value Laplacian</strong>: This metric uses spectral clustering on a graph where nodes are responses and the edge weights are pairwise similarities. Eigenvalues far different from one indicate weaker clustering and thus higher uncertainty.</li>
<li><strong>Degree Matrix (Jaccard Index)</strong>: The similarity between two responses is computed by the size of the intersection of their word sets divided by the size of their union. This score is then used to build a degree matrix for uncertainty calculation.</li>
<li><strong>Degree Matrix (NLI)</strong>: This metric is similar to the Jaccard Index version, but the similarity is computed by an NLI classifier&rsquo;s entailment predictions.</li>
<li><strong>Eccentricity</strong>: This graph-based metric also uses the degree matrix.
They found that the Eccentricity and the Degree Matrix (Jaccard) were able to reduce the number of retrieval calls by nearly half, with minimal performance drop compared to an &ldquo;Always Retrieve&rdquo; baseline. Comparatively, Eccentricity offered the best overall performance and efficiency, while Degree Matrix (Jaccard) required the fewest retrieval operations. The authors note that computing some of these UD metrics can be computationally more expensive than simple retrieval methods, which may outweigh the benefit of skipping retrieval.</li>
</ul>
<h3 id="cross-examine-models-consistency-on-perturbed-questions" class="headerLink">
    <a href="#cross-examine-models-consistency-on-perturbed-questions" class="header-mark"></a>Cross-examine Model&rsquo;s Consistency on Perturbed Questions</h3><p>In Rowen (<strong>R</strong>etrieve <strong>o</strong>nly <strong>w</strong>h<strong>en</strong> it needs), Ding et al.<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup> hypothesized that if an LLM is truly confident about a fact, it should provide a consistent answer even when the question is phrased differently, asked in another language, or posed to a different model. As shown in the figure below, the authors first prompt an LLM to generate a preliminary answer using its own internal knowledge, often guided by a Chain-of-Thought process to elicit its reasoning.</p>
<p>Next, a consistency-based detection module generates several semantically equivalent versions of the original question to measure the model&rsquo;s uncertainty. The model is asked the same question in different languages (e.g., English and Chinese). Optionally, an additional &ldquo;verifier&rdquo; LLM can also be used to answer the same set of questions. A checking operator then determines if the different answers are semantically consistent by producing a consistency score. If the score is high, it indicates that the original answer is reliable and can be returned to the user; otherwise, a retrieval-augmented generation procedure is triggered. The code for Rowen is available on <a href="https://github.com/dhx20150812/Rowen" target="_blank" rel="noopener noreferrer">GitHub</a>.</p>
<p><div class="details admonition quote">
    <div class="details-summary admonition-title">
        <span class="icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"/></svg></span>Rowen&#39;s prompt for determining whether two QA pairs are equivalent<span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
    </div>
    <div class="details-content">
        <div class="admonition-content">Given the question Q, and two potential answers:<br>
Answer A in English and answer B in Chinese. Your task is to determine if the content and meaning of A and B are equivalent in the context of answering Q. Consider linguistic nuances, cultural variations, and the overall conveyance of information. Respond with a binary classification. If A and B are equivalent, output ’True.’, otherwise output ’False’.</div></div></div>
<figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/rowen_framework.png" title="Overview of Rowen framework" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/rowen_framework.png" data-sub-html="<h2>Overview of Rowen framework</h2><p>Overview of Rowen framework</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/rowen_framework.png'   alt="Overview of Rowen framework"  ></a><figcaption class="image-caption">Overview of Rowen framework</figcaption>
</figure>
The framework incurs increased cost and latency due to requiring multiple API calls for generating the initial answer, perturbed questions, and the final correction.</p>
<h3 id="dual-generation-as-a-retrieval-gate" class="headerLink">
    <a href="#dual-generation-as-a-retrieval-gate" class="header-mark"></a>Dual Generation as a Retrieval Gate</h3><p>In PAIRS (Parametric-verified Adaptive Information Retrieval and Selection), Chen et al.<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup> propose a training-free framework aimed at fully leveraging LLM&rsquo;s own parametric knowledge to decide whether external retrieval is necessary. The goal is to improve efficiency without sacrificing accuracy, especially for the simple queries that the LLM can already answer.
Given a query, the LLM is prompted to generate two answers in parallel:</p>
<ul>
<li><strong>A direct answer</strong>, using only the model&rsquo;s parametric knowledge.</li>
<li><strong>A context-augmented answer</strong>, which is based on a self-generated pseudo-context. The pseudo-context is created by the LLM to simply mimic the retrieved information..
<div class="details admonition quote">
    <div class="details-summary admonition-title">
        <span class="icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"/></svg></span>Prompt for generating pseudo-context<span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
    </div>
    <div class="details-content">
        <div class="admonition-content">Your task is to write a detailed passage to answer the given question. Question: {q}</div></div></div></li>
</ul>
<p><figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/pairs_framework.png" title="The PAIRS framework" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/pairs_framework.png" data-sub-html="<h2>The PAIRS framework</h2><p>The PAIRS framework</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/pairs_framework.png'   alt="The PAIRS framework"  ></a><figcaption class="image-caption">The PAIRS framework</figcaption>
</figure></p>
<p>Next, the system checks if the two answers converge or are semantically consistent. If the answers agree, PAIRS concludes that the LLM is confident. It then skips the external retrieval entirely and returns the final answer. However, if the answers differ, suggesting uncertainty in the model&rsquo;s parametric knowledge, the system activates its Dual Path Retrieval and Adaptive Information Selection (DPR-AIS) module.
First, DPR executes two parallel retrieval operations, one using the original query and the other using a self-generated pseudo-context. The results are merged, and the AIS module then scores each candidate document based on its joint relevance to both sources. This dual-source approach makes retrieval more robust against sparse queries.</p>
<h3 id="trade-offs-and-limitations-1" class="headerLink">
    <a href="#trade-offs-and-limitations-1" class="header-mark"></a>Trade-offs and Limitations</h3><p>To sum up, consistency-based ARAG methods are generally more robust, as they compare multiple outputs to capture true semantic uncertainty and are less susceptible to a single, anomalous generation. Like prompting, these methods also work with any black-box model. However, generating 10, 20, or even more responses for a single query dramatically increases inference cost and response time. Additionally, they may not detect systematic knowledge gaps that produce consistent but wrong answers.</p>
<h2 id="probing-the-models-mind-internal-state-based-adaptive-rag" class="headerLink">
    <a href="#probing-the-models-mind-internal-state-based-adaptive-rag" class="header-mark"></a>Probing the Model&rsquo;s Mind: Internal State-based Adaptive RAG</h2><p>Finally, we switch our attention to the set of Adaptive RAG methods that bypass the text output entirely to analyze the model&rsquo;s internal activations and representations.</p>
<h3 id="retrieve-when-llm-generates-low-confidence-tokens" class="headerLink">
    <a href="#retrieve-when-llm-generates-low-confidence-tokens" class="header-mark"></a>Retrieve When LLM Generates Low Confidence Tokens</h3><p>In FLARE (<strong>F</strong>orward-<strong>L</strong>ooking <strong>A</strong>ctive <strong>RE</strong>trieval), Jiang et al.<sup id="fnref1:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup> hypothesize that in a RAG system, retrieval queries should reflect the intents of future generations. They propose anticipating the future by generating a temporary sentence, using it as the retrieval query if it contains low-confidence tokens, and then regenerating the next sentence conditioning on the retrieved document. This iterative framework continues until the next sentence generation reaches the end. The code and data for FLARE are available on <a href="https://github.com/jzbjyb/FLARE/" target="_blank" rel="noopener noreferrer">Github</a>.
<figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/flare_framework.png" title="An overview of FLARE" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/flare_framework.png" data-sub-html="<h2>An overview of FLARE. x represents input, Dx is initial retrieval results, the temporary next sentence is shown in gray italics, and low-probability tokens are shown with underline.</h2><p>An overview of FLARE</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/flare_framework.png'   alt="An overview of FLARE"  ></a><figcaption class="image-caption">An overview of FLARE. x represents input, Dx is initial retrieval results, the temporary next sentence is shown in gray italics, and low-probability tokens are shown with underline.</figcaption>
</figure></p>
<p>The authors proposed 2 variants of FLARE that differ in their retrieval triggering mechanism. Broadly, these methods either encourage LLMs to generate special search tokens or use the confidence for tokens in the next sentence.</p>
<ol>
<li>FLARE with Retrieval Instructions ($\text{FLARE}_{instruct}$): Inspired by Toolformer, this method instructs the LM to generate a retrieval query <code>[Search(query)]</code> when additional information is needed. Though not as reliable as a fine-tuned model, black-box models can elicit this behavior through few-shot prompting. When LM generates a retrieval query, generation is stopped, and the query term is used to retrieve relevant documents, which are prepended to the user input to aid future generation.
<div class="details admonition quote">
    <div class="details-summary admonition-title">
        <span class="icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"/></svg></span>Prompt: retrieval instructions<span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
    </div>
    <div class="details-content">
        <div class="admonition-content"><pre><code>Skill 1. An instruction to guide LMs to generate search queries. Several search-related exemplars.

Skill 2. An instruction to guide LMs to perform a specific downstream task (e.g., multihop QA). Several task-related exemplars.

An instruction to guide LMs to combine skills 1 and 2 for the test case. The input of the test case.
</code></pre>
</div></div></div></li>
<li>Direct FLARE ($\text{FLARE}_{direct}$): This method directly uses the LM&rsquo;s generation as search queries. Retrieval can be triggered if any token of the temporary next sentence has a probability lower than a threshold $\theta$ ($\theta=0$ means never retrieve and $\theta=1$ means always retrieve). Alternatively, the temporary next sentence can be directly used as the query. In the latter approach, there&rsquo;s a risk of perpetuating errors when the temporary sentence is incorrect. To avoid such errors, the authors propose either masking out low-confidence tokens or using the following prompt to generate questions for the low-confidence span.
<div class="details admonition quote">
    <div class="details-summary admonition-title">
        <span class="icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"/></svg></span>Prompt: zero-shot question generation<span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
    </div>
    <div class="details-content">
        <div class="admonition-content"><pre><code>User input x.

Generated output so far y ≤ t.

Given the above passage, ask a question to which the answer is the term/entity/phrase “z”. 
</code></pre>
</div></div></div></li>
</ol>
<p>FLARE is applicable to any existing LM at inference time without additional training. However, interleaving retrieval and generation increases the overhead and cost of generation. Its query formulation strategy hinges on the assumption that LMs are well-calibrated, and a low probability/confidence of each token indicates a lack of knowledge. Many tokens are functional words (such as &lsquo;am&rsquo;, &rsquo;to&rsquo;, &lsquo;in&rsquo;, etc.), lacking semantic importance or influence. Confidence scores for these tokens may not capture the real retrieval needs <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>. LLMs are also prone to self-bias, so relying on the knowledge sufficiency of LLMs solely based on their outputs can become problematic <sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>. Some studies found FLARE&rsquo;s performance to vary significantly among different datasets <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>, and subpar with self-aware scenarios <sup id="fnref1:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>. Its efficacy also depends on the meticulously crafted few-shot prompts and manual rules.</p>
<h3 id="improving-flares-understanding-of-llms-information-needs-during-generation" class="headerLink">
    <a href="#improving-flares-understanding-of-llms-information-needs-during-generation" class="header-mark"></a>Improving FLARE&rsquo;s Understanding of LLM&rsquo;s Information Needs During Generation</h3><p>In DRAGIN (<strong>D</strong>ynamic <strong>RAG</strong> based on <strong>I</strong>nformation <strong>N</strong>eeds of LLMs), Su et al.<sup id="fnref1:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup> argue that dynamic RAG methods like FLARE (discussed above) can be a bit one-dimensional since relying solely on a single metric, like token generation probability, to decide when to retrieve is not enough. A model may be uncertain about an unimportant word or confident about a factually incorrect word, leading to a suboptimal retrieval decision. The authors propose a method called RIND (<strong>R</strong>eal-time <strong>I</strong>nformation <strong>N</strong>eeds <strong>D</strong>etection) that decides to activate retrieval if a $S_{\text{RIND}}$ score value exceeds a predefined threshold. This score is calculated per-token as the product of three factors:</p>
<ul>
<li><strong>Uncertainty</strong>: The entropy of the token&rsquo;s probability distribution. A high entropy score signals that the model is uncertain about which token to generate next.</li>
<li><strong>Influence</strong>: The impact a token has on subsequent tokens, computed by tracking its maximum self-attention value from all the following tokens in the sequence. This helps in identifying words that are critical for shaping the rest of the output.</li>
<li><strong>Semantic Value</strong>: A simple binary filter that removes common stopwords, so that the retrieval decision is based only on tokens with significant meaning.
<figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/dragin_framework.png" title="The DRAGIN Framework" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/dragin_framework.png" data-sub-html="<h2>The DRAGIN Framework</h2><p>The DRAGIN Framework</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/dragin_framework.png'   alt="The DRAGIN Framework"  ></a><figcaption class="image-caption">The DRAGIN Framework</figcaption>
</figure></li>
</ul>
<p>DRAGIN can be integrated into any transformer-based LMs without requiring fine-tuning or prompt engineering. However, its application is limited to open-source models since it accesses the self-attention scores from the last Transformer layer of the LLM. The corresponding code and data for this project are available on <a href="https://github.com/oneal2000/DRAGIN/" target="_blank" rel="noopener noreferrer">GitHub</a>.</p>
<h3 id="detect-entity-level-hallucinations" class="headerLink">
    <a href="#detect-entity-level-hallucinations" class="header-mark"></a>Detect Entity-level Hallucinations</h3><p>In DRAD (<strong>D</strong>ynamic <strong>R</strong>etrieval <strong>A</strong>ugmentation based on hallucination <strong>D</strong>etection), Su et al.<sup id="fnref1:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup> argue that the uncertainty in a model&rsquo;s output is not spread evenly across all words. Instead, hallucinations that stem from a model&rsquo;s knowledge gaps are most likely to occur when the model is generating a fact-based entity like a person, place, or date that it doesn&rsquo;t actually know. As an example, given the input &ldquo;Bill Clinton was born and raised in __&rdquo;, LLM can confidently assign a high probability to the token &ldquo;Arkansas&rdquo; to complete the sentence, since it has likely seen extensive information about Bill Clinton during the pre-training phase. The authors propose a Real-time Hallucination Detection (RHD) module that performs real-time Named Entity Recognition (NER) during text generation, on the LLM&rsquo;s output stream. When an entity is detected, RHD assesses the model&rsquo;s confidence by calculating the following two metrics from the model&rsquo;s internal state:</p>
<ul>
<li><strong>Entity-level Probability</strong>: This is the joint probability across all tokens that make up the detected entity (e.g., &ldquo;King Demetrius II&rdquo;). A low probability indicates that the model is not confident about the entity.</li>
<li><strong>Entity-level Entropy</strong>: This is the uncertainty in the model&rsquo;s output distribution for each token in the entity. A high entropy value indicates the model may have faced a flat probability distribution and may have selected a random entity.
<figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/rhd_framework.png" title="Real-time Hallucination Detection (RHD) module" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/rhd_framework.png" data-sub-html="<h2>Real-time Hallucination Detection (RHD) module of the DRAD framework</h2><p>Real-time Hallucination Detection (RHD) module</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/rhd_framework.png'   alt="Real-time Hallucination Detection (RHD) module"  ></a><figcaption class="image-caption">Real-time Hallucination Detection (RHD) module of the DRAD framework</figcaption>
</figure></li>
</ul>
<p>The two metrics are compared against their respective preset thresholds. A hallucination is flagged if the entity probability score is lower than its threshold or the entropy score is higher than its threshold. Finally, retrieval-augmentation is initiated if RHD detects hallucination. The code and data for this project are available on <a href="https://github.com/oneal2000/EntityHallucination" target="_blank" rel="noopener noreferrer">GitHub</a>.</p>
<h3 id="deciding-when-to-retrieve-by-steering-for-honesty" class="headerLink">
    <a href="#deciding-when-to-retrieve-by-steering-for-honesty" class="header-mark"></a>Deciding When to Retrieve by Steering for Honesty</h3><p>In <strong>C</strong>on<strong>tr</strong>o<strong>l</strong>-based <strong>A</strong>daptive RAG (CTRLA), Liu et al.<sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup> argue that many existing Adaptive RAG approaches that rely on statistical uncertainty, such as output probability or token entropy, have two fundamental problems. First, LLMs are not always &ldquo;honest&rdquo;, i.e., they might generate plausible but incorrect responses, instead of admitting lack of knowledge, meaning that the output tokens may not truly reflect the model&rsquo;s knowledge limitations. Second, low uncertainty doesn&rsquo;t always mean high confidence. For example, a model can be highly certain that it doesn&rsquo;t know something. The authors propose a framework to infer the model&rsquo;s knowledge gaps from a representation perspective, using the model&rsquo;s own internal states to guide retrieval.
<figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/ctrla_framework.png" title="CTRLA Framework" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/ctrla_framework.png" data-sub-html="<h2>CTRLA Framework</h2><p>CTRLA Framework</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/ctrla_framework.png'   alt="CTRLA Framework"  ></a><figcaption class="image-caption">CTRLA Framework</figcaption>
</figure></p>
<ul>
<li><strong>Honesty and Confidence Feature Extraction</strong>: The CTRLA framework begins by extracting directional vectors for honesty and confidence from LLM&rsquo;s internal layers. This is done using contrastive instructions, where pairs of positive &ldquo;Pretend you&rsquo;re an honest/confident person&rdquo; and negative &ldquo;Pretend you&rsquo;re a dishonest/unconfident person&rdquo; instructions, along with a statement, are fed into the LLM to get token representations for each case. For each token and each layer, a contrastive vector is calculated by subtracting the negative representation from the positive one. This vector effectively points in the direction of the desired trait (honesty or confidence). Principal Component Analysis is applied to these vectors at each layer to find the first principal component, which serves as the general directional feature. This one-time, offline process results in honesty and confidence vectors that are used to control LLM behavior and guide retrieval timing.
<figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/ctrla_examples.png" title="Examples of honesty steering and confidence monitoring" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/ctrla_examples.png" data-sub-html="<h2>Examples of honesty steering (top) and confidence monitoring (bottom), unconfident tokens are marked in red.</h2><p>Examples of honesty steering and confidence monitoring</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/ctrla_examples.png'   alt="Examples of honesty steering and confidence monitoring"  ></a><figcaption class="image-caption">Examples of honesty steering (top) and confidence monitoring (bottom), unconfident tokens are marked in red.</figcaption>
</figure></li>
<li><strong>Honesty Steering and Confidence Monitoring</strong>: During inference, when the LLM generates each token, its internal representation is modified at each layer by adding the pre-computed honesty feature vector, nudging the model to be more truthful. Simultaneously, a confidence score is computed for each token by measuring the alignment of its representation with the confidence feature vector (using dot-product followed by mean-pooling across layers and scaling). Finally, the confidence score is monitored for tokens that are not stopwords and not present in the original query or previous generations. Retrieval is triggered if this confidence falls below a certain threshold.
The official implementation for CtrlA is available on <a href="https://github.com/HSLiu-Initial/CtrlA" target="_blank" rel="noopener noreferrer">GitHub</a>.</li>
</ul>
<h3 id="looking-for-signs-of-uncertainty-in-llms-internal-states" class="headerLink">
    <a href="#looking-for-signs-of-uncertainty-in-llms-internal-states" class="header-mark"></a>Looking for Signs of Uncertainty in LLMs&rsquo; Internal States</h3><p>In SeaKR (<strong>SE</strong>lf-<strong>A</strong>ware <strong>K</strong>nowledge <strong>R</strong>etrieval for RAG), Yao et al.<sup id="fnref1:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup> also argue that relying on observing LLM&rsquo;s output to decide when to retrieve is insufficient, as models can be prone to self-bias. The authors hypothesize that LLMs possess a form of self-awareness about their uncertainty, which can be measured by observing the consistency of their internal states. They propose calculating a Self-aware Uncertainty Estimate U(c) by measuring the statistical consistency of the model&rsquo;s internal states, via the Gram determinant of their hidden state vectors, across multiple parallel pseudo-generations.
<figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/seakr_uncertainty_extraction_process.png" title="The Self-Aware Uncertanity Extraction module" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/seakr_uncertainty_extraction_process.png" data-sub-html="<h2>Self-Aware Uncertanity Extraction module of SeaKR</h2><p>The Self-Aware Uncertanity Extraction module</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/seakr_uncertainty_extraction_process.png'   alt="The Self-Aware Uncertanity Extraction module"  ></a><figcaption class="image-caption">Self-Aware Uncertanity Extraction module of SeaKR</figcaption>
</figure>
For a given prompt, SeaKR prompts the LLM to generate multiple (k) different outputs for the same input context in parallel. The authors use k=20 for their experiments. For each of the k generations, SeaKR extracts the hidden representation of the final <code>&lt;EOS&gt;</code> (end-of-sequence) token from one of the model&rsquo;s middle layers. This <code>&lt;EOS&gt;</code> representation is treated as the compressed representation of the entire generation process. The authors then compute consistency among these k hidden state vectors by computing their Gram determinant. A large determinant value implies that the vectors are more scattered and diverse, indicating LLM&rsquo;s higher uncertainty about the task. If this uncertainty score is higher than a pre-defined threshold, SeaKR concludes that LLM lacks sufficient internal knowledge to answer the question and triggers retrieval.</p>
<p>As SeaKR requires access to the internal state of LLMs, its use is limited to open-source models. Conducting k parallel pseudo-generations is also computationally expensive. The code for SeaKR is available on <a href="https://github.com/THU-KEG/SeaKR" target="_blank" rel="noopener noreferrer">GitHub</a>.</p>
<h3 id="using-llm-confidence-shift-as-a-retrieval-gate" class="headerLink">
    <a href="#using-llm-confidence-shift-as-a-retrieval-gate" class="header-mark"></a>Using LLM Confidence Shift as a Retrieval Gate</h3><p>In CBDR (Confidence-based Dynamic Retrieval), Jin et al.<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup> propose an ARAG method that uses LLM confidence to not only decide when to retrieve, but also to optimize what gets retrieved. The authors hypothesize that LLM&rsquo;s internal hidden state reflected confidence serves as a strong indicator of its confidence. More importantly, the confidence shift observed when LLM processes different retrieved contexts for the same question reveals its intrinsic preference among those contexts. This intrinsic preference signal can guide a reranker model to filter and optimize contexts post-retrieval to boost the RAG system&rsquo;s efficacy.</p>
<p><figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/two_phases_of_cbdr_framework.png" title="The CBDR framework." data-thumbnail="/img/posts/2025/retrieval-timing-judgement/two_phases_of_cbdr_framework.png" data-sub-html="<h2>The two phases of CBDR framework.</h2><p>The CBDR framework.</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/two_phases_of_cbdr_framework.png'   alt="The CBDR framework."  ></a><figcaption class="image-caption">The two phases of CBDR framework.</figcaption>
</figure></p>
<p>Their framework operates in two phases:</p>
<p><strong>Phase 1: Offline Reranker Alignment</strong></p>
<ol>
<li>
<p><strong>Confidence Detection Model Training</strong>: First, a binary classifier is trained to act as a &ldquo;Confidence Detection Model&rdquo;. This model takes the LLM&rsquo;s middle-layer hidden state, captured just before the first answer token is generated, and predicts whether the LLM will answer the query correctly. The output of the model is a confidence score ($\text{Conf}(H_M,Q)$), which represents the probability of a correct answer.</p>
</li>
<li>
<p><strong>Preference Dataset Creation</strong>: This confidence model is then used to measure how much a given context $C_i$ helps the LLM. This is calculated as a &ldquo;confidence shift&rdquo; or increase:
$$ \text{Inc}(Q, C_i) = \text{Conf}(H_{M,Q+C_i}) - \text{Conf}(H_{M,Q}) $$
Contexts that increase confidence ($\text{Inc} \gt 0$) are labeled as positive preferences, while those that decrease it are labeled as negative. This process builds a preference dataset (called NQ_Rerank) based on the LLM&rsquo;s actual internal state changes.</p>
</li>
<li>
<p><strong>Reranker Fine-tuning</strong>: Finally, a standard reranker is fine-tuned on the NQ_Rerank dataset. This teaches the reranker to prioritize documents that will produce the largest positive confidence shift in downstream LLM, creating a powerful alignment between the two components instead of relying on simply semantic similarity.)</p>
</li>
</ol>
<p><figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/cbdr_reranking_strategies.png" title="Comparison of CBDR reranking strategy" data-thumbnail="/img/posts/2025/retrieval-timing-judgement/cbdr_reranking_strategies.png" data-sub-html="<h2>A conventional similarity-based reranker (left) prioritizes contexts solely through textual matching between questions and documents. In CBDR (right), LLM&#39;s intrinsic preferences are leveraged to rerank contexts according to their ability to enhance the model&#39;s answer confidence.</h2><p>Comparison of CBDR reranking strategy</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/cbdr_reranking_strategies.png'   alt="Comparison of CBDR reranking strategy"  ></a><figcaption class="image-caption">A conventional similarity-based reranker (left) prioritizes contexts solely through textual matching between questions and documents. In CBDR (right), LLM's intrinsic preferences are leveraged to rerank contexts according to their ability to enhance the model's answer confidence.</figcaption>
</figure></p>
<p><strong>Phase 2: Online Dynamic Retrieval</strong></p>
<p><figure><a class="lightgallery" href="/img/posts/2025/retrieval-timing-judgement/cbdr_workflow.png" title="The CBDR Workflow." data-thumbnail="/img/posts/2025/retrieval-timing-judgement/cbdr_workflow.png" data-sub-html="<h2>The CBDR Workflow.</h2><p>The CBDR Workflow.</p>"><img  loading="lazy" src='/img/posts/2025/retrieval-timing-judgement/cbdr_workflow.png'   alt="The CBDR Workflow."  ></a><figcaption class="image-caption">The CBDR Workflow.</figcaption>
</figure></p>
<p>During inference, the system uses the trained confidence model to gate the retrieval process efficiently. For any incoming query, the system first calculates the LLM&rsquo;s initial confidence, $\text{Conf}(H_{M,Q})$. This score is then compared against a predefined threshold $\beta$. If $\text{Conf}(H_{M,Q}) \gt \beta$, retrieval is skipped entirely and the LLM generates the answer using only parametric knowledge. Otherwise, the full retrieval-reranking pipeline is activated.</p>
<h3 id="trade-offs-and-limitations-2" class="headerLink">
    <a href="#trade-offs-and-limitations-2" class="header-mark"></a>Trade-offs and Limitations</h3><p>Internal state-based ARAG approaches provide the most direct and potentially the most accurate measure of the model&rsquo;s internal uncertainty. Once the initial setup is complete, monitoring the hidden states during a single generation pass is also computationally efficient compared to approaches that generate multiple full responses. However, their biggest limitation is that they require white-box access to model weights and internal states, making them incompatible with closed-source APIs. They also require an understanding of model architecture, may not generalize across different model families, and are significantly more complex than simple text-based methods.</p>
<h2 id="summary" class="headerLink">
    <a href="#summary" class="header-mark"></a>Summary</h2><p>This post dived deep into the confidence-based selective retrieval paradigm. It introduced three categories of approaches for deciding whether an LLM needs external help by detecting the model&rsquo;s knowledge boundary. We started with simple prompt-based methods that rely on LLM self-reflection, then look at consistency-based approaches that infer uncertainty from behavioral patterns, and finally, the sophisticated internal state analysis that directly probes the model&rsquo;s hidden representations. We established that there is no universal best approach and each category presents a unique set of trade-offs between accuracy, cost, latency, and implementation complexity. In the next post of this Adaptive RAG series, we will shift focus to training-based approaches for treat adaptive retrieval as a learned skill.</p>
<h2 id="references" class="headerLink">
    <a href="#references" class="header-mark"></a>References</h2><div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Ni, S., Bi, K., Guo, J., &amp; Cheng, X. (2024). When Do LLMs Need Retrieval Augmentation? Mitigating LLMs&rsquo; Overconfidence Helps Retrieval Augmentation. <em>ArXiv</em>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Zhang, Z., Fang, M., &amp; Chen, L. (2024). RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering. <em>ArXiv</em>.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Wang, X., Sen, P., Li, R., &amp; Yilmaz, E. (2024). Adaptive Retrieval-Augmented Generation for Conversational Systems. <em>ArXiv</em>.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Wang, H., Xue, B., Zhou, B., Zhang, T., Wang, C., Wang, H., Chen, G., &amp; Wong, K. (2024). Self-DC: When to Reason and When to Act? Self Divide-and-Conquer for Compositional Unknown Questions. <em>ArXiv</em>.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Li, J., Tang, T., Zhao, W. X., Wang, J., Nie, J., &amp; Wen, J. (2023). The Web Can Be Your Oyster for Improving Large Language Models. <em>ArXiv</em>.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Zubkova, H., Park, J., &amp; Lee, S. (2025). SUGAR: Leveraging Contextual Confidence for Smarter Retrieval. <em>ArXiv</em>.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Dhole, K. D. (2025). To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic Retrieval Augmented Generation. <em>ArXiv</em>.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Jiang, Z., Xu, F. F., Gao, L., Sun, Z., Liu, Q., Yang, Y., Callan, J., &amp; Neubig, G. (2023). Active Retrieval Augmented Generation. <em>ArXiv</em>.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Cheng, Q., Li, X., Li, S., Zhu, Q., Yin, Z., Shao, Y., Li, L., Sun, T., Yan, H., &amp; Qiu, X. (2024). Unified Active Retrieval for Retrieval Augmented Generation. <em>ArXiv</em>.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Chen, W., Qi, G., Li, W., Li, Y., Xia, D., &amp; Huang, J. (2025). PAIRS: Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG. <em>ArXiv</em>.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Su, W., Tang, Y., Ai, Q., Wang, C., Wu, Z., &amp; Liu, Y. (2024). Mitigating Entity-Level Hallucination in Large Language Models. <em>ArXiv</em>.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Yao, Z., Qi, W., Pan, L., Cao, S., Hu, L., Liu, W., Hou, L., &amp; Li, J. (2024). SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented Generation. <em>ArXiv</em>.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Su, W., Tang, Y., Ai, Q., Wu, Z., &amp; Liu, Y. (2024). DRAGIN: Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models. <em>ArXiv</em>.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Liu, H., Zhang, H., Guo, Z., Wang, J., Dong, K., Li, X., Lee, Y. Q., Zhang, C., &amp; Liu, Y. (2024). CtrlA: Adaptive Retrieval-Augmented Generation via Inherent Control. <em>ArXiv</em>.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Jin, H., Li, R., Lu, Z., &amp; Miao, Q. (2025). Rethinking LLM Parametric Knowledge as Post-retrieval Confidence for Dynamic Retrieval and Reranking. <em>ArXiv</em>.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
</div>
		
        


<h2>Related Content</h2>
<div class="related-container">
    <div class="related-item-container">
            <div class="related-image">
                <a href="/posts/2025/09/deciding-when-not-to-retrieve/"><img   src='/posts/2025/09/deciding-when-not-to-retrieve/featured-image-preview.webp'    height="200" width="400"></a>
            </div><h2 class="related-title">
                <a href="/posts/2025/09/deciding-when-not-to-retrieve/">Deciding When Not to Retrieve: Adaptive RAG, Part 2</a>
            </h2>
        </div>
    <div class="related-item-container">
            <div class="related-image">
                <a href="/posts/2025/09/problems-with-naive-rag/"><img   src='/posts/2025/09/problems-with-naive-rag/featured-image-preview.webp'    height="200" width="400"></a>
            </div><h2 class="related-title">
                <a href="/posts/2025/09/problems-with-naive-rag/">The Hidden Costs of Naive Retrieval: Adaptive RAG, Part 1</a>
            </h2>
        </div>
    <div class="related-item-container">
            <div class="related-image">
                <a href="/posts/2023/12/towards-ranking-aware-llms/"><img   src='/posts/2023/12/towards-ranking-aware-llms/featured-image-preview.webp'    height="200" width="400"></a>
            </div><h2 class="related-title">
                <a href="/posts/2023/12/towards-ranking-aware-llms/">Strategies for Effective and Efficient Text Ranking Using Large Language Models</a>
            </h2>
        </div>
    <div class="related-item-container">
            <div class="related-image">
                <a href="/posts/2023/12/prompting-llm-for-ranking/"><img   src='/posts/2023/12/prompting-llm-for-ranking/featured-image-preview.webp'    height="200" width="400"></a>
            </div><h2 class="related-title">
                <a href="/posts/2023/12/prompting-llm-for-ranking/">Prompting-based Methods for Text Ranking Using Large Language Models</a>
            </h2>
        </div>
    <div class="related-item-container">
            <div class="related-image">
                <a href="/posts/2023/09/generative-retrieval/"><img   src='/posts/2023/09/generative-retrieval/featured-image-preview.webp'    height="200" width="400"></a>
            </div><h2 class="related-title">
                <a href="/posts/2023/09/generative-retrieval/">Generative Retrieval for End-to-End Search Systems</a>
            </h2>
        </div>
    

</div>


        <script src="https://f.convertkit.com/ckjs/ck.5.js"></script>
      <form action="https://app.convertkit.com/forms/4932644/subscriptions" class="seva-form formkit-form" method="post" data-sv-form="4932644" data-uid="e309c832a6" data-format="inline" data-version="5" data-options="{&quot;settings&quot;:{&quot;after_subscribe&quot;:{&quot;action&quot;:&quot;message&quot;,&quot;success_message&quot;:&quot;Success! Now check your email to confirm your subscription.&quot;,&quot;redirect_url&quot;:&quot;&quot;},&quot;analytics&quot;:{&quot;google&quot;:null,&quot;fathom&quot;:null,&quot;facebook&quot;:null,&quot;segment&quot;:null,&quot;pinterest&quot;:null,&quot;sparkloop&quot;:null,&quot;googletagmanager&quot;:null},&quot;modal&quot;:{&quot;trigger&quot;:&quot;timer&quot;,&quot;scroll_percentage&quot;:null,&quot;timer&quot;:5,&quot;devices&quot;:&quot;all&quot;,&quot;show_once_every&quot;:15},&quot;powered_by&quot;:{&quot;show&quot;:true,&quot;url&quot;:&quot;https://convertkit.com/features/forms?utm_campaign=poweredby&amp;utm_content=form&amp;utm_medium=referral&amp;utm_source=dynamic&quot;},&quot;recaptcha&quot;:{&quot;enabled&quot;:false},&quot;return_visitor&quot;:{&quot;action&quot;:&quot;show&quot;,&quot;custom_content&quot;:&quot;&quot;},&quot;slide_in&quot;:{&quot;display_in&quot;:&quot;bottom_right&quot;,&quot;trigger&quot;:&quot;timer&quot;,&quot;scroll_percentage&quot;:null,&quot;timer&quot;:5,&quot;devices&quot;:&quot;all&quot;,&quot;show_once_every&quot;:15},&quot;sticky_bar&quot;:{&quot;display_in&quot;:&quot;top&quot;,&quot;trigger&quot;:&quot;timer&quot;,&quot;scroll_percentage&quot;:null,&quot;timer&quot;:5,&quot;devices&quot;:&quot;all&quot;,&quot;show_once_every&quot;:15}},&quot;version&quot;:&quot;5&quot;}" min-width="400 500 600 700 800" style="background-color: rgb(249, 250, 251); border-radius: 4px;"><div class="formkit-background" style="opacity: 0.33;"></div><div data-style="minimal"><div class="formkit-header" data-element="header" style="color: rgb(77, 77, 77); font-size: 27px; font-weight: 700;"><h2>Be the First to Know</h2></div><div class="formkit-subheader" data-element="subheader" style="color: rgb(104, 104, 104); font-size: 18px;"><p>Subscribe to get notified when I write a new post.</p></div><ul class="formkit-alert formkit-alert-error" data-element="errors" data-group="alert"></ul><div data-element="fields" data-stacked="false" class="seva-fields formkit-fields"><div class="formkit-field"><input class="formkit-input" name="email_address" aria-label="Email Address" placeholder="Email Address" required="" type="email" style="color: rgb(0, 0, 0); border-color: rgb(227, 227, 227); border-radius: 4px; font-weight: 400;"></div><button data-element="submit" class="formkit-submit formkit-submit" style="color: rgb(255, 255, 255); background-color: rgb(22, 119, 190); border-radius: 4px; font-weight: 400;"><div class="formkit-spinner"><div></div><div></div><div></div></div><span class="">Subscribe</span></button></div><div class="formkit-guarantee" data-element="guarantee" style="color: rgb(77, 77, 77); font-size: 13px; font-weight: 400;"><p>We won't send you spam. Unsubscribe at any time.</p></div><div class="formkit-powered-by-convertkit-container"><a href="https://convertkit.com/features/forms?utm_campaign=poweredby&amp;utm_content=form&amp;utm_medium=referral&amp;utm_source=dynamic" data-element="powered-by" class="formkit-powered-by-convertkit" data-variant="dark" target="_blank" rel="nofollow">Built with ConvertKit</a></div></div><style>.formkit-form[data-uid="e309c832a6"] *{box-sizing:border-box;}.formkit-form[data-uid="e309c832a6"]{-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;}.formkit-form[data-uid="e309c832a6"] legend{border:none;font-size:inherit;margin-bottom:10px;padding:0;position:relative;display:table;}.formkit-form[data-uid="e309c832a6"] fieldset{border:0;padding:0.01em 0 0 0;margin:0;min-width:0;}.formkit-form[data-uid="e309c832a6"] body:not(:-moz-handler-blocked) fieldset{display:table-cell;}.formkit-form[data-uid="e309c832a6"] h1,.formkit-form[data-uid="e309c832a6"] h2,.formkit-form[data-uid="e309c832a6"] h3,.formkit-form[data-uid="e309c832a6"] h4,.formkit-form[data-uid="e309c832a6"] h5,.formkit-form[data-uid="e309c832a6"] h6{color:inherit;font-size:inherit;font-weight:inherit;}.formkit-form[data-uid="e309c832a6"] h2{font-size:1.5em;margin:1em 0;}.formkit-form[data-uid="e309c832a6"] h3{font-size:1.17em;margin:1em 0;}.formkit-form[data-uid="e309c832a6"] p{color:inherit;font-size:inherit;font-weight:inherit;}.formkit-form[data-uid="e309c832a6"] ol:not([template-default]),.formkit-form[data-uid="e309c832a6"] ul:not([template-default]),.formkit-form[data-uid="e309c832a6"] blockquote:not([template-default]){text-align:left;}.formkit-form[data-uid="e309c832a6"] p:not([template-default]),.formkit-form[data-uid="e309c832a6"] hr:not([template-default]),.formkit-form[data-uid="e309c832a6"] blockquote:not([template-default]),.formkit-form[data-uid="e309c832a6"] ol:not([template-default]),.formkit-form[data-uid="e309c832a6"] ul:not([template-default]){color:inherit;font-style:initial;}.formkit-form[data-uid="e309c832a6"] .ordered-list,.formkit-form[data-uid="e309c832a6"] .unordered-list{list-style-position:outside !important;padding-left:1em;}.formkit-form[data-uid="e309c832a6"] .list-item{padding-left:0;}.formkit-form[data-uid="e309c832a6"][data-format="modal"]{display:none;}.formkit-form[data-uid="e309c832a6"][data-format="slide in"]{display:none;}.formkit-form[data-uid="e309c832a6"][data-format="sticky bar"]{display:none;}.formkit-sticky-bar .formkit-form[data-uid="e309c832a6"][data-format="sticky bar"]{display:block;}.formkit-form[data-uid="e309c832a6"] .formkit-input,.formkit-form[data-uid="e309c832a6"] .formkit-select,.formkit-form[data-uid="e309c832a6"] .formkit-checkboxes{width:100%;}.formkit-form[data-uid="e309c832a6"] .formkit-button,.formkit-form[data-uid="e309c832a6"] .formkit-submit{border:0;border-radius:5px;color:#ffffff;cursor:pointer;display:inline-block;text-align:center;font-size:15px;font-weight:500;cursor:pointer;margin-bottom:15px;overflow:hidden;padding:0;position:relative;vertical-align:middle;}.formkit-form[data-uid="e309c832a6"] .formkit-button:hover,.formkit-form[data-uid="e309c832a6"] .formkit-submit:hover,.formkit-form[data-uid="e309c832a6"] .formkit-button:focus,.formkit-form[data-uid="e309c832a6"] .formkit-submit:focus{outline:none;}.formkit-form[data-uid="e309c832a6"] .formkit-button:hover > span,.formkit-form[data-uid="e309c832a6"] .formkit-submit:hover > span,.formkit-form[data-uid="e309c832a6"] .formkit-button:focus > span,.formkit-form[data-uid="e309c832a6"] .formkit-submit:focus > span{background-color:rgba(0,0,0,0.1);}.formkit-form[data-uid="e309c832a6"] .formkit-button > span,.formkit-form[data-uid="e309c832a6"] .formkit-submit > span{display:block;-webkit-transition:all 300ms ease-in-out;transition:all 300ms ease-in-out;padding:12px 24px;}.formkit-form[data-uid="e309c832a6"] .formkit-input{background:#ffffff;font-size:15px;padding:12px;border:1px solid #e3e3e3;-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;line-height:1.4;margin:0;-webkit-transition:border-color ease-out 300ms;transition:border-color ease-out 300ms;}.formkit-form[data-uid="e309c832a6"] .formkit-input:focus{outline:none;border-color:#1677be;-webkit-transition:border-color ease 300ms;transition:border-color ease 300ms;}.formkit-form[data-uid="e309c832a6"] .formkit-input::-webkit-input-placeholder{color:inherit;opacity:0.8;}.formkit-form[data-uid="e309c832a6"] .formkit-input::-moz-placeholder{color:inherit;opacity:0.8;}.formkit-form[data-uid="e309c832a6"] .formkit-input:-ms-input-placeholder{color:inherit;opacity:0.8;}.formkit-form[data-uid="e309c832a6"] .formkit-input::placeholder{color:inherit;opacity:0.8;}.formkit-form[data-uid="e309c832a6"] [data-group="dropdown"]{position:relative;display:inline-block;width:100%;}.formkit-form[data-uid="e309c832a6"] [data-group="dropdown"]::before{content:"";top:calc(50% - 2.5px);right:10px;position:absolute;pointer-events:none;border-color:#4f4f4f transparent transparent transparent;border-style:solid;border-width:6px 6px 0 6px;height:0;width:0;z-index:999;}.formkit-form[data-uid="e309c832a6"] [data-group="dropdown"] select{height:auto;width:100%;cursor:pointer;color:#333333;line-height:1.4;margin-bottom:0;padding:0 6px;-webkit-appearance:none;-moz-appearance:none;appearance:none;font-size:15px;padding:12px;padding-right:25px;border:1px solid #e3e3e3;background:#ffffff;}.formkit-form[data-uid="e309c832a6"] [data-group="dropdown"] select:focus{outline:none;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"]{text-align:left;margin:0;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"]{margin-bottom:10px;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] *{cursor:pointer;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"]:last-of-type{margin-bottom:0;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] input[type="checkbox"]{display:none;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] input[type="checkbox"] + label::after{content:none;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] input[type="checkbox"]:checked + label::after{border-color:#ffffff;content:"";}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] input[type="checkbox"]:checked + label::before{background:#10bf7a;border-color:#10bf7a;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] label{position:relative;display:inline-block;padding-left:28px;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] label::before,.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] label::after{position:absolute;content:"";display:inline-block;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] label::before{height:16px;width:16px;border:1px solid #e3e3e3;background:#ffffff;left:0px;top:3px;}.formkit-form[data-uid="e309c832a6"] [data-group="checkboxes"] [data-group="checkbox"] label::after{height:4px;width:8px;border-left:2px solid #4d4d4d;border-bottom:2px solid #4d4d4d;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);left:4px;top:8px;}.formkit-form[data-uid="e309c832a6"] .formkit-alert{background:#f9fafb;border:1px solid #e3e3e3;border-radius:5px;-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;list-style:none;margin:25px auto;padding:12px;text-align:center;width:100%;}.formkit-form[data-uid="e309c832a6"] .formkit-alert:empty{display:none;}.formkit-form[data-uid="e309c832a6"] .formkit-alert-success{background:#d3fbeb;border-color:#10bf7a;color:#0c905c;}.formkit-form[data-uid="e309c832a6"] .formkit-alert-error{background:#fde8e2;border-color:#f2643b;color:#ea4110;}.formkit-form[data-uid="e309c832a6"] .formkit-spinner{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:0px;width:0px;margin:0 auto;position:absolute;top:0;left:0;right:0;width:0px;overflow:hidden;text-align:center;-webkit-transition:all 300ms ease-in-out;transition:all 300ms ease-in-out;}.formkit-form[data-uid="e309c832a6"] .formkit-spinner > div{margin:auto;width:12px;height:12px;background-color:#fff;opacity:0.3;border-radius:100%;display:inline-block;-webkit-animation:formkit-bouncedelay-formkit-form-data-uid-e309c832a6- 1.4s infinite ease-in-out both;animation:formkit-bouncedelay-formkit-form-data-uid-e309c832a6- 1.4s infinite ease-in-out both;}.formkit-form[data-uid="e309c832a6"] .formkit-spinner > div:nth-child(1){-webkit-animation-delay:-0.32s;animation-delay:-0.32s;}.formkit-form[data-uid="e309c832a6"] .formkit-spinner > div:nth-child(2){-webkit-animation-delay:-0.16s;animation-delay:-0.16s;}.formkit-form[data-uid="e309c832a6"] .formkit-submit[data-active] .formkit-spinner{opacity:1;height:100%;width:50px;}.formkit-form[data-uid="e309c832a6"] .formkit-submit[data-active] .formkit-spinner ~ span{opacity:0;}.formkit-form[data-uid="e309c832a6"] .formkit-powered-by[data-active="false"]{opacity:0.35;}.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit-container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;width:100%;z-index:5;margin:10px 0;position:relative;}.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit-container[data-active="false"]{opacity:0.35;}.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:#ffffff;border:1px solid #dde2e7;border-radius:4px;color:#373f45;cursor:pointer;display:block;height:36px;margin:0 auto;opacity:0.95;padding:0;-webkit-text-decoration:none;text-decoration:none;text-indent:100%;-webkit-transition:ease-in-out all 200ms;transition:ease-in-out all 200ms;white-space:nowrap;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:190px;background-repeat:no-repeat;background-position:center;background-image:url("data:image/svg+xml;charset=utf8,%3Csvg width='162' height='20' viewBox='0 0 162 20' fill='none' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='M83.0561 15.2457C86.675 15.2457 89.4722 12.5154 89.4722 9.14749C89.4722 5.99211 86.8443 4.06563 85.1038 4.06563C82.6801 4.06563 80.7373 5.76407 80.4605 8.28551C80.4092 8.75244 80.0387 9.14403 79.5686 9.14069C78.7871 9.13509 77.6507 9.12841 76.9314 9.13092C76.6217 9.13199 76.3658 8.88106 76.381 8.57196C76.4895 6.38513 77.2218 4.3404 78.618 2.76974C80.1695 1.02445 82.4289 0 85.1038 0C89.5979 0 93.8406 4.07791 93.8406 9.14749C93.8406 14.7608 89.1832 19.3113 83.1517 19.3113C78.8502 19.3113 74.5179 16.5041 73.0053 12.5795C72.9999 12.565 72.9986 12.5492 73.0015 12.534C73.0218 12.4179 73.0617 12.3118 73.1011 12.2074C73.1583 12.0555 73.2143 11.907 73.2062 11.7359L73.18 11.1892C73.174 11.0569 73.2075 10.9258 73.2764 10.8127C73.3452 10.6995 73.4463 10.6094 73.5666 10.554L73.7852 10.4523C73.9077 10.3957 74.0148 10.3105 74.0976 10.204C74.1803 10.0974 74.2363 9.97252 74.2608 9.83983C74.3341 9.43894 74.6865 9.14749 75.0979 9.14749C75.7404 9.14749 76.299 9.57412 76.5088 10.1806C77.5188 13.1 79.1245 15.2457 83.0561 15.2457Z' fill='%23373F45'/%3E%3Cpath d='M155.758 6.91365C155.028 6.91365 154.804 6.47916 154.804 5.98857C154.804 5.46997 154.986 5.06348 155.758 5.06348C156.53 5.06348 156.712 5.46997 156.712 5.98857C156.712 6.47905 156.516 6.91365 155.758 6.91365ZM142.441 12.9304V9.32833L141.415 9.32323V8.90392C141.415 8.44719 141.786 8.07758 142.244 8.07986L142.441 8.08095V6.55306L144.082 6.09057V8.08073H145.569V8.50416C145.569 8.61242 145.548 8.71961 145.506 8.81961C145.465 8.91961 145.404 9.01047 145.328 9.08699C145.251 9.16351 145.16 9.2242 145.06 9.26559C144.96 9.30698 144.853 9.32826 144.745 9.32822H144.082V12.7201C144.082 13.2423 144.378 13.4256 144.76 13.4887C145.209 13.5629 145.583 13.888 145.583 14.343V14.9626C144.029 14.9626 142.441 14.8942 142.441 12.9304Z' fill='%23373F45'/%3E%3Cpath d='M110.058 7.92554C108.417 7.88344 106.396 8.92062 106.396 11.5137C106.396 14.0646 108.417 15.0738 110.058 15.0318C111.742 15.0738 113.748 14.0646 113.748 11.5137C113.748 8.92062 111.742 7.88344 110.058 7.92554ZM110.07 13.7586C108.878 13.7586 108.032 12.8905 108.032 11.461C108.032 10.1013 108.878 9.20569 110.071 9.20569C111.263 9.20569 112.101 10.0995 112.101 11.459C112.101 12.8887 111.263 13.7586 110.07 13.7586Z' fill='%23373F45'/%3E%3Cpath d='M118.06 7.94098C119.491 7.94098 120.978 8.33337 120.978 11.1366V14.893H120.063C119.608 14.893 119.238 14.524 119.238 14.0689V10.9965C119.238 9.66506 118.747 9.16047 117.891 9.16047C117.414 9.16047 116.797 9.52486 116.502 9.81915V14.069C116.502 14.1773 116.481 14.2845 116.44 14.3845C116.398 14.4845 116.337 14.5753 116.261 14.6519C116.184 14.7284 116.093 14.7891 115.993 14.8305C115.893 14.8719 115.786 14.8931 115.678 14.8931H114.847V8.10918H115.773C115.932 8.10914 116.087 8.16315 116.212 8.26242C116.337 8.36168 116.424 8.50033 116.46 8.65577C116.881 8.19328 117.428 7.94098 118.06 7.94098ZM122.854 8.09713C123.024 8.09708 123.19 8.1496 123.329 8.2475C123.468 8.34541 123.574 8.48391 123.631 8.64405L125.133 12.8486L126.635 8.64415C126.692 8.48402 126.798 8.34551 126.937 8.2476C127.076 8.1497 127.242 8.09718 127.412 8.09724H128.598L126.152 14.3567C126.091 14.5112 125.986 14.6439 125.849 14.7374C125.711 14.831 125.549 14.881 125.383 14.8809H124.333L121.668 8.09713H122.854Z' fill='%23373F45'/%3E%3Cpath d='M135.085 14.5514C134.566 14.7616 133.513 15.0416 132.418 15.0416C130.496 15.0416 129.024 13.9345 129.024 11.4396C129.024 9.19701 130.451 7.99792 132.191 7.99792C134.338 7.99792 135.254 9.4378 135.158 11.3979C135.139 11.8029 134.786 12.0983 134.38 12.0983H130.679C130.763 13.1916 131.562 13.7662 132.615 13.7662C133.028 13.7662 133.462 13.7452 133.983 13.6481C134.535 13.545 135.085 13.9375 135.085 14.4985V14.5514ZM133.673 10.949C133.785 9.87621 133.061 9.28752 132.191 9.28752C131.321 9.28752 130.734 9.93979 130.679 10.9489L133.673 10.949Z' fill='%23373F45'/%3E%3Cpath d='M137.345 8.11122C137.497 8.11118 137.645 8.16229 137.765 8.25635C137.884 8.35041 137.969 8.48197 138.005 8.62993C138.566 8.20932 139.268 7.94303 139.759 7.94303C139.801 7.94303 140.068 7.94303 140.489 7.99913V8.7265C140.489 9.11748 140.15 9.4147 139.759 9.4147C139.31 9.4147 138.651 9.5829 138.131 9.8773V14.8951H136.462V8.11112L137.345 8.11122ZM156.6 14.0508V8.09104H155.769C155.314 8.09104 154.944 8.45999 154.944 8.9151V14.8748H155.775C156.23 14.8748 156.6 14.5058 156.6 14.0508ZM158.857 12.9447V9.34254H157.749V8.91912C157.749 8.46401 158.118 8.09506 158.574 8.09506H158.857V6.56739L160.499 6.10479V8.09506H161.986V8.51848C161.986 8.97359 161.617 9.34254 161.161 9.34254H160.499V12.7345C160.499 13.2566 160.795 13.44 161.177 13.503C161.626 13.5774 162 13.9024 162 14.3574V14.977C160.446 14.977 158.857 14.9086 158.857 12.9447ZM98.1929 10.1124C98.2033 6.94046 100.598 5.16809 102.895 5.16809C104.171 5.16809 105.342 5.44285 106.304 6.12953L105.914 6.6631C105.654 7.02011 105.16 7.16194 104.749 6.99949C104.169 6.7702 103.622 6.7218 103.215 6.7218C101.335 6.7218 99.9169 7.92849 99.9068 10.1123C99.9169 12.2959 101.335 13.5201 103.215 13.5201C103.622 13.5201 104.169 13.4717 104.749 13.2424C105.16 13.0799 105.654 13.2046 105.914 13.5615L106.304 14.0952C105.342 14.7819 104.171 15.0566 102.895 15.0566C100.598 15.0566 98.2033 13.2842 98.1929 10.1124ZM147.619 5.21768C148.074 5.21768 148.444 5.58663 148.444 6.04174V9.81968L151.82 5.58131C151.897 5.47733 151.997 5.39282 152.112 5.3346C152.227 5.27638 152.355 5.24607 152.484 5.24611H153.984L150.166 10.0615L153.984 14.8749H152.484C152.355 14.8749 152.227 14.8446 152.112 14.7864C151.997 14.7281 151.897 14.6436 151.82 14.5397L148.444 10.3025V14.0508C148.444 14.5059 148.074 14.8749 147.619 14.8749H146.746V5.21768H147.619Z' fill='%23373F45'/%3E%3Cpath d='M0.773438 6.5752H2.68066C3.56543 6.5752 4.2041 6.7041 4.59668 6.96191C4.99219 7.21973 5.18994 7.62695 5.18994 8.18359C5.18994 8.55859 5.09326 8.87061 4.8999 9.11963C4.70654 9.36865 4.42822 9.52539 4.06494 9.58984V9.63379C4.51611 9.71875 4.84717 9.88721 5.05811 10.1392C5.27197 10.3882 5.37891 10.7266 5.37891 11.1543C5.37891 11.7314 5.17676 12.1841 4.77246 12.5122C4.37109 12.8374 3.81152 13 3.09375 13H0.773438V6.5752ZM1.82373 9.22949H2.83447C3.27393 9.22949 3.59473 9.16064 3.79688 9.02295C3.99902 8.88232 4.1001 8.64502 4.1001 8.31104C4.1001 8.00928 3.99023 7.79102 3.77051 7.65625C3.55371 7.52148 3.20801 7.4541 2.7334 7.4541H1.82373V9.22949ZM1.82373 10.082V12.1167H2.93994C3.37939 12.1167 3.71045 12.0332 3.93311 11.8662C4.15869 11.6963 4.27148 11.4297 4.27148 11.0664C4.27148 10.7324 4.15723 10.4849 3.92871 10.3237C3.7002 10.1626 3.35303 10.082 2.88721 10.082H1.82373Z' fill='%23373F45'/%3E%3Cpath d='M13.011 6.5752V10.7324C13.011 11.207 12.9084 11.623 12.7034 11.9805C12.5012 12.335 12.2068 12.6089 11.8201 12.8022C11.4363 12.9927 10.9763 13.0879 10.4402 13.0879C9.6433 13.0879 9.02368 12.877 8.5813 12.4551C8.13892 12.0332 7.91772 11.4531 7.91772 10.7148V6.5752H8.9724V10.6401C8.9724 11.1704 9.09546 11.5615 9.34155 11.8135C9.58765 12.0654 9.96557 12.1914 10.4753 12.1914C11.4656 12.1914 11.9607 11.6714 11.9607 10.6313V6.5752H13.011Z' fill='%23373F45'/%3E%3Cpath d='M15.9146 13V6.5752H16.9649V13H15.9146Z' fill='%23373F45'/%3E%3Cpath d='M19.9255 13V6.5752H20.9758V12.0991H23.696V13H19.9255Z' fill='%23373F45'/%3E%3Cpath d='M28.2828 13H27.2325V7.47607H25.3428V6.5752H30.1724V7.47607H28.2828V13Z' fill='%23373F45'/%3E%3Cpath d='M41.9472 13H40.8046L39.7148 9.16796C39.6679 9.00097 39.6093 8.76074 39.539 8.44727C39.4687 8.13086 39.4262 7.91113 39.4116 7.78809C39.3823 7.97559 39.3339 8.21875 39.2665 8.51758C39.2021 8.81641 39.1479 9.03905 39.1039 9.18554L38.0405 13H36.8979L36.0673 9.7832L35.2236 6.5752H36.2958L37.2143 10.3193C37.3578 10.9199 37.4604 11.4502 37.5219 11.9102C37.5541 11.6611 37.6025 11.3828 37.6669 11.0752C37.7314 10.7676 37.79 10.5186 37.8427 10.3281L38.8886 6.5752H39.9301L41.0024 10.3457C41.1049 10.6943 41.2133 11.2158 41.3276 11.9102C41.3715 11.4912 41.477 10.958 41.644 10.3105L42.558 6.5752H43.6215L41.9472 13Z' fill='%23373F45'/%3E%3Cpath d='M45.7957 13V6.5752H46.846V13H45.7957Z' fill='%23373F45'/%3E%3Cpath d='M52.0258 13H50.9755V7.47607H49.0859V6.5752H53.9155V7.47607H52.0258V13Z' fill='%23373F45'/%3E%3Cpath d='M61.2312 13H60.1765V10.104H57.2146V13H56.1643V6.5752H57.2146V9.20312H60.1765V6.5752H61.2312V13Z' fill='%23373F45'/%3E%3C/svg%3E");}.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit:hover,.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit:focus{background-color:#ffffff;-webkit-transform:scale(1.025) perspective(1px);-ms-transform:scale(1.025) perspective(1px);transform:scale(1.025) perspective(1px);opacity:1;}.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit[data-variant="dark"],.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit[data-variant="light"]{background-color:transparent;border-color:transparent;width:166px;}.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit[data-variant="light"]{color:#ffffff;background-image:url("data:image/svg+xml;charset=utf8,%3Csvg width='162' height='20' viewBox='0 0 162 20' fill='none' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='M83.0561 15.2457C86.675 15.2457 89.4722 12.5154 89.4722 9.14749C89.4722 5.99211 86.8443 4.06563 85.1038 4.06563C82.6801 4.06563 80.7373 5.76407 80.4605 8.28551C80.4092 8.75244 80.0387 9.14403 79.5686 9.14069C78.7871 9.13509 77.6507 9.12841 76.9314 9.13092C76.6217 9.13199 76.3658 8.88106 76.381 8.57196C76.4895 6.38513 77.2218 4.3404 78.618 2.76974C80.1695 1.02445 82.4289 0 85.1038 0C89.5979 0 93.8406 4.07791 93.8406 9.14749C93.8406 14.7608 89.1832 19.3113 83.1517 19.3113C78.8502 19.3113 74.5179 16.5041 73.0053 12.5795C72.9999 12.565 72.9986 12.5492 73.0015 12.534C73.0218 12.4179 73.0617 12.3118 73.1011 12.2074C73.1583 12.0555 73.2143 11.907 73.2062 11.7359L73.18 11.1892C73.174 11.0569 73.2075 10.9258 73.2764 10.8127C73.3452 10.6995 73.4463 10.6094 73.5666 10.554L73.7852 10.4523C73.9077 10.3957 74.0148 10.3105 74.0976 10.204C74.1803 10.0974 74.2363 9.97252 74.2608 9.83983C74.3341 9.43894 74.6865 9.14749 75.0979 9.14749C75.7404 9.14749 76.299 9.57412 76.5088 10.1806C77.5188 13.1 79.1245 15.2457 83.0561 15.2457Z' fill='white'/%3E%3Cpath d='M155.758 6.91365C155.028 6.91365 154.804 6.47916 154.804 5.98857C154.804 5.46997 154.986 5.06348 155.758 5.06348C156.53 5.06348 156.712 5.46997 156.712 5.98857C156.712 6.47905 156.516 6.91365 155.758 6.91365ZM142.441 12.9304V9.32833L141.415 9.32323V8.90392C141.415 8.44719 141.786 8.07758 142.244 8.07986L142.441 8.08095V6.55306L144.082 6.09057V8.08073H145.569V8.50416C145.569 8.61242 145.548 8.71961 145.506 8.81961C145.465 8.91961 145.404 9.01047 145.328 9.08699C145.251 9.16351 145.16 9.2242 145.06 9.26559C144.96 9.30698 144.853 9.32826 144.745 9.32822H144.082V12.7201C144.082 13.2423 144.378 13.4256 144.76 13.4887C145.209 13.5629 145.583 13.888 145.583 14.343V14.9626C144.029 14.9626 142.441 14.8942 142.441 12.9304Z' fill='white'/%3E%3Cpath d='M110.058 7.92554C108.417 7.88344 106.396 8.92062 106.396 11.5137C106.396 14.0646 108.417 15.0738 110.058 15.0318C111.742 15.0738 113.748 14.0646 113.748 11.5137C113.748 8.92062 111.742 7.88344 110.058 7.92554ZM110.07 13.7586C108.878 13.7586 108.032 12.8905 108.032 11.461C108.032 10.1013 108.878 9.20569 110.071 9.20569C111.263 9.20569 112.101 10.0995 112.101 11.459C112.101 12.8887 111.263 13.7586 110.07 13.7586Z' fill='white'/%3E%3Cpath d='M118.06 7.94098C119.491 7.94098 120.978 8.33337 120.978 11.1366V14.893H120.063C119.608 14.893 119.238 14.524 119.238 14.0689V10.9965C119.238 9.66506 118.747 9.16047 117.891 9.16047C117.414 9.16047 116.797 9.52486 116.502 9.81915V14.069C116.502 14.1773 116.481 14.2845 116.44 14.3845C116.398 14.4845 116.337 14.5753 116.261 14.6519C116.184 14.7284 116.093 14.7891 115.993 14.8305C115.893 14.8719 115.786 14.8931 115.678 14.8931H114.847V8.10918H115.773C115.932 8.10914 116.087 8.16315 116.212 8.26242C116.337 8.36168 116.424 8.50033 116.46 8.65577C116.881 8.19328 117.428 7.94098 118.06 7.94098ZM122.854 8.09713C123.024 8.09708 123.19 8.1496 123.329 8.2475C123.468 8.34541 123.574 8.48391 123.631 8.64405L125.133 12.8486L126.635 8.64415C126.692 8.48402 126.798 8.34551 126.937 8.2476C127.076 8.1497 127.242 8.09718 127.412 8.09724H128.598L126.152 14.3567C126.091 14.5112 125.986 14.6439 125.849 14.7374C125.711 14.831 125.549 14.881 125.383 14.8809H124.333L121.668 8.09713H122.854Z' fill='white'/%3E%3Cpath d='M135.085 14.5514C134.566 14.7616 133.513 15.0416 132.418 15.0416C130.496 15.0416 129.024 13.9345 129.024 11.4396C129.024 9.19701 130.451 7.99792 132.191 7.99792C134.338 7.99792 135.254 9.4378 135.158 11.3979C135.139 11.8029 134.786 12.0983 134.38 12.0983H130.679C130.763 13.1916 131.562 13.7662 132.615 13.7662C133.028 13.7662 133.462 13.7452 133.983 13.6481C134.535 13.545 135.085 13.9375 135.085 14.4985V14.5514ZM133.673 10.949C133.785 9.87621 133.061 9.28752 132.191 9.28752C131.321 9.28752 130.734 9.93979 130.679 10.9489L133.673 10.949Z' fill='white'/%3E%3Cpath d='M137.345 8.11122C137.497 8.11118 137.645 8.16229 137.765 8.25635C137.884 8.35041 137.969 8.48197 138.005 8.62993C138.566 8.20932 139.268 7.94303 139.759 7.94303C139.801 7.94303 140.068 7.94303 140.489 7.99913V8.7265C140.489 9.11748 140.15 9.4147 139.759 9.4147C139.31 9.4147 138.651 9.5829 138.131 9.8773V14.8951H136.462V8.11112L137.345 8.11122ZM156.6 14.0508V8.09104H155.769C155.314 8.09104 154.944 8.45999 154.944 8.9151V14.8748H155.775C156.23 14.8748 156.6 14.5058 156.6 14.0508ZM158.857 12.9447V9.34254H157.749V8.91912C157.749 8.46401 158.118 8.09506 158.574 8.09506H158.857V6.56739L160.499 6.10479V8.09506H161.986V8.51848C161.986 8.97359 161.617 9.34254 161.161 9.34254H160.499V12.7345C160.499 13.2566 160.795 13.44 161.177 13.503C161.626 13.5774 162 13.9024 162 14.3574V14.977C160.446 14.977 158.857 14.9086 158.857 12.9447ZM98.1929 10.1124C98.2033 6.94046 100.598 5.16809 102.895 5.16809C104.171 5.16809 105.342 5.44285 106.304 6.12953L105.914 6.6631C105.654 7.02011 105.16 7.16194 104.749 6.99949C104.169 6.7702 103.622 6.7218 103.215 6.7218C101.335 6.7218 99.9169 7.92849 99.9068 10.1123C99.9169 12.2959 101.335 13.5201 103.215 13.5201C103.622 13.5201 104.169 13.4717 104.749 13.2424C105.16 13.0799 105.654 13.2046 105.914 13.5615L106.304 14.0952C105.342 14.7819 104.171 15.0566 102.895 15.0566C100.598 15.0566 98.2033 13.2842 98.1929 10.1124ZM147.619 5.21768C148.074 5.21768 148.444 5.58663 148.444 6.04174V9.81968L151.82 5.58131C151.897 5.47733 151.997 5.39282 152.112 5.3346C152.227 5.27638 152.355 5.24607 152.484 5.24611H153.984L150.166 10.0615L153.984 14.8749H152.484C152.355 14.8749 152.227 14.8446 152.112 14.7864C151.997 14.7281 151.897 14.6436 151.82 14.5397L148.444 10.3025V14.0508C148.444 14.5059 148.074 14.8749 147.619 14.8749H146.746V5.21768H147.619Z' fill='white'/%3E%3Cpath d='M0.773438 6.5752H2.68066C3.56543 6.5752 4.2041 6.7041 4.59668 6.96191C4.99219 7.21973 5.18994 7.62695 5.18994 8.18359C5.18994 8.55859 5.09326 8.87061 4.8999 9.11963C4.70654 9.36865 4.42822 9.52539 4.06494 9.58984V9.63379C4.51611 9.71875 4.84717 9.88721 5.05811 10.1392C5.27197 10.3882 5.37891 10.7266 5.37891 11.1543C5.37891 11.7314 5.17676 12.1841 4.77246 12.5122C4.37109 12.8374 3.81152 13 3.09375 13H0.773438V6.5752ZM1.82373 9.22949H2.83447C3.27393 9.22949 3.59473 9.16064 3.79688 9.02295C3.99902 8.88232 4.1001 8.64502 4.1001 8.31104C4.1001 8.00928 3.99023 7.79102 3.77051 7.65625C3.55371 7.52148 3.20801 7.4541 2.7334 7.4541H1.82373V9.22949ZM1.82373 10.082V12.1167H2.93994C3.37939 12.1167 3.71045 12.0332 3.93311 11.8662C4.15869 11.6963 4.27148 11.4297 4.27148 11.0664C4.27148 10.7324 4.15723 10.4849 3.92871 10.3237C3.7002 10.1626 3.35303 10.082 2.88721 10.082H1.82373Z' fill='white'/%3E%3Cpath d='M13.011 6.5752V10.7324C13.011 11.207 12.9084 11.623 12.7034 11.9805C12.5012 12.335 12.2068 12.6089 11.8201 12.8022C11.4363 12.9927 10.9763 13.0879 10.4402 13.0879C9.6433 13.0879 9.02368 12.877 8.5813 12.4551C8.13892 12.0332 7.91772 11.4531 7.91772 10.7148V6.5752H8.9724V10.6401C8.9724 11.1704 9.09546 11.5615 9.34155 11.8135C9.58765 12.0654 9.96557 12.1914 10.4753 12.1914C11.4656 12.1914 11.9607 11.6714 11.9607 10.6313V6.5752H13.011Z' fill='white'/%3E%3Cpath d='M15.9146 13V6.5752H16.9649V13H15.9146Z' fill='white'/%3E%3Cpath d='M19.9255 13V6.5752H20.9758V12.0991H23.696V13H19.9255Z' fill='white'/%3E%3Cpath d='M28.2828 13H27.2325V7.47607H25.3428V6.5752H30.1724V7.47607H28.2828V13Z' fill='white'/%3E%3Cpath d='M41.9472 13H40.8046L39.7148 9.16796C39.6679 9.00097 39.6093 8.76074 39.539 8.44727C39.4687 8.13086 39.4262 7.91113 39.4116 7.78809C39.3823 7.97559 39.3339 8.21875 39.2665 8.51758C39.2021 8.81641 39.1479 9.03905 39.1039 9.18554L38.0405 13H36.8979L36.0673 9.7832L35.2236 6.5752H36.2958L37.2143 10.3193C37.3578 10.9199 37.4604 11.4502 37.5219 11.9102C37.5541 11.6611 37.6025 11.3828 37.6669 11.0752C37.7314 10.7676 37.79 10.5186 37.8427 10.3281L38.8886 6.5752H39.9301L41.0024 10.3457C41.1049 10.6943 41.2133 11.2158 41.3276 11.9102C41.3715 11.4912 41.477 10.958 41.644 10.3105L42.558 6.5752H43.6215L41.9472 13Z' fill='white'/%3E%3Cpath d='M45.7957 13V6.5752H46.846V13H45.7957Z' fill='white'/%3E%3Cpath d='M52.0258 13H50.9755V7.47607H49.0859V6.5752H53.9155V7.47607H52.0258V13Z' fill='white'/%3E%3Cpath d='M61.2312 13H60.1765V10.104H57.2146V13H56.1643V6.5752H57.2146V9.20312H60.1765V6.5752H61.2312V13Z' fill='white'/%3E%3C/svg%3E");}@-webkit-keyframes formkit-bouncedelay-formkit-form-data-uid-e309c832a6-{0%,80%,100%{-webkit-transform:scale(0);-ms-transform:scale(0);transform:scale(0);}40%{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}@keyframes formkit-bouncedelay-formkit-form-data-uid-e309c832a6-{0%,80%,100%{-webkit-transform:scale(0);-ms-transform:scale(0);transform:scale(0);}40%{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}.formkit-form[data-uid="e309c832a6"] blockquote{padding:10px 20px;margin:0 0 20px;border-left:5px solid #e1e1e1;}.formkit-form[data-uid="e309c832a6"] .seva-custom-content{padding:15px;font-size:16px;color:#fff;mix-blend-mode:difference;}.formkit-form[data-uid="e309c832a6"] .formkit-modal.guard{max-width:420px;width:100%;} .formkit-form[data-uid="e309c832a6"]{border:1px solid #e3e3e3;max-width:700px;position:relative;overflow:hidden;}.formkit-form[data-uid="e309c832a6"] .formkit-background{width:100%;height:100%;position:absolute;top:0;left:0;background-size:cover;background-position:center;opacity:0.3;}.formkit-form[data-uid="e309c832a6"] [data-style="minimal"]{padding:20px;width:100%;position:relative;}.formkit-form[data-uid="e309c832a6"] .formkit-header{margin:0 0 27px 0;text-align:center;}.formkit-form[data-uid="e309c832a6"] .formkit-subheader{margin:18px 0;text-align:center;}.formkit-form[data-uid="e309c832a6"] .formkit-guarantee{font-size:13px;margin:10px 0 15px 0;text-align:center;}.formkit-form[data-uid="e309c832a6"] .formkit-guarantee > p{margin:0;}.formkit-form[data-uid="e309c832a6"] .formkit-powered-by-convertkit-container{margin-bottom:0;}.formkit-form[data-uid="e309c832a6"] .formkit-fields{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;margin:25px auto 0 auto;}.formkit-form[data-uid="e309c832a6"] .formkit-field{min-width:220px;}.formkit-form[data-uid="e309c832a6"] .formkit-field,.formkit-form[data-uid="e309c832a6"] .formkit-submit{margin:0 0 15px 0;-webkit-flex:1 0 100%;-ms-flex:1 0 100%;flex:1 0 100%;}.formkit-form[data-uid="e309c832a6"][min-width~="600"] [data-style="minimal"]{padding:40px;}.formkit-form[data-uid="e309c832a6"][min-width~="600"] .formkit-fields[data-stacked="false"]{margin-left:-5px;margin-right:-5px;}.formkit-form[data-uid="e309c832a6"][min-width~="600"] .formkit-fields[data-stacked="false"] .formkit-field,.formkit-form[data-uid="e309c832a6"][min-width~="600"] .formkit-fields[data-stacked="false"] .formkit-submit{margin:0 5px 15px 5px;}.formkit-form[data-uid="e309c832a6"][min-width~="600"] .formkit-fields[data-stacked="false"] .formkit-field{-webkit-flex:100 1 auto;-ms-flex:100 1 auto;flex:100 1 auto;}.formkit-form[data-uid="e309c832a6"][min-width~="600"] .formkit-fields[data-stacked="false"] .formkit-submit{-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;} </style></form>

		<div class="sponsor print:tw-hidden">
        <div class="sponsor-avatar"></div><p class="sponsor-bio"><em>Did you find this article helpful?</em></p><div class="sponsor-custom"><script type="text/javascript" src="https://cdnjs.buymeacoffee.com/1.0.0/button.prod.min.js" data-name="bmc-button" data-slug="reachsumit" data-color="#FFDD00" data-emoji=""  data-font="Cookie" data-text="Buy me a coffee" data-outline-color="#000000" data-font-color="#000000" data-coffee-color="#ffffff" ></script></div></div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2025-09-27</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line print:!tw-hidden">
            <div class="post-info-md"></div>
            <div class="post-info-share"><button title="Share on Facebook" data-sharer="facebook" data-url="https://blog.reachsumit.com/posts/2025/09/probing-llms-knowledge-boundary/" data-hashtag="retrieval"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14 0 55.52 4.84 55.52 4.84v61h-31.28c-30.8 0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"/></svg></button><button title="Share on Linkedin" data-sharer="linkedin" data-url="https://blog.reachsumit.com/posts/2025/09/probing-llms-knowledge-boundary/"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg></button><button title="Share on WhatsApp" data-sharer="whatsapp" data-url="https://blog.reachsumit.com/posts/2025/09/probing-llms-knowledge-boundary/" data-title="Probing LLMs&#39; Knowledge Boundary: Adaptive RAG, Part 3" data-web><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M380.9 97.1C339 55.1 283.2 32 223.9 32c-122.4 0-222 99.6-222 222 0 39.1 10.2 77.3 29.6 111L0 480l117.7-30.9c32.4 17.7 68.9 27 106.1 27h.1c122.3 0 224.1-99.6 224.1-222 0-59.3-25.2-115-67.1-157zm-157 341.6c-33.2 0-65.7-8.9-94-25.7l-6.7-4-69.8 18.3L72 359.2l-4.4-7c-18.5-29.4-28.2-63.3-28.2-98.2 0-101.7 82.8-184.5 184.6-184.5 49.3 0 95.6 19.2 130.4 54.1 34.8 34.9 56.2 81.2 56.1 130.5 0 101.8-84.9 184.6-186.6 184.6zm101.2-138.2c-5.5-2.8-32.8-16.2-37.9-18-5.1-1.9-8.8-2.8-12.5 2.8-3.7 5.6-14.3 18-17.6 21.8-3.2 3.7-6.5 4.2-12 1.4-32.6-16.3-54-29.1-75.5-66-5.7-9.8 5.7-9.1 16.3-30.3 1.8-3.7.9-6.9-.5-9.7-1.4-2.8-12.5-30.1-17.1-41.2-4.5-10.8-9.1-9.3-12.5-9.5-3.2-.2-6.9-.2-10.6-.2-3.7 0-9.7 1.4-14.8 6.9-5.1 5.6-19.4 19-19.4 46.3 0 27.3 19.9 53.7 22.6 57.4 2.8 3.7 39.1 59.7 94.8 83.8 35.2 15.2 49 16.5 66.6 13.9 10.7-1.6 32.8-13.4 37.4-26.4 4.6-13 4.6-24.1 3.2-26.4-1.3-2.5-5-3.9-10.5-6.6z"/></svg></button><button title="Share on Hacker News" data-sharer="hackernews" data-url="https://blog.reachsumit.com/posts/2025/09/probing-llms-knowledge-boundary/" data-title="Probing LLMs&#39; Knowledge Boundary: Adaptive RAG, Part 3"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M0 32v448h448V32H0zm21.2 197.2H21c.1-.1.2-.3.3-.4 0 .1 0 .3-.1.4zm218 53.9V384h-31.4V281.3L128 128h37.3c52.5 98.3 49.2 101.2 59.3 125.6 12.3-27 5.8-24.4 60.6-125.6H320l-80.8 155.1z"/></svg></button><button title="Share on Reddit" data-sharer="reddit" data-url="https://blog.reachsumit.com/posts/2025/09/probing-llms-knowledge-boundary/"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M201.5 305.5c-13.8 0-24.9-11.1-24.9-24.6 0-13.8 11.1-24.9 24.9-24.9 13.6 0 24.6 11.1 24.6 24.9 0 13.6-11.1 24.6-24.6 24.6zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-132.3-41.2c-9.4 0-17.7 3.9-23.8 10-22.4-15.5-52.6-25.5-86.1-26.6l17.4-78.3 55.4 12.5c0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.3 24.9-24.9s-11.1-24.9-24.9-24.9c-9.7 0-18 5.8-22.1 13.8l-61.2-13.6c-3-.8-6.1 1.4-6.9 4.4l-19.1 86.4c-33.2 1.4-63.1 11.3-85.5 26.8-6.1-6.4-14.7-10.2-24.1-10.2-34.9 0-46.3 46.9-14.4 62.8-1.1 5-1.7 10.2-1.7 15.5 0 52.6 59.2 95.2 132 95.2 73.1 0 132.3-42.6 132.3-95.2 0-5.3-.6-10.8-1.9-15.8 31.3-16 19.8-62.5-14.9-62.5zM302.8 331c-18.2 18.2-76.1 17.9-93.6 0-2.2-2.2-6.1-2.2-8.3 0-2.5 2.5-2.5 6.4 0 8.6 22.8 22.8 87.3 22.8 110.2 0 2.5-2.2 2.5-6.1 0-8.6-2.2-2.2-6.1-2.2-8.3 0zm7.7-75c-13.6 0-24.6 11.1-24.6 24.9 0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.1 24.9-24.6 0-13.8-11-24.9-24.9-24.9z"/></svg></button><button title="Share on Line" data-sharer="line" data-url="https://blog.reachsumit.com/posts/2025/09/probing-llms-knowledge-boundary/" data-title="Probing LLMs&#39; Knowledge Boundary: Adaptive RAG, Part 3"><svg class="icon" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>LINE</title><path d="M19.365 9.863c.349 0 .63.285.63.631 0 .345-.281.63-.63.63H17.61v1.125h1.755c.349 0 .63.283.63.63 0 .344-.281.629-.63.629h-2.386c-.345 0-.627-.285-.627-.629V8.108c0-.345.282-.63.63-.63h2.386c.346 0 .627.285.627.63 0 .349-.281.63-.63.63H17.61v1.125h1.755zm-3.855 3.016c0 .27-.174.51-.432.596-.064.021-.133.031-.199.031-.211 0-.391-.09-.51-.25l-2.443-3.317v2.94c0 .344-.279.629-.631.629-.346 0-.626-.285-.626-.629V8.108c0-.27.173-.51.43-.595.06-.023.136-.033.194-.033.195 0 .375.104.495.254l2.462 3.33V8.108c0-.345.282-.63.63-.63.345 0 .63.285.63.63v4.771zm-5.741 0c0 .344-.282.629-.631.629-.345 0-.627-.285-.627-.629V8.108c0-.345.282-.63.63-.63.346 0 .628.285.628.63v4.771zm-2.466.629H4.917c-.345 0-.63-.285-.63-.629V8.108c0-.345.285-.63.63-.63.348 0 .63.285.63.63v4.141h1.756c.348 0 .629.283.629.63 0 .344-.282.629-.629.629M24 10.314C24 4.943 18.615.572 12 .572S0 4.943 0 10.314c0 4.811 4.27 8.842 10.035 9.608.391.082.923.258 1.058.59.12.301.079.766.038 1.08l-.164 1.02c-.045.301-.24 1.186 1.049.645 1.291-.539 6.916-4.078 9.436-6.975C23.176 14.393 24 12.458 24 10.314"/></svg></button><button title="Share on Pocket" data-sharer="pocket" data-url="https://blog.reachsumit.com/posts/2025/09/probing-llms-knowledge-boundary/"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M407.6 64h-367C18.5 64 0 82.5 0 104.6v135.2C0 364.5 99.7 464 224.2 464c124 0 223.8-99.5 223.8-224.2V104.6c0-22.4-17.7-40.6-40.4-40.6zm-162 268.5c-12.4 11.8-31.4 11.1-42.4 0C89.5 223.6 88.3 227.4 88.3 209.3c0-16.9 13.8-30.7 30.7-30.7 17 0 16.1 3.8 105.2 89.3 90.6-86.9 88.6-89.3 105.5-89.3 16.9 0 30.7 13.8 30.7 30.7 0 17.8-2.9 15.7-114.8 123.2z"/></svg></button><button title="Share on 微博" data-sharer="weibo" data-url="https://blog.reachsumit.com/posts/2025/09/probing-llms-knowledge-boundary/" data-title="Probing LLMs&#39; Knowledge Boundary: Adaptive RAG, Part 3" data-image="featured-image.webp"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M407 177.6c7.6-24-13.4-46.8-37.4-41.7-22 4.8-28.8-28.1-7.1-32.8 50.1-10.9 92.3 37.1 76.5 84.8-6.8 21.2-38.8 10.8-32-10.3zM214.8 446.7C108.5 446.7 0 395.3 0 310.4c0-44.3 28-95.4 76.3-143.7C176 67 279.5 65.8 249.9 161c-4 13.1 12.3 5.7 12.3 6 79.5-33.6 140.5-16.8 114 51.4-3.7 9.4 1.1 10.9 8.3 13.1 135.7 42.3 34.8 215.2-169.7 215.2zm143.7-146.3c-5.4-55.7-78.5-94-163.4-85.7-84.8 8.6-148.8 60.3-143.4 116s78.5 94 163.4 85.7c84.8-8.6 148.8-60.3 143.4-116zM347.9 35.1c-25.9 5.6-16.8 43.7 8.3 38.3 72.3-15.2 134.8 52.8 111.7 124-7.4 24.2 29.1 37 37.4 12 31.9-99.8-55.1-195.9-157.4-174.3zm-78.5 311c-17.1 38.8-66.8 60-109.1 46.3-40.8-13.1-58-53.4-40.3-89.7 17.7-35.4 63.1-55.4 103.4-45.1 42 10.8 63.1 50.2 46 88.5zm-86.3-30c-12.9-5.4-30 .3-38 12.9-8.3 12.9-4.3 28 8.6 34 13.1 6 30.8.3 39.1-12.9 8-13.1 3.7-28.3-9.7-34zm32.6-13.4c-5.1-1.7-11.4.6-14.3 5.4-2.9 5.1-1.4 10.6 3.7 12.9 5.1 2 11.7-.3 14.6-5.4 2.8-5.2 1.1-10.9-4-12.9z"/></svg></button><button title="Share on Evernote" data-sharer="evernote" data-url="https://blog.reachsumit.com/posts/2025/09/probing-llms-knowledge-boundary/" data-title="Probing LLMs&#39; Knowledge Boundary: Adaptive RAG, Part 3"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M120.82 132.21c1.6 22.31-17.55 21.59-21.61 21.59-68.93 0-73.64-1-83.58 3.34-.56.22-.74 0-.37-.37L123.79 46.45c.38-.37.6-.22.38.37-4.35 9.99-3.35 15.09-3.35 85.39zm79 308c-14.68-37.08 13-76.93 52.52-76.62 17.49 0 22.6 23.21 7.95 31.42-6.19 3.3-24.95 1.74-25.14 19.2-.05 17.09 19.67 25 31.2 24.89A45.64 45.64 0 0 0 312 393.45v-.08c0-11.63-7.79-47.22-47.54-55.34-7.72-1.54-65-6.35-68.35-50.52-3.74 16.93-17.4 63.49-43.11 69.09-8.74 1.94-69.68 7.64-112.92-36.77 0 0-18.57-15.23-28.23-57.95-3.38-15.75-9.28-39.7-11.14-62 0-18 11.14-30.45 25.07-32.2 81 0 90 2.32 101-7.8 9.82-9.24 7.8-15.5 7.8-102.78 1-8.3 7.79-30.81 53.41-24.14 6 .86 31.91 4.18 37.48 30.64l64.26 11.15c20.43 3.71 70.94 7 80.6 57.94 22.66 121.09 8.91 238.46 7.8 238.46C362.15 485.53 267.06 480 267.06 480c-18.95-.23-54.25-9.4-67.27-39.83zm80.94-204.84c-1 1.92-2.2 6 .85 7 14.09 4.93 39.75 6.84 45.88 5.53 3.11-.25 3.05-4.43 2.48-6.65-3.53-21.85-40.83-26.5-49.24-5.92z"/></svg></button><button title="Share on Trello" data-sharer="trello" data-url="https://blog.reachsumit.com/posts/2025/09/probing-llms-knowledge-boundary/" data-title="Probing LLMs&#39; Knowledge Boundary: Adaptive RAG, Part 3" data-description=""><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M392.3 32H56.1C25.1 32 0 57.1 0 88c-.1 0 0-4 0 336 0 30.9 25.1 56 56 56h336.2c30.8-.2 55.7-25.2 55.7-56V88c.1-30.8-24.8-55.8-55.6-56zM197 371.3c-.2 14.7-12.1 26.6-26.9 26.6H87.4c-14.8.1-26.9-11.8-27-26.6V117.1c0-14.8 12-26.9 26.9-26.9h82.9c14.8 0 26.9 12 26.9 26.9v254.2zm193.1-112c0 14.8-12 26.9-26.9 26.9h-81c-14.8 0-26.9-12-26.9-26.9V117.2c0-14.8 12-26.9 26.8-26.9h81.1c14.8 0 26.9 12 26.9 26.9v142.1z"/></svg></button></div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M497.941 225.941L286.059 14.059A48 48 0 0 0 252.118 0H48C21.49 0 0 21.49 0 48v204.118a48 48 0 0 0 14.059 33.941l211.882 211.882c18.744 18.745 49.136 18.746 67.882 0l204.118-204.118c18.745-18.745 18.745-49.137 0-67.882zM112 160c-26.51 0-48-21.49-48-48s21.49-48 48-48 48 21.49 48 48-21.49 48-48 48zm513.941 133.823L421.823 497.941c-18.745 18.745-49.137 18.745-67.882 0l-.36-.36L527.64 323.522c16.999-16.999 26.36-39.6 26.36-63.64s-9.362-46.641-26.36-63.64L331.397 0h48.721a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882z"/></svg>&nbsp;<a href="/tags/retrieval/">Retrieval</a>,&nbsp;<a href="/tags/rag/">Rag</a>,&nbsp;<a href="/tags/adaptive-rag/">Adaptive-Rag</a></section>
        <section class="print:!tw-hidden">
            <span><button class="tw-text-fgColor-link-muted hover:tw-text-fgColor-link-muted-hover" onclick="window.history.back();">Back</button></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav print:tw-hidden"><a href="/posts/2025/09/deciding-when-not-to-retrieve/" class="prev" rel="prev" title="Deciding When Not to Retrieve: Adaptive RAG, Part 2"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M31.7 239l136-136c9.4-9.4 24.6-9.4 33.9 0l22.6 22.6c9.4 9.4 9.4 24.6 0 33.9L127.9 256l96.4 96.4c9.4 9.4 9.4 24.6 0 33.9L201.7 409c-9.4 9.4-24.6 9.4-33.9 0l-136-136c-9.5-9.4-9.5-24.6-.1-34z"/></svg>Deciding When Not to Retrieve: Adaptive RAG, Part 2</a></div>
</div>
<div id="comments" class="print:!tw-hidden tw-pt-32 tw-pb-8"><div id="gitalk" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://github.com/gitalk/gitalk"></a>Gitalk</a>.
            </noscript></div></article></main><footer class="footer">
        <div class="footer-container"><div class="footer-line"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M256 8C119.033 8 8 119.033 8 256s111.033 248 248 248 248-111.033 248-248S392.967 8 256 8zm0 448c-110.532 0-200-89.451-200-200 0-110.531 89.451-200 200-200 110.532 0 200 89.451 200 200 0 110.532-89.451 200-200 200zm107.351-101.064c-9.614 9.712-45.53 41.396-104.065 41.396-82.43 0-140.484-61.425-140.484-141.567 0-79.152 60.275-139.401 139.762-139.401 55.531 0 88.738 26.62 97.593 34.779a11.965 11.965 0 0 1 1.936 15.322l-18.155 28.113c-3.841 5.95-11.966 7.282-17.499 2.921-8.595-6.776-31.814-22.538-61.708-22.538-48.303 0-77.916 35.33-77.916 80.082 0 41.589 26.888 83.692 78.277 83.692 32.657 0 56.843-19.039 65.726-27.225 5.27-4.857 13.596-4.039 17.82 1.738l19.865 27.17a11.947 11.947 0 0 1-1.152 15.518z"/></svg>2020 - 2025<span class="author">&nbsp;<a href="https://reachsumit.com" target="_blank" rel="noopener noreferrer">Sumit Kumar</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
            <div class="footer-line"></div>
            <div class="footer-line">
            </div>
        </div></footer><div class="print:!tw-hidden tw-flex tw-flex-col tw-fixed tw-right-4 tw-bottom-4 tw-gap-2"><a href="#back-to-top" id="back-to-top-button" class="tw-transition-opacity tw-opacity-0 tw-block tw-bg-bgColor-secondary tw-rounded-full" style="padding: 0.6rem; line-height: 1.3rem; font-size: 1rem;" title="Back to Top">
      <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M34.9 289.5l-22.2-22.2c-9.4-9.4-9.4-24.6 0-33.9L207 39c9.4-9.4 24.6-9.4 33.9 0l194.3 194.3c9.4 9.4 9.4 24.6 0 33.9L413 289.4c-9.5 9.5-25 9.3-34.3-.4L264 168.6V456c0 13.3-10.7 24-24 24h-32c-13.3 0-24-10.7-24-24V168.6L69.2 289.1c-9.3 9.8-24.8 10-34.3.4z"/></svg>
  </a>

  <button id="toc-drawer-button" class="tw-block tw-bg-bgColor-secondary tw-rounded-full md:tw-hidden" style="padding: 0.6rem; line-height: 1.3rem; font-size: 1rem;">
      <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"/></svg>
  </button><a href="#comments" id="view-comments" class="tw-block tw-bg-bgColor-secondary tw-rounded-full" style="padding: 0.6rem; line-height: 1.3rem; font-size: 1rem;" title="View Comments">
      <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M256 32C114.6 32 0 125.1 0 240c0 49.6 21.4 95 57 130.7C44.5 421.1 2.7 466 2.2 466.5c-2.2 2.3-2.8 5.7-1.5 8.7S4.8 480 8 480c66.3 0 116-31.8 140.6-51.4 32.7 12.3 69 19.4 107.4 19.4 141.4 0 256-93.1 256-208S397.4 32 256 32z"/></svg>
  </a></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.css"><link rel="preload" as="style" onload="this.onload=null;this.rel='stylesheet'" href="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/css/lightgallery.min.css">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/css/lightgallery.min.css"></noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><link rel="preload" as="style" onload="this.onload=null;this.rel='stylesheet'" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/copy-tex.min.css">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/copy-tex.min.css"></noscript>
<script>window.config={"algoliasearch.min.js":"https://cdn.jsdelivr.net/npm/algoliasearch@4.11.0/dist/algoliasearch.umd.min.js","autocomplete.min.js":"https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.0/dist/autocomplete.min.js","comment":{"gitalk":{"admin":["reachsumit"],"clientID":"e13962a172516867862a","clientSecret":"43e82fd70da96d006eec6c9bee0a861aaa13ee89","id":"2025-09-27T00:00:00Z","owner":"reachsumit","repo":"reachsumit-blog-gitalk","title":"Probing LLMs' Knowledge Boundary: Adaptive RAG, Part 3"}},"data":{"desktop-header-typeit":"Sumit's Diary","mobile-header-typeit":"Sumit's Diary"},"lightGallery":{"actualSize":false,"exThumbImage":"data-thumbnail","hideBarsDelay":2000,"selector":".lightgallery","speed":400,"thumbContHeight":80,"thumbWidth":80,"thumbnail":true},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"LV11CUNTAX","algoliaIndex":"blog_reachsumit","algoliaSearchKey":"98d868016771f8a06b967e7eb3eaf63a","highlightTag":"em","maxResultLength":10,"noResultsFound":"No results found","snippetLength":30,"type":"algolia"},"sharerjs":true,"table":{"sort":true},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"desktop-header-typeit":["desktop-header-typeit"],"mobile-header-typeit":["mobile-header-typeit"]},"duration":2700,"speed":100}};</script><script
    src="https://cdn.jsdelivr.net/npm/tablesort@5.3.0/src/tablesort.min.js"
    
  ></script><script
    src="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/js/lightgallery.min.js"
    
      defer
    
  ></script><script
    src="https://cdn.jsdelivr.net/npm/lg-thumbnail.js@1.2.0/dist/lg-thumbnail.min.js"
    
      defer
    
  ></script><script
    src="https://cdn.jsdelivr.net/npm/lg-zoom.js@1.3.0/dist/lg-zoom.min.js"
    
      defer
    
  ></script><script
    src="https://cdn.jsdelivr.net/npm/sharer.js@0.4.2/sharer.min.js"
    
  ></script><script
    src="https://cdn.jsdelivr.net/npm/typeit@7.0.4/dist/typeit.min.js"
    
  ></script><script
    src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
    
      defer
    
  ></script><script
    src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
    
      defer
    
  ></script><script
    src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/copy-tex.min.js"
    
      defer
    
  ></script><script
    src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js"
    
      defer
    
  ></script><script
    src="/js/katex.min.js"
    
      defer
    
  ></script><script
    src="/js/theme.min.js"
    
      defer
    
  ></script><script
    src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"
    
  ></script><script
    src="/js/gitalk.min.js"
    
      defer
    
  ></script><script>
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'G-TGH87J92Z3');
        </script><script
    src="https://www.googletagmanager.com/gtag/js?id=G-TGH87J92Z3"
    async
  ></script>

<script type="speculationrules">
  {
    "prerender": [
      {
        "where": { "href_matches": "/*" },
        "eagerness": "moderate"
      }
    ]
  }
</script>
</body>

</html>
