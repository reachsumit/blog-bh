[{"categories":["Recommender systems"],"content":"This article explains what explicit and implicit feedback data means for recommender systems. We discuss their characteristics and peculiarities concerning collaborative filtering based algorithms. Then we go over one of the most popular collaborative filtering algorithms for implicit data and implement it in Python with an example dataset.","date":"2022-09-25","objectID":"/posts/2022/09/explicit-implicit-cf/","tags":["recsys","literature review"],"title":"Collaborative Filtering based Recommender Systems for Implicit Feedback Data","uri":"/posts/2022/09/explicit-implicit-cf/"},{"categories":["Recommender systems"],"content":"Different Flavors of Recommenders At a high-level, Recommender systems work based on two different strategies (or a hybrid of the two) for recommending content. Collaborative Filtering: Algorithms that use usage data, such as explicit or implicit feedback from the user. Content-based Filtering: Algorithms that use content metadata and user profile. For example, a movie can be profiled based on its genre, IMDb ratings, box-office sales, etc., and a user can be profile based on their demographic information or their answers to an onboarding survey. So, while collaborative filtering needs much feedback from the users to work properly, content-based filtering needs good descriptions of the items. ","date":"2022-09-25","objectID":"/posts/2022/09/explicit-implicit-cf/:1:0","tags":["recsys","literature review"],"title":"Collaborative Filtering based Recommender Systems for Implicit Feedback Data","uri":"/posts/2022/09/explicit-implicit-cf/"},{"categories":["Recommender systems"],"content":"Challenges With Using Explicit Data Broadly speaking, the usage data can be either an explicit or implicit signal from the user. Explicit feedback is likely the most accurate input for the recommender system because it is pure information provided by the user about their preference for certain content. This feedback is usually collected using controls such as upvote/downvote buttons or star ratings. Despite their flaws, explicit feedback is used by a vast majority of recommender systems probably thanks to the convenience of using such an input. However, explicit feedback is not always available and often lacks nuances. Not everyone who purchases a product on Amazon or watches a movie on Netflix likes to leave feedback, people might rate a product lower because of their bad experience with an Amazon delivery agent, and someone might rate a movie higher not because of the content of the movie but simply because they liked the movie’s cover art. Explicit feedback collection is not even possible for products like movies or shows broadcasted on television. Similarly, a platform may not have a feedback system with 0 or negative ratings, so if you don’t like a product, the lowest rating you can give to it might still be a 1-star. This dilutes how strongly you may dislike the product, and for the recommender system, it means that the dataset lacks substantial evidence on which products consumers dislike. Despite their flaws, explicit feedback is used by a large number of recommender systems probably thanks to the convenience of using such an input. The missing information is simply considered as empty cells in a sparse matrix input and is omitted from the analysis. Using explicit signals requires additional boosting algorithms to get user feedback on new content because collaborative filtering will not be able to recommend the new content until someone rates it (also known as the cold start problem). ","date":"2022-09-25","objectID":"/posts/2022/09/explicit-implicit-cf/:2:0","tags":["recsys","literature review"],"title":"Collaborative Filtering based Recommender Systems for Implicit Feedback Data","uri":"/posts/2022/09/explicit-implicit-cf/"},{"categories":["Recommender systems"],"content":"Building a Recommender System Using Explicit Feedback Before we talk about using implicit signals, let’s implement a collaborative filtering based recommender system using explicit signals. We will implement a matrix factorization based algorithm using NumPy’s SVD 1 on MovieLens 100K dataset 2. This dataset contains 100,000 ratings given by 943 users for 1682 movies, with each user having rated at least 20 movies. ","date":"2022-09-25","objectID":"/posts/2022/09/explicit-implicit-cf/:3:0","tags":["recsys","literature review"],"title":"Collaborative Filtering based Recommender Systems for Implicit Feedback Data","uri":"/posts/2022/09/explicit-implicit-cf/"},{"categories":["Recommender systems"],"content":"Quick Notes on How Matrix Factorization Works We start with a U x M rating matrix (943x1682 for our dataset) where each value $r_{ui}$ represents the rating given by user u for a movie i. We then divide this matrix into two much smaller matrices such that we approximately get the same U x M matrix back if we multiply the two new matrices together. Basically, we compress a high-dimensional sparse input matrix to a low-dimensional dense matrix space because this generalization leads to a better understanding of the data. Refer to Yehuda Koren’s paper to read more about how the matrix factorization technique is used for recommender systems 3. To learn how SVD works, refer to Jeremy Kun’s article on Math ∩ Programming 4. By splitting this matrix into lower dimensions (factors), we represent each user as well as each movie by a 50-dimensional dense vector. To recommend movies similar to a given movie, we can compute the dot product (cosine similarity) between that movie and other movies and take the closest candidates as the output. Alternatively, we can compute the dot product of a user vector u and an item vector i to get the predicted score and sort all unrated items’ predicted score for a given user to get the list of recommended items for a user. ","date":"2022-09-25","objectID":"/posts/2022/09/explicit-implicit-cf/:3:1","tags":["recsys","literature review"],"title":"Collaborative Filtering based Recommender Systems for Implicit Feedback Data","uri":"/posts/2022/09/explicit-implicit-cf/"},{"categories":["Recommender systems"],"content":"Python Implementation import numpy as np import pandas as pd from numpy import bincount, log, log1p from scipy.sparse import coo_matrix, linalg class ExplicitCF: def __init__(self): self.df = pd.read_csv(\"ml-100k/u.data\", sep='\\t', header=None, names=['user', 'item', 'rating'], usecols=range(3)) self.df['user'] = self.df['user'].astype(\"category\") self.df['item'] = self.df['item'].astype(\"category\") self.df.dropna(inplace=True) self.rating_matrix = coo_matrix((self.df['rating'].astype(float), (self.df['item'].cat.codes, self.df['user'].cat.codes))) def _bm25_weight(self, X, K1=100, B=0.8): \"\"\"Weighs each row of a sparse matrix X by BM25 weighting\"\"\" # calculate idf per term (user) X = coo_matrix(X) N = float(X.shape[0]) idf = log(N) - log1p(bincount(X.col)) # calculate length_norm per document (artist) row_sums = np.ravel(X.sum(axis=1)) average_length = row_sums.mean() length_norm = (1.0 - B) + B * row_sums / average_length # weight matrix rows by bm25 X.data = X.data * (K1 + 1.0) / (K1 * length_norm[X.row] + X.data) * idf[X.col] return X def factorize(self): item_factor, _, user_factor = linalg.svds(self._bm25_weight(self.rating_matrix), 50) return item_factor, user_factor def init_predict(self, x_factors): # fully normalize factors, so can compare with only the dot product norms = np.linalg.norm(x_factors, axis=-1) self.factors = x_factors / norms[:, np.newaxis] def get_related(self, x_id, N=5): scores = self.factors.dot(self.factors[x_id]) best = np.argpartition(scores, -N)[-N:] print(\"Recommendations:\") for _id, score in sorted(zip(best, scores[best]), key=lambda x: -x[1]): print(f\"item id: {_id}, score: {score}\") cf_object = ExplicitCF() print(cf_object.df.head()) # user item rating #0 196 242 3 #1 186 302 3 #2 22 377 1 #3 244 51 2 #4 166 346 1 print(cf_object.df.user.nunique()) # 943 print(cf_object.df.item.nunique()) # 1682 print(cf_object.df.rating.describe()) #count 100000.000000 #mean 3.529860 #std 1.125674 #min 1.000000 #25% 3.000000 #50% 4.000000 #75% 4.000000 #max 5.000000 #Name: rating, dtype: float64 print(cf_object.rating_matrix.shape) # (1682, 943) item_factor, user_factor = cf_object.factorize() print(item_factor.shape) # (1682, 50) print(user_factor.shape) # (50, 943) cf_object.init_predict(item_factor) print(cf_object.factors.shape) # (1682, 50) cf_object.get_related(314) #Recommendations: #item id: 314, score: 1.0 #item id: 315, score: 0.8940031189407059 #item id: 346, score: 0.8509562164687848 #item id: 271, score: 0.8441764974934266 #item id: 312, score: 0.7475076699852435 Code is also available on this GitHub Gist. ","date":"2022-09-25","objectID":"/posts/2022/09/explicit-implicit-cf/:3:2","tags":["recsys","literature review"],"title":"Collaborative Filtering based Recommender Systems for Implicit Feedback Data","uri":"/posts/2022/09/explicit-implicit-cf/"},{"categories":["Recommender systems"],"content":"Challenges With Using Implicit Data Implicit user feedback includes purchase history, browsing history, search patterns, impressions, clicks, or even mouse movements. While with explicit feedback, the user specifies the degree of positive or negative preference for an item, we do not have the same symmetry with implicit feedback. For example, a user might not have read a book either because they didn’t like the book or they didn’t know about the book or the book wasn’t available in their region, or maybe its cost was prohibitive for the user. Focusing on only the gathered implicit signals means that we could end up focusing heavily on positive signals. So we also have to address the missing data because a lot of negative feedback might exist there. This unfortunately also means that we may no longer have a sparse matrix and we may not be able to use compute-efficient sparsity-based algorithmic optimizations. Implicit feedback is also inherently noisy. A user might have purchased an item in the past only to give it away as a gift, someone might have “watched” a 2+ hour long movie in a theatre, but they might have been asleep throughout it, a user might have missed watching a tv show because there was a live sports event at the same time. ","date":"2022-09-25","objectID":"/posts/2022/09/explicit-implicit-cf/:4:0","tags":["recsys","literature review"],"title":"Collaborative Filtering based Recommender Systems for Implicit Feedback Data","uri":"/posts/2022/09/explicit-implicit-cf/"},{"categories":["Recommender systems"],"content":"Preference vs Confidence One way to think about the difference between the two feedback types is that the explicit feedback indicates user preference, whereas the implicit feedback numerical value indicates confidence. For example, using the count of the number of times the user has watched a show does not indicates a higher preference, but it tells us about the confidence we have in a certain observation. A one-time event might be caused by various reasons that have nothing to do with user preferences. However, a recurring event is more likely to reflect the user’s opinion. ","date":"2022-09-25","objectID":"/posts/2022/09/explicit-implicit-cf/:4:1","tags":["recsys","literature review"],"title":"Collaborative Filtering based Recommender Systems for Implicit Feedback Data","uri":"/posts/2022/09/explicit-implicit-cf/"},{"categories":["Recommender systems"],"content":"Building a Recommender System Using Implicit Feedback The most common collaborative filtering algorithms are either neighborhood-based or latent factor models. Neighborhood models, such as clustering or top-N, find the closest users or closest items to generate recommendations. But with implicit data, they do not make a distinction between user preferences and the confidence we might have in those preferences. Factorization models, uncover latent features that explain observed ratings. We already saw an example of a factorization method using explicit data in the previous section. Most of the research in this domain use explicit feedback datasets and model the observed ratings directly as shown below: Parameters in the above equation are often learned through stochastic gradient descent. Work done by Hu et. al divides raw observation $r_{ui}$ into two separate magnitudes with distinct interpretations: preference ($p_{ui}$) and confidence levels ($c_{ui}$). Hu et. al.5 in their seminal paper (Winner of the 2017 IEEE ICDM 10-Year Highest-Impact Paper Award) described an implicit feedback based factorization model idea that gained a lot of popularity. First, they formalized the notion of preference and confidence. Preference $p_{ui}$ is indicated using a binary variable that is 1 for all non-zero and positive $r_{ui}$ values, and is 0 otherwise (when u never consumed i). $p_{ui} = \\begin{cases} 1 \u0026 r_{ui}\u003e0\\ 0 \u0026 r_{ui}=0 \\end{cases}$ $r_{ui}$ indicates the observations for user actions, for example $r_{ui}$ = 0.7 could indicate that the user u watched 70% of the movie i. If $r_{ui}$ grows, so does our confidence that the user indeed likes the item. Hence the confidence $c_{ui}$ in observing $p_{ui}$ is defined as $1 + \\alpha r_{ui}$. This varying level of confidence gracefully handles the noisy implicit feedback issues described previously. Also, the missing values ($p_{ui} = 0$) will have low corresponding confidence. Similar to the least-squares model for explicit feedback, our goal is to find a vector $x_{u}$ (user factor) for each user u and vector $y_{i}$ (item factor) for each item i that will factor respective user and item preferences. In other words, preferences are assumed to be the inner products: $p_{ui} = x_{u}^{T}y_{i}$. Hence the cost function becomes: Note that the cost function now contains $m \\times n$ terms where m and n are the dimensions of the rating matrix. As this number can easily reach a few billion, the author proposed a clever modification to the alternating least squares method (as opposed to using stochastic gradient descent). The algorithm alternates between re-computing user factors and item factors and each step is guaranteed to lower the value of the cost function. Basically $x_{u}$ can be minimized to: $x_{u} = (Y^{T} C^{u}Y + \\lambda I)^{-1} Y^{T} C^{u} p(u)$, which can be further sped up by computing $Y^{T} C^{u}Y$ as $Y^{T}Y + Y^{T}(C^{u}-I)Y$. Similar we calculate $y_{i}$ as $(X^{T} C^{i}X + \\lambda I)^{-1} X^{T} C^{i} p(i)$. A slight modification to this calculation also allows for explainable recommendations. Refer to their paper for further details5. ","date":"2022-09-25","objectID":"/posts/2022/09/explicit-implicit-cf/:5:0","tags":["recsys","literature review"],"title":"Collaborative Filtering based Recommender Systems for Implicit Feedback Data","uri":"/posts/2022/09/explicit-implicit-cf/"},{"categories":["Recommender systems"],"content":"Implementing Implicit Feedback Recommender in Python To implement a recommender based on the above idea, we will use the Last.fm 360K dataset 6, and the Python implementation based on Ben Frederickson’s implicit Python package7. import numpy as np import pandas as pd from numpy import bincount, log, log1p from scipy.sparse import coo_matrix, linalg class ImplicitCF: def __init__(self): self.df = pd.read_csv(\"lastfm-dataset-360K/usersha1-artmbid-artname-plays.tsv\", sep='\\t', header=None, names=['user', 'artist', 'plays'], usecols=[0,2,3]) self.df['user'] = self.df['user'].astype(\"category\") self.df['artist'] = self.df['artist'].astype(\"category\") self.df.dropna(inplace=True) self.plays = coo_matrix((self.df['plays'].astype(float), (self.df['artist'].cat.codes, self.df['user'].cat.codes))) def _bm25_weight(self, X, K1=100, B=0.8): \"\"\"Weighs each row of a sparse matrix X by BM25 weighting\"\"\" # calculate idf per term (user) X = coo_matrix(X) N = float(X.shape[0]) idf = log(N) - log1p(bincount(X.col)) # calculate length_norm per document (artist) row_sums = np.ravel(X.sum(axis=1)) average_length = row_sums.mean() length_norm = (1.0 - B) + B * row_sums / average_length # weight matrix rows by bm25 X.data = X.data * (K1 + 1.0) / (K1 * length_norm[X.row] + X.data) * idf[X.col] return X def _alternating_least_squares(self, Cui, factors, regularization, iterations=20): artists, users = Cui.shape X = np.random.rand(artists, factors) * 0.01 Y = np.random.rand(users, factors) * 0.01 Ciu = Cui.T.tocsr() for iteration in range(iterations): self._least_squares(Cui, X, Y, regularization) self._least_squares(Ciu, Y, X, regularization) return X, Y def _least_squares(self, Cui, X, Y, regularization): artists, factors = X.shape YtY = Y.T.dot(Y) for u in range(artists): # accumulate YtCuY + regularization * I in A A = YtY + regularization * np.eye(factors) # accumulate YtCuPu in b b = np.zeros(factors) for i in Cui[u,:].indices: confidence = Cui[u,i] factor = Y[i] A += (confidence - 1) * np.outer(factor, factor) b += confidence * factor # Xu = (YtCuY + regularization * I)^-1 (YtCuPu) X[u] = np.linalg.solve(A, b) def factorize(self): artist_factor, user_factor = self._alternating_least_squares(self._bm25_weight(self.plays).tocsr(), 50, 1, 10) return artist_factor, user_factor def init_predict(self, x_factors): # fully normalize factors, so can compare with only the dot product norms = np.linalg.norm(x_factors, axis=-1) self.factors = x_factors / norms[:, np.newaxis] def get_related(self, x_id, N=5): scores = self.factors.dot(self.factors[x_id]) best = np.argpartition(scores, -N)[-N:] print(\"Recommendations:\") for _id, score in sorted(zip(best, scores[best]), key=lambda x: -x[1]): print(f\"artist id: {_id}, artist_name: {self.df.artist[_id]} score: {score:.5f}\") cf_object = ImplicitCF() print(cf_object.df.head()) # user artist plays #0 00000c289a1829a808ac09c00daf10bc3c4e223b betty blowtorch 2137 #1 00000c289a1829a808ac09c00daf10bc3c4e223b die Ärzte 1099 #2 00000c289a1829a808ac09c00daf10bc3c4e223b melissa etheridge 897 #3 00000c289a1829a808ac09c00daf10bc3c4e223b elvenking 717 #4 00000c289a1829a808ac09c00daf10bc3c4e223b juliette \u0026 the licks 706 print(cf_object.df.user.nunique()) # 358868 print(cf_object.df.artist.nunique()) # 292364 print(cf_object.df.plays.describe()) #count 1.753565e+07 #mean 2.151932e+02 #std 6.144815e+02 #min 0.000000e+00 #25% 3.500000e+01 #50% 9.400000e+01 #75% 2.240000e+02 #max 4.191570e+05 #Name: plays, dtype: float64 print(cf_object.plays.shape) # (292364, 358868) artist_factor, user_factor = cf_object.factorize() print(artist_factor.shape) # (292364, 50) print(user_factor.shape) # (50, 358868) cf_object.init_predict(artist_factor) print(cf_object.factors.shape) # (292364, 50) cf_object.get_related(2170) #Recommendations: #artist id: 257, artist_name: the beatles score: 1.00000 #artist id: 2170, artist_name: maroon 5 score: 1.00000 #artist id: 170436, artist_name: vilma palma e vampiros score: 1.00000 #artist id: 252491, artist","date":"2022-09-25","objectID":"/posts/2022/09/explicit-implicit-cf/:5:1","tags":["recsys","literature review"],"title":"Collaborative Filtering based Recommender Systems for Implicit Feedback Data","uri":"/posts/2022/09/explicit-implicit-cf/"},{"categories":["Recommender systems"],"content":"Summary In this article, I discussed defining characteristics of explicit and implicit feedback along with their respective shortcomings. Next, I went over one of the most popular research on a factor model which is specially tailored for implicit feedback recommenders. We also implemented factorization-based recommender systems in Python for both explicit and implicit datasets. ","date":"2022-09-25","objectID":"/posts/2022/09/explicit-implicit-cf/:6:0","tags":["recsys","literature review"],"title":"Collaborative Filtering based Recommender Systems for Implicit Feedback Data","uri":"/posts/2022/09/explicit-implicit-cf/"},{"categories":["Recommender systems"],"content":"References https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html ↩︎ https://grouplens.org/datasets/movielens/100k/ ↩︎ Y. Koren, R. Bell and C. Volinsky, “Matrix Factorization Techniques for Recommender Systems,” in Computer, vol. 42, no. 8, pp. 30-37, Aug. 2009, doi: 10.1109/MC.2009.263. https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf ↩︎ https://jeremykun.com/2016/04/18/singular-value-decomposition-part-1-perspectives-on-linear-algebra/ ↩︎ Hu, Yifan \u0026 Koren, Yehuda \u0026 Volinsky, Chris. (2008). Collaborative Filtering for Implicit Feedback Datasets. Proceedings - IEEE International Conference on Data Mining, ICDM. 263-272. 10.1109/ICDM.2008.22. ↩︎ ↩︎ https://www.upf.edu/web/mtg/lastfm360k ↩︎ https://www.benfrederickson.com/matrix-factorization/ ↩︎ ","date":"2022-09-25","objectID":"/posts/2022/09/explicit-implicit-cf/:7:0","tags":["recsys","literature review"],"title":"Collaborative Filtering based Recommender Systems for Implicit Feedback Data","uri":"/posts/2022/09/explicit-implicit-cf/"},{"categories":["Software Engineering"],"content":"This article will explain the basic concepts of each category of the NoSQL database models and analyze the characteristics of the data that each category of the NoSQL database is suitable for processing.","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/","tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/"},{"categories":["Software Engineering"],"content":"In the last article, I did a literature review defining the origin of SQL, NoSQL, and NewSQL. I also went over the important theorems and properties that could help in the categorization and comparison of the different types of databases. If you haven’t read that article yet, I highly recommend you to go over it: SQL vs NoSQL vs NewSQL: An In-depth Literature Review. In this article, we will build upon those concepts and learn how to categorize NoSQL databases based on the type of data we intend to store. ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:0:0","tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/"},{"categories":["Software Engineering"],"content":"Tackling the Data Variety Variety is one of defining aspects, the 7 Vs, of Big Data. It refers to the diverse, complex, and unstructured nature of the data generated by new channels and emerging technologies, such as social media, IoT, mobile devices, and online advertising. A majority of these processes generate semi-structured or unstructured data. This includes tabular data (databases), hierarchical data, documents, XML, emails, blogs, instant messaging, click streams, log files, data metering, images, audio, video, and information about share rates (stock ticker), financial transactions, etc 1. NoSQL provides a simple and flexible technology for the storage and retrieval of such data. This new DBMS drops the structure, entity relationships, and most of the other principles of the ACID (Atomicity, Consistency, Isolation, Durability) model. Instead, it proposes a more flexible model named Basic Availability Soft State Eventual Consistency (BASE). ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:1:0","tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/"},{"categories":["Software Engineering"],"content":"NoSQL Database Models The most commonly employed distinction between NoSQL databases is the way they store and allow access to data. While Relational databases require all data to neatly fit into tables, NoSQL offers different database solutions for specific contexts without having to use a specific schema. The “one size fits all” approach of relational databases no longer applies. According to the classification on the NoSQL database official website 2, there are 15 categories of NoSQL databases. This classification is based on the logical organization of the data (also called data models), meaning the way the data are stored in the database. Its query model specifies how to retrieve and update the data. There are hundreds of readily available NoSQL databases, and each has different use case scenarios. They are usually divided into four categories, according to their data model and storage: Key-Value Stores, Document Stores, Column Stores, and Graph databases. ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:2:0","tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/"},{"categories":["Software Engineering"],"content":"Key-Value Stores As the name suggests a key-value store consists of a set of key-value pairs with unique keys. The data is stored and accessed using keys and almost all queries are key lookup based. Key-value stores do not assume the structure of the stored data, meaning that both the key and the value can be of any structure and NoSQL has no information on the data; it just delivers the data based on the key. If the use-case requires any assumptions about the structure of the stored data, it has to be implicitly encoded in the application logic (i.e. schema-on-read), and not explicitly defined through a data definition language (i.e. schema-on-write) 3. These stores do not support operations beyond the get-put operations-based CRUD (Create, Read, Update, Delete), and their model can be likened to a distributed hash table. They also usually do not support composite keys and secondary indexes. A logical data structure that can contain any number of key-value pairs is called a namespace. Key-value stores commonly apply a hashing function to the key to obtain a specific node of the physical network where the value will be finally stored. To allow easy addition and/or removal of nodes, several databases take advantage of the concept of Consistent Hashing. Amazon’s DynamoDB made further improvements in this partitioning scheme by introducing virtual nodes. Similar to Consistent Hashing, some databases like Membase/CouchBase use Virtual Buckets or vBuckets to overcome the problem of redistributing keys when a node is added or removed. Figure: A simple key-value store example for serving static web content. Key-value stores can be categorized into three types in terms of storage options: temporary, permanent, and hybrid stores. In temporary stores, all the data are stored in memory, hence the access to data is fast. However, data will be lost if the system is down. Permanent key-value stores ensure the high availability of data by storing the data on the hard disk but with the price of the lower speed of I/O operations on the hard disk. Hybrid approaches combine the best of both temporary and permanent types by storing the data into memory and then writing the input to the hard disk when a set of specified conditions are met 4. Some of the in-memory key-value systems are Redis, Ehcache, Aerospike, and Tarantool. Persistent key-value systems include RocksDB, and LevelDB 5. Strengths of Key-Value Stores The simplicity of such a system makes it attractive in certain circumstances. For example, resource-efficient key-value stores are often applied in embedded systems or as high-performance in-process databases They are horizontally scalable. Partitioning and data querying is easy because of their simple abstraction. They provide low latency and high throughput. The time complexity of its data access is O(1) They can be optimized for reads or writes. Weaknesses of Key-Value Stores They are not powerful enough if an application requires complex operations such as range querying. KV Stores needs to return the entire value when accessing a key because the stored data is schemaless. In-place updates are generally not supported. It is hard for these stores to support reporting, analytics, aggregation, or ordered values. Example Applications of Key-value Stores Key-value stores are generally good solutions if you have a simple application with only one kind of object, and you only need to look up objects based on one attribute 4. Some examples of use-cases for KV stores are listed below 6. Data storage with simple querying needs Profiles, user preferences Data from a cart of purchases Sensor data, logs Data cache for rarely updated data Large-scale sessions management for users ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:2:1","tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/"},{"categories":["Software Engineering"],"content":"Document-Oriented Stores Document stores extend the unstructured storage paradigm of key-value stores and make the values to be semi-structured, specifically attribute name/value pairs, called documents. A document, in this context, is a JSON or JSON-like object (such as XML, YAML, or BSON). This restriction brings flexibility in accessing the data because it is now possible to fetch either an entire document by its key or to only retrieve parts of the document. The document-oriented database still regards the document as a whole, instead of partitioning the document into many key/value pairs. This enables the documents of different structures to be put into the same set. The document-oriented database supports document index, including not only primary identifiers but also document properties 7. Also, unlike key-value stores, both keys and values are fully searchable in document databases. They also usually support complex keys along with secondary indexes. Each document can store hundreds of attributes, and the number and type of attributes stored can vary from document to document (schema-free!). The attribute names are dynamically defined for each document at runtime, and values can be nested documents, or lists, as well as scalar values. Most of the databases available under this category provide data access typically over HTTP protocol using RESTful API or over Apache Thrift protocol for cross-language interoperability. A group of documents is called a collection. The documents within a collection are usually related to the same subject, such as employees, products, and so on. Figure: A simple Document store example for customer data. Document stores make it possible to add and delete value fields, modify certain fields, and query the database by fields. Queries can be done on any field using patterns, so, ranges, logical operators, wildcards, and more, can also be used in queries. The drawback is that for each type of query a new index needs to be created 8. According to db-engines 9, some highly ranked document-oriented systems include MongoDB, Couchbase, CouchDB, Realm, MarkLogic, OrientDB, RavenDB, PouchDB, and RethinkDB. Strengths of Document Stores They tend to support more complex data models than key-value stores. They usually support multiple indexes and range querying. They support data access over RESTful API. Documents can be nested due to the schemaless nature of the store. They typically support low latency reads. Weaknesses of Document Stores Joins are still not available within the database. A query requiring documents from two collections will require two separate queries. A lot of document stores have security concerns in regards to data leakage on web apps. However, this problem is prevalent among other NoSQL models as well. Example Applications of Document Stores A Document-oriented database is useful when the number of fields cannot be fully determined at application design time. Some examples of use-cases for document stores are as follows. Content Management Systems Real-time Web Analytics Products Catalog Blogs Analytical Platforms ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:2:2","tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/"},{"categories":["Software Engineering"],"content":"Column-Oriented Stores While the relational databases store and process the data with a row as the unit, column-oriented stores process and store the data with a column as the unit. A column-oriented store (also known as extensible record stores, wide-column stores, and column-family stores) stores each row by splitting it vertically and horizontally across the physical nodes of distributed systems. Each row can have one or more column families and is identified by a row key. Each column family can also act as a key to manipulate one or more columns. Technically a wide-column store is closer to a distributed multi-level sorted map (or a two-dimensional key-value store): the first-level keys identify rows that themselves consist of key-value pairs. The first-level keys are called row keys, the second-level keys are called column keys 3. Figure: General structure for Wide-Column databases. A Keyspace is used as the repository for the tables. The row key is similar to the key used in key-value systems for data manipulation. The set of all columns is partitioned into so-called column families to colocate columns on disks that are usually accessed together. Each column family acts as a key to manipulate one or more containing columns. Wide column stores are also called extensible record stores because they store data in records with the ability to hold very large numbers (billions) of columns (schema-free!). These column-wide data management systems are flexible enough to store the data of any format or type. The following figure represents a sample table in a wide-column datastore 4. Figure: An example of a data table in a wide column store. On disk, wide-column stores do not co-locate all data from each row, but instead values of the same column family and from the same row. Hence, an entity (a row) cannot be retrieved by one single lookup as in a document store but has to be joined together from the columns of all column families. However, this storage layout usually enables highly efficient data compression and makes retrieving only a portion of an entity very efficient. According to db-engines 10, some of the column-oriented systems that are ranked high include Cassandra, HBase, Google Cloud Bigtable, Accumulo, and ScyllaDB. Strengths of Column-Oriented Stores Wide Column or Column Families databases store data by columns and do not impose a rigid scheme on user data. This means that some rows may or may not have columns of a certain type. Since data stored by column have the same data type, compression algorithms can be used to decrease the required space. Because there is no column key without a corresponding value, null values can be stored without any space overhead. It is also possible to do functional partitioning per column so that columns that are frequently accessed are placed in the same physical location. The column-oriented database is so scalable that the increase in data size will not result in decreased processing speed. So it is well-suited for processing a huge amount of data. They have better querying capabilities than the key-value stores due to the usage of row-level and column-level keys. Wide-column stores allow different access levels for different columns. Weaknesses of Column-Oriented Stores Complexity in the development of such databases is considerably high. The application domain of Wide Column databases is limited to particular problems: data to be stored need to be structured and potentially reach the order of petabytes, but the search can only be done through the primary key, i.e., the ID of the row. Queries on certain columns are not possible as this would imply having an index on the entire dataset or traversing it 8. Referential integrity and joins are not supported. They are much less efficient when processing many columns simultaneously. Designing an effective indexing schema is difficult and time-consuming. Even then, the said schema would still not be as effective as simple relational data","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:2:3","tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/"},{"categories":["Software Engineering"],"content":"Graph-Oriented Stores Graph-oriented systems are based on graph theory that represents the objects as nodes of the graph and their relationships as the edges. In graph-oriented systems, the main focus is on the relationships, which naturally represent data and their relationships. This reduces the overhead of joins that involves the merging of columns from one or more tables. Instead of joins, each node possesses a record list of the relationships that it holds with the other nodes, eliminating the need for foreign keys and join operations 11. The graph-oriented systems are widely used in applications that embody complex relationships such as recommendation systems and social networking applications. Graph databases help us to implement graph processing requirements in the same query language level as we use for fetching graph data without the extra layer of abstraction for graph nodes and edges. This means less overhead for graph-related processing and more flexibility and performance. Figure: Example of vertex representation in a Graph-oriented database. The above figure shows an example where users in a system may have friendship relationships (i.e., undirected edges) and send messages to each other (i.e., directed edges). Undirected edges are commutative and can be stored once, whereas directed edges are often stored in two different structures to provide faster access for queries in different directions. Additionally, vertices and edges have specific properties associated with them, and thereby, these properties must be stored in separate structures. According to db-engines 12, some of the graph-oriented systems that are ranked high include Neo4j, JanusGraph, TigerGraph, Dgraph, Giraph, Nebula Graph, and FlockDB. Strengths of Graph-Oriented Stores They provide support for graph-based algorithms and queries. For example, deep traversals in graph databased systems are faster than relational databases. Graph-based stores allow for very fast execution of complex pattern-matching queries. They also allow for a compact representation of the data, basing its implementation on bitmaps. Most of them are transactional. Weaknesses of Graph-Oriented Stores Distributed graph processing is challenging. Additionally, a partitioning strategy is difficult to express in source code because the structure of the graph (unlike lists or hashes) is not known a priori. This is one of the greatest pitfalls in the application of MapReduce-based frameworks in graph processing 8. Similar to partitioning, computing locality is also challenging. Traversal algorithms generate a significant communication overhead when the vertices are located in different computing nodes. Most graph-based stores do not have a declarative language. Example Applications of Graph-Oriented Stores Generating recommendations Business Intelligence (BI) Semantic Web, Social networks analysis Geospatial Data, geolocation Genealogy, Web of things, Routing Services Authentication and authorization systems Fraud detection in financial services ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:2:4","tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/"},{"categories":["Software Engineering"],"content":"Other NoSQL Stores So far we have seen the four major categories of NoSQL database models. NoSQL website defines 11 other categories: Multi-model databases, Object databases, Grid/Cloud databases, XML databases, Multidimensional databases, Multi-value databases, Event Sourcing databases, Time Series databases, Scientific/Specialized databases, Others, and Unresolved and Uncategorized. The following table summarizes the suitable data processing feature for each of these databases 13. Figure: Summary of suitable data features for NoSQL databases. ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:2:5","tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/"},{"categories":["Software Engineering"],"content":"Comparing NoSQL Database Models Within the family of NoSQL databases, different systems handle size, data model, and query complexity differently. Big data requires NoSQL database solutions to address velocity and variety of data more than data volume. This notion comes from the belief that 90% of web-based companies will likely never rise to the volume levels implied by NoSQL 4. The following figure plots the different NoSQL data stores on the data volume vs. modeling complexity dimensions. There is a tradeoff between size and complexity, as can be noticed from the figure. Figure: NoSQL solutions; complexity vs size. Further, these NoSQL stores can also be compared based on non-functional requirements as shown below 14. Data Store Type Performance Scalability Flexibility Complexity Key-value store high high high none Column store high high moderate low Document store high variable (high) high low Graph store variable variable high high ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:3:0","tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/"},{"categories":["Software Engineering"],"content":"Summary The most commonly employed distinction between NoSQL databases is the way they store and allow access to data. Understanding the storage model characteristics of the NoSQL databases can help individuals or organizations in selecting the most appropriate class of databases for specific data feature requirements. This article explored the four most popular NoSQL data storage models. We looked at strengths, weaknesses, and specific application domains for each of those database models, along with a basic comparison among them. ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:4:0","tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/"},{"categories":["Software Engineering"],"content":"What’s Next In the next article, we will look at the most popular database examples from each of the NoSQL models explained above. We will compare those individual databases against each other on functional and non-functional requirements. That comparison will further help the users in selecting the most appropriate database for their specific data application requirements. ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:5:0","tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/"},{"categories":["Software Engineering"],"content":"References Alexandru, Adriana \u0026 Alexandru, Cristina \u0026 Coardos, Dora \u0026 Tudora, Eleonora. (2016). Big data: Concepts, Technologies and Applications in the Public Sector. International Journal of Computer, Electrical, Automation, Control and Information Engineering. 10. 1629-1635. ↩︎ NoSQL Databases. Available online: http://nosql-database.org/ ↩︎ Gessert, Felix \u0026 Wingerath, Wolfram \u0026 Friedrich, Steffen \u0026 Ritter, Norbert. (2017). NoSQL database systems: a survey and decision guidance. Computer Science - Research and Development. 32. 10.1007/s00450-016-0334-3 ↩︎ ↩︎ Khazaei, Hamzeh \u0026 Fokaefs, Marios \u0026 Zareian, Saeed \u0026 Beigi, Nasim \u0026 Ramprasad, Brian \u0026 Shtern, Mark \u0026 Gaikwad, Purwa \u0026 Litoiu, Marin. (2015). How do I choose the right NoSQL solution? A comprehensive theoretical and experimental survey. Journal of Big Data and Information Analytics (BDIA). 2. 10.3934/bdia.2016004. ↩︎ ↩︎ ↩︎ ↩︎ DB-Engines Ranking of Key-value Stores. Available online: https://db-engines.com/en/ranking/key-value+store ↩︎ Hajoui, Omar \u0026 Dehbi, Rachid \u0026 Talea, Mohamed \u0026 Ibn Batouta, Zouhair. (2015). An advanced comparative study of the most promising NoSQL and NewSQL databases with a multi-criteria analysis method. 81. 579-588. ↩︎ Han, Jing, E. Haihong, Guan Le and Jian Du. “Survey on NoSQL database.” 2011 6th International Conference on Pervasive Computing and Applications (2011): 363-366. ↩︎ Corbellini, Alejandro \u0026 Mateos, Cristian \u0026 Zunino, Alejandro \u0026 Godoy, Daniela \u0026 Schiaffino, Silvia. (2017). Persisting big data: The NoSQL landscape. Information Systems. 63. 1-23. 10.1016/j.is.2016.07.009. ↩︎ ↩︎ ↩︎ DB-Engines Ranking of Document Stores. Available online: https://db-engines.com/en/ranking/document+store ↩︎ DB-Engines Ranking of Wide-Column Stores. Available online: https://db-engines.com/en/ranking/wide+column+store ↩︎ Chaudhry, Natalia \u0026 Yousaf, Muhammad. (2020). Architectural assessment of NoSQL and NewSQL systems. Distributed and Parallel Databases. 38. 10.1007/s10619-020-07310-1. ↩︎ DB-Engines Ranking of Graph DBMS. Available online: https://db-engines.com/en/ranking/graph+dbms ↩︎ Chen, \u0026 Lee,. (2019). An Introduction of NoSQL Databases Based on Their Categories and Application Industries. Algorithms. 12. 106. 10.3390/a12050106. ↩︎ Kotecha, Bansari H., and Hetal Joshiyara. “A Survey of Non-Relational Databases with Big Data.” International Journal on Recent and Innovation Trends in Computing and Communication 5.11 (2017): 143-148. ↩︎ ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:6:0","tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/"},{"categories":["Software Engineering"],"content":"In this article, we look at SQL, NoSQL and NewSQL database technologies. We go over the origin, strengths, weaknesses, application areas, and fundamental concepts for each of the database types to make it easy to select the most appropriate database type for a given application.","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/","tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/"},{"categories":["Software Engineering"],"content":"SQL Databases Origin The concept of Relational Databases was originally developed in the 1970s by IBM 1. They are also known as SQL (Structured Query Language) databases, named after the query language used for managing data in Relational Database Management Systems (RDBMS). Over multiple years of research and development, an unmatched level of reliability, stability, and strong mechanisms to store and query data have been baked into Relational Databases. They have been the storage of choice for a majority of transactional data management applications such as banking, airline reservation, online e-commerce, and supply chain management applications. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:1:0","tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/"},{"categories":["Software Engineering"],"content":"Relations in Relational Databases Relational Databases model the data to be stored as a collection of relations. A relation is a two-dimensional, normalized table to organize data. Each relation is composed of two parts: relation schema and relation instances. Relation schema includes the name of the relation, names of the attributes in the relation, along with their domain. Relation instances refer to the data records stored in the relation at a specific time. The following table gives an example of a relation 2. SID: Char(5) Name: Char(10) Telephone: Char(11) Birthday: Date (dd/mm/yy) S0001 Alice 05-65976597 10/4/1994 S0002 Dora 06-45714571 15/5/1995 S0003 Ella 07-57865869 20/6/1996 S0004 Kevin 06-57995611 22/7/1997 Table Name: Students Here “Students” is the relation; SID, name, telephone, and birthday are the names of the attributes; domain for Birthday is Date, and the relation has four data records stored. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:1:1","tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/"},{"categories":["Software Engineering"],"content":"Characteristics of Relational Databases Mature Ecosystem: Relational databases are well known for reliability, stability, and support achieved through decades of development. Support for Vertical Scaling: They can be scaled vertically to handle growing data volume by beefing up the existing server. Fixed Schema: They enforce strict structure (schema) constraints on the data to be stored, and provide powerful mechanisms to store and query this structured data. Constraints: As part of an RDB design, integrity constraints are used to check the correctness of the data input into the database. Integrity constraints not only prevent authorized users from storing illegal data into the database but also avoid data inconsistency between relations. The four common kinds of integrity constraints are: Primary Key constraints, Domain constraints, Entity integrity constraints, and Referential integrity constraints. RDBMS uses a relational model which has a relationship between tables using foreign keys, primary keys, and indexes. Because of this, fetching and storing of data becomes faster than the older Navigational models that RDBMS replaced 3. Support for Transactions: RDBs especially shine when it comes to managing transactions. The concept of a transaction was first proposed by Jim Gray in 1970 4, and it represents a work unit in a database system that contains a set of operations. For a transaction to behave in a safe manner it should exhibit four main properties: atomicity, consistency, isolation, and durability (ACID). ACID Properties This set of properties, known as ACID, increases the complexity of database systems 5, but also guarantees strong consistency and serializability. Atomicity: a guarantee that all of the operations of a transaction will be completed as a whole or none at all Consistency: a guarantee that the saved data will always be valid and the database will be consistent both before and after the transaction Integrity: a guarantee that the different transactions will not interfere with each other Durability: a guarantee that the changes of a successful transaction will persist permanently in the database even if a system failure occurs Relational databases are ACID-compliant and hence simplify the work of the developer by guaranteeing that every operation will leave the database in a consistent state. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:1:2","tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/"},{"categories":["Software Engineering"],"content":"The Rise of NoSQL Databases ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:2:0","tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/"},{"categories":["Software Engineering"],"content":"Big Data Era In recent years, the amount of useful data in some application areas has become so vast that it cannot be stored or processed by traditional database solutions. With the launch of Web 2.0, a large amount of valuable business data started being generated. This data can be structured or unstructured and can come from multiple sources. Social networks, products viewed in virtual stores, information read by sensors, GPS signals from mobile devices, lP addresses, cookies, bar codes, etc. are examples of this phenomenon commonly referred to as Big Data 6. Big Data!\rBig data is often characterized by 7 Vs: Volume (great size), Velocity (rapid procreation), Variety (various types), Veracity (data messiness or constancy), Value (huge value but low density), Variability (constantly changing), and Visualization (data presentation).\r","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:2:1","tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/"},{"categories":["Software Engineering"],"content":"Shortcomings of Relational Databases Big Data significantly changed the way organizations viewed data. The data volume, accumulation speed, and various data types required operating in ways that Relational databases were not quite designed for. A popular survey suggested that 37.5% of leading companies were unable to analyze big data 7. Some of the major shortcomings that Relational databases exhibited are as follows. Scalability: Most SQL-oriented databases do not support distributed data processing and storage, making it a challenge to work with data of high volume, therefore, resulting in a need for a server with exceptional computing capabilities to handle the large data volume, which is an expensive undesirable solution. Flexibility: Big Data systems are primarily unstructured in nature, which meant that the need for a predefined structure of the data, i.e. explicitly defining a schema for the data being stored became difficult. Not all data could be fit into tables, so custom solutions targeting the new data models were required. Complexity: Database performance often decreases significantly since joins and transactions are costly in distributed environments. Others: Servers based on SQL standards are now prone to memory footprint, security risks, and performance issues 8. All in all, this does not mean RDBMSs have become obsolete, but rather they have been designed with other requirements in mind and work well when extreme scalability is not required. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:2:2","tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/"},{"categories":["Software Engineering"],"content":"Introducing NoSQL databases Traditional databases could not cope with the generation of massive amounts of information by different devices, including GPS information, RFIDs, IP addresses, unique Identifiers, data and metadata about the devices, sensor data, and historical data. A class of novel data storage systems able to cope with Big Data has subsumed under the term NoSQL databases. NoSQL!\rThe term “NoSQL” was first coined in 1998 by Carlo Strozzi for his RDBMS, Strozzi NoSQL. However, Strozzi used the term simply to distinguish his solution from other RDBMS. He used the term NoSQL just for the reason that his database did not expose a SQL interface. It was then brought back in 2009 for naming an event that highlighted new non-relational databases, such as BigTable and Dynamo. While originally the term stood for “No SQL”, it has been restated as “Not Only SQL” to highlight that these systems rarely fully drop the relational model. Thus, despite being a recurrent theme in literature, NoSQL is a very broad term, encompassing very distinct database systems [^18].\r","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:2:3","tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/"},{"categories":["Software Engineering"],"content":"NoSQL Properties The major properties of NoSQL databases are as follows 9 10. Highly and Easily Scalable: NoSQL databases are designed to expand horizontally, meaning that you scale out by adding more machines into your pool of resources, while still being able to run CRUD operations over many servers. Sharding: Data can also be distributed across nodes in a non-overlapping way; this technique is known as Sharding, and the locations of the stored data are managed by metadata. Replication: To provision scaling, a lot of NoSQL databases are designed under a “let it crash” philosophy, where nodes are allowed to crash and their replicas are always ready to receive requests. NoSQL databases mostly support master-slave replication or peer-to-peer replication, making it easier for NoSQL databases to ensure high availability. No-Sharing Architecture: They are based on ‘No Sharing Architecture’ in which neither memory nor storage is shared. This enables each node to operate independently. The scaling thus becomes very convenient as new nodes can be easily added or removed from the system to meet the data processing requirement. Flexible Schema: The data to be stored in NoSQL databases do not need to conform to a strict schema. This also enables the ability to dynamically add new attributes to data records. Less expensive to maintain: The scalability of NoSQL databases allows the database admins to make use of pay-as-you-go pricing models of cloud-based database service providers. NoSQL databases can run on low specs devices. Integrated Caching: In general, as NoSQL databases are designed to be distributed, the location of the data in the cluster is leveraged to improve network usage, usually by caching remote data, and making queries to those nodes located closer in the network topology. This mechanism is often referred to as location awareness or data affinity. No Support for Joins: In general, NoSQL databases do not use a relational database model, and do not support SQL join operations or have very limited support for it. So the related data needs to be stored together to improve the speed of data access. Others: Unlike most RDBs that require a fee to purchase, most NoSQL databases are open source and free to download. NoSQL databases often have easy-to-use APIs as well. Past researches, such as Stonebraker et al. 11, have argued that the main reasons to move to NoSQL databases are performance and flexibility. Performance is mainly focused on sharing and management of distributed data (i.e. dealing with “Big Data”), while flexibility relates to the semi-structured or unstructured data that may arise on the web. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:2:4","tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/"},{"categories":["Software Engineering"],"content":"Most Suited Applications for NoSQL NoSQL is primarily suited 12 for applications with: No need for ACID properties Flexible Schema No constraints and validation to be executed in database Temporary data Different data types High data volume ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:2:5","tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/"},{"categories":["Software Engineering"],"content":"Limitations of NoSQL Databases NoSQL databases are open-source which is its greatest strength but at the same time, it can be considered its greatest weakness because there are not many defined standards for NoSQL databases; so, no two NoSQL databases are equal. Tools like GUI mode, management console, etc. to access the database are not widely available in the market. Joins and transactions are costly operations for NoSQL databases. Most NoSQL databases do not support ACID properties, hence data consistency remains a fundamental challenge for NoSQL. Due to a lack of standardization, the design and query languages for NoSQL databases vary widely. This results in a steeper learning curve for NoSQL databases. Despite being designed with horizontal scaling in mind, not all NoSQL databases are good at automating the process of sharding. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:2:6","tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/"},{"categories":["Software Engineering"],"content":"The NewSQL Paradigm NewSQL is a class of modern relational database management systems that aim to combine the best of SQL and NoSQL databases. NoSQL databases seek to provide the same scalable performance as NoSQL systems for online transaction processing(OLTP) read-write workloads while still maintaining the ACID guarantees of a traditional database system. Basically, it is a combination of NoSQL (Scalable) properties with Relational (ACID) Properties 13. Example systems in this category are NuoDB, VoltDB, Google Spanner, and Clustrix. NewSQL!\rNewSQL is not only used to overcome some limitations of the SQL, but it could also be an alternative for NoSQL in certain applications where the need for analytics and decision making has to be found on request with high consistency [^12]. It helps in executing read-write transactions that are short-lived and operated using index loops and executing the same number of queries with different inputs. It works on lock-free concurrency control and share-nothing architecture [^10].\r","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:3:0","tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/"},{"categories":["Software Engineering"],"content":"Characteristics of NewSQL Databases Distributed: NewSQL databases are relational databases supporting share-nothing architecture, sharding, automatic replication, and distributed transaction processing, i.e., providing ACID guarantees even across shards. Schemas: NewSQL databases support fixed schemas as well as schema-free databases. ACID-compliant: NewSQL systems preserve the ACID properties of relational databases. SQL Support: NewSQL databases support SQL but query complexity is very high. They also support SQL extensions. RAM Support: NewSQL gives high performance by keeping all data in RAM and enabling in-memory computations. Data Affinity: Scalability is provided by employing partitioning and replication in such a way that queries generally do not have to communicate between multiple machines. They get the required information from a single host. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:3:1","tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/"},{"categories":["Software Engineering"],"content":"Application of NewSQL: Google Spanner Google’s Spanner is a globally distributed database system created at Google that supports distributed transactions, designed as a replacement to Megastore, a BigTable-based storage. It is a database that performs sharding of data that is a horizontal partition of data and it is spread across many Paxos state machines. Paxos state machines are used to solve consensus which is the process of agreeing upon one result when there are multiple participants. The datacenters are spread all over the world. The databases should be available globally. So for this purpose replication is used. The replication isn’t completely random. It considers the geographic locality and what kind of data is required more frequently. Spanner works dynamically. It reshards and migrates data automatically for the purpose of load balancing. For achieving low latency and high availability most applications would probably replicate data over three or five datacenters in one geographic region. Spanner is useful when applications want strong consistency and are distributed over a large area. Spanner performs versioning of the data and stores the time stamp which is the same as the commit time. Unrequired data can be deleted with proper policies for handling old data. Spanner is very useful for OLTP concerning Big Data. It also uses SQL query language. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:3:2","tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/"},{"categories":["Software Engineering"],"content":"SQL vs NoSQL vs NewSQL database comparison Distinguishing Feature SQL NoSQL NewSQL Relational Yes No Yes ACID Yes No Yes SQL Yes No Yes OLTP Not fully Supported Supported Fully Supported Horizontal Scaling No Yes Yes Query Complexity Low High Very High Distributed No Yes Yes Source [^3] ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:4:0","tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/"},{"categories":["Software Engineering"],"content":"Transactional Properties and Theorems Now we turn our attention to some fundamental properties and theorems that will lay the foundations for understanding the key differentiating core philosophies among the different database implementations. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:5:0","tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/"},{"categories":["Software Engineering"],"content":"ACID Properties We discussed ACID properties earlier on in this article. ACID is a combination of four properties namely Atomicity, Consistency, Isolation, and Durability. These transactional properties of the database guarantee the reliability of transactions 14. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:5:1","tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/"},{"categories":["Software Engineering"],"content":"The CAP Theorem The CAP Theorem, also known as Brewer’s theorem was presented by Eric Brewer in 2000 and later proven by Gilbert and Lynch 15, is one of the truly influential impossibility results in the field of distributed computing, because it places an ultimate upper bound on what can be accomplished by a distributed system 6. It states that it is impossible for a distributed computer system to simultaneously provide all three of the following guarantees: Consistency (C): Reads and writes are always executed atomically and are strictly consistent (linearizable). Put differently, all clients have the same view on the data at all times. Availability (A): All clients can always find at least one copy of the requested data, even if some of the machines in a cluster are down. Put differently, it is a guarantee that every request receives a response about whether it succeeded or failed. Partition Tolerance (P): The system can continue operating normally in the presence of network partitions. These occur if two or more “islands” of network nodes arise that (temporarily or permanently) cannot connect to each other due to network failures. Eric Brewer conjectured that at any given moment in time only two out of the three mentioned characteristics can be guaranteed, concluding that only distributed systems accomplishing the following combinations can be created: AP (Availability-Partition Tolerance), CP (Consistency-Partition Tolerance), or AC (Availability-Consistency). Most of the databases fall in the “AP” (eventual consistent systems) or the “CP” group because resigning P (Partition Tolerance) in a distributed system means assuming that the underlying network will never drop packages or disconnect, which is not feasible. Some systems usually are available and consistent, but fail completely when there is a partition (CA), for example, single-node relational database systems. Figure: CAP theorem with databases that “choose” CA, CP, and AP [^18] ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:5:2","tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/"},{"categories":["Software Engineering"],"content":"BASE Properties Many of the NoSQL databases have loosened up the requirements on Consistency to achieve better Availability and Partitioning. This resulted in systems known as the BASE (Basically Available, Soft-state, Eventually consistent). The idea behind the systems implementing this concept is to allow partial failures instead of full system failure, which leads to a perception of greater system availability. Basically Available: Basically available ensures the availability of data. It means that there will always be a guaranteed response to each request. Some NoSQL DBs typically keep several copies of specific data on different servers, which allows the DB system to respond to all queries even if a few of the servers fail. Soft-state: Soft state means the system may change as it switches from one state to another even without any input, i.e. the data can be volatile, stored, and recovered easily and can be regenerated, unlike the hard state of the system in which the predictable inputs produce the predictable outputs. This soft state is due to the concept of eventual consistency in which the state of the system may become consistent after some time even without input. Eventually consistent: Eventually consistent means that the consistency is achieved after some indeterminate time because after a write operation, the replicas need to be synchronized. Therefore, strict consistency is difficult to ensure. The design of BASE systems, and in particular BASE NoSQL databases, allows certain operations to be performed leaving the replicas (i.e., copies of the data) in an inconsistent state. ACID principles are used when data reliability and consistency are important because the focus is on consistency and availability. Whereas BASE principles are used when data viability and speed are important because the focus is on focuses on availability and partition tolerance 16. Figure: Big Data characteristics and NoSQL System features ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:5:3","tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/"},{"categories":["Software Engineering"],"content":"PACELC Theorem It is important to note that the CAP Theorem actually does not state anything on normal operation; it merely tells us whether a system favors availability or consistency in the face of a network partition. The CAP theorem assumes a failure model that allows arbitrary messages to be dropped, reordered, or delayed indefinitely. This lack of the CAP Theorem is addressed in an article by Daniel Abadi 17 in which he points out that the CAP Theorem fails to capture the trade-off between latency and consistency during normal operation. He formulates PACELC which unifies both trade-offs and thus portrays the design space of distributed systems more accurately. From PACELC, we learn that in the case of a Partition, there is an Availability-Consistency trade-off; Else, i.e. in normal operation, there is a Latency-Consistency trade-off. This classification basically offers two possible choices for the partition scenario (A/C) and also two for normal operation (L/C) and thus appears more fine-grained than the CAP classification. However, many systems cannot be assigned exclusively to one single PACELC class and one of the four PACELC classes, namely PC/EL, can hardly be assigned to any system 18. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:5:4","tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/"},{"categories":["Software Engineering"],"content":"BASIC Properties SQL databases use ACID, which is the strongest consistency level, while NoSQL databases use BASE, which is a weak consistency model. Some NewSQL databases 19 support a consistency level between ACID and BASE, called BASIC (Basic Availability, Scalability, Instant Consistency) 20. Basic Availability: The system always responds to read/write queries. Scalability: Scalability allows for adding more resources if the workload is increased. Instant Consistency: Instant consistency, a better consistency level than eventual consistency, ensures that if a write and read operation is carried out consecutively, then the result returned by a read operation must be the same as written by a write operation. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:5:5","tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/"},{"categories":["Software Engineering"],"content":"Summary SQL Databases, also known as RDBMS (Relational Database Management Systems) are the most common and traditional approach to database solutions. SQL solves problems ranging from fast write-oriented transactions to scan-intensive deep analytics. SQL allows users to apply their knowledge across systems and provides support for third-party add-ons and tools because it is standardized. The data is stored in a structured way in form of tables or Relations. With the advent of Big Data, however, the structured approach falls short to serve the needs of Big Data systems which are primarily unstructured in nature. Increasing the capacity of SQL although allows a huge amount of data to be managed, it does not really count as a solution to Big Data needs, which expects a fast response and quick scalability. To solve this problem a new kind of Database system called NoSQL was introduced to provide the scalability and unstructured platform for Big Data applications. The data structures used by NoSQL databases (e.g. key-value, graph, or document) differ slightly from those used by default in relational databases, making some operations faster in NoSQL and others faster in relational databases. NoSQL stands for Not Only SQL. It is also horizontally Scalable as opposed to vertical scaling in RDBMS. NoSQL was introduced to us for resolving scalability issues but consistency issues after scalability moved us from NoSQL to NewSQL. NoSQL provided great promises to be a perfect database system for Big Data applications; it however falls short because of some major drawbacks like NoSQL does not guarantee ACID properties (Atomicity, Consistency, Isolation, and Durability) of SQL systems. It is also not compatible with earlier versions of the database. This is where NewSQL comes into the picture. NewSQL is the latest development in the world of database systems, and it is basically a Relational Database with the scalability properties of NoSQL. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:6:0","tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/"},{"categories":["Software Engineering"],"content":"What’s Next I hope you found this database technologies review to be useful. In the next few articles, I will compare different NoSQL data models on several criteria, then I will compare the most popular databases from each NoSQL model, followed by explaining structured approaches for picking the best database for your use-case. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:7:0","tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/"},{"categories":["Software Engineering"],"content":"References Don Chamberlin : “SQL”, IBM Almaden Research Center, San Jose, CA ↩︎ Chen, J.-K.; Lee, W.-Z. An Introduction of NoSQL Databases Based on Their Categories and Application Industries. Algorithms 2019, 12, 106. ↩︎ Binani, Sneha \u0026 Gutti, Ajinkya \u0026 Upadhyay, Shivam. (2016). SQL vs. NoSQL vs. NewSQL- A Comparative Study. Communications on Applied Electronics. 6. 43-46. 10.5120/cae2016652418. ↩︎ Jim Gray, Andreas Reuter, Transaction Processing: Concepts and Techniques, 1st ed. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1992 ↩︎ Corbellini, Alejandro \u0026 Mateos, Cristian \u0026 Zunino, Alejandro \u0026 Godoy, Daniela \u0026 Schiaffino, Silvia. (2017). Persisting big data: The NoSQL landscape. Information Systems. 63. 1-23. 10.1016/j.is.2016.07.009. ↩︎ Gessert, Felix \u0026 Wingerath, Wolfram \u0026 Friedrich, Steffen \u0026 Ritter, Norbert. (2017). NoSQL database systems: a survey and decision guidance. Computer Science - Research and Development. 32. 10.1007/s00450-016-0334-3. ↩︎ ↩︎ S. J. Veloso, 2015, Data Analytics Topic: Big Data [Online]. ↩︎ Venkatraman, S., Fahd, K., Kaspi, S., \u0026 Venkatraman, R. (2016). SQL Versus NoSQL Movement with Big Data Analytics. International Journal of Information Technology and Computer Science, 8, 59-66. ↩︎ Kotecha, Bansari H., and Hetal Joshiyara. “A Survey of Non-Relational Databases with Big Data.” International Journal on Recent and Innovation Trends in Computing and Communication 5.11 (2017): 143-148. ↩︎ Hajoui, Omar \u0026 Dehbi, Rachid \u0026 Talea, Mohamed \u0026 Ibn Batouta, Zouhair. (2015). An advanced comparative study of the most promising NoSQL and NewSQL databases with a multi-criteria analysis method. 81. 579-588. ↩︎ Stonebraker M (2010) Sql databases v. nosql databases. Commun ACM. 53(4):10–11 ↩︎ Khasawneh, Tariq \u0026 Alsahlee, Mahmoud \u0026 Safieh, Ali. (2020). SQL, NewSQL, and NOSQL Databases: A Comparative Survey. 013-021. 10.1109/ICICS49469.2020.239513. ↩︎ Chandra, Umesh. (2017). A Comparative Study On: Nosql, Newsql And Polygot Persistence. International Journal Of Soft Computing and Engineering (IJSE). 7. ↩︎ Chaudhry, Natalia \u0026 Yousaf, Muhammad. (2020). Architectural assessment of NoSQL and NewSQL systems. Distributed and Parallel Databases. 38. 10.1007/s10619-020-07310-1. ↩︎ Gilbert S, Lynch N (2002) Brewer’s conjecture and the feasibility of consistent, available, partition-tolerant web services. SIGACT News 33(2):51–59 ↩︎ Khazaei, Hamzeh \u0026 Fokaefs, Marios \u0026 Zareian, Saeed \u0026 Beigi, Nasim \u0026 Ramprasad, Brian \u0026 Shtern, Mark \u0026 Gaikwad, Purwa \u0026 Litoiu, Marin. (2015). How do I choose the right NoSQL solution? A comprehensive theoretical and experimental survey. Journal of Big Data and Information Analytics (BDIA). 2. 10.3934/bdia.2016004. ↩︎ Abadi D (2012) Consistency tradeoffs in modern distributed data- base system design: cap is only part of the story. Computer 45(2):37–42 ↩︎ Lourenço, João \u0026 Cabral, Bruno \u0026 Carreiro, Paulo \u0026 Vieira, Marco \u0026 Bernardino, Jorge. (2015). Choosing the right NoSQL database for the job: a quality attribute evaluation. Journal of Big Data. 2. 18. 10.1186/s40537-015-0025-0. ↩︎ Yuan, Li-Yan \u0026 Wu, Lengdong \u0026 You, Jia-Huai \u0026 Shanghai, Yan \u0026 Software, Shifang. (2015). A Demonstration of Rubato DB: A Highly Scalable NewSQL Database System for OLTP and Big Data Applications. ↩︎ Wu, Lengdong \u0026 Yuan, Li-Yan \u0026 You, Jia-Huai. (2014). BASIC: An alternative to BASE for large-scale data management system. Proceedings - 2014 IEEE International Conference on Big Data, IEEE Big Data 2014. 10.1109/BigData.2014.7004206. ↩︎ ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:8:0","tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/"},{"categories":["NLP"],"content":"Recognizing feelings in the conversation partner and replying empathetically is a trivial skill for humans. But how can we infuse empathy into responses generated by a conversational dialogue agent or any of the text generation algorithm in Natural Language Processing? In this article, I will describe what empathy means through the lens of various academic disciplines and then do an in-depth review of the prior and current state-of-the-art NLU systems that can simulate empathy.","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/","tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/"},{"categories":["NLP"],"content":"Recognizing feelings in the conversation partner and replying empathetically is a trivial skill for humans. But how can we infuse empathy into responses generated by a conversational dialogue agent or any of the text generation algorithm in Natural Language Processing? In this article, I will describe what empathy means through the lens of various academic disciplines and then do an in-depth review of the prior and current state-of-the-art NLU systems that can simulate empathy. ","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:0:0","tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/"},{"categories":["NLP"],"content":"Empathy and its Linguistic Origin The word empathy has a rather interesting linguistic history. In 1909, this word was first introduced in English by psychologist Edward Titchener as a translation of the German word Einfühlung, which means “feeling into” or “in-feeling”. The German word itself was adapted from the Ancient Greek word “ἐμπάθεια” or “empátheia” meaning “in passion” (from Greek ’en pathos’). Einfühlung first appeared in Robert Vischer’s 1873 Ph.D. dissertation, where Vischer used it to describe the human ability to enter into a piece of art or literature and feel the emotions of its creator1. Even though in modern Greek, the word empátheia has an opposite meaning, a strong negative feeling or prejudice against someone; the English word empathy does not carry those negative connotations. The modern-day usage of the word empathy pertains to the range of psychological capacities that play a central role in establishing humans as social and moral animals. It enables us to “put ourselves into someone else’s shoes”. Before the introduction of “empathy” in the English language, the word sympathy was used to describe a related phenomenon of understanding others’ feelings2. However, empathy is used as a broader concept to address a phenomenon of not just understanding someone’s emotions and but also viscerally feeling them. ","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:1:0","tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/"},{"categories":["NLP"],"content":"The Importance of Empathy Theodor Lipps was a German philosopher who posited the theory that empathy should be understood as the primary epistemic means for gaining knowledge of other minds. While this theory has been a topic of contentious debate in the field of philosophy, the study and scientific exploration of empathy as a social science phenomenon have been less critical. There are two major focus areas involving empathy in social science. The first one treats empathy as a cognitive phenomenon and attempts to measure the accuracy of one’s abilities to recognize others’ personality, attitude, and moral traits. It concerns with the factors that affect empathy. For example, do age, gender, upbringing, family history, relationships impact empathy in a person3? The second focus area treats empathy as a rather emotional phenomenon and finds means to measure empathy and other perceptual factors that trigger empathetic responses. The interdisciplinary field of neuroscience, on the other hand, researches into the processes that neurologically enable a person to feel what another is feeling4. Empathy enhances social functioning5. The ability to understand and share feelings of other people around us enables us to also understand their present and future mental state and actions. It can even encourage prosocial behaviors by motivating humans to act altruistically towards kin, mates and, allies6 7. In their book “Empathy Reconsidered: New Directions in Psychotherapy”, Arthur Bohart and Leslie Greenberg, explored the role that empathy plays in psychotherapy8. Their work propounded that all forms of psychotherapy are effective as a result of empathetic processes, and made the case for ensuring that psychotherapists are empathetically engaging with their clients. Other researches have shown the positive impact of empathy in mental healthcare, nursing, and even primary care. Researchers Stewart Mercer and William Reynolds highlighted the importance of empathy in the quality of primary care, in their paper titled “Empathy and quality of care”9. There are a substantial number of similar studies that show a wide range of applications of empathy in healthcare. ","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:2:0","tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/"},{"categories":["NLP"],"content":"What does Empathy Mean for NLP? Natural Language Processing has proved to be an exceedingly viable tool to bridge the conversational gap between humans and machines. Various industries are now using conversational AI assistants (or chatbots) to improve their customer service. Not only these artificial conversational agents can understand users’ intent and respond to them, but they are also increasingly becoming capable of understanding users’ emotions. NLP researchers are looking into ways to infuse the human trait of empathy into conversational agents to create more empathetic end-user experience. As an example, consider the following two scenarios where an Amazon customer reaches out to a customer service bot to complain about their order not being delivered on time. The choice of words used by the customer in the two scenarios convey differently charged emotions. Compared to scenario 1, the customer sounds more distressed in scenario 2. An optimal response from the bot in the second scenario should not feel like an off-the-shelf template response. An ideal response may start with first acknowledging the understanding of the customer’s frustration and displaying empathy to subdue their negatively charged emotions. A compassionate choice of words in the response can not only alleviate some of the customer annoyance, but it may also help in better customer retention in the long run. ","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:3:0","tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/"},{"categories":["NLP"],"content":"NLP Research on Empathy ","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:4:0","tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/"},{"categories":["NLP"],"content":"Early Works The topics of identifying and generating empathy in natural langue processing haven’t seen as explosive research growth and application as topics like sentiment analysis. Perhaps the earliest work in identifying empathy in text data was done by Xiao et al., 201210, when they developed an N-gram language model-based maximum likelihood strategy to classify empathetic vs non-empathetic utterances from a dataset of clinical trial studies on substance use by college students. In 2015, Gibson et al.11 proposed computation of features based upon psycholinguistic norms, and in 2017, Khanpour et al.12 used a simple combination of Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks to identify empathetic messages in online health communities. It’s worth highlighting that most of these researches used datasets that weren’t publicly available to the NLP community. In 2018, Buechel et al.13 published their research which they claimed to be the first gold-standard for empathy prediction. Their paper looked at a more nuanced form of empathy that was based on psychology and included empathic concern, and personal distress. Also, the empathy ratings in their dataset were provided by writers instead of other annotators. They used Ridge regression, simple feed-forward neural nets, and CNN for their empathy prediction tasks on newswire articles. In somewhat related research, Perez-Rosas et al.14 looked into behavioral counseling and proposed a quantitative approach to understand the dynamics of counseling interactions and counselor empathy during motivational interviewing. They first identified linguistic and acoustic empathy markers and used those along with raw features to train classifiers that were able to predict counselor empathy. Some of the other prominent researches are listed below: Zara The Supergirl Zara was an Empathetic Personality Recognition System created by Fung, Dey et al.15. Their research showed an interactive dialogue system that did sentiment analysis, emotion recognition, facial and speech recognition to extract user emotions in a human-robot conversational setup. They deployed their virtual robot as a webapp where users can interact with an animated cartoon character, Zara. You can watch their demo video on YouTube and also interact with their online webapp. Zara assesses a user’s personality by asking a series of questions, along with the follow-up inquiries, on different topics like the user’s childhood memory, last vacation, challenges at work, etc. It uses OpenSmile and Kaldi to perform emotion recognition from user audio, keyword matches from a pool of positive and negative emotion lexicons to perform sentiment analysis, and then uses these results to calculate the scores in four personality dimensions - extroversion, intuitive, judging, perceiving. Nora the Empathetic Psychologist Similar to Zara, researchers Winata, Kampman et al.16 crated a virtual psychologist, an empathetic dialogue system named Nora, that could mimic a conversation with a psychologist. Nora employed a natural language understanding (NLU) module to classify user intent and slots, a dialogue management module to evaluate NLU output and manage dialog turns, and a language generation module to respond to the user. A simple CNN was used to detect stress, personality, sentiment, and six different emotions from audio. Real-Time Speech Emotion and Sentiment Recognition for Interactive Dialogue Systems Bertero, Siddique et al.17 also used a CNN-based approach to recognize emotion and sentiment from raw audio data in real-time to enable an empathetic conversational dialogue system. They used the Kaldi speech recognition toolkit to train deep neural network hidden Markov models (DNN-HMMs) that used the raw audio together with encode-decode parallel audio and outperformed the SVM baseline. Their work avoided any feature engineering to enable real-time speech processing. Generating Emotionally Flexible Responses One of","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:4:1","tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/"},{"categories":["NLP"],"content":"Current State-of-the-Art Research on Empathetic Response Generation The sheer pace of progress in NLP makes it difficult to keep track of current state-of-the-art researches. And, often it is difficult to pin-down one research work as the current state-of-the-art, simply because there might be some other research that shows improvement in certain but not all of the shared metrics or different research work might have used completely different dataset for training and evaluation. However, the following two recent papers stand out for the quality of their results for empathetic response generation in dialogue systems on empathetic-dialogues dataset23. MoEL: Mixture of Empathetic Listeners The authors of this research (Lin et al., 2019)26 argue that prior researches made assumptions such as- understanding the emotional state of the user is enough for the model to implicitly learn how to respond appropriately without any additional inductive bias. However, this assumption may lead to generic response outputs from the single decoder that is learning to produce all emotions. Some of the other works assume that the emotion to condition the generation on is given as input, which may not always be true. There could be multiple emotions present in different turns. Hence a dialogue state embedding is also incorporated along with word embedding and standard positional embedding to produce context embedding. This idea was originally proposed by Wolf et al., 201927. Similar to the prior art, MoEL first encodes the dialogue context and uses it to recognize the emotional state (out of n possible states). But this architecture contains n decoders, called listeners, which are optimized to react to each context emotion. There is another listener, called meta-listener, that is trained along with other listeners and learns to softly combine the output states of all decoders according to the emotional classification distribution. This idea of having independent specialized experts (Listeners) was originally inspired by Shazeer et al. (2017)28. The 3-main components in this architecture are described below. Emotion Tracker: It is simply a standard Transformer encoder that encodes context and also computes a distribution over the possible user emotions. A query token QRY at the beginning of each input sequence, as in BERT, to compute the weighted sum of the output tensor. Emotion-aware Listeners: These are standard Transformer decoders that independently attend to the distribution produced by the emotional tracker and compute their own representation. There is also a shared listener that learns shared information for all emotions. The output from the shared listener is expected to be a general representation that can help the model to capture the dialogue context. But each empathetic listener learns how to respond to a particular emotion. Hence, different weights are assigned to each empathetic listener according to the user emotion distribution, while assigning a fixed weight of 1 to the shared listener. Meta Listener: Finally, the meta listener takes the weighted sum of representations from the listeners and generates the final response. The intuition is that each listener specializes in a certain emotion and the Meta Listener gathers the opinions generated by multiple listeners to produce the final response. In the experiments conducted on empathetic-dialogues dataset23, MoEL shows improvements over the baseline in Empathy and Relevance, while the baseline had a higher score on Fluency. MIME: MIMicking Emotions for Empathetic Response Generation Mimicry is one of the key components related to empathy. Research in Psychology shows that mimicry contributes substantially to an empathic response. Sonnby-Borgstrom, 200229 proposed that mimicry enables one to automatically share and understand another’s emotions. Their proposal also receives support from studies showing a (notably, weak) correlation between the strength of the mimicry response and trait","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:4:2","tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/"},{"categories":["NLP"],"content":"Conclusion In this article, we looked at what Empathy means from a philosophical, psychological, and neuroscience perspective. Then we did a deep dive into research on making Empathetic NLU systems. We went through several datasets and model architectures, including a peek into the current state-of-the-art systems that can generate empathetic responses in conversational dialogue systems. I hope you learned some new things from this post. And, I will love to hear your feedback in the comments below. ","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:5:0","tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/"},{"categories":["NLP"],"content":"References Empathy, Stanford Encyclopedia of Philosophy ↩︎ Einfühlung and Empathy: What do they mean?, Karal McLaren ↩︎ Personality and Empathy, Rosalind Daymond ↩︎ The Social Neuroscience of Empathy, Tania Singer and Claus Lamm ↩︎ ↩︎ Machiavellian intelligence: Social expertise and the evolution of intellect in monkeys, apes, and humans, Richard Byrne ↩︎ Empathy: A social psychological approach, Mark H. Davis ↩︎ Altruism and human nature: Resolving the evolutionary paradox, Ian Vine ↩︎ Empathy Reconsidered: New Directions in Psychotherapy, Arthur Bohart, Leslie Greenberg ↩︎ Empathy and quality of care, Stewart Mercer; William Reynolds ↩︎ Analyzing the Language of Therapist Empathy in Motivational Interview based Psychotherapy, Xiao et al ↩︎ Predicting Therapist Empathy in Motivational Interviews using Language Features Inspired by Psycholinguistic Norms, Gibson et al. ↩︎ Identifying Empathetic Messages in Online Health Communities, Khanour et al. ↩︎ Modeling Empathy and Distress in Reaction to News Stories, Buechel et al. ↩︎ Understanding and Predicting Empathic Behavior in Counseling Therapy, Perez-Rosas et al. ↩︎ Zara The Supergirl: An Empathetic Personality Recognition System, Fung, Dey et al. ↩︎ Nora the Empathetic Psychologist, Winata, Kampman et al. ↩︎ Real-Time Speech Emotion and Sentiment Recognition for Interactive Dialogue Systems, Bertero, Siddique et al. ↩︎ Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory, Zhou, Huang et al. ↩︎ Affect-lm: A neural language model for customizable affective text generation, Ghosh et al. ↩︎ MojiTalk: Generating Emotional Responses at Scale, Zhou and Wang, 2018 ↩︎ SentiGAN: Generating Sentimental Texts via Mixture Adversarial Networks, Wang and Wan, 2018 ↩︎ A Simple Dual-decoder Model for Generating Response with Sentiment, Xiuyu and Yunfang, 2019; [Source Code] ↩︎ ↩︎ I Know The Feeling: Learning To Converse With Empathy, Rashkin, Smith et al. 2019 ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ Eliciting Positive Emotion through Affect-Sensitive Dialogue Response Generation: A Neural Network Approach, Lubis et al. 2018 ↩︎ Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models, Serban et al. 2016 ↩︎ MoEL: Mixture of Empathetic Listeners, Lin et al., 2019; [Source Code] ↩︎ TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents, Wolf et al., 2019 ↩︎ Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer, Shazeer et al. (2017) ↩︎ Automatic mimicry reactions as related to differences in emotional empathy, Sonnby–Borgström, 2008 ↩︎ Neural mechanisms of empathy in humans: A relay from neural systems for imitation to limbic areas, Carr et al., 2003 ↩︎ MIME: MIMicking Emotions for Empathetic Response Generation, Majumder et al. 2020 [Source Code] ↩︎ ","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:6:0","tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/"},{"categories":["Software Engineering"],"content":"Identifying patterns among questions is quite an effective strategy when you are grinding LeetCode in preparation for your upcoming software engineering interviews. In this article, you will develop intuitions about Sliding Window pattern. You will also get a template approach to write code to solve these problems. I will also walk you through some LeetCode questions to show how to apply the template and at the end, there will be some LeetCode exercises for you to practice what you learn.","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/","tags":["leetcode","algorithms"],"title":"Effective LeetCode: Understanding the Sliding Window Pattern","uri":"/posts/2020/10/leetcode-sliding-window/"},{"categories":["Software Engineering"],"content":"Identifying patterns among questions is quite an effective strategy when you are grinding LeetCode in preparation for your upcoming software engineering interviews. In this article, you will develop intuitions about Sliding Window pattern. You will also get a template approach to write code to solve these problems. I will also walk you through some LeetCode questions to show how to apply the template and at the end, there will be some LeetCode exercises for you to practice what you learn. ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:0:0","tags":["leetcode","algorithms"],"title":"Effective LeetCode: Understanding the Sliding Window Pattern","uri":"/posts/2020/10/leetcode-sliding-window/"},{"categories":["Software Engineering"],"content":"Why Should One Focus on Programming Question Patterns? Whether you like them or not, solving programming challenges is a prevalent part of software engineering interviews. Most newbies and seasoned developers turn to leetcode.com to find a wide set of questions and discussion forums to help their interview preparations. However, many are baffled upon finding a more than a thousand questions on the website and not knowing where to start from. Many people in LeetCode and other discussion forums have shared their learning paths with other learners in the form of compiled list of questions that vary from difficulty level, question types, interviewing company and so on. A common occurring themes among those recommendations is to spend time on identifying patterns among questions. It helps in reinforcing your brain to think about the solution in more general terms so that you don’t have to cram your memory with specific details for individual questions. This way you will be better prepared to take on such programming challenges that may quiz you on a diverse range of Data Structure and Algorithm questions. Through a series of articles on these patterns, I will share the tips and tricks from what I have learned while solving LeetCode problems. And hopefully it will help you to prepare more effectively and faster for your software programming rounds of interviews. ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:1:0","tags":["leetcode","algorithms"],"title":"Effective LeetCode: Understanding the Sliding Window Pattern","uri":"/posts/2020/10/leetcode-sliding-window/"},{"categories":["Software Engineering"],"content":"Sliding Window Pattern Generally speaking, in sliding window problems, you would be given a sequence of values, and your task will be to return the best solution given a limiting constraint. The best solution would be defined in terms of a maximum or minimum value that occurs in a range or “window” over the given sequence. I know, it sounds a bit too generic. So, let’s try to materialize the idea with a fun example. ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:2:0","tags":["leetcode","algorithms"],"title":"Effective LeetCode: Understanding the Sliding Window Pattern","uri":"/posts/2020/10/leetcode-sliding-window/"},{"categories":["Software Engineering"],"content":"An Intuitive Analogy Let’s assume you are playing an Animal-Crossing-meets-SIMS game. And you are in-charge of many cute little characters currently living on different islands in the game doing some fun tasks. This game allows these characters to collaborate such that they can visit other islands and do tasks together. However, in order to connect any two islands, you will have to make some standard-length connecting bridges. And, some of the islands may be farther away from each other, so they may require more than one connecting bridge. For simplicity, we are going to assume that the islands are located linearly as shown in the figure. As you play new quests in the game, and finish daily tasks, you can unlock more of these connecting bridges. But at any given point of time, you will have a limited number of bridges in your collection. Now your task is to use the available bridge count and connect the islands such that you end up with maximum possible number of characters connected with each other. Say if you had 2 bridges available, would you choose move 1 or move 2 from below? With move 1, you connect the first two islands and the two characters on those islands together. However, with move 2, you connect the last three islands and four of your characters together. So the second move give you the best answer. As an exercise, can you think of any other moves in the above setting? What results would you get in those? While thinking of a solution to the above questions, you were given a constraint (limited number of available bridges) and you found an optimized solution (maximum number of characters connected). Because there were a limited number of islands and characters, you were able to simply eyeball and easily come up with a solution that was the best with the given constraint. But as you can imagine, the things can get a lot trickier if you have tens of thousands of islands with thousands of characters living on them and maybe hundreds of bridges available at your disposal. So, let’s summon the coder in you and formulate this problem as a programming challenge. ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:2:1","tags":["leetcode","algorithms"],"title":"Effective LeetCode: Understanding the Sliding Window Pattern","uri":"/posts/2020/10/leetcode-sliding-window/"},{"categories":["Software Engineering"],"content":"The Sliding Window Approach First, we are going to represent the arrangement of islands, bridges and characters in terms of a sequence of values, i.e. an array. So, let’s assume that an index in array with value -1 mean an empty location between islands where you could build a bridge, value 0 signifies the presence of a bridge at that location, and any other number is simply a count of the number of characters on the island at that index. So your goal is to find the maximum count for connected characters given a fixed number of bridges, B (for example, B=2). Before jumping into code, we need to first develop solid intuitions about Sliding Window approach and how it can help us in this problem. As you saw in the last section, we got our solution from a contiguous subsequence, i.e. a subarray. This subarray highlighted in red, is what we call a window. The left end, L, of the window is at index 3 and right end, R, is at index 7 (assuming index start at 0). This window spans over 5 array elements, so we can also say that the window length is 5. In Sliding Window pattern problems, we will calculate multiple solutions over varying length of the window, but we will only save the most optimal solution. A naïve approach could be to evaluate the input array for all possible length of windows, i.e. all possible placements for windows of length 1, 2, 3, 4, … , 8 (size of array), and calculate the result for each window, but save only the optimal value (maximum, in our example). However, as any seasoned LeetCode-er will tell you, your program will easily hit a Time Limit Exceeded wall even with a moderate sized array, because you have way too many potential solutions in your search space. So we need a better solution. Let’s solve the above problem that had a constraint of 2 bridges with a Sliding Window approach. In a sliding Window based solution, we will generally start from the left of the array and with a window of size 1, i.e. both the left and right ends, L and R respectively, of the window will be at index 0. At each step, we also calculate the current answer, i.e. the current number of connected characters inside the window, and save the most optimal solution, i.e. the maximum count so far. Now, let’s expand the window by moving the right end, R, as much as our constraint allows us (think of this as an outer loop in the code). The constraint in this example being the count of available bridges (B = 2). We used one bridge, but we do have one more left. So we keep expanding to right. We are out of bridges to use, but we can still move to the right, as there’s an island there and our constraint will still hold (max 2 bridges). We now have 2 connected characters, which is also our best answer so far. But we also have a problem, we can’t move to the right because that’s an empty slot and we are out of bridges to use. So now we will start to shrink our window from the left marker one step at a time, and keep doing it until we are allowed to move R to the right and still satisfy our constraint (think of it as another loop inside the outer one). At this point, we still can’t move R to the right, because we are out of bridges. So let’s move L to the right one more time and reclaim one bridge. Now we’re back on track. Moving R to the right requires a new bridge, but we do have that in inventory. Moving R to the right is still valid, so let’s do it. Moving R requires a bridge. So we go back to moving L to the right, and reclaim one bridge with the very first move. Moving R will make us use the bridge that we have in inventory. We can still move R more to the right. We can’t move R anymore. That brings us to the end of our algorithm. The best answer that we have is 4, which is also the most optimal answer. 😊 Notice the followings: We started with 2 markers left (L) and right (R) at index 0 R moved to the right towards the end of the array (we can use an outer loop for this) Inside the above looping process, if we hit a state where moving R wi","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:2:2","tags":["leetcode","algorithms"],"title":"Effective LeetCode: Understanding the Sliding Window Pattern","uri":"/posts/2020/10/leetcode-sliding-window/"},{"categories":["Software Engineering"],"content":"Solving LeetCode Problems with Sliding Window Pattern ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:3:0","tags":["leetcode","algorithms"],"title":"Effective LeetCode: Understanding the Sliding Window Pattern","uri":"/posts/2020/10/leetcode-sliding-window/"},{"categories":["Software Engineering"],"content":"LeetCode 1004: Max Consecutive Ones III Let’s try to apply what we you have learned above and solve this LeetCode problem: https://leetcode.com/problems/max-consecutive-ones-iii/ You are given an array input that has 0s and 1s. You are given a constraint that you can change only K number of values from 0 to 1. And, your task is to find the longest subarray that contains only 1s. So, we are looking for the maximum window size (i.e. R - L + 1), such that the window contains all 1s, and given the constraint that we can change up to K number of values in the window from 0 to 1. Let’s first setup some basic variables: current_change_count = 0 # This is our \"inventory\", i.e. a count for the number of changes from 0 to 1 L = 0 # This is the left marker of our Sliding Window answer = -1 # This is the variable that will store the best answer Based on the earlier template, we need an outer loop, that will move R to the right. for R in range(len(A)): # Here A is the input array ... Inside the for loop, we need to update our “inventory”, i.e. current_change_count, if required i.e. if the value of the current position is 0, we need to make it 1 to include it in the window if A[R] == 0: current_change_count += 1 And we need an inner loop to move L to the right, if required i.e. if the constraint “current_change_count \u003c= K” doesn’t hold. while current_change_count \u003e K and L \u003c len(A): if A[L] == 0: current_change_count -= 1 L += 1 Finally we save the current answer as the best answer, if required, i.e. if the current window length (R-L+1) is greater than answer. answer = max(answer, R-L+1) So, our complete solution will be: class Solution: def longestOnes(self, A: List[int], K: int) -\u003e int: if not A: return 0 current_change_count = 0 L = 0 answer = -1 for R in range(len(A)): if A[R] == 0: current_change_count += 1 while current_change_count \u003e K and L \u003c len(A): if A[L] == 0: current_change_count -= 1 L += 1 answer = max(answer, R-L+1) return answer ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:3:1","tags":["leetcode","algorithms"],"title":"Effective LeetCode: Understanding the Sliding Window Pattern","uri":"/posts/2020/10/leetcode-sliding-window/"},{"categories":["Software Engineering"],"content":"LeetCode 1358: Number of Substrings Containing All Three Characters Let’s take another example: https://leetcode.com/problems/number-of-substrings-containing-all-three-characters/ A valid window in this case is the one that satisfies the constraint that all of the three characters ‘a’, ‘b’ and ‘c’ are at least once present in the window. Let’s setup some basic variables: current_answer = 0 looking_for = [0, 0, 0] # We will save the count of 'a', 'b' and 'c' occurances in the window on the 3 indexes in this array/list L = 0 best_answer = 0 The outer loop will simply be: for R in S: ... Our inventory, in this case, keeps track of the count of ‘a’, ‘b’ and ‘c’ in “looking_for” list. Since we know each character in the input will be one of the three characters, we can directly update the index like this: looking_for[ord(R)-ord('a')] += 1 and the inner loop will move L to the right if the window has ‘a’, ‘b’ and ‘c’ occurring at least once. while L \u003c len(s) and looking_for[0] and looking_for[1] and looking_for[2]: looking_for[ord(s[L])-ord('a')] -= 1 # subtract count for the character at L L += 1 current_answer += 1 The optimal answer will simply add up the overall answers found in the Sliding Window. best_answer += current_answer So, our complete solution will be: class Solution: def numberOfSubstrings(self, s: str) -\u003e int: if len(s) \u003c 3: return 0 current_answer = 0 looking_for = [0, 0, 0] L = 0 best_answer = 0 for R in s: looking_for[ord(R)-ord('a')] += 1 while L \u003c len(s) and looking_for[0] and looking_for[1] and looking_for[2]: looking_for[ord(s[L])-ord('a')] -= 1 L += 1 current_answer += 1 best_answer += current_answer return best_answer ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:3:2","tags":["leetcode","algorithms"],"title":"Effective LeetCode: Understanding the Sliding Window Pattern","uri":"/posts/2020/10/leetcode-sliding-window/"},{"categories":["Software Engineering"],"content":"LeetCode 904: Fruit Into Baskets Let’s solve yet another LeetCode question with this approach: https://leetcode.com/problems/fruit-into-baskets/ In this problem, we maintain an inventory of two baskets with a each count containing a fruit type (kind of a “key”), and a fruit count (kind of a “value”), so we can simply make it a hashmap, i.e. a Python dictionary. The constraint here is that we can only have maximum 2 baskets, with each having a unique fruit type. Our goal is to find the maximum count of fruits in both baskets. First, we setup some basic variables: L = 0 inventory_hashmap = {} best_answer = 0 Then we setup an outer loop for moving R. for R in range(len(tree)): ... We update the inventory, i.e. our hashmap with keys representing the fruit type and the values representing the number of fruits picked during current window. inventory_hashmap[tree[R]] = inventory_hashmap.get(tree[R], 0) + 1 Next, we move L to the right, and remove the corresponding count and fruit type from the inventory, if the constraint is broken. while L \u003c len(tree) and len(inventory_hashmap) \u003e 2: inventory_hashmap[tree[L]]-= 1 if inventory_hashmap[tree[L]] == 0: del inventory_hashmap[tree[L]] L += 1 Finally, we update the best answer by finding the maximum value between the best answer so far, and the current number of fruits in our inventory. best_answer = max(best_answer, sum(inventory_hashmap.values())) Adding all the components together, our solution will be this: class Solution: def totalFruit(self, tree: List[int]) -\u003e int: if not tree: return 0 L = 0 inventory_hashmap = {} best_answer = 0 for R in range(len(tree)): inventory_hashmap[tree[R]] = inventory_hashmap.get(tree[R], 0) + 1 while L \u003c len(tree) and len(inventory_hashmap) \u003e 2: inventory_hashmap[tree[L]]-= 1 if inventory_hashmap[tree[L]] == 0: del inventory_hashmap[tree[L]] L += 1 best_answer = max(best_answer, sum(inventory_hashmap.values())) return best_answer ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:3:3","tags":["leetcode","algorithms"],"title":"Effective LeetCode: Understanding the Sliding Window Pattern","uri":"/posts/2020/10/leetcode-sliding-window/"},{"categories":["Software Engineering"],"content":"Summing Up I hope these examples were good enough to drive home the intuitions required to build a Sliding Window based solution. I would also like to point out that the template that I talked explained in this article is the most common strategy with Sliding Window problems, however it is not the only kind of Sliding Window approach. As you saw, in our template, the right marker R moves faster than the left marker L, therefore this approach is sometimes referred to as Fast/Slow Sliding Window approach. There are other approaches like Fast/Catch-Up and Front/Back that I will talk about in a future post. ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:4:0","tags":["leetcode","algorithms"],"title":"Effective LeetCode: Understanding the Sliding Window Pattern","uri":"/posts/2020/10/leetcode-sliding-window/"},{"categories":["Software Engineering"],"content":"Exercise for the Readers If you want to apply what you have learned in this article, and solve a few LeetCode challenges, then you can try your hand on the following Sliding Window problems on LeetCode website: https://leetcode.com/problems/longest-continuous-subarray-with-absolute-diff-less-than-or-equal-to-limit/ https://leetcode.com/problems/number-of-substrings-containing-all-three-characters/ https://leetcode.com/problems/replace-the-substring-for-balanced-string/ https://leetcode.com/problems/max-consecutive-ones-iii/ https://leetcode.com/problems/subarrays-with-k-different-integers/ https://leetcode.com/problems/fruit-into-baskets/ https://leetcode.com/problems/get-equal-substrings-within-budget/ https://leetcode.com/problems/longest-repeating-character-replacement/ https://leetcode.com/problems/shortest-subarray-with-sum-at-least-k/ https://leetcode.com/problems/minimum-size-subarray-sum/ https://leetcode.com/problems/sliding-window-maximum/ I also want to thank the LeetCode user wh0ami for compiling this list of questions and sharing this idea, in his C++ post at LeetCode discussion forums here. ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:5:0","tags":["leetcode","algorithms"],"title":"Effective LeetCode: Understanding the Sliding Window Pattern","uri":"/posts/2020/10/leetcode-sliding-window/"},{"categories":["Software Engineering"],"content":"In June 2020, Twitter announced a major overhaul in its storage and retrieval systems. These changes allowed Twitter to reduce the search index latency from 15 seconds to 1 second. So, what did they do to get such impressive gains? Allow me to explain!","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/","tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/"},{"categories":["Software Engineering"],"content":"In June 2020, Twitter announced a major overhaul in its storage and retrieval systems. These changes allowed Twitter to reduce the search index latency from 15 seconds to 1 second. So, what did they do to get such impressive gains? Allow me to explain! ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:0:0","tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/"},{"categories":["Software Engineering"],"content":"Twitter’s Search Architecture Designing a search system for Twitter is an incredibly complex task. Twitter has more the 145 million daily users 1 and over half a billion tweets are sent each day 2. Their search system gets hundreds of thousands of search queries per second. For any search system indexing latency is an important metric. The indexing latency can be defined as the amount of time it takes for new information (“tweets” in this case) to be available in the search index. And as you can imagine, with a large amount of data generated every day, we have an interesting engineering problem at hand. But why is search indexing latency important for Twitter?\rNote that not every service out there needs to have low update their search index quickly. For example, a Costco warehouse could update their search index once every couple of hours or so. For Twitter, however, real-time access to the content can be really important, for cases like following some breaking news, ongoing conversations, delivering timelines etc.\rBefore we take a look at what’s new, let’s first understand how the search workflow looked like at Twitter before these changes took place. In their 2012 paper titled: “Earlybird: Real-Time Search at Twitter” 3 4, Twitter released details about its search system project codenamed “Earlybird”. With Earlybird Twitter adopted their custom implementation of Apache Lucene which was a replacement for Twitter’s earlier MySQL-indexes based search algorithm. Some of the enhancements included image/video search support, searchable IndexWriter buffer, efficient relevance based search in time sorted index etc. This enabled Twitter to launch relevance-based product features like ranked home timeline. Although the search was still limited to last x days, but they later added the support for performing archive search on SSD with vanilla Lucene, as shown by the bottom row in the diagram below. ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:1:0","tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/"},{"categories":["Software Engineering"],"content":"Lucene Basics Knowing basics of Lucene helps to better understand Twitter’s implementation for their search system. Lucene is an open-source search engine library that is completely written in Java and is used by tech companies like LinkedIn, Twitter, Slack, Evernote etc. As you can imagine, we can’t afford to search record-by-record on large data specially for a time-sensitive application, we use a Lucene like solution that provides us Inverted Indexes. In information retrieval terminologies, a string is called a “term” (for example, English words), a sequence of terms is called a “field” (for example, sentences), a sequence of fields is called a document (for example, tweets), and a sequence of documents is called an index 5. Following is an example of inverted index creation, taken from “Inverted files for text search engines” research paper  6 document id document text 1 The old night keeper keeps the keep in the town 2 In the big old house in the big old gown 3 The house in the town had the big old keep 4 Where the old night keeper never did sleep 5 The night keeper keeps the keep in the night 6 And keeps in the dark and sleeps in the night Table: Input documents term‏‏‎ | freq inverted list for t and | 1 \u003c6\u003e big | 2 \u003c2\u003e, \u003c3\u003e dark | 1 \u003c6\u003e did | 1 \u003c4\u003e gown | 1 \u003c2\u003e had | 1 \u003c3\u003e house | 2 \u003c2\u003e, \u003c3\u003e in | 5 \u003c1\u003e, \u003c2\u003e, \u003c3\u003e, \u003c5\u003e, \u003c6\u003e keep | 3 \u003c1\u003e, \u003c3\u003e, \u003c5\u003e keeper | 3 \u003c1\u003e, \u003c4\u003e, \u003c5\u003e keeps | 3 \u003c1\u003e, \u003c5\u003e, \u003c6\u003e light | 1 \u003c6\u003e never | 1 \u003c4\u003e night |3 \u003c1\u003e, \u003c4\u003e, \u003c5\u003e old | 4 \u003c1\u003e, \u003c2\u003e, \u003c3\u003e, \u003c4\u003e sleep | 1 \u003c4\u003e sleeps | 1 \u003c6\u003e the | 6 \u003c1\u003e, \u003c2\u003e, \u003c3\u003e, \u003c4\u003e, \u003c5\u003e, \u003c6\u003e town | 2 \u003c1\u003e, \u003c3\u003e where | 1 \u003c4\u003e Table: inverted file You can use the above inverted index to answer questions like “Which documents have the word house in them?” Also, note that a term or a document could also be a single word, a conjunction, a phrase or a number. Each row of this index above, maps a term to a Posting List. In real implementations, the posting list may also include additional information, such as the position (index) of the term in the document or some other application-specific payload. ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:1:1","tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/"},{"categories":["Software Engineering"],"content":"Changes in Ingestion Pipeline Let’s get back to Twitter’s ingestion pipeline, which used to look like this: Click to zoom\rFlaw 1: Synchronous Workflow\rOne major problem with this design is that the Tweet data that we want to index isn’t available as soon as the Tweet is created. With ranked Home timeline feature, the timeline needs additional “relevance” detail about each Tweet. Among other factors, this relevance depends on fields that may not be immediately available. For example, a shortened URL (https://t.co/foo) needs to be expanded to provide more context for ranking, geo-coding resolution might take longer.\rFix: Twitter decided to stop waiting for the delayed fields to become available by adding an asynchronous part to their ingestion pipeline. click to zoom\rNow most of the data will be sent to indexing system as soon as the Tweet is posted. Another update will be sent once the additional information is available. Even though the indexing service doesn’t have full information at the beginning, at least for search system we have enough information to fetch results. ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:2:0","tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/"},{"categories":["Software Engineering"],"content":"Changes in Data Structures and Algorithms ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:3:0","tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/"},{"categories":["Software Engineering"],"content":"Eliminating Sorting Step Lucene’s vanilla implementation stored document ids using delta encoding, such that the key at an index depends on the previous index. Also, the document ids were generated starting from 0 up to the size of the segment storing those documents (a segment is just a “committed” i.e. immutable index). This essentially meant that the search could only be performed from left to right, i.e. from the oldest tweet to latest tweet. However, like many other products at Twitter, search is also designed to prioritize recent tweets over the older ones. So, twitter decided to customize this implementation in Earlybird. Earlybird diverged from standard Lucene approach and allocated document ids to incoming tweet from high value to low value, i.e. from the {size of the index - 1} to 0. This lets searching algorithm to traverse from latest tweet to oldest tweet and with the possibility of returning early if a client-specified number of hits have been recorded. Flaw 2: Decoupled Search System and Tweet Creation System\rBy design, Tweet creation system at Twitter is decoupled from the search system. Hence the indexing service can’t guarantee that the order in which it receives the tweets is also the order of their creation. Hence to fix this, Twitter had to include a buffer in their ingestion pipeline to temporarily store and then sort all of the incoming tweets to the indexing service. This unfortunately adds additional delays in search indexing.\rFIx: Twitter changed it’s ID assignment scheme such that each tweet is given a document ID based on its time of creation. They fit the document id to 31 bits (positive Java Integer), such that 27 bits store the timestamp with microseconds granularity. The rest 4 bits are used as a counter for the all of the tweets received at the same microsecond. Although the probability of such \\( 2^4\\) events is rare, but all tweets after 16, that are created at the same microsecond precision, would be assigned to the next segment, and would appear slightly out of order in search results, if selected. This means that Twitter can eliminate the need for explicit sorting step and reduce latency even further. ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:3:1","tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/"},{"categories":["Software Engineering"],"content":"Optimizing Operations on Posting Lists For their posting list implementation, Twitter had been using Unrolled Linked Lists. This implementation had only one writer thread and multiple searcher threads (one per CPU core, with tens of thousands of cores per server) without any locks. Writing would be done only at the head of the list, and the searcher would search starting from the new head pointer or the older head (still, valid) pointer to the end of the list. (notice in the figure above how document id assignment is done from high value to low value. ) Unrolled linked lists are cache friendly data structures that also reduce some of the overhead of storing pointer references. However, we can’t use them to cost-effectively insert at arbitrary location in the list, they also depend largely upon the assumption that the input will be strictly ordered. Fix: Twitter decided to opt for Skip Lists as the replacement for unrolled linked lists. Skip Lists allow for O(\\( \\log{n}\\)) search and insertions, and also support concurrency. I wrote an introductory article on Skip Lists here. I highly recommend you to read through the article to better understand the concept behind Skip Lists. Twitter’s Skip List implementation Skip Lists are probabilistic data structures, such that after inserting a new node at list \\( L_{i} \\), we insert the node at the layer \\( L_{i+1}\\) with some probability. Twitter’s found 20% ( \\( \\frac{1}{5}\\) ) is a good tradeoff between space and time complexity (memory used vs. speed). In their implementation, Twitter used several optimizations. The implemented the Skip List in a single flat array, such that the “next” pointer becomes just an index into the array. Here’s a breakdown of the other optimizations in their Skip List: Insertion would simply append the new value at the end of the array, and the corresponding pointer will be updated appropriately after traversing the Skip List. New values at the higher levels will be added with a 20% probability. At every search operation, the descent down the different levels is recorded and saved as a “search finger”. This helps in lookups when we are finding a document with id greater than the one for which we already have the search finger. It reduces the lookup time complexity from O(\\( \\log{n} \\)) to O(\\( \\log{d} \\)), where n is the number of items in the posting list and d is the number of items in the list between the first value and the second value, in case of conjunction search queries. Using primitive array also has an advantage of reducing pointer management overheads, such as garbage collection. Allocating vertical levels of Skip List in adjacent location, eliminates the need for storing “above” and “below” pointers. One obvious disadvantage, however, is that the Skip Lists are not cache friendly. However, being able to run in logarithmic time in case of sparse documents is a big advantage. ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:3:2","tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/"},{"categories":["Software Engineering"],"content":"Additional Details Twitter’s new Skip List implementation also uses a “published pointer” that points to the current maximum pointer into array such that searchers never traverse beyond this pointer. This allows for ensuring atomicity in search and retrievals, and a document is searched only if all of its terms are indexed. Considering this was a change of large magnitude, Twitter rolled it out gradually by first evaluating the results in Dark Launch mode. This was done to ensure that the clients do not have any reliance or assumptions on the earlier 15 seconds delay in search indexing. ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:4:0","tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/"},{"categories":["Software Engineering"],"content":"Conclusion In order to reduce search indexing latency, Twitter made big changes to its storage system and retrieval system. They shared their approach and learnings in a whitepaper 7 that I summarized here. The search indexing improvement from 15 seconds to 1 second means that the new content should be available for Twitter users to access almost instantaneously. Twitter Q3-2019 - Selected Financial and Metrics ↩︎ Twitter Engagement Report 2018 ↩︎ http://users.umiacs.umd.edu/~jimmylin/publications/Busch_etal_ICDE2012.pdf ↩︎ https://www.youtube.com/watch?v=KUmFJc3fFuM ↩︎ https://lucene.apache.org/core/8_6_0/core/org/apache/lucene/codecs/lucene86/package-summary.html#package.description ↩︎ http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.105.8844 ↩︎ Twitter Infrastructure: Reducing search indexing latency ↩︎ ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:5:0","tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/"},{"categories":["NLP"],"content":"Word vector representations with subword information are great for NLP modeling. But can we make lexical corrections using a trained embeddings space? Can its accuracy be high enough to beat Peter Norvig's spell-corrector? Let's find out!","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/"},{"categories":["NLP"],"content":"Word vector representations with subword information are great for NLP modeling. But can we make lexical corrections using a trained embeddings space? Can its accuracy be high enough to beat Peter Norvig’s spell-corrector? Let’s find out! ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:0:0","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/"},{"categories":["NLP"],"content":"Introduction Let’s first define some terminology. ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:1:0","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/"},{"categories":["NLP"],"content":"What are word embeddings? Word Embedding techniques have been an important factor behind recent advancements and successes in the field of Natural Language Processing. Word Embeddings provide a way for Machine Learning modelers to represent textual information as the input to ML algorithms. Simply put, they are a hashmap where the key is a language word, and the corresponding value is a vector of real numbers fed to the models in place of that word. There are different kinds of word embeddings available out there that vary in the way they learn and transform a word to a vector. The word vector representations can be as simple as a hot-encoded vector, or they can be more complex (and more successful) representations that are trained on large corpus, take context into account, break the words into subword representations etc ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:1:1","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/"},{"categories":["NLP"],"content":"What are subword embeddings? Suppose you have a deep learning NLP model, say a chatbot, running in production. Your customers are directly interacting with the ML model. The model is being fed the vector values, such that each word from the customer is queried against a hashmap and the value corresponding to the word is the input vector. All of the keys in your hashmap represents the vocabulary, i.e. all the words you know. Now, how would you handle a case where the customer uses a word that is not already present in your vocabulary? There are many ways to solve this out-of-vocabulary (OOV) problem. One popular approach is to split the words into “subword” units, and use those subwords to learn your hashmap during model training stage. At the time of inference, you would again divide each incoming word into smaller subword units, find the word vector corresponding to each subword unit using the hashmap, then aggregate each subword vector to get the vector representation for the complete word. For example, let’s say we have the word tiktok, the corresponding subwords could be tik, ikt, kto, tok, tikt, ikto, ktok etc. This is the character n-gram division for the input word, where n is the subword sequence length, and is fixed by the modeler. Figure: example subword representations ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:1:2","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/"},{"categories":["NLP"],"content":"What is FastText? FastText is an open-source project from Facebook Research. It is a library for fast text-representations and classifications. It is written in C++ and supports multiprocessing. It can be used to train unsupervised word vectors and supervised classification tasks. For learning word-vectors it supports both skipgram and continuous bag-of-words approaches. FastText supports subword representations such that each word can be represented as a bag of character n-grams in addition to the word itself. Incorporating finer (subword level) information is pretty good for handling rare words. You can read more about the FastText approach in their paper here . For an example, let’s say you have a word “superman” in FastText trained word embeddings (“hashmap”). Let’s assume the hyperparameters minimum and maximum length of ngram was set to 4. Corresponding to this word, the hashmap would have the following keys: Original word: superman n-gram size subword 4 \u003csup 4 supe 4 uper 4 perm 4 erma 4 rman 4 man\u003e where “\u003c” and “\u003e” characters mark the start and end of a word respectively ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:1:3","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/"},{"categories":["NLP"],"content":"The Task: Spell-Checking In NLP, the learnt word embedding vectors not only have lexical representations for words, but the vector values also have semantically related positioning in the embedding space. We are going to try and build a spell-checker application based on FastText word vectors such that given a misspelled word, our task will be to find the word vector representation closest to the vector representation of that word in trained embedding space. We will work based on this simple heuristic: heuristic\rIF word exists in the Vocabulary ​ Do not change the word ELSE ​ Replace the word with the one closest to its sub-word representation ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:2:0","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/"},{"categories":["NLP"],"content":"Dataset \u0026 Benchmark For fun, let’s build and evaluate our spell-checker on the same training and testing data as this classic article: “How to write a Spelling Corrector” by Peter Norvig , Director of Research at Google. In this article, Norvig build a simple spelling corrector based on basic probability theory. Let’s see how does this FastText based approach hold up against it. ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:2:1","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/"},{"categories":["NLP"],"content":"Installing FastText Installation for FastText is straightforward. FastText can be used as a command line tool or via Python client. Click here to access the latest installation instructions for both approaches. ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:2:2","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/"},{"categories":["NLP"],"content":"Method 1: Using Pre-trained Word Vectors FastText provides pretrained word vectors based on common-crawl and wikipedia datasets. The details and download instructions for the embeddings can be found here. For a quick experiment, let’s load the largest pretrained model available from FastText and use that to perform spelling-correction. Download and unzip the trained vectors and binary model file. -\u003e wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip -\u003e unzip crawl-300d-2M-subword.zip There are two files inside the zip: crawl-300d-2M-subword.vec : This file contains the number of words (2M) and the size of each word (vector dimensions; 300) in the first line. All of the following lines start with the word (or the subword) followed by the 300 real number values representing the learnt word vector. crawl-300d-2M-subword.bin: This binary file is the exported model trained on the Common-Crawl dataset. As mentioned in his article, Norvig used spell-testset1.txt and spell-testset2.txt as development and test set respectively, to evaluate the performance of his spelling-corrector. I’m going to load the pretrained FastText model and make predictions based on the heurisitic defined above. I’m also going to borrow some of the evaluation code from Norvig. import io import fasttext def load_vectors(fname): fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore') n, d = map(int, fin.readline().split()) data = {} for line in fin: tokens = line.rstrip().split(' ') data[tokens[0]] = map(float, tokens[1:]) return data def spelltest(tests, model, vocab): \"Run correction(wrong) on all (right, wrong) pairs; report results.\" import time start = time.clock() good, unknown = 0, 0 n = len(tests) for right, wrong in tests: w = wrong if w in vocab: print('word: {} exists in the vocabulary. No correction required'.format(w)) else: w_old = w w = model.get_nearest_neighbors(w, k=1)[0][1] print(\"found replacement: {} for word: {}\".format(w, w_old)) good += (w == right) dt = time.clock() - start print('{:.0%} of {} correct at {:.0f} words per second ' .format(good / n, n, n / dt)) def Testset(lines): \"Parse 'right: wrong1 wrong2' lines into [('right', 'wrong1'), ('right', 'wrong2')] pairs.\" return [(right, wrong) for (right, wrongs) in (line.split(':') for line in lines) for wrong in wrongs.split()] if __name__ == \"__main__\": model = fasttext.load_model(\"crawl-300d-2M-subword.bin\") vocab = load_vectors(\"crawl-300d-2M-subword.vec\") spelltest(Testset(open('spell-testset1.txt')), model, vocab) spelltest(Testset(open('spell-testset2.txt')), model, vocab) For the sake of brevity, I’m reducing the output log to just the metrics trace. output\r… 0% of 270 correct at 3 words per second. … 0% of 400 correct at 4 words per second.\rThat didn’t go well! 😅 None of the corrections from the model were right. Let’s dig into the results. Here are a few snapshots from the output log: output\r… word: sysem exists in the vocabulary. No correction required word: controled exists in the vocabulary. No correction required word: reffered exists in the vocabulary. No correction required word: irrelavent exists in the vocabulary. No correction required word: financialy exists in the vocabulary. No correction required word: whould exists in the vocabulary. No correction required … found replacement: reequipped for word: reequired found replacement: putput for word: oputput found replacement: catecholate for word: colate found replacement: yeahNow for word: yesars found replacement: detale for word: segemnt found replacement: \u003cli\u003e\u003cstrong\u003eStyle for word: earlyest …\rLooks like there are a lot of garbage subwords in the pretrained vocabulary that directly matches our misspelled input. Also, the performed lexical corrections show that the model replaced misspelled input words with the closest semantic neighbor. However none of those neighbors are meaningful English words. We can verify this by checking out the neighbors for random misspel","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:2:3","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/"},{"categories":["NLP"],"content":"Method 2: Trained Our Own Word Vectors For a fair comparison with Norvig’s spell-checker, let’s use the same training data that he used (big.txt). From Norvig's article:\r… by counting the number of times each word appears in a text file of about a million words, big.txt. It is a concatenation of public domain book excerpts from Project Gutenberg and lists of most frequent words from Wiktionary and the British National Corpus.\r-\u003e wc big.txt 128457 1095695 6488666 big.txt The training data has around 128K lines, 1M words, 6.5M characters. As discussed above, we should try with keeping the wordNgrams hyperparameter to 1, and use the trained FastText model to perform spell-checking. import io import fasttext def load_vectors(fname): fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore') n, d = map(int, fin.readline().split()) data = {} for line in fin: tokens = line.rstrip().split(' ') data[tokens[0]] = map(float, tokens[1:]) return data def spelltest(tests, model): \"Run correction(wrong) on all (right, wrong) pairs; report results.\" import time start = time.clock() good, unknown = 0, 0 n = len(tests) for right, wrong in tests: w_old = wrong w = wrong if w in model.words: pass else: w = model.get_nearest_neighbors(w, k=1)[0][1] good += (w == right) if not (w == right): if w_old != w: print(\"Edited {} to {}, but the correct word is: {}\".format(w_old, w, right)) dt = time.clock() - start print('{:.0%} of {} correct at {:.0f} words per second ' .format(good / n, n, n / dt)) def Testset(lines): \"Parse 'right: wrong1 wrong2' lines into [('right', 'wrong1'), ('right', 'wrong2')] pairs.\" return [(right, wrong) for (right, wrongs) in (line.split(':') for line in lines) for wrong in wrongs.split()] if __name__ == \"__main__\": model = fasttext.train_unsupervised('big.txt', wordNgrams=1, minn=1, maxn=2, dim=300, ws=8, neg=8, epoch=4, minCount=1, bucket=900000) spelltest(Testset(open('spell-testset1.txt')), model) spelltest(Testset(open('spell-testset2.txt')), model) There are a couple of changes in this code. We are training the model with specific hyper-parameters, and the output traces will inform us what went wrong in our decisions. Note that FastText doesn’t provide an option to set seed in the above implementation, so the results may vary by 1%-2% on every execution. -\u003e python fasttext_trained.py Read 1M words Number of words: 81398 Number of labels: 0 Progress: 100.0% words/sec/thread: 20348 lr: 0.000000 avg. loss 1.943424 ETA: 0h 0m 0s ... 73% of 270 correct at 46 words per second. ... 69% of 400 correct at 48 words per second. This is a big improvement! 😄 We have almost the same accuracy as Norvig’s spell-corrector. Here are Norvig’s results for comparison. 75% of 270 correct at 41 words per second 68% of 400 correct at 35 words per second Looking at some of the edits in the output traces, I can see that the model output isn’t essentially incorrect, but the model is biased to certain edit-based operations. output\r… Edited reffered to referred, but the correct word is: refered Edited applogised to apologized, but the correct word is: apologised Edited speeking to seeking, but the correct word is: speaking Edited guidlines to guideline, but the correct word is: guidelines Edited throut to throughout, but the correct word is: through Edited nite to unite, but the correct word is: night Edited oranised to organised, but the correct word is: organized Edited thay to thy, but the correct word is: they Edited stoped to stooped, but the correct word is: stopped Edited upplied to supplied, but the correct word is: applied Edited goegraphicaly to geographical, but the correct word is: geographically …\rAs you can see our trained model is nicely producing dictionary-based words as output. It’s likely that with contextual training approaches and evaluations, along with more training data, we can come up with an even better approaches that would understand context in a full sentence and produce the correct word as the spell-chec","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:2:4","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/"},{"categories":["NLP"],"content":"Conclusion Our model was able to match Peter Norvig’s spell-corrector. 😊 Looking at the failing cases, we realize that the model could potentially do even better with more training data and a contextual training and evaluation strategy. ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:3:0","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/"},{"categories":["Software Engineering"],"content":"Analyzing randomness is hard. So why would you want to choose a probabilistic data structure in your system implementation? Allow me to explain!","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Analyzing randomness is hard. So why would you want to choose a probabilistic data structure in your system implementation? Allow me to explain! ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:0:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Why am I writing about Skip Lists? Skip List is one of those data structures that I have seen briefly mentioned in academic books; but never seen it being used in academic or industrial projects (or in LeetCode questions 🙂 ). However I was pleasantly surprised to see Skip List being one of the major contributors behind Twitter’s recent success in reducing their search indexing latency by more than 90% (absolute). This encouraged me to brush up my understanding of Skip List and also learn from Twitter’s solution. ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:1:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Why do we need Skip Lists? I believe it’s easier to understand the concept of Skip Lists by seeing their application. So let’s first build a case for Skip List. ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:2:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"The Problem: Search Let’s say you have an array of sorted values as shown below, and your task is to find whether a given value exists in the array or not. Given an array: find whether value 57 exists in this array or not ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:2:1","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Option 1: Linearly Searching an Array As the name suggests, in linear search you can simply traverse the list from one end to the other and compare each item you see with the target value you’re looking for. You can stop the linear traversal if you find a matching value. You can stop the search if at any point of time you see an item whose value is bigger than the one you’re looking for. Is this a good solution? [Click here]\rThis solution is alright. But as you would notice, it doesn’t take advantage of the fact that the given sequence of values are in sorted order. The worst case time complexity for this algorithm would be order of the number of values in the sequence, i.e. O(n). We will be comparing our target value with each member of sequence if the element is at the very end of the sequence or does not exist in the sequence.\rCan we do better?\rYes, with Binary Search!\r","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:3:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Option 2: Binary Search on an Array The binary search concept is simple. Think of how you search for a given word in a dictionary. Say if you are trying to find the word “quirky” in the dictionary, you do not need to linearly read each word in the dictionary one-by-one to know if the word exists in the dictionary. Because you have established that the dictionary has the all of the words in alphabetical sorted order. You can simply flip through the book, notice which word you are at, then either go left or right depending on whether the word starts with a character that comes before or after ‘q’. Going back to our example, let’s first check the value at the center of the sequence, if the value matches our target. we can successfully end the search. But if it is not the value we are looking for, we will compare the value of the middle position with our target value. If our target value is smaller, we need to look into smaller numbers, so we go towards left and ignore the values on the right. By following this binary search approach have reduced our search space and the required number of comparisons by half. Also, we repeat the same process on the “valid” half, i.e. we directly go the middle value of the left sub-array and see if we have a match, if not, we again decide whether to go left or right, and hence reduce our candidate search space by half again. What is the time complexity now?\rThe time complexity of this algorithm is again defined by the number of comparisons. If you recognize the pattern in this algorithm, you would know that the algorithm belongs to the category for which the size of a variable is continuously divided by a constant (2 in this case). And hence our algorithmic time complexity is O(\\(log_2n\\)). [Read this for an intuitive explanation]\rThis is excellent. Logarithmic time complexity means that our algorithm will scale really well with large number of values being stored in the sequential array. Is this a perfect solution?\rOne big issue, here, is that while sequential storage (for example, arrays) makes in easy and cheap to design binary search algorithms, it makes it costlier for us to implement an insertion and deletion strategy. For example, if you insert an item at a specific index in the array, you will have to shift the values on its right by one place to the right. Similarly if you delete a value at an arbitrary index inside the array, you will have to left shift all the values to the right of that index by one place. (see examples below)\rAs you can imagine, having extra space on the right of the array would be convenient else you would have to call for dynamic allocation of memory at every insertion and copy over the whole array to newly allocated array memory. List data structure in Python, allocates list in a similar way that leaves capacity at the end of the sequence for future expansion. If we run out of space in the list, Python allocates a new and much larger (double the size, for example) sized list, copies over each value from the old list to new list, and then deletes the old list. (This is why the insertion in a Python list is Amortized O(1) complexity) Could there be a way to have a data structure with better insertion and deletion costs?\rYes, we can use Linked Lists!\r","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:4:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Option 3: Linear search with Linked Lists Linked Lists are dynamic data structures that store values in non-contiguous memory blocks (called nodes). Each block refers to the other using memory pointers. These pointers are sort of the links in a chain that define the ordering among the non-contiguous nodes. A Singly Linked list represents a sequential structure where each node maintains a pointer to the element that follows it in the sequence. In linked lists we also maintain additional reference to the list nodes. Head is one such reference which is variable that stores the address for the first element in the list (stores null pointer if the list is empty). Insertion and deletion at any arbitrary location is straightforward, as you would simply reconnect the links of the nodes previous and next positions from the target position. We can also have a Doubly Linked List where each node stores the pointer to the previous as well as the next node in the sequence. A tail reference may also be used to store pointer to the last node in the linked list. As you would notice, random access in linked lists are not possible. And to find a value in the list, we would simply have to traverse the list from one end, say from the node referred by Head, towards the other end till we find what we are looking for. So, even though you can insert and delete in O(1), you still need the reference to the target location. Finding the target location is again O(n). With Linked Lists, while we solved the issue of costly insertions and deletions, we haven’t quite found a good solution to quickly access a target value in the list (or find if the value exists). Wouldn’t it be great if we could have the random access (or something close to it) in the list to simply our find, insert and delete operations? Well this is where Skip List come handy! ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:5:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Option 4: Pseudo-binary search with Linked Lists [Skip List] Technical advances have made low latency highly desirable for software applications. How well the underlying algorithm scales to demand can make or break a product. On the other hand, the memory/storage costs is getting cheaper by day. This propels many system designers to make decisions that balance compromises that spend extra money on storage costs, while opting for algorithms with low time complexities. Skip Lists is a good example of one such compromise. Skip Lists were proposed by WIlliam Pugh in 1989 (read the original paper  here) as a probabilistic alternative to balanced trees. Skip lists are relatively easy to implement, specially when compared to balanced trees like AVL Trees or Red Black Trees. If you have used the Balanced Trees before, you may remember how they are great data structures to maintain relative order of elements in a sequence, while making search operations easy. However, insertions and deletions in Balanced Trees are quite complex as we have the additional task to strictly ensure the tree is balanced at all times. Skip Lists introduce randomness in by making balancing of the data structure to be probabilistic. While it definitely makes evaluating complexity of the algorithm to be much harder, the insertion, deletion operations become much faster and simpler to implement. The complexities are calculated based on expectation, but they have a high probability, such that we can say with high confidence (but not absolute guarantee) that the run time complexity would be O(log(n)) apart from some specific worst-case scenarios as we would see soon. ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:6:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Skip Lists - Intuition Following is the map of New York Subway’s “7” train. Each point on the horizontal line represents on train stop on a specific street. The line at the top is the local train that has much more number of stops compared to the express line on the below it. Government decides where to put the stops based on factors like nearby facilities, neighborhood, business centers etc. Let’s simplify the above route map and display stations by the street numbers. In the simplified map above, the express line is shown at the top and local line is below it. Now if you had this map and you were supposed to travel from stop 14 to stop 59. How would you plan your trip route? For simplicity let’s assume we can’t overshoot, meaning that we can’t go some station to the right and then travel back towards left. You would probably want to take the express line from stop 14 and get down on stop 42. Then take the local train from stop 42 and reach the destination 59 from there, as shown below. As you would notice, by taking the express route you would be able to skip some stations along the way and maybe even reach the destination much faster compared to taking the local train from a original stop and traveling each station one by one to reach a destination stop. This is exactly the core idea behind Skip List. With Skip Lists, we introduce redundant travel paths at the cost of additional space, but the additional travel paths have lesser and lesser number of “stops” such that the linear traversal is much faster if we travel in those additional lanes. ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:6:1","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Skip Lists - Basics Let’s now build the concept of Skips Lists from bottom to top. Assume that we have a Doubly Linked List which is unsorted. What do you think would be the search complexity for this list? Search would be done in O(n), because you would have to traverse the list linearly to find your target node. Let’s assume that the list is now sorted as shown below. What do you think is the search complexity now? Well, it’s still O(n). Even if it sorted, the linked list structure doesn’t provide us random access, hence we can’t simply calculate the next middle index and access that middle node in O(1). Now, let’s put our “express line” on top of the existing structure. We are going to create a second Doubly Linked List and place the second list above of the first list such that each node has four reference to neighboring nodes: left, right, above, below. which can be visualized as: Now given this Skip Lists structure, you can find the number 73 with lesser number of comparison operations. You can start traversing from the top list first, and at every node see if the next node overshoots the value that you are looking for. If it doesn’t simply go the next node in the list on the top, else go to the list on the bottom and continue your linear search in the bottom list. We can summarize the searching approach in a simple algorithm. Algorithm\rWalk right in the Linked List on top (L1) until going right is going too far Walk down to the bottom list (L0) Walk right in L0 until element is found or we overshoot. So what is the kind of search cost we are looking at? We would want to travel the length of L1 as much as possible because that gives us skips over unnecessary comparisons. And eventually we may go down to L0 and traverse a portion of it. The idea is to uniformly disperse the nodes in the top lane. Search Cost = \\(|L1| + \\frac{|L0|}{|L1|}\\) How many “stops” (nodes) do you need in the top lane? It can be mathematically shown1 that if the cardinality of the bottom list is n (i.e. it has n nodes), we will get an optimal solution if we have \\(\\sqrt{n}\\) nodes in the top layer. So, Search Cost = \\(\\sqrt{n} + \\frac{n}{\\sqrt{n}}\\) = 2\\(\\sqrt{n}\\) = O(\\(\\sqrt{n})\\)) However, our Skip List don’t have to limited to 2 lists. We can have more lists (“lanes”), meaning our search costs would scale as: ‎‏‏‎2 sorted lists → \\(2 * \\sqrt[2]{n}\\) 3 sorted lists → \\(3 * \\sqrt[3]{n}\\) ‎‏‏‎… k sorted lists → \\(k * \\sqrt[k]{n}\\) (if we have log(n) number of sorted lists:)‎‎‎‎‎‎‎‎‎‎‎‏‏‎ ‎‎‎‎‎‎‎‎‎‎‎log(n) sorted lists → \\(log(n) * \\sqrt[log(n)]{n}\\) = 2 \\(\\log{n}\\) ‎‎‎‎‎‏‎‎‏‏ So, if the nodes are optimally dispersed, theoretically the structure gives us our favorite logarithmic complexity 🙂 (Although in implementation when we use randomization to insert the nodes, things get a little tricky). Now think of the structure we have created. At the bottom we have n nodes, the layer on top has interspersed \\(\\frac{n}{2}\\) nodes and the layer above it has \\(\\frac{n}{4}\\) nodes. Do you how this structure looks quite similar to a tree? We just need to figure out how “balancing” works in this tree (hint: it’s probabilistic). ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:6:2","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Skip List - Definition Skip List S consists of a series of lists {\\(S_0,S_1 .. S_h\\)} such that each list \\(S_i\\) stores a subset of the items sorted by increasing keys. Also note:\rh represents the height of the Skip List S each list also has two sentinel nodes -\\(\\infty\\) and +\\(\\infty\\) . -\\(\\infty\\) is smaller than any possible key in the list and +\\(\\infty\\) is greater than any possible key in the list. Hence the node with -\\(\\infty\\) key is always on the leftmost position and node with +\\(\\infty\\) key is always the rightmost node in the list For visualization, it is customary to have the \\(S_0\\) list on the bottom and lists \\(S_1, S_2, .. S_h\\) above it Each node in a list has to be “sitting” on another node with same key below it. Meaning that if \\(L_{i+1}\\) has a node with key k, then \\(L_i, L_{i-1} ..\\) all valid lists below it will have the same key in them We may chose to opt for an implementation of Skip List that only uses Single Linked Lists. However it may only improve the asymptotic complexity by a constant factor Skip List also needs to maintain the “head” pointer, i.e. the reference to the first member (sentinel node -\\(\\infty\\)) in the topmost list On average the Skip List will have O(n) space complexity Let’s first simplify the visualization for our nodes and connection in Skip List. A standard, simplified Skip List would look like this: Now, let’s see how some of the basic operations are performed on a Skip List. ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:6:3","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Searching in Skip List Searching in a Skip List in straightforward. We have already seen an example above with two lists. We can extend the same approach to a Skip List with h lists. We will simply start with the leftmost node in the list on top. Go down if the next value in the same list is greater than the one we are looking for, go right otherwise. The main idea is again to skip comparisons with as many keys as possible, while compromising a little on the extra storage required in the additional lists containing subset of all keys. On average, “expected” Search complexity for Skip List is O(log(n)) ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:6:4","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Inserting in Skip List To insert a new node, we would first find the location where this new key should be inserted in the list at the very bottom. This can be simply done be re-using the search logic, we start traversing to the right from the list on top and we go one step down if the next item is bigger than the key that we want to insert, else go right. Once we reach at a position in bottom list where we can’t go more to the right, we insert the new value on the right side of that position. Skip List after inserting 67: Now comes the probabilistic part, after inserting the new node in the list \\(L_i\\) we have to also insert the node in list above it, i.e. list \\(L_{i+1}\\) with probability \\(\\frac{1}{2}\\). We do this with a simple coin toss, such that if we get a head we insert the node in the list \\(L_{i+1}\\) and toss the coin again, we stop when we get a tail. We may repeat the coin toss till we keep getting heads, even if we have to insert a new layer (list) at the top. Note that we also need to “re-hook” the links for the newly inserted node. The new node needs to have it’s ‘below’ reference pointing to the node below, and the node below would have ‘above’ reference pointer to the new node. To find the ’left’ neighbor for the new node, we simply traverse towards left from the node below it and return ‘above’ pointer from a node for which ‘above’ pointer is not null. The ‘right’ reference of the ’left’ neighbor is used to update the ‘right’ reference of the new node. Also, the ’left’ reference for the new node’s right neighbor is also updated to the new node. This re-hooking operation is actually pretty easy to implement with Linked Lists. Let’s toss the coin again. Last toss gave us another Head, so let’s toss the coin again. We got a Tail this time, so no more insertions are required. As you see the probability of a node also getting inserted in the layer above it get reduced by half after every layer. Still, there is a worst case possibility that you would keep getting Heads indefinitely, although the probability of that happening is extremely small. To avoid such cases when you may get a large number of heads sequentially, you could also use a termination condition where you stop inserting if you reach a predefined threshold for the number of layers (height) or a predefined threshold for the number of nodes in a specific layer. Although the expected time complexity would still be O(log(n)) for all operations. ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:6:5","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Deletion in Skip List Deletion operation in Skip List is pretty straightforward. We first perform search operation to find the location of the node. If we find the node, we simply delete it at all levels. Before deleting a node, we simply ensure that no other node is pointing to it. Release all references to this node. And move to the node below it. Remove all references to this node and release it. Move to the node below in the bottom list. Release this node as well. This is how our Skip List looks like after deletion. ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:6:6","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Conclusion In this article we saw how Skip List is a probabilistic data structure that loosely resembles a balanced binary tree. It is also easy to implement and very fast as the search, insert, delete operations have an expected time complexity of O(log(n)), along with O(n) expected space complexity. in next article, I will introduce a practical application of Skip List and explain how Twitter used Skip List to get a huge improvement in their search indexing latency. ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:7:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Reference and Further Reading MIT OCW 6.046 Design and Analysis of Algorithms Lecture 7: Randomization: Skip Lists ↩︎ ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:8:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":null,"content":" Sumit Kumar - Senior Machine Learning Engineer at TikTok US; UChicago Alum. Previous: Research Scientist 2 at Alexa AI, Amazon, SDE2@Amazon, Lead Engineer@Samsung R\u0026D, SSE@Altran ‎‎‎‎‎‏‏‎ ‎ ","date":"2020-07-14","objectID":"/about/:0:0","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"Brief My name is Sumit Kumar. I love learning new concepts and building things. I have more than 10 years of work experience in Software Development, Machine Learning/Deep Learning research. Currently I work as a Senior Machine Learning Engineer at TikTok USA. And I work with research and development of Recommendation Systems built on Deep Learning concepts. In the past I have worked as a Research Scientist II in Natural Language Processing (NLP) for Alexa AI, Amazon in Seattle, USA, and Lead Engineer for Samsung Research Labs, India. ","date":"2020-07-14","objectID":"/about/:0:1","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"Long Form 😄 [This introduction is from my uChicago faculty profile from Dec 2018] Sumit joined the University of Chicago’s Graham School as a full-time, international student in the autumn 2017 cohort. He is currently a software development engineer 2 at Amazon’s headquarters in Seattle. He has seven years of experience in software engineering and research and has worked extensively with major programming languages including C, C++, JavaScript, Java, MATLAB, R, and Python. After earning his bachelor’s degree in computer science, he started his career at Aricent Group as a software developer where he worked on different telecom projects, writing software for simulating radio network controllers that were used by Nokia-Siemens Networks. In 2012, Sumit joined Samsung R\u0026D in Noida, India and developed the L3 software protocol stack used today in Samsung’s smartphone modems. He has worked on various telecom projects in Canada, Australia, South Korea, and India, where he developed features and algorithms compliant with 3GPP standard for WCDMA/LTE wireless communication. During this period, he also collaborated with various telecom vendors across the world and was instrumental in success of many critical projects. In 2015, Sumit was promoted to lead engineer and started working in the advanced R\u0026D division at Samsung. He mentored engineers in addition to his independent research work. Sumit developed various novel solutions and algorithms during his research at Samsung and is the inventor and co-author for seven algorithms filed for patents by Samsung. Apart from his research work, he also created tools to automate many manual processes at Samsung. He did an extensive amount of machine-learning based research in audio DSP domain to solve problems such as Blind Source Separation (Cocktail Party Problem) and Audio Directionality and Speaker Diarization. During this time, he also volunteered as a teacher in Samsung’s Corporate Social Responsibility (CSR) mission to help under-funded schools. During his time as a student at the Graham School, Sumit also worked as a research assistant at the Research Computing Center (RCC), UChicago. There he worked on web development projects, based on Django frameworks in Python, to help researchers from different universities connect with each other and share large volumes of data. He was the International Student Representative for the Master of Science in Analytics (MScA) program at Graham School to enhance international students’ experience and engagement and was also a member of the UChicago cricket team. Sumit has also taught Advanced Analytics and Machine Learning certificate course at The University of Chicago and Python for Internet Programming certificate course at The University of Washington. [Source] [Web Archive backup] ","date":"2020-07-14","objectID":"/about/:0:2","tags":null,"title":"About me","uri":"/about/"}]