[{"categories":["Recommender systems"],"content":"The Mixture-of-Experts (MoE) is a classical ensemble learning technique originally proposed by Jacobs et. al1 in 1991. MoEs have the capability to substantially scale up the model capacity and only introduce small computation overhead. This ability combined with recent innovations in the deep learning domain has led to the wide-scale adoption of MoEs in healthcare, finance, pattern recognition, etc. They have been successfully utilized in large-scale applications such as Large Language Modeling (LLM), Machine Translation, and Recommendations. This article gives an introduction to Mixture-of-Experts and some of the most important enhancements made to the original MoE proposal. Then we look at how MoEs have been adapted to compute recommendations by looking at examples of such systems in production. ","date":"2023-04-23","objectID":"/posts/2023/04/moe-for-recsys/:0:0","series":null,"tags":["literature review","recsys","Mixture-of-experts"],"title":"Mixture-of-Experts based Recommender Systems","uri":"/posts/2023/04/moe-for-recsys/#"},{"categories":["Recommender systems"],"content":"\rIntroduction to Mixture-of-Experts (MoE)Generally speaking, MoE architecture is an ensemble of many models (aka experts). We train a number of these experts, each of which can be trained on a subspace of the problem, and then specialize in that specific part of the input space. The underlying assumption is that our data comes from different regimes. Many datasets can be naturally divided into that correspond to different subtasks. For example: in financial data, the state of the economy has a big effect on mapping inputs to outputs. So you might want different models for different states of the economy. Each of the experts can be any machine learning algorithm such as an SVM, HMM, a small neural network, or even just a simple matrix multiplication. These experts often have the same architecture and are also trained by the same algorithm. Additionally, MoEs include a gating (or routing) function that acts as a manager that forwards individual inputs to the best experts based on their specialization. A simple classification example for MoE\rIn the classification example shown above, the blue circles and red diamonds correspond to two different classes. The gate does a soft partition of the input space and defines the regions where the individual experts are trustworthy. For example, Expert 1 is responsible for the left of the gating line, whereas Expert 2’s opinion is considered for the data points to the right of it. This approach simplifies a non-linear classification problem into two linear classification problems2. This divide-and-conquer approach to problem-solving also underlies many other predictive modeling ensembles, like Boosting methods. However, all the trained models in Boosting are given some weightage regardless of the current input we are dealing with, so the result will be combined from all models. On the other hand, the routing scheme in MoE only incurs computational cost corresponding to the selected experts. Hence MoEs enable a paradigm called Conditional Computation and perform effectively when there is a large amount of training data. Also, because the input is fractionated, MoE models often do not perform well with small datasets. ","date":"2023-04-23","objectID":"/posts/2023/04/moe-for-recsys/:1:0","series":null,"tags":["literature review","recsys","Mixture-of-experts"],"title":"Mixture-of-Experts based Recommender Systems","uri":"/posts/2023/04/moe-for-recsys/#introduction-to-mixture-of-experts-moe"},{"categories":["Recommender systems"],"content":"\rMake experts complete, not cooperateTraining a single, multilayer network to perform different subtasks on different occasions can lead to strong interference effects that lead to slow learning and poor generalization. If the number of subtasks is known prior to training, one option, for example, is to make the final output of the whole system to be a linear combination of the outputs of the local experts, each given their own proportional weights. So, the error value on case c can be calculated as: $$ E^c = || y^c - \\sum_{i} w_i^c o_i^c ||^2 $$ where $o_i^c$ is the output vector of $i$th expert and $w_i^c$ is the proportional contribution of expert $i$, and $y^c$ is the desired output. Note that the CART algorithm uses a similar approach. One problem with such an error measure is that it encourages cooperation as there is a strong coupling between the experts. Each local expert will have to make its output cancel the residual error that is left by the combined effect of other experts. And, if the weight of one expert changes, the residual error changes, and the error derivatives for all other local experts also change. Mixture of Experts\rThe interference can be reduced by using a system composed of several different “expert” networks plus a gating network that decides which of the experts should be used for each training case. Instead of using a linear combination of experts, we can use a manager layer (aka a gating network) that makes a stochastic decision about which single expert to use for specific input. The gating network allows the mixing proportions of the experts to be determined by the input vector. This gives us a system of competing local experts where each expert is required to produce the whole of the output vector rather than a residual effectively making the experts independent of each other. $$ E^c = || y^c - o_i^c ||^2 = \\sum_{i} p_i^c || y^c - o_i^c ||^2 $$ Here $p_i^c$ represents the probability that the gating network will select the output from the expert $i$ for an input $c$. If an expert gives less error than the weighted average of all experts, its responsibility for that case will be increased. There is still some indirect coupling because if some other expert changes its weights, it may cause the gating network to alter the responsibilities that get assigned to the experts, but at least these responsibility changes cannot alter the sign of the error that a local expert senses on a given training case. The original authors proposed to further optimize the above error function to a negative log probability of generating the desired output vector under the mixture of Gaussian model1. $$ E^c = - \\log{ \\sum_i p_i^c e^{- \\frac{1}{2} || y^c - o_i^c || }} $$ ","date":"2023-04-23","objectID":"/posts/2023/04/moe-for-recsys/:1:1","series":null,"tags":["literature review","recsys","Mixture-of-experts"],"title":"Mixture-of-Experts based Recommender Systems","uri":"/posts/2023/04/moe-for-recsys/#make-experts-complete-not-cooperate"},{"categories":["Recommender systems"],"content":"\rAdvantages of MoE MoEs are can be flexibly combined with different models. For example, when combined with SVMs they can allocate different kernel functions to different input regions, which is not possible with vanilla SVMs. They find the subsets of patterns that naturally exist within the data. This makes MoEs a great choice for problems that contain non-stationary, piecewise continuous data and datasets that contain a naturally distinctive subset of patterns. The divide-and-conquer strategy used by MoEs allows for building larger networks that are still cheap to compute at test time, and more parallelizable at training time3. Empirical studies show that the MoE model does not collapse into a single model4. ","date":"2023-04-23","objectID":"/posts/2023/04/moe-for-recsys/:1:2","series":null,"tags":["literature review","recsys","Mixture-of-experts"],"title":"Mixture-of-Experts based Recommender Systems","uri":"/posts/2023/04/moe-for-recsys/#advantages-of-moe"},{"categories":["Recommender systems"],"content":"\rProblems with MoE At the beginning of the training, all experts are randomly initialized, have the same network, and are trained by the same algorithm. This makes it hard for the gating network to learn the right features because all the experts look the same. Hence the gating mistakes get amplified. The gating network can converge to a state where it relies heavily on a specific set of experts. This problems is called the expert load imbalance during training and it can lead to performance issues. Some researchers have suggested adding a load imbalance loss value to mitigate this problem5. ","date":"2023-04-23","objectID":"/posts/2023/04/moe-for-recsys/:1:3","series":null,"tags":["literature review","recsys","Mixture-of-experts"],"title":"Mixture-of-Experts based Recommender Systems","uri":"/posts/2023/04/moe-for-recsys/#problems-with-moe"},{"categories":["Recommender systems"],"content":"\rMoE VariantsSince its inception over 30 years ago, MoEs have been revised many times and several improvements have been proposed. This section highlights a few of them. Hierarchical Mixture of ExpertsThe Hierarchical Mixture-of-Experts (HME) algorithm proposed by Jordan et. al6 takes a recursive approach to decompose the predictive modeling task. During MoE training, the gate and expert parameters are decoupled and estimated separately. This modularity led to the development of HMEs. HME learns a hierarchy of gating networks in a tree structure, such that the output is conditioned on multiple levels of gating functions. The outputs are then mixed according to the gating weights at each node. Two-level HME Architecture\rThe above example shows a two-level HME architecture with two MoE components at the bottom, each consisting of a single gate and two experts. Compared to the CART algorithm’s greedy approach, splits in HME are probabilistic functions. So HME can also be thought of as a statistical approach to decision tree modeling that has the capability of recovering from a poor decision somewhere further up the tree. Deep Mixture of ExpertsEigen et. al7 proposed a Deep Mixture of Experts (DMoE) model that stacks MoEs such that a suitable expert combination is dynamically assembled for a given input. DMoE uses multiple sets of gating and experts to exponentially increase the number of effective paths, yet maintains a modest model size by associating each input with a subset of experts at each layer. DMoE Architecture\rThe above example shows a DMoE with two sets of experts with gating networks. It dynamically computes a suitable expert combination by conditioning the experts and gating networks on the output of the previous layers. To ensure that the experts at the first layer do not overpower the remaining experts, the authors propose an explicit constraint on relative gating assignments temporarily during training. Sparsely-gated Mixture of ExpertsOne of the most influential improvements to MoEs was proposed by Shazeer et. al5 which offered a dramatic increase in model capacity without a proportional increase in computation. Their Sparsely-gated Mixture-of-Experts model can achieve more than 1000x improvement in model capacity while incurring only a minor loss in computational efficiency. Scaling a traditional deep learning model costs a significant amount of compute because the entire model is activated for every example. Their gating method selects a sparse combination of experts to process each input. Before taking the softmax in a standard gating network, they add a tunable Gaussian noise, and then keep only the top-k values, setting the rest to $-\\infty$ (making the corresponding gate outputs to be 0). Sparsely-gated MoE\rThe above example shows their proposed MoE layer embedded within a recurrent model. The sparse gating function in the example has selected two experts to perform computations. They trained the model on language modeling and machine translation tasks and show that different experts tend to become highly specialized based on input syntax and semantics. The paper also proposes parallelism techniques for increasing batch sizes and additional loss terms to balance the utilization of experts. Some recent studies show that using sparsely-gated MoEs for Large Language Modeling (LLM) can lead to similar performance as achieved by a dense model that requires four times as much compute8. Meta also used the sparsely-gated MoE technique for its No Language Left Behind (NLLB-200) model that provides translation capability for low-resource languages with relatively little data9. Multi-gate Mixture-of-ExpertsIn Multi-gate Mixture-of-Experts (MMoE), Ma el. al10 adapted the MoE structure to multi-task learning. The goal of multi-task learning is to build a single model that learns multiple tasks simultaneously. The authors hypothesize that such learning is sensitive to the differences in data distributions and the relatio","date":"2023-04-23","objectID":"/posts/2023/04/moe-for-recsys/:1:4","series":null,"tags":["literature review","recsys","Mixture-of-experts"],"title":"Mixture-of-Experts based Recommender Systems","uri":"/posts/2023/04/moe-for-recsys/#moe-variants"},{"categories":["Recommender systems"],"content":"\rMoE VariantsSince its inception over 30 years ago, MoEs have been revised many times and several improvements have been proposed. This section highlights a few of them. Hierarchical Mixture of ExpertsThe Hierarchical Mixture-of-Experts (HME) algorithm proposed by Jordan et. al6 takes a recursive approach to decompose the predictive modeling task. During MoE training, the gate and expert parameters are decoupled and estimated separately. This modularity led to the development of HMEs. HME learns a hierarchy of gating networks in a tree structure, such that the output is conditioned on multiple levels of gating functions. The outputs are then mixed according to the gating weights at each node. Two-level HME Architecture\rThe above example shows a two-level HME architecture with two MoE components at the bottom, each consisting of a single gate and two experts. Compared to the CART algorithm’s greedy approach, splits in HME are probabilistic functions. So HME can also be thought of as a statistical approach to decision tree modeling that has the capability of recovering from a poor decision somewhere further up the tree. Deep Mixture of ExpertsEigen et. al7 proposed a Deep Mixture of Experts (DMoE) model that stacks MoEs such that a suitable expert combination is dynamically assembled for a given input. DMoE uses multiple sets of gating and experts to exponentially increase the number of effective paths, yet maintains a modest model size by associating each input with a subset of experts at each layer. DMoE Architecture\rThe above example shows a DMoE with two sets of experts with gating networks. It dynamically computes a suitable expert combination by conditioning the experts and gating networks on the output of the previous layers. To ensure that the experts at the first layer do not overpower the remaining experts, the authors propose an explicit constraint on relative gating assignments temporarily during training. Sparsely-gated Mixture of ExpertsOne of the most influential improvements to MoEs was proposed by Shazeer et. al5 which offered a dramatic increase in model capacity without a proportional increase in computation. Their Sparsely-gated Mixture-of-Experts model can achieve more than 1000x improvement in model capacity while incurring only a minor loss in computational efficiency. Scaling a traditional deep learning model costs a significant amount of compute because the entire model is activated for every example. Their gating method selects a sparse combination of experts to process each input. Before taking the softmax in a standard gating network, they add a tunable Gaussian noise, and then keep only the top-k values, setting the rest to $-\\infty$ (making the corresponding gate outputs to be 0). Sparsely-gated MoE\rThe above example shows their proposed MoE layer embedded within a recurrent model. The sparse gating function in the example has selected two experts to perform computations. They trained the model on language modeling and machine translation tasks and show that different experts tend to become highly specialized based on input syntax and semantics. The paper also proposes parallelism techniques for increasing batch sizes and additional loss terms to balance the utilization of experts. Some recent studies show that using sparsely-gated MoEs for Large Language Modeling (LLM) can lead to similar performance as achieved by a dense model that requires four times as much compute8. Meta also used the sparsely-gated MoE technique for its No Language Left Behind (NLLB-200) model that provides translation capability for low-resource languages with relatively little data9. Multi-gate Mixture-of-ExpertsIn Multi-gate Mixture-of-Experts (MMoE), Ma el. al10 adapted the MoE structure to multi-task learning. The goal of multi-task learning is to build a single model that learns multiple tasks simultaneously. The authors hypothesize that such learning is sensitive to the differences in data distributions and the relatio","date":"2023-04-23","objectID":"/posts/2023/04/moe-for-recsys/:1:4","series":null,"tags":["literature review","recsys","Mixture-of-experts"],"title":"Mixture-of-Experts based Recommender Systems","uri":"/posts/2023/04/moe-for-recsys/#hierarchical-mixture-of-experts"},{"categories":["Recommender systems"],"content":"\rMoE VariantsSince its inception over 30 years ago, MoEs have been revised many times and several improvements have been proposed. This section highlights a few of them. Hierarchical Mixture of ExpertsThe Hierarchical Mixture-of-Experts (HME) algorithm proposed by Jordan et. al6 takes a recursive approach to decompose the predictive modeling task. During MoE training, the gate and expert parameters are decoupled and estimated separately. This modularity led to the development of HMEs. HME learns a hierarchy of gating networks in a tree structure, such that the output is conditioned on multiple levels of gating functions. The outputs are then mixed according to the gating weights at each node. Two-level HME Architecture\rThe above example shows a two-level HME architecture with two MoE components at the bottom, each consisting of a single gate and two experts. Compared to the CART algorithm’s greedy approach, splits in HME are probabilistic functions. So HME can also be thought of as a statistical approach to decision tree modeling that has the capability of recovering from a poor decision somewhere further up the tree. Deep Mixture of ExpertsEigen et. al7 proposed a Deep Mixture of Experts (DMoE) model that stacks MoEs such that a suitable expert combination is dynamically assembled for a given input. DMoE uses multiple sets of gating and experts to exponentially increase the number of effective paths, yet maintains a modest model size by associating each input with a subset of experts at each layer. DMoE Architecture\rThe above example shows a DMoE with two sets of experts with gating networks. It dynamically computes a suitable expert combination by conditioning the experts and gating networks on the output of the previous layers. To ensure that the experts at the first layer do not overpower the remaining experts, the authors propose an explicit constraint on relative gating assignments temporarily during training. Sparsely-gated Mixture of ExpertsOne of the most influential improvements to MoEs was proposed by Shazeer et. al5 which offered a dramatic increase in model capacity without a proportional increase in computation. Their Sparsely-gated Mixture-of-Experts model can achieve more than 1000x improvement in model capacity while incurring only a minor loss in computational efficiency. Scaling a traditional deep learning model costs a significant amount of compute because the entire model is activated for every example. Their gating method selects a sparse combination of experts to process each input. Before taking the softmax in a standard gating network, they add a tunable Gaussian noise, and then keep only the top-k values, setting the rest to $-\\infty$ (making the corresponding gate outputs to be 0). Sparsely-gated MoE\rThe above example shows their proposed MoE layer embedded within a recurrent model. The sparse gating function in the example has selected two experts to perform computations. They trained the model on language modeling and machine translation tasks and show that different experts tend to become highly specialized based on input syntax and semantics. The paper also proposes parallelism techniques for increasing batch sizes and additional loss terms to balance the utilization of experts. Some recent studies show that using sparsely-gated MoEs for Large Language Modeling (LLM) can lead to similar performance as achieved by a dense model that requires four times as much compute8. Meta also used the sparsely-gated MoE technique for its No Language Left Behind (NLLB-200) model that provides translation capability for low-resource languages with relatively little data9. Multi-gate Mixture-of-ExpertsIn Multi-gate Mixture-of-Experts (MMoE), Ma el. al10 adapted the MoE structure to multi-task learning. The goal of multi-task learning is to build a single model that learns multiple tasks simultaneously. The authors hypothesize that such learning is sensitive to the differences in data distributions and the relatio","date":"2023-04-23","objectID":"/posts/2023/04/moe-for-recsys/:1:4","series":null,"tags":["literature review","recsys","Mixture-of-experts"],"title":"Mixture-of-Experts based Recommender Systems","uri":"/posts/2023/04/moe-for-recsys/#deep-mixture-of-experts"},{"categories":["Recommender systems"],"content":"\rMoE VariantsSince its inception over 30 years ago, MoEs have been revised many times and several improvements have been proposed. This section highlights a few of them. Hierarchical Mixture of ExpertsThe Hierarchical Mixture-of-Experts (HME) algorithm proposed by Jordan et. al6 takes a recursive approach to decompose the predictive modeling task. During MoE training, the gate and expert parameters are decoupled and estimated separately. This modularity led to the development of HMEs. HME learns a hierarchy of gating networks in a tree structure, such that the output is conditioned on multiple levels of gating functions. The outputs are then mixed according to the gating weights at each node. Two-level HME Architecture\rThe above example shows a two-level HME architecture with two MoE components at the bottom, each consisting of a single gate and two experts. Compared to the CART algorithm’s greedy approach, splits in HME are probabilistic functions. So HME can also be thought of as a statistical approach to decision tree modeling that has the capability of recovering from a poor decision somewhere further up the tree. Deep Mixture of ExpertsEigen et. al7 proposed a Deep Mixture of Experts (DMoE) model that stacks MoEs such that a suitable expert combination is dynamically assembled for a given input. DMoE uses multiple sets of gating and experts to exponentially increase the number of effective paths, yet maintains a modest model size by associating each input with a subset of experts at each layer. DMoE Architecture\rThe above example shows a DMoE with two sets of experts with gating networks. It dynamically computes a suitable expert combination by conditioning the experts and gating networks on the output of the previous layers. To ensure that the experts at the first layer do not overpower the remaining experts, the authors propose an explicit constraint on relative gating assignments temporarily during training. Sparsely-gated Mixture of ExpertsOne of the most influential improvements to MoEs was proposed by Shazeer et. al5 which offered a dramatic increase in model capacity without a proportional increase in computation. Their Sparsely-gated Mixture-of-Experts model can achieve more than 1000x improvement in model capacity while incurring only a minor loss in computational efficiency. Scaling a traditional deep learning model costs a significant amount of compute because the entire model is activated for every example. Their gating method selects a sparse combination of experts to process each input. Before taking the softmax in a standard gating network, they add a tunable Gaussian noise, and then keep only the top-k values, setting the rest to $-\\infty$ (making the corresponding gate outputs to be 0). Sparsely-gated MoE\rThe above example shows their proposed MoE layer embedded within a recurrent model. The sparse gating function in the example has selected two experts to perform computations. They trained the model on language modeling and machine translation tasks and show that different experts tend to become highly specialized based on input syntax and semantics. The paper also proposes parallelism techniques for increasing batch sizes and additional loss terms to balance the utilization of experts. Some recent studies show that using sparsely-gated MoEs for Large Language Modeling (LLM) can lead to similar performance as achieved by a dense model that requires four times as much compute8. Meta also used the sparsely-gated MoE technique for its No Language Left Behind (NLLB-200) model that provides translation capability for low-resource languages with relatively little data9. Multi-gate Mixture-of-ExpertsIn Multi-gate Mixture-of-Experts (MMoE), Ma el. al10 adapted the MoE structure to multi-task learning. The goal of multi-task learning is to build a single model that learns multiple tasks simultaneously. The authors hypothesize that such learning is sensitive to the differences in data distributions and the relatio","date":"2023-04-23","objectID":"/posts/2023/04/moe-for-recsys/:1:4","series":null,"tags":["literature review","recsys","Mixture-of-experts"],"title":"Mixture-of-Experts based Recommender Systems","uri":"/posts/2023/04/moe-for-recsys/#sparsely-gated-mixture-of-experts"},{"categories":["Recommender systems"],"content":"\rMoE VariantsSince its inception over 30 years ago, MoEs have been revised many times and several improvements have been proposed. This section highlights a few of them. Hierarchical Mixture of ExpertsThe Hierarchical Mixture-of-Experts (HME) algorithm proposed by Jordan et. al6 takes a recursive approach to decompose the predictive modeling task. During MoE training, the gate and expert parameters are decoupled and estimated separately. This modularity led to the development of HMEs. HME learns a hierarchy of gating networks in a tree structure, such that the output is conditioned on multiple levels of gating functions. The outputs are then mixed according to the gating weights at each node. Two-level HME Architecture\rThe above example shows a two-level HME architecture with two MoE components at the bottom, each consisting of a single gate and two experts. Compared to the CART algorithm’s greedy approach, splits in HME are probabilistic functions. So HME can also be thought of as a statistical approach to decision tree modeling that has the capability of recovering from a poor decision somewhere further up the tree. Deep Mixture of ExpertsEigen et. al7 proposed a Deep Mixture of Experts (DMoE) model that stacks MoEs such that a suitable expert combination is dynamically assembled for a given input. DMoE uses multiple sets of gating and experts to exponentially increase the number of effective paths, yet maintains a modest model size by associating each input with a subset of experts at each layer. DMoE Architecture\rThe above example shows a DMoE with two sets of experts with gating networks. It dynamically computes a suitable expert combination by conditioning the experts and gating networks on the output of the previous layers. To ensure that the experts at the first layer do not overpower the remaining experts, the authors propose an explicit constraint on relative gating assignments temporarily during training. Sparsely-gated Mixture of ExpertsOne of the most influential improvements to MoEs was proposed by Shazeer et. al5 which offered a dramatic increase in model capacity without a proportional increase in computation. Their Sparsely-gated Mixture-of-Experts model can achieve more than 1000x improvement in model capacity while incurring only a minor loss in computational efficiency. Scaling a traditional deep learning model costs a significant amount of compute because the entire model is activated for every example. Their gating method selects a sparse combination of experts to process each input. Before taking the softmax in a standard gating network, they add a tunable Gaussian noise, and then keep only the top-k values, setting the rest to $-\\infty$ (making the corresponding gate outputs to be 0). Sparsely-gated MoE\rThe above example shows their proposed MoE layer embedded within a recurrent model. The sparse gating function in the example has selected two experts to perform computations. They trained the model on language modeling and machine translation tasks and show that different experts tend to become highly specialized based on input syntax and semantics. The paper also proposes parallelism techniques for increasing batch sizes and additional loss terms to balance the utilization of experts. Some recent studies show that using sparsely-gated MoEs for Large Language Modeling (LLM) can lead to similar performance as achieved by a dense model that requires four times as much compute8. Meta also used the sparsely-gated MoE technique for its No Language Left Behind (NLLB-200) model that provides translation capability for low-resource languages with relatively little data9. Multi-gate Mixture-of-ExpertsIn Multi-gate Mixture-of-Experts (MMoE), Ma el. al10 adapted the MoE structure to multi-task learning. The goal of multi-task learning is to build a single model that learns multiple tasks simultaneously. The authors hypothesize that such learning is sensitive to the differences in data distributions and the relatio","date":"2023-04-23","objectID":"/posts/2023/04/moe-for-recsys/:1:4","series":null,"tags":["literature review","recsys","Mixture-of-experts"],"title":"Mixture-of-Experts based Recommender Systems","uri":"/posts/2023/04/moe-for-recsys/#multi-gate-mixture-of-experts"},{"categories":["Recommender systems"],"content":"\rMixture-of-Expert for RecommendationsA lot of recommender system applications are built on a multi-task learning framework. The key idea is to model multiple related objectives, like user engagement (such as clicks, engagement time), user satisfaction (such as ratings, like rate), and user purchases together for a more holistic view of the users. Such joint modeling allows for efficient and effective knowledge and data sharing across relevant tasks. This is especially useful for tasks with sparse data availability. Multi-task learning can also act as a regularizer such that the inductive biases introduced by the auxiliary tasks also improve the generalization of the main task. Some of the widely used multi-task learning models are based on a shared-bottom model structure. These systems suffer from optimization conflicts caused by different tasks sharing the same set of parameters. Data sparsity, data heterogeneity, and users’ complex underlying intents are some of the common challenges faced by these multi-task learning systems. Recently, some large-scale recommendation systems have started to utilize MMoE for multi-task modeling and achieve state-of-the-art results for recommendations. This section highlights some notable examples from the industry. ","date":"2023-04-23","objectID":"/posts/2023/04/moe-for-recsys/:2:0","series":null,"tags":["literature review","recsys","Mixture-of-experts"],"title":"Mixture-of-Experts based Recommender Systems","uri":"/posts/2023/04/moe-for-recsys/#mixture-of-expert-for-recommendations"},{"categories":["Recommender systems"],"content":"\rYouTube’s Next Video RecommendationZhao et. al11 from Google proposed an MMoE-based multi-objective ranking system for recommending what video to watch next on YouTube. They first group their task objective into two categories: engagement and satisfaction. Given the list of candidate videos from the retrieval step, their ranking system uses candidate, user, and context features to learn to predict the probabilities corresponding to the two categories of user behavior. YouTube's MMoE\rThey substitute the last shared-bottom layer in their previous model with the MoE layer with task-specific gating and experts shared across tasks. They chose to not apply the MoE layer directly to the input because the high dimensionality of input would lead to significant model training and serving costs. In the paper, they also describe a shallow tower learned alongside this MMoE to reduce selection biases. ","date":"2023-04-23","objectID":"/posts/2023/04/moe-for-recsys/:2:1","series":null,"tags":["literature review","recsys","Mixture-of-experts"],"title":"Mixture-of-Experts based Recommender Systems","uri":"/posts/2023/04/moe-for-recsys/#youtubes-next-video-recommendation"},{"categories":["Recommender systems"],"content":"\rGMail’s Mixture of Sequential ExpertsQin et. al12 from Google, proposed a Mixture-of-Sequential-Experts (MoSE) framework for multi-task modeling that focuses on non-sequential data (like user query and context) as well as sequential data (like user behavior from search or browsing logs). In order to incorporate temporal dependencies from sequential data modeling into the MMoE structure, they utilized an LSTM shared-bottom, LSTM-based experts and LSTM task-towers. MoSE model structure\rTheir experiments showed that the LSTM-based MMoE (MoSE) performed better at modeling user activity streams than the FFN-based MMoE model. ","date":"2023-04-23","objectID":"/posts/2023/04/moe-for-recsys/:2:2","series":null,"tags":["literature review","recsys","Mixture-of-experts"],"title":"Mixture-of-Experts based Recommender Systems","uri":"/posts/2023/04/moe-for-recsys/#gmails-mixture-of-sequential-experts"},{"categories":["Recommender systems"],"content":"\rKuaishou’s Kwai Recommender SystemIn Personalized Cold Start Modules (POSO), Dai et. al13 explained their recommendation system used in the Kwai app. The input consists of sequential features (like the user’s historical interaction) and non-sequential features (like user ID). All of these features are first mapped into low-dimensional vectors using an embedding look-up table. For each sequential feature, a multi-head attention (MHA) module is used to fuse the embeddings into a concatenation of vector outputs from each head. These sequential and non-sequential vectors are then concatenated and fed into an MMoE layer, which is followed by a task-specific MLP that predicts the probabilities for user behavior specific to the task. Kwai's recommender system in production\r","date":"2023-04-23","objectID":"/posts/2023/04/moe-for-recsys/:2:3","series":null,"tags":["literature review","recsys","Mixture-of-experts"],"title":"Mixture-of-Experts based Recommender Systems","uri":"/posts/2023/04/moe-for-recsys/#kuaishous-kwai-recommender-system"},{"categories":["Recommender systems"],"content":"\rSummaryMixture-of-Experts have seen a wide-scale adoption in the industry for a number of applications. Their learning procedure divides up the task into appropriate subtasks, each of which can be solved by a very simple expert network. A gating network further reduces the computational overhead by deciding which of the experts should be used for each input. Hence MoEs can automatically adjust parameterization between modeling shared information and modeling task-specific information. MoE layers in deep learning achieve conditional computation where only parts of a network are active on a per-example basis at both training and serving time. This capability translates to parallelizable training and fast inference, making MoEs lucrative for large-scale systems. This article introduced MoE and its variants in detail and highlighted several industrial recommender systems that utilized mixture-of-experts. ","date":"2023-04-23","objectID":"/posts/2023/04/moe-for-recsys/:3:0","series":null,"tags":["literature review","recsys","Mixture-of-experts"],"title":"Mixture-of-Experts based Recommender Systems","uri":"/posts/2023/04/moe-for-recsys/#summary"},{"categories":["Recommender systems"],"content":"\rReferences Jacobs, R.A., Jordan, M.I., Nowlan, S.J., \u0026 Hinton, G.E. (1991). Adaptive Mixtures of Local Experts. Neural Computation, 3, 79-87. ↩︎ ↩︎ Yüksel, S.E., Wilson, J.N., \u0026 Gader, P.D. (2012). Twenty Years of Mixture of Experts. IEEE Transactions on Neural Networks and Learning Systems, 23, 1177-1193. ↩︎ Kalyan, S.K. (2017). Designing mixture of deep experts. ↩︎ Chen, Z., Deng, Y., Wu, Y., Gu, Q., \u0026 Li, Y. (2022). Towards Understanding Mixture of Experts in Deep Learning. ArXiv, abs/2208.02813. ↩︎ Shazeer, N.M., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q.V., Hinton, G.E., \u0026 Dean, J. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. ArXiv, abs/1701.06538. ↩︎ ↩︎ Jordan, M.I., \u0026 Jacobs, R.A. (1993). Hierarchical Mixtures of Experts and the EM Algorithm. Neural Computation, 6, 181-214. ↩︎ Eigen, D., Ranzato, M., \u0026 Sutskever, I. (2013). Learning Factored Representations in a Deep Mixture of Experts. CoRR, abs/1312.4314. ↩︎ Artetxe, M., Bhosale, S., Goyal, \u0026 Stoyanov, V. (2021). Efficient Large Scale Language Modeling with Mixtures of Experts. Conference on Empirical Methods in Natural Language Processing. ↩︎ Team, N. (2022). No Language Left Behind: Scaling Human-Centered Machine Translation. ArXiv. /abs/2207.04672 ↩︎ Ma, J., Zhao, Z., Yi, X., Chen, J., Hong, L., \u0026 Chi, E.H. (2018). Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts. Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \u0026 Data Mining. ↩︎ Zhao, Z., Hong, L., Wei, L., Chen, J., Nath, A., Andrews, S., Kumthekar, A., Sathiamoorthy, M., Yi, X., \u0026 Chi, E.H. (2019). Recommending what video to watch next: a multitask ranking system. Proceedings of the 13th ACM Conference on Recommender Systems. ↩︎ Qin, Z., Cheng, Y., Zhao, Z., Chen, Z., Metzler, D., \u0026 Qin, J. (2020). Multitask Mixture of Sequential Experts for User Activity Streams. Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \u0026 Data Mining. ↩︎ Dai, S., Lin, H., Zhao, Z., Lin, J., Wu, H., Wang, Z., Yang, S., \u0026 Liu, J. (2021). POSO: Personalized Cold Start Modules for Large-scale Recommender Systems. ArXiv, abs/2108.04690. ↩︎ ","date":"2023-04-23","objectID":"/posts/2023/04/moe-for-recsys/:4:0","series":null,"tags":["literature review","recsys","Mixture-of-experts"],"title":"Mixture-of-Experts based Recommender Systems","uri":"/posts/2023/04/moe-for-recsys/#references"},{"categories":["Recommender systems"],"content":"\rGenerative Modeling for Recommender SystemsThe task of learning a user’s preference based on historical interactions and predicting the next item is essential to most modern recommendation systems. Most of the traditional deep learning approaches for sequential recommendations, like MLPs, CNNs, RNNs, and Transformers, represent items as a dense vector in latent space, and suffer from two issues: A fixed-length vector does not fully capture the uncertainty and diversity of a user’s interests. These methods lead to exposure bias because of the underlying assumption that the true interacted item is the most relevant one. To address these shortcomings, several generative models such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) are utilized to infer the user interaction probabilities over non-interacted items. These models use a generative process in which the user preferences are considered latent factors that determine users’ interaction behaviors. Sampling from the learned probability distribution mimics the uncertainty of user behavior. GANs learn a generator and a discriminator adversarially. While the generator estimates users’ interaction probabilities, the discriminator provides guidance to the generator. For example, in IRGAN1 researchers proposed a generative retrieval GAN model to serve item recommendations, and the SD-GAR2 paper further improved the process with a sampling-decomposable generator. However, this adversarial training often leads to poor performance due to the instability of adversarial training. VAEs maximize the likelihood of historical interactions to learn a posterior distribution over latent factors. For example, Liang et. al3 used VAEs to perform collaborative filtering over implicit feedback, and in ACVAE4 authors augmented a sequential VAE with adversarial training to capture user preferences. Despite outperforming GANs for recommendations, VAEs struggle to balance the tradeoff when a simple encoder fails to capture heterogeneous user preference and a complex one makes the posterior distribution calculation to be intractable. ","date":"2023-04-17","objectID":"/posts/2023/04/diffusion-for-recsys/:1:0","series":null,"tags":["literature review","recsys","diffusion"],"title":"Diffusion Modeling based Recommender Systems","uri":"/posts/2023/04/diffusion-for-recsys/#generative-modeling-for-recommender-systems"},{"categories":["Recommender systems"],"content":"\rTowards Diffusion Models for RecommendationsApart from the problems mentioned above, both GANs and VAEs suffer from posterior collapse due to information bottleneck, so the hidden representation may not contain information about the user preference. These methods are also prone to mode collapse where they generate only a small set of outputs over multiple iterations. To tackle these challenges, a new paradigm of generative models called diffusion models has been utilized by various researchers recently. These diffusion models overcome the issues of traditional generative models and produce more accurate modeling of the complex user interaction in a denoising manner. ","date":"2023-04-17","objectID":"/posts/2023/04/diffusion-for-recsys/:2:0","series":null,"tags":["literature review","recsys","diffusion"],"title":"Diffusion Modeling based Recommender Systems","uri":"/posts/2023/04/diffusion-for-recsys/#towards-diffusion-models-for-recommendations"},{"categories":["Recommender systems"],"content":"\rIntroduction to DiffusionThe Diffusion Model (DMs) is a new generative AI paradigm that is inspired by nonequilibrium thermodynamics. These models have achieved state-of-the-art results in various fields such as image synthesis, audio generation, etc. Dall-E2 by OpenAI, Imagen by Google, Stable Diffusion by StabilityAI, and Midjourney are some of the most popular diffusion models. Visual representation of a Variational Diffusion Model\rThe diffusion process works in the following two steps: Forward Process: Given an input data sample $x_{0}$, sampled from a distribution $q(x)$, forward diffusion creates latent variables $x_{1:T}$, using a Markov chain of $T$ steps, by gradually adding Gaussian noises at each step. Formally, we can define forward transition ${x}_{t-1} \\to x_t$ as: $$q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}{x}_{t-1}, \\beta_t{I})$$ where $t$ is the diffusion step, $\\mathcal{N}$ is the Gaussian distribution, $I$ is an identity matrix, and $\\beta_t \\in (0,1)$ is the noise scale at each step. $\\beta_{t}$ can also be defined using a linear, quadratic, sqrt, or cosine variance schedule. For $T\\to\\infty$, $x_T$ approaches a standard Gaussian distribution. Thanks to the reparameterization trick and additivity of two Gaussian noises, we can directly obtain $x_{t}$ from $x_{0}$. Reverse Process: In this step, a neural network (parameterized by $p_{\\theta}$), such as a Transformer or a U-Net, learns to remove the added noises from $x_t$ to recover $x_{t-1}$ . The network takes $x_t$ as input and learns to denoise it to $x_{t-1}$, iteratively by: $$p_{\\theta}(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_{\\theta}(x_t, t), \\Sigma_{\\theta}(x_t, t))$$ where $\\mu_{\\theta(x_t,t)}$ and ${\\Sigma}_\\theta({x}_t,t)$ are the mean and covariance of the Gaussian distribution predicted by the network with parameters $\\theta$. Essentially, in the forward process, DMs gradually convert the original data to Gaussian noises using a Markov chain. In the reverse process, a neural network takes the noised data and reconstructs the original data in multiple diffusion steps. Having no learnable parameters in the forward chain helps in avoiding the posterior collapse, and having only a single neural network (as opposed to generator-discriminator or encoder-decoder in GANs or VAEs) avoids the mode collapse problem. Diffusion models are optimized by maximizing the Evidence Lower Bound (ELBO) of the likelihood of observed input data $x_{0}$: ELBO\rHere reconstruction term is similar to the ELBO of a VAE and measures the recovery probability of $x_0$, the prior matching term is a constant value that is ignored during training, and the denoising matching term is the difference between the desired and approximated denoising step. For further details, please refer to Luo et. al5. At inference time, diffusion models draw $x_T \\sim \\mathcal{N}(0,I)$ and utilize the denoising step $p_{\\theta(x_{t-1} | x_t)}$ to iteratively repeat the generation process $x_T \\to x_{T-1} \\to \\dots \\to x_0$. This iterative process makes them slow at sampling, at least compared to GANs. ","date":"2023-04-17","objectID":"/posts/2023/04/diffusion-for-recsys/:2:1","series":null,"tags":["literature review","recsys","diffusion"],"title":"Diffusion Modeling based Recommender Systems","uri":"/posts/2023/04/diffusion-for-recsys/#introduction-to-diffusion"},{"categories":["Recommender systems"],"content":"\rWhy Should You Use Diffusion for Recommendations?Real-world recommendation applications often have to deal with complex scenarios that cannot be encoded in a single vector representation. Such as: Users’ interests and preferences are dynamic, diverse, and inconsistent. Items contain multiple complex latent aspects. Users’ currently known preferences are always uncertain to some degree due to diversity and the evolution of interests. Soft-attention and dynamic-routing-based methods like DIN, MIND, and ComiRec can do multi-interest modeling, but they also require a predefined number of interests. It is possible to address some of these concerns by modeling users’ current interests as a distribution. Generative methods like VAEs can model the multiple latent aspects of an item as a distribution and inject uncertainty as Gaussian noises. But as explained earlier, these traditional methods suffer from a lot of issues like mode collapse, and Diffusion Models are much better alternatives. The goal of recommender systems is to predict future interaction probabilities based on historical interactions. In practice, the collected user interactions in implicit feedback can be considered naturally noisy (or corrupt) due to false-positive and false-negative interactions. Hence the objective of the diffusion model to predict probabilities based on the iterative denoising process aligns well with recommender systems. We can use DMs to predict users’ future interaction probabilities based on corrupted interactions by learning to recover the original interactions. Then these probabilities can be used to rank and recommend items. ","date":"2023-04-17","objectID":"/posts/2023/04/diffusion-for-recsys/:2:2","series":null,"tags":["literature review","recsys","diffusion"],"title":"Diffusion Modeling based Recommender Systems","uri":"/posts/2023/04/diffusion-for-recsys/#why-should-you-use-diffusion-for-recommendations"},{"categories":["Recommender systems"],"content":"\rDiffusion-based Recommender SystemsIn the last couple of weeks, several researchers have proposed ways to utilize diffusion models for recommendation tasks. This section highlights some of those ideas. ","date":"2023-04-17","objectID":"/posts/2023/04/diffusion-for-recsys/:3:0","series":null,"tags":["literature review","recsys","diffusion"],"title":"Diffusion Modeling based Recommender Systems","uri":"/posts/2023/04/diffusion-for-recsys/#diffusion-based-recommender-systems"},{"categories":["Recommender systems"],"content":"\rDiffRec by Soochow UniversityDu et. al6 proposed DiffRec to adapt the diffusion model to sequential recommendations task. Diffusion models originally process continuous data (like images, and audio). To adapt them to discrete recommendation data, this paper added one additional transition that maps discrete items into hidden representation in the forward process. Another transition was added in the reverse process that converts hidden representation back to the probability distribution over the items by using a linear layer. An additional KL-Divergence term is also introduced in the objective function to control the behavior of this additional transition. Additional transitions proposed by DiffRec converts discrete item v to hidden representation h in the forward step and convert h to probability distribution over the items in the reverse step\rTo further make the diffusion model suitable for the recommendations domain, noising, and denoising are done only on the hidden representation of the target item instead of the whole sequence. The authors also proposed a simplification trick that rewrites the KL divergence term in the objective function as the mean square error. This allows for a neural network $f_{\\theta}$ that can directly predict $h_T^0$ based on $h_T^n$. The paper used a Transformer Encoder as the neural network and named it Denoise Sequential Recommender (DNR). Modern recommender systems can have millions or billions of users and items. This means that the training and inference cost for the multiple diffusion steps can be extremely costly. To alleviate this, only the important diffusion steps (highest MSE loss) are sampled, as opposed to calculating model output at all diffusion steps, during training and benefit from the direct inference of $h_T^0$ from any diffusion step during inference. The experiments conducted by the paper used three public benchmark datasets and showed that DiffRec outperforms the state-of-the-art sequential recommendation models like GRU4Rec, SASRec, BERT4Rec, and STOSA, and several generative and contrastive methods as well. ","date":"2023-04-17","objectID":"/posts/2023/04/diffusion-for-recsys/:3:1","series":null,"tags":["literature review","recsys","diffusion"],"title":"Diffusion Modeling based Recommender Systems","uri":"/posts/2023/04/diffusion-for-recsys/#diffrec-by-soochow-university"},{"categories":["Recommender systems"],"content":"\rDiffRec by National University of SingaporeWang et. al7 proposed a Diffusion Recommender Model (DiffRec) to infer user interaction probabilities in a denoising manner to serve sequential recommendations. In the forward step, DiffRec uses a linear noise schedule to gradually corrupt user interaction histories by injecting scheduled Gaussian noises. During the reverse step, it recovers the original interaction from corrupted interactions iteratively by predicting Gaussian parameters via a parameterized neural network (like an MLP). The authors hypothesize that using pure noising like in image synthesis for interaction histories can hurt personalized recommendations, so they keep the added noise in forward pass to a minimum, meaning that the latent variable $x_T$ does not approach the standard Gaussian noises. They also propose a simplification of denoising matching terms in ELBO to make the approximation tractable and use the importance sampling similar to the previous method. The paper proposed two extensions of DiffRec: Latent DiffRec (L-DiffRec) for scalable and cost-efficient large-scale item prediction and Temporal DiffRec (T-DiffRec) for temporal modeling to facilitate the use of DiffRec in practical recommender systems. In L-DiffRec, the k-means method is used to cluster the item set into $C$ categories using the item embeddings learned from a LightGCN algorithm. The user interaction vector $x_0$ is then divided into $C$ parts according to the clusters. A variational encoder is then used to compress these interactions by predicting the mean and variance of the variational distribution. The diffusion process is conducted over these latent space representations. This method significantly reduces the model parameters and memory costs for deploying diffusion models for large-scale recommender systems. T-DiffRec uses a time-aware reweighting strategy that assigns larger weight to recent user interactions. The underlying hypothesis is that user preferences shift over time and more recent interactions can better represent users’ current preferences. These weights are defined through a simple time-aware linear schedule: $w_m = w_{\\min} + \\dfrac{m-1}{M-1}(w_{\\max}-w_{\\min})$ for an interaction sequence $\\mathcal{S}={i_1, i_2, \\dots, i_M}$ of M items, where $w_{\\min}\u003cw_{\\max}\\in(0,1]$ represent two hyperparameters for the lower an upper bound of the interaction weights. The code and data for DiffRec are available on GitHub. ","date":"2023-04-17","objectID":"/posts/2023/04/diffusion-for-recsys/:3:2","series":null,"tags":["literature review","recsys","diffusion"],"title":"Diffusion Modeling based Recommender Systems","uri":"/posts/2023/04/diffusion-for-recsys/#diffrec-by-national-university-of-singapore"},{"categories":["Recommender systems"],"content":"\rDiffuRec by Wuhan UniversityLi et. al8 proposed DiffuRec, a diffusion-based sequential recommender system, for modeling items’ latent aspects and users’ multi-level interests. The overall architecture for DiffuRec is shown below. DiffuRec Architecture\rDuring the diffusion phase, Gaussian noise is gradually added into the target embedding using a truncated linear schedule to generate a noised item representation. The authors hypothesize that this noise helps in injecting some uncertainty into the recommendation process. A step index is sampled from a uniform distribution and the corresponding step embedding is fed into the Approximator for model training. A Transformer model is used as an Approximator (shown on the left) that iteratively reconstructs target item representation for model training. At inference time, this Approximator iteratively reverses pure Gaussian noise into target item distribution representation to predict the users’ current interests. While the earlier work simplified the KL divergence in the objective function to mean square error, the authors of DiffuRec utilize cross-entropy loss for model optimization. Their experiments show that the DiffuRec model outperforms nine strong baseline methods. ","date":"2023-04-17","objectID":"/posts/2023/04/diffusion-for-recsys/:3:3","series":null,"tags":["literature review","recsys","diffusion"],"title":"Diffusion Modeling based Recommender Systems","uri":"/posts/2023/04/diffusion-for-recsys/#diffurec-by-wuhan-university"},{"categories":["Recommender systems"],"content":"\rSummaryGenerative methods have shown considerable success in the recommender systems domain by modeling historical interactions to learn posterior distributions over latent factors. These factors characterize highly entangled user preferences when executing an intention. Diffusion Models present a new and powerful paradigm to effectively model this latent space. This article introduced several recent proposals that adapt diffusion models to the recommendations domain. ","date":"2023-04-17","objectID":"/posts/2023/04/diffusion-for-recsys/:4:0","series":null,"tags":["literature review","recsys","diffusion"],"title":"Diffusion Modeling based Recommender Systems","uri":"/posts/2023/04/diffusion-for-recsys/#summary"},{"categories":["Recommender systems"],"content":"\rReferences Wang, Jun \u0026 Yu, Lantao \u0026 Zhang, Weinan \u0026 Gong, Yu \u0026 Xu, Yinghui \u0026 Wang, Benyou \u0026 Zhang, Peng \u0026 Zhang, Dell. (2017). IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models. 10.1145/3077136.3080786. ↩︎ Jin, B., Lian, D., Liu, Z., Liu, Q., Ma, J., Xie, X., \u0026 Chen, E. (2020). Sampling-Decomposable Generative Adversarial Recommender. ArXiv. /abs/2011.00956 ↩︎ Liang, D., Krishnan, R. G., Hoffman, M. D., \u0026 Jebara, T. (2018). Variational Autoencoders for Collaborative Filtering. ArXiv. /abs/1802.05814 ↩︎ Xie, Z., Liu, C., Zhang, Y., Lu, H., Wang, D., \u0026 Ding, Y. (2021). Adversarial and Contrastive Variational Autoencoder for Sequential Recommendation. ArXiv. /abs/2103.10693 ↩︎ Luo, C. (2022). Understanding Diffusion Models: A Unified Perspective. ArXiv. /abs/2208.11970 ↩︎ Du, H., Yuan, H., Huang, Z., Zhao, P., \u0026 Zhou, X. (2023). Sequential Recommendation with Diffusion Models. ArXiv. /abs/2304.04541 ↩︎ Wang, W., Xu, Y., Feng, F., Lin, X., He, X., \u0026 Chua, T. (2023). Diffusion Recommender Model. ArXiv. /abs/2304.04971 ↩︎ Li, Z., Sun, A., \u0026 Li, C. (2023). DiffuRec: A Diffusion Model for Sequential Recommendation. ArXiv. /abs/2304.00686 ↩︎ ","date":"2023-04-17","objectID":"/posts/2023/04/diffusion-for-recsys/:5:0","series":null,"tags":["literature review","recsys","diffusion"],"title":"Diffusion Modeling based Recommender Systems","uri":"/posts/2023/04/diffusion-for-recsys/#references"},{"categories":["Recommender systems"],"content":"Recent developments in Large Language Models (LLMs) have brought a significant paradigm shift in Natural Language Processing (NLP) domain. These pretrained language models encode an extensive amount of world knowledge, and they can be applied to a multitude of downstream NLP applications with zero or just a handful of demonstrations. While existing recommender systems mainly focus on behavior data, large language models encode extensive world knowledge mined from large-scale web corpora. Hence these LLMs store knowledge that can complement the behavior data. For example, an LLM-based system, like ChatGPT, can easily recommend buying turkey on Thanksgiving day, in a zero-shot manner, even without having click behavior data related to turkeys or Thanksgiving. Many researchers have recently proposed different approaches to building recommender systems using LLMs. These methods convert different recommendation tasks into either language understanding or language generation templates. This article highlights the prominent work done on this theme. ","date":"2023-04-10","objectID":"/posts/2023/04/llm-for-recsys/:0:0","series":null,"tags":["literature review","recsys","LLM"],"title":"Zero and Few Shot Recommender Systems based on Large Language Models","uri":"/posts/2023/04/llm-for-recsys/#"},{"categories":["Recommender systems"],"content":"\rWhy should you use LLMs for building recommendation systems?A lot of characteristics of LLMs make them a great choice for building recommender systems. By expressing user behavior data as a part of the task descriptions (aka prompts), the knowledge stored in LLM parameters can be used to generate personalized recommendations. LLMs’ reasoning capability can infer user interest from the context provided through prompts. Research work done in zero and few-shot domain adaptions for LLMs have proven their ability to effectively adapt to downstream domains. This allows startups and large businesses to expand into new domains and set up recommendation applications even with a limited amount of task-specific data. Large-scale recommender systems are often built as a multi-step cascade pipeline. Using a single LLM framework for recommendations can help in unifying common improvements, like bias reduction, which are usually tuned at each step. It can even minimize the carbon footprint by avoiding training a separate model for each downstream task. In the majority of workflows, these different recommendation tasks share a common user-item pool and have overlapping contextual features. Hence they have a high likelihood of benefitting from unified frameworks for representing inputs and jointly learning from them. Such a learned model will also likely generalize well on unseen tasks. Their interactive nature can help with model explainability. Their feedback mechanism can help enhance the overall user experience. Next, we will see various ways in which researchers have proposed to use LLMs for recommender systems. ","date":"2023-04-10","objectID":"/posts/2023/04/llm-for-recsys/:1:0","series":null,"tags":["literature review","recsys","LLM"],"title":"Zero and Few Shot Recommender Systems based on Large Language Models","uri":"/posts/2023/04/llm-for-recsys/#why-should-you-use-llms-for-building-recommendation-systems"},{"categories":["Recommender systems"],"content":"\rPretraining Foundation Language Model on Recommendation TasksIn Pretrain, Personalized Prompt \u0026 Predict Paradigm (P5), Geng et al.1 proposed a unified text-to-text paradigm for building recommender systems. Their framework combines five types of recommendation tasks: Sequential Recommendation, Rating Prediction, Explanation Generation, Review Summarization, and Direct Recommendation. During pretraining, all of these tasks are learned using the same language modeling objective. All of the available data, such as user information, item metadata, user reviews, and user-item interactions are converted to natural language sequences. This rich information in multitask learning setup helps P5 to learn the semantic information required for making personalized recommendations. P5 Model\rAs shown above, P5 learns these related recommendation tasks together with a unified sequence-to-sequence framework by formulating tasks in prompt-based natural language format. The features input is fed to the model using adaptive personalized prompt templates. P5 treats all personalized tasks as a conditional text generation problem and solves them using an encoder-decoder Transformer model pretrained with instruction-based prompts. These personalized prompts have fields for user and item information and can be further categorized into three prompt templates. The following figure shows examples of each prompt template built using the beauty products category from an Amazon dataset. P5: Personalized Prompt Templates\rUsing the above prompt templates, all input and output data is converted to the same token sequence format. The following figure shows P5’s Transformer model architecture along with an example prompt What star rating do you think user_23 will give item_7391? as input. Note that instead of the sentence-piece tokenization process that divides the user_23 token to user, _, and 23, an independent extra token \u003cuser_23\u003e can also be used, however, this approach will incur a significant amount of extra compute. P5 learns to minimize the negative log-likelihood of target tokens given the input tokens along with different performance metrics for different tasks. Note that P5 uses pretrained T5 model checkpoints as the backbone. P5: Encoder-Decoder Architecture\rThe shared task framework enables knowledge generalization such that P5 can serve as a foundation model for various unseen downstream recommendation tasks in a zero-shot or few-shot manner with novel downstream prompts. Through experiments, authors exhibit P5’s zero-shot generalization capability through transfer to unseen personalized prompts and new items in new domains. ","date":"2023-04-10","objectID":"/posts/2023/04/llm-for-recsys/:2:0","series":null,"tags":["literature review","recsys","LLM"],"title":"Zero and Few Shot Recommender Systems based on Large Language Models","uri":"/posts/2023/04/llm-for-recsys/#pretraining-foundation-language-model-on-recommendation-tasks"},{"categories":["Recommender systems"],"content":"\rFine-Tuning Pretrained Language ModelsCui et al.2 built a foundation model, M6-Rec, using Alibaba’s pretrained LLM called M6. To train the model, they added a negligible amount (1%) of task-specific parameters without making changes to the original M6 Transformer model. The following diagram shows the M6 architecture that takes sentence inputs to its bidirectional and autoregressive regions and calculates autoregressive loss on the output of the autoregressive region. M6 Model Architecture\rThe authors represented user behavior as plain texts and converted various recommendation tasks, such as retrieval, scoring, explanation generation, conversational recommendation, etc. to either language understanding or generation. To further adapt to open-domain recommendations, M6-Rec avoids using user IDs or item IDs as input. This enables M6-Rec to be applied to target domains that consist of unseen items. For scoring tasks, such as click-through rate prediction and conversion rate prediction, M6-Rec uses the following template. prompt\r[BOS′] December. Beijing, China. Cold weather. A male user in their early twenties searched “winter stuff” 23 minutes ago, clicked a product of the category “jacket” named “men’s lightweight warm winter hooded jacket” 19 minutes ago, clicked … [EOS’] [BOS] The user is now recommended a product of category “boots” named “waterproof hiking shoes men’s outdoor”. The product has had a high population-level CTR in the past 14 days, among the top 5%. The user clicked the category 4 times in the last 2 years. [EOS]\rThe text between [BOS'] and [EOS'] tokens represents user-side features and is fed to the bidirectional region, the text between [BOS] and [EOS] represents items as well as information common to both user and items. For generation tasks, such as product design and explainable recommendations, it uses the following template. prompt\r[BOS’] … [EOS’] [BOS] The user now purchases a product of category “…” named “…”. Product details: … The user likes it because … [EOS]\rThis prompt is populated using information mined from the product description and user’s review. For zero-shot scoring tasks, for example, for a user who clicks hiking shoes prefers trekking poles or yoga knee pads, M6-Rec uses the following template: prompt\r[BOS’] A user clicks hiking shoes [EOS’] [BOS] also clicks trekking poles [EOS] [BOS’] A user clicks hiking shoes [EOS’] [BOS] also clicks yoga knee pads [EOS] The plausibility of each is calculated based on token probability from M6. Similarly, for retrieval tasks, M6-Rec feeds user and item prompt information separately to the two regions followed by a nearest-neighbor approach over learned representations. The authors also showed that the trained model could be deployed to edge devices in Alipay through various optimizations such as pruning, early exiting, 8-bit quantization, and parameter sharing. ","date":"2023-04-10","objectID":"/posts/2023/04/llm-for-recsys/:3:0","series":null,"tags":["literature review","recsys","LLM"],"title":"Zero and Few Shot Recommender Systems based on Large Language Models","uri":"/posts/2023/04/llm-for-recsys/#fine-tuning-pretrained-language-models"},{"categories":["Recommender systems"],"content":"\rPrompt-Tuning Pretrained Language ModelsSeveral researchers have proposed various prompt-based fine-tuning methods that enable downstream adaption by using a few examples without any gradient updates or fine-tuning. ","date":"2023-04-10","objectID":"/posts/2023/04/llm-for-recsys/:4:0","series":null,"tags":["literature review","recsys","LLM"],"title":"Zero and Few Shot Recommender Systems based on Large Language Models","uri":"/posts/2023/04/llm-for-recsys/#prompt-tuning-pretrained-language-models"},{"categories":["Recommender systems"],"content":"\rLMRecSysIn Language Model Recommender Systems (LMRecSys), Zhang et al.3 use prompts to reformulate the session-based recommendation task as a multi-token cloze task. They use the user’s past interaction history (watched movies), taken from the MovieLens-1M dataset, to predict the next movie that the user would watch. First, they use a rule-based system to create a personalized prompt, similar to the one shown below. prompt\rA user watched One Flew Over the Cuckoo’s Nest, James and the Giant Peach, and My Fair Lady. Now the user wants to watch [MASK].\rNext, they use a GPT-2 to estimate the probability distribution of the next item by multi-token inference. The probability of the ground-truth item is maximized using a cross-entropy loss. MRR@K and Recall@K metrics are used for evaluation. Through experiments, they showed that in zero-shot settings, the pretrained language model (PLM) outperformed the random recommendations baseline, but also had linguistic biases. In few-shot settings, the PLM underperformed the GRU4Rec baseline. ","date":"2023-04-10","objectID":"/posts/2023/04/llm-for-recsys/:4:1","series":null,"tags":["literature review","recsys","LLM"],"title":"Zero and Few Shot Recommender Systems based on Large Language Models","uri":"/posts/2023/04/llm-for-recsys/#lmrecsys"},{"categories":["Recommender systems"],"content":"\rNIRIn Zero-Shot Next-Item Recommendation (NIR), Wang et al.4 proposed a prompting strategy to perform next-item recommendations in a zero-shot setting. Their method works in the following three steps. Candidate Generation: The authors argue that a PLM, like GPT-3, will exhibit poor performance when given a zero-shot prompt like: Based on the movies I have watched, including..., can you recommend 10 movies to me? because the recommendation space is large. Hence, in their experiment, they used a user-filtering or item-filtering-based external recommendation system component to first construct a candidate set of items. Three-step Prompting: This step uses GPT-3 and three prompts. First, they prompt GPT-3 to summarize the user’s preferences using the items from the watch history. Next, they use GPT-3’s response and the candidate movies in a prompt to request GPT-3 to select representative movies in descending order of preference. Finally, they create a recommendation prompt to guide GPT-3 to recommend 10 movies from the candidate set that is the most similar to the representative movies. Answer Extraction: They use a rule-based extraction method to get the recommended items from GPT-3’s final response. The extracted items can be used in downstream applications, or for performance evaluation. Zero-shot NIR Prompts\r","date":"2023-04-10","objectID":"/posts/2023/04/llm-for-recsys/:4:2","series":null,"tags":["literature review","recsys","LLM"],"title":"Zero and Few Shot Recommender Systems based on Large Language Models","uri":"/posts/2023/04/llm-for-recsys/#nir"},{"categories":["Recommender systems"],"content":"\rChat-RECIn Chat-GPT Augmented Recommender System (Chat-REC), Gao et al.5 used an LLM to augment their conversational recommender system. They first created prompts using user profiles, like age, gender, location, interests, etc., historical interactions, like clicked, purchased, or rated items, and history of dialogues (optional). The LLM is then asked to summarize user preferences. Next, a conventional recommender system is used to generate a large set of candidate items, and finally, LLM narrows down the candidate set to generate the final set of recommendations. Overview of Chat-Rec\rThe authors leverage OpenAI’s Chat-GPT as their recommender system interface to enable multi-round recommendations, interactivity, and explainability. To tackle the cold item recommendations (note that current ChatGPT’s training data is capped till September 2021), they recommend using an item-to-item similarity approach using an external source of current information. In their experiments, Chat-REC exhibited good performance on zero-shot and cross-domain recommendation tasks. ","date":"2023-04-10","objectID":"/posts/2023/04/llm-for-recsys/:4:3","series":null,"tags":["literature review","recsys","LLM"],"title":"Zero and Few Shot Recommender Systems based on Large Language Models","uri":"/posts/2023/04/llm-for-recsys/#chat-rec"},{"categories":["Recommender systems"],"content":"\rSummaryLanguage grounding is a powerful medium to describe different tasks and modalities in the form of natural language. Pretrained language models (PLMs) have shown remarkable performance on natural language tasks. Expressing different recommendation tasks in natural language opens up a world of possibilities for using large language models for effective recommendations. In this article, I summarized various methods that combine language modeling with user behavior data through personalized prompts for building recommender systems. ","date":"2023-04-10","objectID":"/posts/2023/04/llm-for-recsys/:5:0","series":null,"tags":["literature review","recsys","LLM"],"title":"Zero and Few Shot Recommender Systems based on Large Language Models","uri":"/posts/2023/04/llm-for-recsys/#summary"},{"categories":["Recommender systems"],"content":"\rReferences Geng, S., Liu, S., Fu, Z., Ge, Y., \u0026 Zhang, Y. (2022). Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt \u0026 Predict Paradigm (P5). Proceedings of the 16th ACM Conference on Recommender Systems. ↩︎ Cui, Z., Ma, J., Zhou, C., Zhou, J., \u0026 Yang, H. (2022). M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems. ArXiv, abs/2205.08084. ↩︎ Zhang, Y., Ding, H., Shui, Z., Ma, Y., Zou, J.Y., Deoras, A., \u0026 Wang, H. (2021). Language Models as Recommender Systems: Evaluations and Limitations. ↩︎ (2023). Zero-Shot Next-Item Recommendation using Large Pretrained Language Models. ↩︎ Gao, Y., Sheng, T., Xiang, Y., Xiong, Y., Wang, H., \u0026 Zhang, J. (2023). Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System. ↩︎ ","date":"2023-04-10","objectID":"/posts/2023/04/llm-for-recsys/:6:0","series":null,"tags":["literature review","recsys","LLM"],"title":"Zero and Few Shot Recommender Systems based on Large Language Models","uri":"/posts/2023/04/llm-for-recsys/#references"},{"categories":["Recommender systems"],"content":"\rTwitter Releases “The Algorithm”Last week, Twitter released a lot of details about the recommendation algorithm that powers their For You feed. The release included an announcement, two new GitHub repositories (main repo, ml repo), and an accompanying post on the engineering blog. Their release also includes some model configs, hyperparameters, input feature descriptions, feature weights, combined score calculation formula, etc. that are immensely valuable for ML practitioners. This article gives an overview of the end-to-end machine learning components made public by Twitter. Today marks a new era of transparency for Twitter. 🧵 We’re sharing much of the source code that powers our platform with the world. Visit our blog to learn more about this initiative: https://t.co/hTHVpuMDz8 — Twitter Engineering (@TwitterEng) March 31, 2023 ","date":"2023-04-04","objectID":"/posts/2023/04/the-twitter-ml-algo/:1:0","series":null,"tags":["literature review","recsys","ranking"],"title":"Twitter's For You Recommendation Algorithm","uri":"/posts/2023/04/the-twitter-ml-algo/#twitter-releases-the-algorithm"},{"categories":["Recommender systems"],"content":"\rThe AlgorithmAt a broad level, we can think of Twitter’s For You timeline generation as the sequence of following steps that are also common to large-scale recommendation systems. We will look at each of these components one by one. Note that while Twitter has shared a lot of information on the code and algorithms for most of these steps, they haven’t still shared details about components such as input data processing, model configs, serving infrastructure, source code for modules like Candidate Generation, Trust \u0026 Safety, Ads, etc. ","date":"2023-04-04","objectID":"/posts/2023/04/the-twitter-ml-algo/:2:0","series":null,"tags":["literature review","recsys","ranking"],"title":"Twitter's For You Recommendation Algorithm","uri":"/posts/2023/04/the-twitter-ml-algo/#the-algorithm"},{"categories":["Recommender systems"],"content":"\rData SourcingTwitter has previously shared details about its data infrastructure in another blog. Twitter processes close to 500 million daily tweets, and 400 billion real-time events. The event data is sourced from various platforms, storage systems, and distributed databases. The user engagement data is collected from various real-time streams, server, and client-side logs, etc. They also utilize various proprietary tools such as Scalding for batch processing, Heron for streaming, and TSAR (TimeSeries AggregatoR) for processing temporal signals. In their recent redesign (see figure below) proposal1 they built an on-premise event processor along with deduplication and BigTable storage on the Google Cloud Platform (GCP). Note that Twitter exactly specified any data schema or preprocessing applied to the data for the recommender system. Twitter's redesigned data pipeline. Source: [^1]\r","date":"2023-04-04","objectID":"/posts/2023/04/the-twitter-ml-algo/:2:1","series":null,"tags":["literature review","recsys","ranking"],"title":"Twitter's For You Recommendation Algorithm","uri":"/posts/2023/04/the-twitter-ml-algo/#data-sourcing"},{"categories":["Recommender systems"],"content":"\rCandidate GenerationThe job of the candidate generation module is to select 1500 most relevant tweets from various sources. The first half of these candidates are from the people that you follow (in-network tweets), while the other half is from the people who you do not follow (out-of-network tweets). In-Network TweetsMost of the in-network tweets are sourced from Twitter’s RealGraph framework (originally deployed in 20122). RealGraph framework uses a directed, weighted graph to model users (tweeps) as nodes. An edge from source to target node indicates that the source user follows the target user or the target user is in the source user’s contact book, or the source user recently interacted with the target user. Each type of edge has its own set of features represented by weights, such as days since edge creation, days since last interaction, number of common friends, explicit and implicit engagement statistics, etc. Similarly, nodes have features such as the number of retweets in the last week, number of followers, demographics, PageRank on the follow graph, etc. Note that Twitter has also shared the implementation details of its PageRank algorithm called TweepCred. A logistic regression model is trained on all of these features to predict interaction in a future time window. Note that the graph itself is updated daily in batch mode. In their announcement, Twitter also mentioned that the logistic regression model was trained several years ago and is in the process of being redesigned. Twitter's RealGraph Framework. Source: [^2]\rOut-of-Network Tweets\rSocial GraphFor some of the out-of-network tweets, the retrieval process uses a graph processing engine called GraphJet3. GraphJet maintains a dynamic (real-time), undirected, bipartite graph between users and tweets in memory on a single server (no partitioning!) through clever edge encoding and dynamic memory allocation schemes. The nodes in the graph represent the users and the tweets, while the edges represent the actions (favorite, tweet, retweet, reply, click, etc.) within a rolling temporal window. To ensure the efficient growth of the graph structure, GraphJet maintains a list of temporally-ordered index segments such that only the latest segment can be written into (see the figure below). GraphJet Architecture. Source: [^3]\rA SALSA (Stochastic Approach for Link-Structure Analysis) random walk over this graph is done to get the tweets that a might be interested in (for example, tweets that the user’s connection recently engaged with). A seed set of user nodes called the circle of trust (personalized PageRank), is used for cold users or users with no recent engagement history. Similar to RealGraph, a logistic regression model is used to predict the probability of engagement. GraphJet is implemented in Java, and powers about 15% of tweets in the For You feed. Embedding-based Semantic SpacesThe majority of out-of-network tweets are retrieved through embedding-based methods described below. SimClustersTwitter has used its SimClusters4 embedding space to identify 145,000 communities that are either based on a common theme (such as “machine learning”, or “K-pop”), or social relationships (such as users who went to the same school). Using SimClusters, Twitter has a representation layer where both users and content are mapped in the same space. First, using a user-user graph (based on follow relationship) k number of possible overlapping communities are discovered. While this can be technically done using a sparse non-negative matrix factorization (NMF) technique, Twitter devised a custom matrix factorization technique (called Sparse Binary Factorization or SBF) that can also scale for large graphs. Next, an item representation can be calculated using a simple aggregation, such as an exponentially time-decayed average, over the representations of all the users who engaged with that item (see the figure below). Overview of SimClusters. Source: [^4]\rTwitter retrieves out-of-networ","date":"2023-04-04","objectID":"/posts/2023/04/the-twitter-ml-algo/:2:2","series":null,"tags":["literature review","recsys","ranking"],"title":"Twitter's For You Recommendation Algorithm","uri":"/posts/2023/04/the-twitter-ml-algo/#candidate-generation"},{"categories":["Recommender systems"],"content":"\rCandidate GenerationThe job of the candidate generation module is to select 1500 most relevant tweets from various sources. The first half of these candidates are from the people that you follow (in-network tweets), while the other half is from the people who you do not follow (out-of-network tweets). In-Network TweetsMost of the in-network tweets are sourced from Twitter’s RealGraph framework (originally deployed in 20122). RealGraph framework uses a directed, weighted graph to model users (tweeps) as nodes. An edge from source to target node indicates that the source user follows the target user or the target user is in the source user’s contact book, or the source user recently interacted with the target user. Each type of edge has its own set of features represented by weights, such as days since edge creation, days since last interaction, number of common friends, explicit and implicit engagement statistics, etc. Similarly, nodes have features such as the number of retweets in the last week, number of followers, demographics, PageRank on the follow graph, etc. Note that Twitter has also shared the implementation details of its PageRank algorithm called TweepCred. A logistic regression model is trained on all of these features to predict interaction in a future time window. Note that the graph itself is updated daily in batch mode. In their announcement, Twitter also mentioned that the logistic regression model was trained several years ago and is in the process of being redesigned. Twitter's RealGraph Framework. Source: [^2]\rOut-of-Network Tweets\rSocial GraphFor some of the out-of-network tweets, the retrieval process uses a graph processing engine called GraphJet3. GraphJet maintains a dynamic (real-time), undirected, bipartite graph between users and tweets in memory on a single server (no partitioning!) through clever edge encoding and dynamic memory allocation schemes. The nodes in the graph represent the users and the tweets, while the edges represent the actions (favorite, tweet, retweet, reply, click, etc.) within a rolling temporal window. To ensure the efficient growth of the graph structure, GraphJet maintains a list of temporally-ordered index segments such that only the latest segment can be written into (see the figure below). GraphJet Architecture. Source: [^3]\rA SALSA (Stochastic Approach for Link-Structure Analysis) random walk over this graph is done to get the tweets that a might be interested in (for example, tweets that the user’s connection recently engaged with). A seed set of user nodes called the circle of trust (personalized PageRank), is used for cold users or users with no recent engagement history. Similar to RealGraph, a logistic regression model is used to predict the probability of engagement. GraphJet is implemented in Java, and powers about 15% of tweets in the For You feed. Embedding-based Semantic SpacesThe majority of out-of-network tweets are retrieved through embedding-based methods described below. SimClustersTwitter has used its SimClusters4 embedding space to identify 145,000 communities that are either based on a common theme (such as “machine learning”, or “K-pop”), or social relationships (such as users who went to the same school). Using SimClusters, Twitter has a representation layer where both users and content are mapped in the same space. First, using a user-user graph (based on follow relationship) k number of possible overlapping communities are discovered. While this can be technically done using a sparse non-negative matrix factorization (NMF) technique, Twitter devised a custom matrix factorization technique (called Sparse Binary Factorization or SBF) that can also scale for large graphs. Next, an item representation can be calculated using a simple aggregation, such as an exponentially time-decayed average, over the representations of all the users who engaged with that item (see the figure below). Overview of SimClusters. Source: [^4]\rTwitter retrieves out-of-networ","date":"2023-04-04","objectID":"/posts/2023/04/the-twitter-ml-algo/:2:2","series":null,"tags":["literature review","recsys","ranking"],"title":"Twitter's For You Recommendation Algorithm","uri":"/posts/2023/04/the-twitter-ml-algo/#in-network-tweets"},{"categories":["Recommender systems"],"content":"\rCandidate GenerationThe job of the candidate generation module is to select 1500 most relevant tweets from various sources. The first half of these candidates are from the people that you follow (in-network tweets), while the other half is from the people who you do not follow (out-of-network tweets). In-Network TweetsMost of the in-network tweets are sourced from Twitter’s RealGraph framework (originally deployed in 20122). RealGraph framework uses a directed, weighted graph to model users (tweeps) as nodes. An edge from source to target node indicates that the source user follows the target user or the target user is in the source user’s contact book, or the source user recently interacted with the target user. Each type of edge has its own set of features represented by weights, such as days since edge creation, days since last interaction, number of common friends, explicit and implicit engagement statistics, etc. Similarly, nodes have features such as the number of retweets in the last week, number of followers, demographics, PageRank on the follow graph, etc. Note that Twitter has also shared the implementation details of its PageRank algorithm called TweepCred. A logistic regression model is trained on all of these features to predict interaction in a future time window. Note that the graph itself is updated daily in batch mode. In their announcement, Twitter also mentioned that the logistic regression model was trained several years ago and is in the process of being redesigned. Twitter's RealGraph Framework. Source: [^2]\rOut-of-Network Tweets\rSocial GraphFor some of the out-of-network tweets, the retrieval process uses a graph processing engine called GraphJet3. GraphJet maintains a dynamic (real-time), undirected, bipartite graph between users and tweets in memory on a single server (no partitioning!) through clever edge encoding and dynamic memory allocation schemes. The nodes in the graph represent the users and the tweets, while the edges represent the actions (favorite, tweet, retweet, reply, click, etc.) within a rolling temporal window. To ensure the efficient growth of the graph structure, GraphJet maintains a list of temporally-ordered index segments such that only the latest segment can be written into (see the figure below). GraphJet Architecture. Source: [^3]\rA SALSA (Stochastic Approach for Link-Structure Analysis) random walk over this graph is done to get the tweets that a might be interested in (for example, tweets that the user’s connection recently engaged with). A seed set of user nodes called the circle of trust (personalized PageRank), is used for cold users or users with no recent engagement history. Similar to RealGraph, a logistic regression model is used to predict the probability of engagement. GraphJet is implemented in Java, and powers about 15% of tweets in the For You feed. Embedding-based Semantic SpacesThe majority of out-of-network tweets are retrieved through embedding-based methods described below. SimClustersTwitter has used its SimClusters4 embedding space to identify 145,000 communities that are either based on a common theme (such as “machine learning”, or “K-pop”), or social relationships (such as users who went to the same school). Using SimClusters, Twitter has a representation layer where both users and content are mapped in the same space. First, using a user-user graph (based on follow relationship) k number of possible overlapping communities are discovered. While this can be technically done using a sparse non-negative matrix factorization (NMF) technique, Twitter devised a custom matrix factorization technique (called Sparse Binary Factorization or SBF) that can also scale for large graphs. Next, an item representation can be calculated using a simple aggregation, such as an exponentially time-decayed average, over the representations of all the users who engaged with that item (see the figure below). Overview of SimClusters. Source: [^4]\rTwitter retrieves out-of-networ","date":"2023-04-04","objectID":"/posts/2023/04/the-twitter-ml-algo/:2:2","series":null,"tags":["literature review","recsys","ranking"],"title":"Twitter's For You Recommendation Algorithm","uri":"/posts/2023/04/the-twitter-ml-algo/#out-of-network-tweets"},{"categories":["Recommender systems"],"content":"\rCandidate GenerationThe job of the candidate generation module is to select 1500 most relevant tweets from various sources. The first half of these candidates are from the people that you follow (in-network tweets), while the other half is from the people who you do not follow (out-of-network tweets). In-Network TweetsMost of the in-network tweets are sourced from Twitter’s RealGraph framework (originally deployed in 20122). RealGraph framework uses a directed, weighted graph to model users (tweeps) as nodes. An edge from source to target node indicates that the source user follows the target user or the target user is in the source user’s contact book, or the source user recently interacted with the target user. Each type of edge has its own set of features represented by weights, such as days since edge creation, days since last interaction, number of common friends, explicit and implicit engagement statistics, etc. Similarly, nodes have features such as the number of retweets in the last week, number of followers, demographics, PageRank on the follow graph, etc. Note that Twitter has also shared the implementation details of its PageRank algorithm called TweepCred. A logistic regression model is trained on all of these features to predict interaction in a future time window. Note that the graph itself is updated daily in batch mode. In their announcement, Twitter also mentioned that the logistic regression model was trained several years ago and is in the process of being redesigned. Twitter's RealGraph Framework. Source: [^2]\rOut-of-Network Tweets\rSocial GraphFor some of the out-of-network tweets, the retrieval process uses a graph processing engine called GraphJet3. GraphJet maintains a dynamic (real-time), undirected, bipartite graph between users and tweets in memory on a single server (no partitioning!) through clever edge encoding and dynamic memory allocation schemes. The nodes in the graph represent the users and the tweets, while the edges represent the actions (favorite, tweet, retweet, reply, click, etc.) within a rolling temporal window. To ensure the efficient growth of the graph structure, GraphJet maintains a list of temporally-ordered index segments such that only the latest segment can be written into (see the figure below). GraphJet Architecture. Source: [^3]\rA SALSA (Stochastic Approach for Link-Structure Analysis) random walk over this graph is done to get the tweets that a might be interested in (for example, tweets that the user’s connection recently engaged with). A seed set of user nodes called the circle of trust (personalized PageRank), is used for cold users or users with no recent engagement history. Similar to RealGraph, a logistic regression model is used to predict the probability of engagement. GraphJet is implemented in Java, and powers about 15% of tweets in the For You feed. Embedding-based Semantic SpacesThe majority of out-of-network tweets are retrieved through embedding-based methods described below. SimClustersTwitter has used its SimClusters4 embedding space to identify 145,000 communities that are either based on a common theme (such as “machine learning”, or “K-pop”), or social relationships (such as users who went to the same school). Using SimClusters, Twitter has a representation layer where both users and content are mapped in the same space. First, using a user-user graph (based on follow relationship) k number of possible overlapping communities are discovered. While this can be technically done using a sparse non-negative matrix factorization (NMF) technique, Twitter devised a custom matrix factorization technique (called Sparse Binary Factorization or SBF) that can also scale for large graphs. Next, an item representation can be calculated using a simple aggregation, such as an exponentially time-decayed average, over the representations of all the users who engaged with that item (see the figure below). Overview of SimClusters. Source: [^4]\rTwitter retrieves out-of-networ","date":"2023-04-04","objectID":"/posts/2023/04/the-twitter-ml-algo/:2:2","series":null,"tags":["literature review","recsys","ranking"],"title":"Twitter's For You Recommendation Algorithm","uri":"/posts/2023/04/the-twitter-ml-algo/#social-graph"},{"categories":["Recommender systems"],"content":"\rCandidate GenerationThe job of the candidate generation module is to select 1500 most relevant tweets from various sources. The first half of these candidates are from the people that you follow (in-network tweets), while the other half is from the people who you do not follow (out-of-network tweets). In-Network TweetsMost of the in-network tweets are sourced from Twitter’s RealGraph framework (originally deployed in 20122). RealGraph framework uses a directed, weighted graph to model users (tweeps) as nodes. An edge from source to target node indicates that the source user follows the target user or the target user is in the source user’s contact book, or the source user recently interacted with the target user. Each type of edge has its own set of features represented by weights, such as days since edge creation, days since last interaction, number of common friends, explicit and implicit engagement statistics, etc. Similarly, nodes have features such as the number of retweets in the last week, number of followers, demographics, PageRank on the follow graph, etc. Note that Twitter has also shared the implementation details of its PageRank algorithm called TweepCred. A logistic regression model is trained on all of these features to predict interaction in a future time window. Note that the graph itself is updated daily in batch mode. In their announcement, Twitter also mentioned that the logistic regression model was trained several years ago and is in the process of being redesigned. Twitter's RealGraph Framework. Source: [^2]\rOut-of-Network Tweets\rSocial GraphFor some of the out-of-network tweets, the retrieval process uses a graph processing engine called GraphJet3. GraphJet maintains a dynamic (real-time), undirected, bipartite graph between users and tweets in memory on a single server (no partitioning!) through clever edge encoding and dynamic memory allocation schemes. The nodes in the graph represent the users and the tweets, while the edges represent the actions (favorite, tweet, retweet, reply, click, etc.) within a rolling temporal window. To ensure the efficient growth of the graph structure, GraphJet maintains a list of temporally-ordered index segments such that only the latest segment can be written into (see the figure below). GraphJet Architecture. Source: [^3]\rA SALSA (Stochastic Approach for Link-Structure Analysis) random walk over this graph is done to get the tweets that a might be interested in (for example, tweets that the user’s connection recently engaged with). A seed set of user nodes called the circle of trust (personalized PageRank), is used for cold users or users with no recent engagement history. Similar to RealGraph, a logistic regression model is used to predict the probability of engagement. GraphJet is implemented in Java, and powers about 15% of tweets in the For You feed. Embedding-based Semantic SpacesThe majority of out-of-network tweets are retrieved through embedding-based methods described below. SimClustersTwitter has used its SimClusters4 embedding space to identify 145,000 communities that are either based on a common theme (such as “machine learning”, or “K-pop”), or social relationships (such as users who went to the same school). Using SimClusters, Twitter has a representation layer where both users and content are mapped in the same space. First, using a user-user graph (based on follow relationship) k number of possible overlapping communities are discovered. While this can be technically done using a sparse non-negative matrix factorization (NMF) technique, Twitter devised a custom matrix factorization technique (called Sparse Binary Factorization or SBF) that can also scale for large graphs. Next, an item representation can be calculated using a simple aggregation, such as an exponentially time-decayed average, over the representations of all the users who engaged with that item (see the figure below). Overview of SimClusters. Source: [^4]\rTwitter retrieves out-of-networ","date":"2023-04-04","objectID":"/posts/2023/04/the-twitter-ml-algo/:2:2","series":null,"tags":["literature review","recsys","ranking"],"title":"Twitter's For You Recommendation Algorithm","uri":"/posts/2023/04/the-twitter-ml-algo/#embedding-based-semantic-spaces"},{"categories":["Recommender systems"],"content":"\rCandidate GenerationThe job of the candidate generation module is to select 1500 most relevant tweets from various sources. The first half of these candidates are from the people that you follow (in-network tweets), while the other half is from the people who you do not follow (out-of-network tweets). In-Network TweetsMost of the in-network tweets are sourced from Twitter’s RealGraph framework (originally deployed in 20122). RealGraph framework uses a directed, weighted graph to model users (tweeps) as nodes. An edge from source to target node indicates that the source user follows the target user or the target user is in the source user’s contact book, or the source user recently interacted with the target user. Each type of edge has its own set of features represented by weights, such as days since edge creation, days since last interaction, number of common friends, explicit and implicit engagement statistics, etc. Similarly, nodes have features such as the number of retweets in the last week, number of followers, demographics, PageRank on the follow graph, etc. Note that Twitter has also shared the implementation details of its PageRank algorithm called TweepCred. A logistic regression model is trained on all of these features to predict interaction in a future time window. Note that the graph itself is updated daily in batch mode. In their announcement, Twitter also mentioned that the logistic regression model was trained several years ago and is in the process of being redesigned. Twitter's RealGraph Framework. Source: [^2]\rOut-of-Network Tweets\rSocial GraphFor some of the out-of-network tweets, the retrieval process uses a graph processing engine called GraphJet3. GraphJet maintains a dynamic (real-time), undirected, bipartite graph between users and tweets in memory on a single server (no partitioning!) through clever edge encoding and dynamic memory allocation schemes. The nodes in the graph represent the users and the tweets, while the edges represent the actions (favorite, tweet, retweet, reply, click, etc.) within a rolling temporal window. To ensure the efficient growth of the graph structure, GraphJet maintains a list of temporally-ordered index segments such that only the latest segment can be written into (see the figure below). GraphJet Architecture. Source: [^3]\rA SALSA (Stochastic Approach for Link-Structure Analysis) random walk over this graph is done to get the tweets that a might be interested in (for example, tweets that the user’s connection recently engaged with). A seed set of user nodes called the circle of trust (personalized PageRank), is used for cold users or users with no recent engagement history. Similar to RealGraph, a logistic regression model is used to predict the probability of engagement. GraphJet is implemented in Java, and powers about 15% of tweets in the For You feed. Embedding-based Semantic SpacesThe majority of out-of-network tweets are retrieved through embedding-based methods described below. SimClustersTwitter has used its SimClusters4 embedding space to identify 145,000 communities that are either based on a common theme (such as “machine learning”, or “K-pop”), or social relationships (such as users who went to the same school). Using SimClusters, Twitter has a representation layer where both users and content are mapped in the same space. First, using a user-user graph (based on follow relationship) k number of possible overlapping communities are discovered. While this can be technically done using a sparse non-negative matrix factorization (NMF) technique, Twitter devised a custom matrix factorization technique (called Sparse Binary Factorization or SBF) that can also scale for large graphs. Next, an item representation can be calculated using a simple aggregation, such as an exponentially time-decayed average, over the representations of all the users who engaged with that item (see the figure below). Overview of SimClusters. Source: [^4]\rTwitter retrieves out-of-networ","date":"2023-04-04","objectID":"/posts/2023/04/the-twitter-ml-algo/:2:2","series":null,"tags":["literature review","recsys","ranking"],"title":"Twitter's For You Recommendation Algorithm","uri":"/posts/2023/04/the-twitter-ml-algo/#simclusters"},{"categories":["Recommender systems"],"content":"\rCandidate GenerationThe job of the candidate generation module is to select 1500 most relevant tweets from various sources. The first half of these candidates are from the people that you follow (in-network tweets), while the other half is from the people who you do not follow (out-of-network tweets). In-Network TweetsMost of the in-network tweets are sourced from Twitter’s RealGraph framework (originally deployed in 20122). RealGraph framework uses a directed, weighted graph to model users (tweeps) as nodes. An edge from source to target node indicates that the source user follows the target user or the target user is in the source user’s contact book, or the source user recently interacted with the target user. Each type of edge has its own set of features represented by weights, such as days since edge creation, days since last interaction, number of common friends, explicit and implicit engagement statistics, etc. Similarly, nodes have features such as the number of retweets in the last week, number of followers, demographics, PageRank on the follow graph, etc. Note that Twitter has also shared the implementation details of its PageRank algorithm called TweepCred. A logistic regression model is trained on all of these features to predict interaction in a future time window. Note that the graph itself is updated daily in batch mode. In their announcement, Twitter also mentioned that the logistic regression model was trained several years ago and is in the process of being redesigned. Twitter's RealGraph Framework. Source: [^2]\rOut-of-Network Tweets\rSocial GraphFor some of the out-of-network tweets, the retrieval process uses a graph processing engine called GraphJet3. GraphJet maintains a dynamic (real-time), undirected, bipartite graph between users and tweets in memory on a single server (no partitioning!) through clever edge encoding and dynamic memory allocation schemes. The nodes in the graph represent the users and the tweets, while the edges represent the actions (favorite, tweet, retweet, reply, click, etc.) within a rolling temporal window. To ensure the efficient growth of the graph structure, GraphJet maintains a list of temporally-ordered index segments such that only the latest segment can be written into (see the figure below). GraphJet Architecture. Source: [^3]\rA SALSA (Stochastic Approach for Link-Structure Analysis) random walk over this graph is done to get the tweets that a might be interested in (for example, tweets that the user’s connection recently engaged with). A seed set of user nodes called the circle of trust (personalized PageRank), is used for cold users or users with no recent engagement history. Similar to RealGraph, a logistic regression model is used to predict the probability of engagement. GraphJet is implemented in Java, and powers about 15% of tweets in the For You feed. Embedding-based Semantic SpacesThe majority of out-of-network tweets are retrieved through embedding-based methods described below. SimClustersTwitter has used its SimClusters4 embedding space to identify 145,000 communities that are either based on a common theme (such as “machine learning”, or “K-pop”), or social relationships (such as users who went to the same school). Using SimClusters, Twitter has a representation layer where both users and content are mapped in the same space. First, using a user-user graph (based on follow relationship) k number of possible overlapping communities are discovered. While this can be technically done using a sparse non-negative matrix factorization (NMF) technique, Twitter devised a custom matrix factorization technique (called Sparse Binary Factorization or SBF) that can also scale for large graphs. Next, an item representation can be calculated using a simple aggregation, such as an exponentially time-decayed average, over the representations of all the users who engaged with that item (see the figure below). Overview of SimClusters. Source: [^4]\rTwitter retrieves out-of-networ","date":"2023-04-04","objectID":"/posts/2023/04/the-twitter-ml-algo/:2:2","series":null,"tags":["literature review","recsys","ranking"],"title":"Twitter's For You Recommendation Algorithm","uri":"/posts/2023/04/the-twitter-ml-algo/#twhin"},{"categories":["Recommender systems"],"content":"\rRanking\rLight RankingA simple and efficient logistic-regression-based light ranker is used for ranking both in-network and out-of-network tweets. Twitter’s core retrieval system (called EarlyBird6) builds and maintains inverted indexes required to perform the real-time search and retrieval of tweets. Instead of relying on the full social graph, In earlier iterations, EarlyBird utilized Bloom Filter to compress relevant social graph information for each user. Apart from being space-efficient, this implementation also enabled constant-time set-membership operations. The logistic regression model uses various static and real-time features listed here. Twitter notes that “the current model was last trained several years ago, and uses some very strange features.” and they are working on training a new model. The source code for this light ranking is released here, along with the details about input features(in-network, out-of-network), feature weights, and time decay used by the model. It is mentioned that EarlyBird ranks a lesser number of tweets than heavy ranker, but the exact order and counts corresponding to light ranker execution are not specified. Heavy RankingTwitter uses MaskNet7 to perform computationally costlier ranking for the 1500 retrieved tweets. Similar to a lot of other rankers in the Click-Through Rate (CTR) estimation domain, MaskNet emphasizes effectively capturing the high-order feature interactions in an explicit manner similar to the earlier Deep \u0026 Cross Network (DCN). MaskNet introduces a basic building block called a MaskBlock that contains three components: an “instance-guided mask”, a feed-forward hidden layer, and a layer normalization module. The instance-guided mask performs element-wise product on feature embeddings and feed-forward layers to highlight the global context information. Twitter uses the parallel MaskNet variant (~48M parameters) where multiple MaskBlocks on feature embeddings are applied in parallel (similar to MMoE design). Instance-guided Mask(left). Parallel variant of MaskNet (right). Souce: [^7]\rTwitter has released its source code for MaskNet implementation here, along with model hyperparameters, and the details of aggregate, non-aggregate, and embedding input features used by the model. MaskNet outputs 10 different probabilities, and the final score calculation is calculated by weighing the output probabilities as shown below. These weights are hard coded and were likely fixed after a lot of A/B testing. $$ Combined Score = P(favorite) \\times 0.5 + max(P(good click and reply or like), P(good click and stay for at least 2 minutes)) \\times 11 + P(negative reaction) \\times -74 + P(author profile clicked and profile engaged) \\times 12 + P(reply) \\times 27 + P(reply and author engaged) \\times 75 + P(report) \\times -369 + P(retweet) * 1 + P(watch half the video) \\times 0.005 $$ ","date":"2023-04-04","objectID":"/posts/2023/04/the-twitter-ml-algo/:2:3","series":null,"tags":["literature review","recsys","ranking"],"title":"Twitter's For You Recommendation Algorithm","uri":"/posts/2023/04/the-twitter-ml-algo/#ranking"},{"categories":["Recommender systems"],"content":"\rRanking\rLight RankingA simple and efficient logistic-regression-based light ranker is used for ranking both in-network and out-of-network tweets. Twitter’s core retrieval system (called EarlyBird6) builds and maintains inverted indexes required to perform the real-time search and retrieval of tweets. Instead of relying on the full social graph, In earlier iterations, EarlyBird utilized Bloom Filter to compress relevant social graph information for each user. Apart from being space-efficient, this implementation also enabled constant-time set-membership operations. The logistic regression model uses various static and real-time features listed here. Twitter notes that “the current model was last trained several years ago, and uses some very strange features.” and they are working on training a new model. The source code for this light ranking is released here, along with the details about input features(in-network, out-of-network), feature weights, and time decay used by the model. It is mentioned that EarlyBird ranks a lesser number of tweets than heavy ranker, but the exact order and counts corresponding to light ranker execution are not specified. Heavy RankingTwitter uses MaskNet7 to perform computationally costlier ranking for the 1500 retrieved tweets. Similar to a lot of other rankers in the Click-Through Rate (CTR) estimation domain, MaskNet emphasizes effectively capturing the high-order feature interactions in an explicit manner similar to the earlier Deep \u0026 Cross Network (DCN). MaskNet introduces a basic building block called a MaskBlock that contains three components: an “instance-guided mask”, a feed-forward hidden layer, and a layer normalization module. The instance-guided mask performs element-wise product on feature embeddings and feed-forward layers to highlight the global context information. Twitter uses the parallel MaskNet variant (~48M parameters) where multiple MaskBlocks on feature embeddings are applied in parallel (similar to MMoE design). Instance-guided Mask(left). Parallel variant of MaskNet (right). Souce: [^7]\rTwitter has released its source code for MaskNet implementation here, along with model hyperparameters, and the details of aggregate, non-aggregate, and embedding input features used by the model. MaskNet outputs 10 different probabilities, and the final score calculation is calculated by weighing the output probabilities as shown below. These weights are hard coded and were likely fixed after a lot of A/B testing. $$ Combined Score = P(favorite) \\times 0.5 + max(P(good click and reply or like), P(good click and stay for at least 2 minutes)) \\times 11 + P(negative reaction) \\times -74 + P(author profile clicked and profile engaged) \\times 12 + P(reply) \\times 27 + P(reply and author engaged) \\times 75 + P(report) \\times -369 + P(retweet) * 1 + P(watch half the video) \\times 0.005 $$ ","date":"2023-04-04","objectID":"/posts/2023/04/the-twitter-ml-algo/:2:3","series":null,"tags":["literature review","recsys","ranking"],"title":"Twitter's For You Recommendation Algorithm","uri":"/posts/2023/04/the-twitter-ml-algo/#light-ranking"},{"categories":["Recommender systems"],"content":"\rRanking\rLight RankingA simple and efficient logistic-regression-based light ranker is used for ranking both in-network and out-of-network tweets. Twitter’s core retrieval system (called EarlyBird6) builds and maintains inverted indexes required to perform the real-time search and retrieval of tweets. Instead of relying on the full social graph, In earlier iterations, EarlyBird utilized Bloom Filter to compress relevant social graph information for each user. Apart from being space-efficient, this implementation also enabled constant-time set-membership operations. The logistic regression model uses various static and real-time features listed here. Twitter notes that “the current model was last trained several years ago, and uses some very strange features.” and they are working on training a new model. The source code for this light ranking is released here, along with the details about input features(in-network, out-of-network), feature weights, and time decay used by the model. It is mentioned that EarlyBird ranks a lesser number of tweets than heavy ranker, but the exact order and counts corresponding to light ranker execution are not specified. Heavy RankingTwitter uses MaskNet7 to perform computationally costlier ranking for the 1500 retrieved tweets. Similar to a lot of other rankers in the Click-Through Rate (CTR) estimation domain, MaskNet emphasizes effectively capturing the high-order feature interactions in an explicit manner similar to the earlier Deep \u0026 Cross Network (DCN). MaskNet introduces a basic building block called a MaskBlock that contains three components: an “instance-guided mask”, a feed-forward hidden layer, and a layer normalization module. The instance-guided mask performs element-wise product on feature embeddings and feed-forward layers to highlight the global context information. Twitter uses the parallel MaskNet variant (~48M parameters) where multiple MaskBlocks on feature embeddings are applied in parallel (similar to MMoE design). Instance-guided Mask(left). Parallel variant of MaskNet (right). Souce: [^7]\rTwitter has released its source code for MaskNet implementation here, along with model hyperparameters, and the details of aggregate, non-aggregate, and embedding input features used by the model. MaskNet outputs 10 different probabilities, and the final score calculation is calculated by weighing the output probabilities as shown below. These weights are hard coded and were likely fixed after a lot of A/B testing. $$ Combined Score = P(favorite) \\times 0.5 + max(P(good click and reply or like), P(good click and stay for at least 2 minutes)) \\times 11 + P(negative reaction) \\times -74 + P(author profile clicked and profile engaged) \\times 12 + P(reply) \\times 27 + P(reply and author engaged) \\times 75 + P(report) \\times -369 + P(retweet) * 1 + P(watch half the video) \\times 0.005 $$ ","date":"2023-04-04","objectID":"/posts/2023/04/the-twitter-ml-algo/:2:3","series":null,"tags":["literature review","recsys","ranking"],"title":"Twitter's For You Recommendation Algorithm","uri":"/posts/2023/04/the-twitter-ml-algo/#heavy-ranking"},{"categories":["Recommender systems"],"content":"\rHeuristics \u0026 FilteringTwitter applies a lot of heuristics and filters on the ranked list of tweets to create a balanced and diverse For You feed. The following list contains some prominent examples. The Visibility Filtering module removes various spam along with the tweet from accounts you have blocked or muted. Remove out-of-network competitor URL tweets. (code reference) Infuse diversity by applying a penalty to too many consecutive tweets from the same author. (code reference) Lower the relevance score for tweets with the author or engaged audience for whom the user had given negative feedback in the past. (code reference) Remove out-of-network tweets that do not have a second-degree connection. (code reference) Boost Twitter Blue Verified authors by a factor of 4.0 for in-network tweets and 2.0 for out-of-network tweets. (code reference) ","date":"2023-04-04","objectID":"/posts/2023/04/the-twitter-ml-algo/:2:4","series":null,"tags":["literature review","recsys","ranking"],"title":"Twitter's For You Recommendation Algorithm","uri":"/posts/2023/04/the-twitter-ml-algo/#heuristics--filtering"},{"categories":["Recommender systems"],"content":"\rMixingAs the last step, the Home Mixer system blends together non-tweet content like ads, promotions, follow recommendations, etc. to the list of ranked and filtered tweets. The output of this step is the final feed that gets sent and displayed on the end-user device. ","date":"2023-04-04","objectID":"/posts/2023/04/the-twitter-ml-algo/:2:5","series":null,"tags":["literature review","recsys","ranking"],"title":"Twitter's For You Recommendation Algorithm","uri":"/posts/2023/04/the-twitter-ml-algo/#mixing"},{"categories":["Recommender systems"],"content":"\rSummaryTwitter has open-sourced a majority of its recommendation algorithm. It offers an exciting opportunity for researchers, industry practitioners, and RecSys enthusiasts to take a close look at how Twitter computes the recommended feed for the For You page. This article described Twitter’s end-to-end recommender system along with relevant literature and code references. ","date":"2023-04-04","objectID":"/posts/2023/04/the-twitter-ml-algo/:3:0","series":null,"tags":["literature review","recsys","ranking"],"title":"Twitter's For You Recommendation Algorithm","uri":"/posts/2023/04/the-twitter-ml-algo/#summary"},{"categories":["Recommender systems"],"content":"\rReferences Processing billions of events in real time at Twitter. Lu Zhang, Chukwudiuto Malife, 22 October 2021. https://blog.twitter.com/engineering/en_us/topics/infrastructure/2021/processing-billions-of-events-in-real-time-at-twitter- ↩︎ Kamath, K.Y., Sharma, A., Wang, D., \u0026 Yin, Z. (2014). RealGraph: User Interaction Prediction at Twitter. ↩︎ Sharma, A., Jiang, J., Bommannavar, P., Larson, B., \u0026 Lin, J.J. (2016). GraphJet: Real-Time Content Recommendations at Twitter. Proc. VLDB Endow., 9, 1281-1292. ↩︎ Satuluri, V., Wu, Y., Zheng, X., Qian, Y., Wichers, B., Dai, Q., Tang, G.M., Jiang, J., \u0026 Lin, J.J. (2020). SimClusters: Community-Based Representations for Heterogeneous Recommendations at Twitter. Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \u0026 Data Mining. ↩︎ El-Kishky, A., Markovich, T., Park, S., Verma, C.K., Kim, B., Eskander, R., Malkov, Y., Portman, F., Samaniego, S., Xiao, Y., \u0026 Haghighi, A. (2022). TwHIN: Embedding the Twitter Heterogeneous Information Network for Personalized Recommendation. Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. ↩︎ Busch, M., Gade, K., Larson, B., Lok, P., Luckenbill, S.B., \u0026 Lin, J.J. (2012). Earlybird: Real-Time Search at Twitter. 2012 IEEE 28th International Conference on Data Engineering, 1360-1369. ↩︎ Wang, Z., She, Q., \u0026 Zhang, J. (2021). MaskNet: Introducing Feature-Wise Multiplication to CTR Ranking Models by Instance-Guided Mask. ArXiv, abs/2102.07619. ↩︎ ","date":"2023-04-04","objectID":"/posts/2023/04/the-twitter-ml-algo/:4:0","series":null,"tags":["literature review","recsys","ranking"],"title":"Twitter's For You Recommendation Algorithm","uri":"/posts/2023/04/the-twitter-ml-algo/#references"},{"categories":["Computer Vision"],"content":"\rNeed for self-supervised learningSupervised deep learning approaches have shown remarkable success in various domains, such as Computer Vision. With the advent of big data, some of these approaches have lagged as the amount of labeled data is often minuscule compared to large-scale unlabeled data, like text, photos, and videos. To address this challenge, a number of algorithms have been proposed in the self-supervised domain that learn robust representations without any labels. A common approach to training such models is to learn a pretext task, which is solved only for the purpose of learning effective data representations to be used in downstream tasks. Some of the pretext tasks for learning image representations include predicting the rotation of images, relative positions or tracking of patches in an image, image colorization, solving jigsaw puzzles, and counting visual primitives. Pretext tasks for video representation learning are more complex due to the presence of temporal dimensions. Some of the pretext tasks in the video domain include predicting future frames or the correct order of shuffled frames, shuffled clips, video rotations, motion and appearance statistics, tracking patches, objects, pixels, etc. These approaches have achieved great performance in learning image/video representations. However, they do not perform competitively with supervised methods because some of these tasks can be trivially solved through learned shortcuts. ","date":"2023-03-27","objectID":"/posts/2023/03/contrastive-video-representations/:1:0","series":null,"tags":["literature review","embeddings"],"title":"Self-Supervised Contrastive Approaches for Video Representation Learning","uri":"/posts/2023/03/contrastive-video-representations/#need-for-self-supervised-learning"},{"categories":["Computer Vision"],"content":"\rThe emergence of contrastive learningContrastive learning is one of the categories of the self-supervised learning paradigm. These methods learn a latent space that draws positive samples together (e.g. frames from the same video) while pushing part negative samples (e.g. frames from different videos). Various contrastive learning algorithms usually differ in the way they define positive and negative samples. For example, algorithms could use multiple augmented views of the same image, clustering methods to find semantically similar examples, sub-clips of the same or different lengths from the same video, etc. In some cases, these methods can also work without explicit negatives1. In video representation learning, contrastive methods could use pretext tasks such as pace prediction, clip shuffling, etc. A lot of recent approaches use a variant of contrastive learning called Instance Discrimination to further close the gap with supervised learning methods. These approaches include data augmentation, contrastive losses, momentum encoders2, and memory banks3. Data augmentation is commonly used by several image-based methods like SimCLR4 to generate positive samples. For example, random image transformations, like random crops, can be applied to an image to obtain multiple views of the same image, which are assumed to be positives. The downside of using such a transformation is that the onus of generalization lies heavily on the data augmentation pipeline which can not cover all possible variances5. InfoNCE is one of the most commonly used contrastive loss functions. Given an embedded video $z$ and corresponding positive embedded video $k^+$ (for example, a random augmentation of $z$) and many negative embedded videos $k^- \\in \\mathcal{N}_i$ respectively, infoNCE loss is defined as: $$ \\mathcal{L}^{\\text{InfoNCE}} = -\\log \\frac{\\exp(z_i \\cdot z_i^+ / \\tau)}{\\exp(z_i \\cdot z_i^+ / \\tau) + \\sum\\limits_{z^- \\in \\mathcal{N}_i} \\exp(z_i \\cdot z^- / \\tau)} $$ where $\\tau$ is the temperature hyperparameter. Similarity among the positive $(z_{i}, z_{i}^+)$ and negative $(z_i, z^-)$ pair is usually defined as dot product (cosine distance) and the embedded vectors are usually $\\ell_2$-normalized. ","date":"2023-03-27","objectID":"/posts/2023/03/contrastive-video-representations/:2:0","series":null,"tags":["literature review","embeddings"],"title":"Self-Supervised Contrastive Approaches for Video Representation Learning","uri":"/posts/2023/03/contrastive-video-representations/#the-emergence-of-contrastive-learning"},{"categories":["Computer Vision"],"content":"\rContrastive learning from unlabeled videos","date":"2023-03-27","objectID":"/posts/2023/03/contrastive-video-representations/:3:0","series":null,"tags":["literature review","embeddings"],"title":"Self-Supervised Contrastive Approaches for Video Representation Learning","uri":"/posts/2023/03/contrastive-video-representations/#contrastive-learning-from-unlabeled-videos"},{"categories":["Computer Vision"],"content":"\rGoogle’s Contrastive Video Representation Learning (CVRL)In the CVRL paper6, researchers at Google built a framework to promote spatial self-supervision signals in videos. They noted that since both spatial and temporal information is crucial for learning video representation, only applying spatial augmentation independent of video frames breaks the natural motion along the time dimension. Instead, they recommended a temporally consistent spatial augmentation method by fixing the randomness across frames. They hypothesized that a pair of positive clips (taken from the same video) that are temporally distant, may contain very different visual content, leading to a low similarity that will be indistinguishable from negative pairs. At the same time, disregarding timewise distant clips reduces the temporal augmentation effect. So their recommended sampling method ensured that the time difference between two positive clips, sampled from the same input video, follows a monotonically decreasing distribution. Effectively, temporally close clips are a much stronger basis for learning as compared to distant clips. It also ensures that the model doesn’t learn temporally invariant features, which is the problem with previous temporal augmentation techniques like sorting video frames or clips, altering playback rates, etc. Google's CVRL Framework\rThese sampled clips are then spatially augmented. Previous spatial augmentation techniques like random cropping, color jittering, and blurring break the motion cues across frames that negatively affect the representation learning along the temporal dimension. To address this, the authors designed the following algorithm that makes the spatial augmentation consistent along the temporal dimension. CVRL's Spatial Augmentation Algorithm\rFollowing this, a video encoder maps the input video clip to a 2048-dimensional latent representation using 3D-ResNets as backbones. This network is a minor modification (different stride and kernel size) of the “slow” pathway of the SlowFast network7. An MLP layer further reduces this representation to a 128-dimensional feature vector that is used to compute InfoNCE loss. As per the standard convention, the MLP is discarded during evaluation. Their representations outperformed all existing baselines for self-supervised learning and they also showed that the CVRL framework benefits from larger datasets and larger networks. The official TensorFlow implementation of CVRL is available on GitHub. ","date":"2023-03-27","objectID":"/posts/2023/03/contrastive-video-representations/:3:1","series":null,"tags":["literature review","embeddings"],"title":"Self-Supervised Contrastive Approaches for Video Representation Learning","uri":"/posts/2023/03/contrastive-video-representations/#googles-contrastive-video-representation-learning-cvrl"},{"categories":["Computer Vision"],"content":"\rFacebook’s $\\rho$-Momentum Contrast ($\\rho$-MoCo)Facebook AI Research (FAIR) proposed a simple method to learn video representations that generalized existing image-based frameworks to space-time. The authors argue that visual content is often temporally-persistent along a timespan in a given video. For example, persistency may involve a person dancing, transitioning from running to walking, etc. covering short to long spans, with different levels of visual invariance. Hence they use an encoder to produce embeddings that are persistent in space-time, over multiple ($\\rho$) temporally distant clips of the same video. Their experiments showed that it is beneficial to sample positives with longer timespans between them. The authors used an encoder design that is similar to Google’s CVRL, i.e. a 3D ResNet-50 ConvNet without temporal pooling in convolutional feature maps, followed by an MLP projection head. For their experiments, they used 2 contrastive approaches which use both positive and negative samples: SimCLR and MoCo, and 2 other methods that only use positive samples: BYOL and SwAV. The original objective of these methods was to learn invariant features for images across different views (augmentations). The authors extended this objective to the temporal domain for videos. The four conceptual frameworks\rSimCLR uses the in-batch negative embeddings of other videos, while MoCo maintains a queue that stores embeddings from previous iterations to fetch negatives. BYOL can be thought of as a form of MoCo without the negative samples but with an extra predictor MLP ($\\theta_p$ in the figure), whereas SwAV can be viewed as a form of SimCLR without the negative samples that also has Sinkhorn-Knopp (SK) transformation step for the targets. Through an extensive set of experiments, the authors share the following findings: The long-spanned persistency can be effective even if the timespan between positives is up to 60 seconds. Beyond that the positive pair might no longer share the same semantic context for learning high-level features corresponding in time, hence the corresponding representations might not be beneficial for downstream tasks. There was no performance difference between contrastive and non-contrastive methods. Multiple samples ($\\rho$) from the same clip are beneficial to achieve high accuracy, with the best results achieved at $\\rho = 4$. While MoCo performed the best for uncurated videos, BYOL and SimCLR showed better performance for short videos (10-16 seconds). Methods that used momentum encoders (like MoCo, BYOL) tend to do better than the methods with contrastive objectives (SimCLR, SwAV). ","date":"2023-03-27","objectID":"/posts/2023/03/contrastive-video-representations/:3:2","series":null,"tags":["literature review","embeddings"],"title":"Self-Supervised Contrastive Approaches for Video Representation Learning","uri":"/posts/2023/03/contrastive-video-representations/#facebooks-rho-momentum-contrast-rho-moco"},{"categories":["Computer Vision"],"content":"\rAmazon’s Inter-Intra Video Contrastive Learning (IIVCL)The authors of Nearest-Neighbor Contrastive Learning of Visual Representations (NNCLR)5 proposed a nearest-neighbor-based approach to sampling the nearest neighbor as a positive sample for images. The authors of IIVCL8 extended this idea to the video domain to provision extra positive samples in addition to the previous CVRL and $\\rho$-MoCo models’ approach of fetching two positive clips from the same video. The authors argue that by only considering clips from the same video to be positive, the previous methods neglect other semantically related videos that may also be useful and relevant as positives for contrastive learning. For example: in the following samples clip A and B are from the same skiing video, hence they are positives, while video 2 and video 3 are negatives for both. But this assumption ignores the fact that video 3 (snowboarding) is more similar to video 1 (skiing) than video 2 (push-ups). Contrastive Learning in CVRL and ρ-MoCo neglects that video 3 is more similar to video 1 than video 2\rHence there is a need to look beyond just the local intra-video similarities and consider global inter-video similarities as well. This ensures additional diversity in viewpoint and scene orientation that cannot be expressed through sampling sub-clips from the same video. And, it also makes it possible to learn from weaker yet relevant and useful positive pairs. However, only using global inter-video semantics will lead to losing granular details that are also important for video understanding. So an effective approach must use both local and global similarities. To achieve this, IIVCL creates a dynamically evolving queue from which the most similar videos (nearest neighbors) are found. Note that the intra-video clips are randomly sampled and there is no temporal ordering among them. Current state-of-the-art (a) vs. inter-video nearest neighbor sampled from a queue(b) vs. IIVCL(c)\rAs contrastive learning requires large batch sizes and computing video embeddings can be very expensive, the FIFO-style queue is updated with embeddings from a momentum encoder. The queue is updated at the end of each training setup (forward pass) by taking the n (batch size) embeddings from the current training step and inserting them into the end of the queue. The nearest neighbors are calculated from this queue by calculating the cosine similarities between the input video and the queue. Over the course of pretraining, the model continually evolves its notion of semantic similarity and becomes better at picking semantically similar nearest-neighbor clips. Hence, no clustering or separate training is required to get the global support set. Separate MLP projection heads are used to process the intra-video and nearest-neighbor positive pairs. Both of these pretext tasks are then combined using a multi-task loss. Similar to other methods, the trained model is evaluated using linear evaluation and fine-tuning protocols. ","date":"2023-03-27","objectID":"/posts/2023/03/contrastive-video-representations/:3:3","series":null,"tags":["literature review","embeddings"],"title":"Self-Supervised Contrastive Approaches for Video Representation Learning","uri":"/posts/2023/03/contrastive-video-representations/#amazons-inter-intra-video-contrastive-learning-iivcl"},{"categories":["Computer Vision"],"content":"\rSummaryAdvances in self-supervised learning have enabled the utilization of large-scale unlabeled data. Contrastive learning methods have further helped to reduce the performance gap between self-supervised and supervised methods. This article summarized some of the most popular ways to apply contrastive methods for effective representation learning for videos. These algorithms proposed different ways to address the spatial and temporal information in video data to achieve state-of-the-art performance for representation learning. ","date":"2023-03-27","objectID":"/posts/2023/03/contrastive-video-representations/:4:0","series":null,"tags":["literature review","embeddings"],"title":"Self-Supervised Contrastive Approaches for Video Representation Learning","uri":"/posts/2023/03/contrastive-video-representations/#summary"},{"categories":["Computer Vision"],"content":"\rReferences Grill, J., Strub, F., Altch’e, F., Tallec, C., Richemond, P.H., Buchatskaya, E., Doersch, C., Pires, B.Á., Guo, Z.D., Azar, M.G., Piot, B., Kavukcuoglu, K., Munos, R., \u0026 Valko, M. (2020). Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning. ArXiv, abs/2006.07733. ↩︎ He, K., Fan, H., Wu, Y., Xie, S., \u0026 Girshick, R.B. (2019). Momentum Contrast for Unsupervised Visual Representation Learning. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 9726-9735. ↩︎ Wu, Z., Xiong, Y., Yu, S.X., \u0026 Lin, D. (2018). Unsupervised Feature Learning via Non-parametric Instance Discrimination. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 3733-3742. ↩︎ Chen, T., Kornblith, S., Norouzi, M., \u0026 Hinton, G. (2020). A Simple Framework for Contrastive Learning of Visual Representations. ArXiv. /abs/2002.05709 ↩︎ Dwibedi, D., Aytar, Y., Tompson, J., Sermanet, P., \u0026 Zisserman, A. (2021). With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations. ArXiv. /abs/2104.14548 ↩︎ ↩︎ Qian, R., Meng, T., Gong, B., Yang, M., Wang, H., Belongie, S.J., \u0026 Cui, Y. (2020). Spatiotemporal Contrastive Video Representation Learning. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 6960-6970. ↩︎ Feichtenhofer, C., Fan, H., Malik, J., \u0026 He, K. (2018). SlowFast Networks for Video Recognition. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), 6201-6210. ↩︎ Fan, D., Yang, D., Li, X., Bhat, V., \u0026 Rohith, M.V. (2023). Nearest-Neighbor Inter-Intra Contrastive Learning from Unlabeled Videos. ArXiv, abs/2303.07317. ↩︎ ","date":"2023-03-27","objectID":"/posts/2023/03/contrastive-video-representations/:5:0","series":null,"tags":["literature review","embeddings"],"title":"Self-Supervised Contrastive Approaches for Video Representation Learning","uri":"/posts/2023/03/contrastive-video-representations/#references"},{"categories":["Information Retrieval"],"content":"\rIntroduction","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:1:0","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#introduction"},{"categories":["Information Retrieval"],"content":"\rEncoders for Learning RepresentationsInformation retrieval problems usually take a query as input and return the most relevant documents from a large corpus. For example, in open-domain question answering, a query represents a question, and documents represent evidence passages containing the answer. In Recommender systems, the query might represent a user query and documents could be a set of candidate items to recommend. A scoring function is commonly used in these approaches to quantify the relevance of a pair containing a query q and document d. As explained in the previous post, these systems are often implemented in a cascade fashion where a less powerful but more efficient “retriever” algorithm (such as ElasticSearch, BM25, or Dual Encoder) first reduces the search space for candidates and then a more complex but powerful “reranker” algorithm (such as a Cross-Encoder) re-ranks the retrieved documents. To balance the efficiency and effectiveness tradeoff, the retriever model is focused on optimizing a high recall as otherwise the most relevant documents will not even be considered in the latter stage1. And, the reranker model focuses more on precision. This article will mainly focus on discussions around the dual-encoder-based dense retriever. BERT-based Dual Encoder (left) and Cross-Encoder (right). Source: [^1]\rBM25 is a widely used approach that is based on token matching and TF-IDF weights. However, it lacks semantic context and can’t be optimized for a specific task. On the other hand, Dual-encoder is an embedding-based method that embeds queries and documents in the same space (aggregating just the [CLS] output) and uses a similarity measure, like the inner product, Euclidean L2 or cosine distance, to measure the similarity between learned representations for queries and documents. The model is usually trained to optimize binary cross entropy (BCE), hinge loss, or negative log-likelihood (NLL) loss. Apart from using just the textual data for query and document towers, we can also add extra contextual information as input. For example, in Facebook Search2 the authors use character n-gram representations for text inputs due to its smaller vocabulary size and robustness to the out-of-vocabulary problem. Additionally, the authors used location signals, like city, region, country, and language for queries, and tagged locations for documents. They also leveraged embeddings generated via a separate model to embed users and entities based on their social graph. Adding location and social features gave their model’s recall an additional boost of 2.20% and 1.77% respectively. Dual Encoder model inputs for embedding-based retrieval in Facebook search\r","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:1:1","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#encoders-for-learning-representations"},{"categories":["Information Retrieval"],"content":"\rNeed for Effective SamplingSampling instances for such information retrieval systems is not straightforward. In systems with a large amount of data, it is infeasible to annotate all candidates for a given query. Annotators are usually given top-k candidates from a simplistic approach like BM25. So it is very likely to have a lot of unlabeled positive data3. One approach to improve the effectiveness of the dual encoders is to also use negative examples to further emphasize the notion of similarity. It helps in separating the irrelevant pairs of queries and documents while keeping the distance smaller for relevant pairs. So, to train most of the existing dense retrieval models, we sample both positive and negative instances from the corpus. Research works, such as Qu et al.3, have shown that the retrieval model performance increases when the number of negatives is increased up to a certain point. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:1:2","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#need-for-effective-sampling"},{"categories":["Information Retrieval"],"content":"\rCalculating Contrastive LossSuch models fall under the self-supervised learning category and are optimized by a contrastive objective in which a model is trained to maximize the scores of positive pairs and minimize the scores of negative ones. Past research shows that contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning approaches4. A contrastive loss is used to calculate the similarities of sample pairs in representation space. The target value in such loss functions doesn’t need to be fixed and it can be a data representation computed by the network on-the-fly during training5. For example, given a query $q$ with an associated positive document $k_{+}$ and a pool of negative documents $(k_{i})_{i=1..K}$, the contrastive InfoNCE loss is defined as6: $$ L(q, k_+) = -\\frac{\\exp\\left(\\frac{s(q, k_+)}{\\tau}\\right)}{\\exp\\left(\\frac{s(q, k_+)}{\\tau}\\right) + \\sum\\limits_{i=1}^{K} \\exp\\left(\\frac{s(q, k_i)}{\\tau}\\right)} $$ where $\\tau$ is a temperature parameter. This loss encourages positive pairs to have high scores and negative pairs to have low scores. Similarly, for a given triplet $(q^{(i)}, d_+^{(i)}, d_-^{(i)})$, where $q^{(i)}$ is a query, $d_+^{(i)}$ and $d_-^{(i)}$ are the corresponding positive and negative documents, we can define the contrastive triplet loss as2: $$ L=\\sum\\limits_{i=1}^{N} \\max\\left(0, D(q^{(i)}, d_+^{(i)}) - D(q^{(i)}, d^{(i)}_-) + m\\right) $$ where $D(u,v)$ is a distance metric and $m$ is the margin enforced between positive and negative pairs, and N is the total number of triplets selected from the training set. This loss function separates positive and negative pairs by a distance margin. For a more comprehensive list of loss functions in contrastive learning, please refer to this post. In the rest of the article, we will look at some of the most common approaches for sampling positive and negative instances. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:1:3","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#calculating-contrastive-loss"},{"categories":["Information Retrieval"],"content":"\rHow to Sample Negative ExamplesFor retrieval problems, usually, the positive examples are explicitly available while negative examples are to be sampled from a large corpus based on some strategy. Negative sampling is a key challenge during representation learning of a dense retriever. The first-stage retriever usually has to look through a large corpus to find the appropriate negative samples, whereas the reranker at a later stage can simply select the irrelevant examples from the previous retrieval stage to be the negative samples7. There are different sampling strategies based on heuristics or empirical results that have been adopted in industrial and academic projects. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:2:0","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#how-to-sample-negative-examples"},{"categories":["Information Retrieval"],"content":"\rExplicit NegativesSome tasks naturally allow for a way to pick negative samples. For example: in Facebook Search, Huang et al.2 added non-clicked impressions, i.e. instances that were impressed but not clicked, as negatives. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:2:1","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#explicit-negatives"},{"categories":["Information Retrieval"],"content":"\rRandom NegativesA simple approach to selecting negative examples is to randomly sample documents from the corpus for each query. In Facebook Search, Huang et al.2 also used random negative samples because they believed it approximates the recall optimization task. Through experiments, they found that the model trained using non-click impressions as negative has significantly worse model recall compared to using random negative. The authors hypothesized that the random documents can be completely irrelevant to the query and do not produce hard enough negative examples. So the resulting dataset would not be challenging enough for the model. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:2:2","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#random-negatives"},{"categories":["Information Retrieval"],"content":"\rBM25 NegativesAnother way to sample negative documents is to take the top documents returned by BM25 which do not contain the answer, but still match the input query tokens. Zhan et al.8 hypothesized that BM25 biases the dense retriever to not retrieve documents with much query term overlapping, which is a distinct characteristic of these negative examples. Such behavior leads to optimization bias and harms retrieval performance. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:2:3","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#bm25-negatives"},{"categories":["Information Retrieval"],"content":"\rGold NegativesFor a given query q, all documents that are paired with other queries in the labeled data can be considered as negative documents for the query q. Karpukhin et al.9 used this approach but didn’t find significant gains over choosing random negatives. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:2:4","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#gold-negatives"},{"categories":["Information Retrieval"],"content":"\rIn-batch NegativesA more effective approach to picking gold negatives is to select gold documents of other queries in the same batch. So for a batch size B, each query can have up to B-1 negative documents. This is one of the most common approaches used to sample negatives for training dual encoders. Qu et al.3 showed that training bi-encoders with many in-batch negatives (batch size up to 4096) significantly improves the model’s effectiveness. Karpukhin et al.9 found that the choice of in-batch negatives – random, BM25, or gold – does not impact the top-k accuracy much when k ≥ 20. They also showed that while in-batch negatives are effective in learning representation, they are not always better than sparse methods like BM25 applied over the whole corpus. While in-batch negatives can be memory-efficient to use (because we reuse the examples already loaded in a mini-batch), some research work shows that this sampling technique requires extremely large batch sizes to work well10. Xiong et al.7 argued that it is unlikely that in-batch negatives can provide meaningful samples. They hypothesized that because the majority of the corpus is only trivially related, and the batch size is much smaller than the corpus size, the probability of a random mini-batch containing meaningful negative is almost zero. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:2:5","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#in-batch-negatives"},{"categories":["Information Retrieval"],"content":"\rCross-batch NegativesQu et al.3 proposed a cross-batch negative sampling method in a multi-GPU environment (shown at the bottom in the figure below). By using their approach, for a given query q, we sample in-batch negatives on the same GPU along with sampling negatives from all other GPUs. So for a setup with A number of GPUs and B batch size, we can sample up to $A \\times B-1$ negatives for a given question. Their experiments showed improved results with negligible additional costs from using cross-batch negatives over the vanilla in-batch negative sampling method. In-batch Negative (top) vs Cross-batch Negative (bottom) Sampling\rThere have been several alternative approaches for sampling negatives across batches. Wu et al.11 proposed storing document representations from previous batches in a queue to be used as negative examples. While this allows for a smaller batch size to be used, it could also lead to a drop in performance when the network changes rapidly from the previous iteration to the next. He et al.5 proposed a related approach called MoCo (Momentum Contrast) that uses a queue of min-batches that stored representations generated by a separate network called the momentum encoder network. Meng et al.12 showed that the MoCo method is quite against noisy pairs, while the in-batch negatives method works better with queries of better quality. They also showed that MoCo’s generalization capability can be improved by using it along with data augmentation methods. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:2:6","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#cross-batch-negatives"},{"categories":["Information Retrieval"],"content":"\rApproximate Nearest NeighborsAnother costly but well-performing sampling strategy was proposed by Xiong et al.7 who used asynchronous approximate nearest neighbors as negatives. After every few thousand steps of training, they used the current model to re-encode and re-index the documents in their corpus. Then they retrieved the top-k documents for the training queries and used them as negatives for the following training iterations until the next refresh. In other words, they picked negatives based on outdated model parameters. However, the near-state-of-the-art performance of their model comes at the cost of significantly higher training time. To tackle this, the authors implement an asynchronous index refresh that updates the ANN index once every few batches. Some recent work showed that the top documents retrieval approach of this method can be biased and unstable because of their lack of overlap with pre-retrieved negatives using random or BM25 methods and proposed an improved method called LTRe (Learning To Retrieve)8. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:2:7","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#approximate-nearest-neighbors"},{"categories":["Information Retrieval"],"content":"\rHybridA lot of research work has chosen to combine some of the methods described above. For example, Karpukhin et al.9 achieved the best performance in their experiments by using a combination of gold passages from the same mini-batch and one BM25 negative passage. Similarly, Luan et al.13 used a combination of BM25 and random sampling to get negative pairs for training. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:2:8","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#hybrid"},{"categories":["Information Retrieval"],"content":"\rHow to Sample Positive Examples","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:3:0","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#how-to-sample-positive-examples"},{"categories":["Information Retrieval"],"content":"\rSupervised Setting\rExplicit PositivesUnder supervised settings, positive instances are explicitly available. These labels are usually task-specific and can be sampled intuitively from user activity logs. For example: in Facebook Search, Huang et al.2 added user-clicked results as likely being positive documents for the input search query. They also tried using impressed documents as positives but it didn’t provide any additional value. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:3:1","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#supervised-setting"},{"categories":["Information Retrieval"],"content":"\rSupervised Setting\rExplicit PositivesUnder supervised settings, positive instances are explicitly available. These labels are usually task-specific and can be sampled intuitively from user activity logs. For example: in Facebook Search, Huang et al.2 added user-clicked results as likely being positive documents for the input search query. They also tried using impressed documents as positives but it didn’t provide any additional value. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:3:1","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#explicit-positives"},{"categories":["Information Retrieval"],"content":"\rSelf-supervised/Unsupervised SettingA lot of research work is based on training dense retrievers in a self-supervised or unsupervised fashion for various tasks. Contrastive Learning has been shown to lead to strong performance in these retrieval settings as well. The distinction between self-supervised and unsupervised learning in the existing literature is informal, but we will limit our discussion to their common idea of pretext tasks. The term “pretext” implies that the task being solved is not of genuine interest, but is solved only for the true purpose of learning a good data representation5. Some of the approaches listed below are also used to pre-train the retriever model14. Inverse Cloze TaskWhile in the standard Cloze task, the goal is to predict masked-out text based on its context, Lee et al.14 proposed Inverse Cloze Task (ICT) that requires predicting the context given a sentence. It generates two mutually exclusive views of a document, first by randomly sampling a span of tokens from a segment of text while using the complement of the span as the second view. The two views are then used as a positive pair. Recurring Span RetrievalIn Spider (Span-based unsupervised dense retriever), Ram et al.15 proposed a self-surprised method called recurring span retrieval for training unsupervised dense retrievers. They leverage recurring spans in different passages of the same document to create positive pairs. They also showed that this method combined with BM25 can train a retriever that sometimes outperforms supervised dense models. Independent CroppingUnder this strategy, two contiguous spans of tokens from the same text are considered a positive pair. Simple Text AugmentationsSimple data augmentations, such as random word deletion, replacement, or masking can also be used to create a positive pair6. OthersOther approaches for creating positive pairs include using masked salient spans in REALM16, random cropping in Contriever6, neighboring text pieces in CPT17, and query and top-k BM25 passages in SPAR Λ18. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:3:2","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#self-supervisedunsupervised-setting"},{"categories":["Information Retrieval"],"content":"\rSelf-supervised/Unsupervised SettingA lot of research work is based on training dense retrievers in a self-supervised or unsupervised fashion for various tasks. Contrastive Learning has been shown to lead to strong performance in these retrieval settings as well. The distinction between self-supervised and unsupervised learning in the existing literature is informal, but we will limit our discussion to their common idea of pretext tasks. The term “pretext” implies that the task being solved is not of genuine interest, but is solved only for the true purpose of learning a good data representation5. Some of the approaches listed below are also used to pre-train the retriever model14. Inverse Cloze TaskWhile in the standard Cloze task, the goal is to predict masked-out text based on its context, Lee et al.14 proposed Inverse Cloze Task (ICT) that requires predicting the context given a sentence. It generates two mutually exclusive views of a document, first by randomly sampling a span of tokens from a segment of text while using the complement of the span as the second view. The two views are then used as a positive pair. Recurring Span RetrievalIn Spider (Span-based unsupervised dense retriever), Ram et al.15 proposed a self-surprised method called recurring span retrieval for training unsupervised dense retrievers. They leverage recurring spans in different passages of the same document to create positive pairs. They also showed that this method combined with BM25 can train a retriever that sometimes outperforms supervised dense models. Independent CroppingUnder this strategy, two contiguous spans of tokens from the same text are considered a positive pair. Simple Text AugmentationsSimple data augmentations, such as random word deletion, replacement, or masking can also be used to create a positive pair6. OthersOther approaches for creating positive pairs include using masked salient spans in REALM16, random cropping in Contriever6, neighboring text pieces in CPT17, and query and top-k BM25 passages in SPAR Λ18. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:3:2","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#inverse-cloze-task"},{"categories":["Information Retrieval"],"content":"\rSelf-supervised/Unsupervised SettingA lot of research work is based on training dense retrievers in a self-supervised or unsupervised fashion for various tasks. Contrastive Learning has been shown to lead to strong performance in these retrieval settings as well. The distinction between self-supervised and unsupervised learning in the existing literature is informal, but we will limit our discussion to their common idea of pretext tasks. The term “pretext” implies that the task being solved is not of genuine interest, but is solved only for the true purpose of learning a good data representation5. Some of the approaches listed below are also used to pre-train the retriever model14. Inverse Cloze TaskWhile in the standard Cloze task, the goal is to predict masked-out text based on its context, Lee et al.14 proposed Inverse Cloze Task (ICT) that requires predicting the context given a sentence. It generates two mutually exclusive views of a document, first by randomly sampling a span of tokens from a segment of text while using the complement of the span as the second view. The two views are then used as a positive pair. Recurring Span RetrievalIn Spider (Span-based unsupervised dense retriever), Ram et al.15 proposed a self-surprised method called recurring span retrieval for training unsupervised dense retrievers. They leverage recurring spans in different passages of the same document to create positive pairs. They also showed that this method combined with BM25 can train a retriever that sometimes outperforms supervised dense models. Independent CroppingUnder this strategy, two contiguous spans of tokens from the same text are considered a positive pair. Simple Text AugmentationsSimple data augmentations, such as random word deletion, replacement, or masking can also be used to create a positive pair6. OthersOther approaches for creating positive pairs include using masked salient spans in REALM16, random cropping in Contriever6, neighboring text pieces in CPT17, and query and top-k BM25 passages in SPAR Λ18. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:3:2","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#recurring-span-retrieval"},{"categories":["Information Retrieval"],"content":"\rSelf-supervised/Unsupervised SettingA lot of research work is based on training dense retrievers in a self-supervised or unsupervised fashion for various tasks. Contrastive Learning has been shown to lead to strong performance in these retrieval settings as well. The distinction between self-supervised and unsupervised learning in the existing literature is informal, but we will limit our discussion to their common idea of pretext tasks. The term “pretext” implies that the task being solved is not of genuine interest, but is solved only for the true purpose of learning a good data representation5. Some of the approaches listed below are also used to pre-train the retriever model14. Inverse Cloze TaskWhile in the standard Cloze task, the goal is to predict masked-out text based on its context, Lee et al.14 proposed Inverse Cloze Task (ICT) that requires predicting the context given a sentence. It generates two mutually exclusive views of a document, first by randomly sampling a span of tokens from a segment of text while using the complement of the span as the second view. The two views are then used as a positive pair. Recurring Span RetrievalIn Spider (Span-based unsupervised dense retriever), Ram et al.15 proposed a self-surprised method called recurring span retrieval for training unsupervised dense retrievers. They leverage recurring spans in different passages of the same document to create positive pairs. They also showed that this method combined with BM25 can train a retriever that sometimes outperforms supervised dense models. Independent CroppingUnder this strategy, two contiguous spans of tokens from the same text are considered a positive pair. Simple Text AugmentationsSimple data augmentations, such as random word deletion, replacement, or masking can also be used to create a positive pair6. OthersOther approaches for creating positive pairs include using masked salient spans in REALM16, random cropping in Contriever6, neighboring text pieces in CPT17, and query and top-k BM25 passages in SPAR Λ18. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:3:2","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#independent-cropping"},{"categories":["Information Retrieval"],"content":"\rSelf-supervised/Unsupervised SettingA lot of research work is based on training dense retrievers in a self-supervised or unsupervised fashion for various tasks. Contrastive Learning has been shown to lead to strong performance in these retrieval settings as well. The distinction between self-supervised and unsupervised learning in the existing literature is informal, but we will limit our discussion to their common idea of pretext tasks. The term “pretext” implies that the task being solved is not of genuine interest, but is solved only for the true purpose of learning a good data representation5. Some of the approaches listed below are also used to pre-train the retriever model14. Inverse Cloze TaskWhile in the standard Cloze task, the goal is to predict masked-out text based on its context, Lee et al.14 proposed Inverse Cloze Task (ICT) that requires predicting the context given a sentence. It generates two mutually exclusive views of a document, first by randomly sampling a span of tokens from a segment of text while using the complement of the span as the second view. The two views are then used as a positive pair. Recurring Span RetrievalIn Spider (Span-based unsupervised dense retriever), Ram et al.15 proposed a self-surprised method called recurring span retrieval for training unsupervised dense retrievers. They leverage recurring spans in different passages of the same document to create positive pairs. They also showed that this method combined with BM25 can train a retriever that sometimes outperforms supervised dense models. Independent CroppingUnder this strategy, two contiguous spans of tokens from the same text are considered a positive pair. Simple Text AugmentationsSimple data augmentations, such as random word deletion, replacement, or masking can also be used to create a positive pair6. OthersOther approaches for creating positive pairs include using masked salient spans in REALM16, random cropping in Contriever6, neighboring text pieces in CPT17, and query and top-k BM25 passages in SPAR Λ18. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:3:2","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#simple-text-augmentations"},{"categories":["Information Retrieval"],"content":"\rSelf-supervised/Unsupervised SettingA lot of research work is based on training dense retrievers in a self-supervised or unsupervised fashion for various tasks. Contrastive Learning has been shown to lead to strong performance in these retrieval settings as well. The distinction between self-supervised and unsupervised learning in the existing literature is informal, but we will limit our discussion to their common idea of pretext tasks. The term “pretext” implies that the task being solved is not of genuine interest, but is solved only for the true purpose of learning a good data representation5. Some of the approaches listed below are also used to pre-train the retriever model14. Inverse Cloze TaskWhile in the standard Cloze task, the goal is to predict masked-out text based on its context, Lee et al.14 proposed Inverse Cloze Task (ICT) that requires predicting the context given a sentence. It generates two mutually exclusive views of a document, first by randomly sampling a span of tokens from a segment of text while using the complement of the span as the second view. The two views are then used as a positive pair. Recurring Span RetrievalIn Spider (Span-based unsupervised dense retriever), Ram et al.15 proposed a self-surprised method called recurring span retrieval for training unsupervised dense retrievers. They leverage recurring spans in different passages of the same document to create positive pairs. They also showed that this method combined with BM25 can train a retriever that sometimes outperforms supervised dense models. Independent CroppingUnder this strategy, two contiguous spans of tokens from the same text are considered a positive pair. Simple Text AugmentationsSimple data augmentations, such as random word deletion, replacement, or masking can also be used to create a positive pair6. OthersOther approaches for creating positive pairs include using masked salient spans in REALM16, random cropping in Contriever6, neighboring text pieces in CPT17, and query and top-k BM25 passages in SPAR Λ18. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:3:2","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#others"},{"categories":["Information Retrieval"],"content":"\rHard Example MiningModern information retrieval tasks work with large and diverse datasets that usually contain an overwhelming number of easy examples and a small number of hard examples. Identifying and utilizing these hard examples can make the model training process more efficient and effective. Hard example mining is the process of selecting hard examples to train machine learning models. This problem has long been studied under the name, ‘Bootstrapping’. The key idea is to grow, or bootstrap, by selecting instances on which our model triggers a false alarm. So we adopt an iterative process that alternates between training a model using selected instances (that may include a random set of negative examples), and then selecting the new instances by removing the ’easy’ ones, and including an additional set on which that the trained model fails to identify correctly. For large and complex models, this process might be iterated only once19. In this section, we will explore hard mining strategies to improve a retriever model’s ability to differentiate between similar documents. A commonly adopted strategy in this domain is to consider instances that are closer to the positive instances in the embedding space as hard negatives in training. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:4:0","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#hard-example-mining"},{"categories":["Information Retrieval"],"content":"\rHard Negative MiningThe training and inference processes for dual-encoder-based retrievers are usually inconsistent. During training, the retriever is given only a small candidate set for each question (usually 1 positive pair per question) to calculate the question-document similarity, while during inference it is required to identify a positive document for a given question out of millions of candidates. Using negative sampling methods, like cross-batch negatives, does help with reducing this vast discrepancy, but the model may still fail to learn effectively because the negatives might be too weak. Also, because fully annotating a large dataset can be prohibitive, there might also be a large number of unlabeled positives that might get picked as negative examples, leading to an increase in false positives by the model. Using reliable, hard negatives helps in alleviating such issues. Top-ranked NegativesA straightforward approach to picking hard negatives is to use the trained retriever to make inferences on negative samples and select the top-k ranked documents as negatives20. This approach can still suffer from false positives as a lot of negative samples could actually be unlabeled positive samples. In-batch Hard NegativesIn Facebook Search2, the authors used the in-batch negatives to first get a set of candidate negative documents. Next, they used an approximate nearest neighbor (ANN) approach on the query and this set of negatives to find the negative closest to the input query. This negative was then used as the hard negative. In their experiments, they found that the optimal setting is at most two hard negatives per positive. Using more than two hard negatives had an adverse impact on the model quality. Denoised Hard NegativesTo solve the false negatives issue, RocketQA3 proposed to utilize a well-trained cross-encoder to remove false negatives from top-k ranked documents. Compared to a dual encoder, a cross-encoder model is inefficient to be used for inference in real-time, but it is also a highly effective and robust method. So a cross-encoder can be used on the top-k ranked documents from a retriever and only keep the documents that are predicted to be negatives with high confidence. The final (denoised) set of negatives is more reliable and can be used as hard negatives. ANCEWe saw earlier how Approximate nearest neighbor Negative Contrastive Learning (ANCE)7 can be used to sample negatives. The main drawback of this approach was that the inference is too expensive to compute per batch as it requires a forward pass on the entire corpus which is much bigger than the training batch. But it also means that the retrieved samples are global negatives from the entire corpus and can be considered hard negatives. They also show that the local, uninformative negative samples (with near zero gradients) can lead to diminishing gradient norms, large stochastic gradient variances, and slow learning convergence. Caution: Note that we do not want all of the negative training instances to be hard negatives. Using only hard negatives will change the representativeness of the retrieval task training data, which might impose a non-trivial bias on the learned embeddings. For example, the model may hardly learn to retrieve if the negatives are too weak; it may be optimized in the wrong direction if the negative samples are biased8. In Facebook Search2, the authors found that models trained simply using hard negatives cannot outperform models trained with random negatives. Their experiments showed that blending random and hard negatives in training is advantageous. Increasing the ratio of easy-to-hard negatives continued the model improvement up to a 100:1 easy-to-hard ratio. They also compared sampling from different rank positions and found that using the hardest examples is not the best strategy, and sampling between ranks 101-500 achieved the best model recall. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:4:1","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#hard-negative-mining"},{"categories":["Information Retrieval"],"content":"\rHard Negative MiningThe training and inference processes for dual-encoder-based retrievers are usually inconsistent. During training, the retriever is given only a small candidate set for each question (usually 1 positive pair per question) to calculate the question-document similarity, while during inference it is required to identify a positive document for a given question out of millions of candidates. Using negative sampling methods, like cross-batch negatives, does help with reducing this vast discrepancy, but the model may still fail to learn effectively because the negatives might be too weak. Also, because fully annotating a large dataset can be prohibitive, there might also be a large number of unlabeled positives that might get picked as negative examples, leading to an increase in false positives by the model. Using reliable, hard negatives helps in alleviating such issues. Top-ranked NegativesA straightforward approach to picking hard negatives is to use the trained retriever to make inferences on negative samples and select the top-k ranked documents as negatives20. This approach can still suffer from false positives as a lot of negative samples could actually be unlabeled positive samples. In-batch Hard NegativesIn Facebook Search2, the authors used the in-batch negatives to first get a set of candidate negative documents. Next, they used an approximate nearest neighbor (ANN) approach on the query and this set of negatives to find the negative closest to the input query. This negative was then used as the hard negative. In their experiments, they found that the optimal setting is at most two hard negatives per positive. Using more than two hard negatives had an adverse impact on the model quality. Denoised Hard NegativesTo solve the false negatives issue, RocketQA3 proposed to utilize a well-trained cross-encoder to remove false negatives from top-k ranked documents. Compared to a dual encoder, a cross-encoder model is inefficient to be used for inference in real-time, but it is also a highly effective and robust method. So a cross-encoder can be used on the top-k ranked documents from a retriever and only keep the documents that are predicted to be negatives with high confidence. The final (denoised) set of negatives is more reliable and can be used as hard negatives. ANCEWe saw earlier how Approximate nearest neighbor Negative Contrastive Learning (ANCE)7 can be used to sample negatives. The main drawback of this approach was that the inference is too expensive to compute per batch as it requires a forward pass on the entire corpus which is much bigger than the training batch. But it also means that the retrieved samples are global negatives from the entire corpus and can be considered hard negatives. They also show that the local, uninformative negative samples (with near zero gradients) can lead to diminishing gradient norms, large stochastic gradient variances, and slow learning convergence. Caution: Note that we do not want all of the negative training instances to be hard negatives. Using only hard negatives will change the representativeness of the retrieval task training data, which might impose a non-trivial bias on the learned embeddings. For example, the model may hardly learn to retrieve if the negatives are too weak; it may be optimized in the wrong direction if the negative samples are biased8. In Facebook Search2, the authors found that models trained simply using hard negatives cannot outperform models trained with random negatives. Their experiments showed that blending random and hard negatives in training is advantageous. Increasing the ratio of easy-to-hard negatives continued the model improvement up to a 100:1 easy-to-hard ratio. They also compared sampling from different rank positions and found that using the hardest examples is not the best strategy, and sampling between ranks 101-500 achieved the best model recall. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:4:1","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#top-ranked-negatives"},{"categories":["Information Retrieval"],"content":"\rHard Negative MiningThe training and inference processes for dual-encoder-based retrievers are usually inconsistent. During training, the retriever is given only a small candidate set for each question (usually 1 positive pair per question) to calculate the question-document similarity, while during inference it is required to identify a positive document for a given question out of millions of candidates. Using negative sampling methods, like cross-batch negatives, does help with reducing this vast discrepancy, but the model may still fail to learn effectively because the negatives might be too weak. Also, because fully annotating a large dataset can be prohibitive, there might also be a large number of unlabeled positives that might get picked as negative examples, leading to an increase in false positives by the model. Using reliable, hard negatives helps in alleviating such issues. Top-ranked NegativesA straightforward approach to picking hard negatives is to use the trained retriever to make inferences on negative samples and select the top-k ranked documents as negatives20. This approach can still suffer from false positives as a lot of negative samples could actually be unlabeled positive samples. In-batch Hard NegativesIn Facebook Search2, the authors used the in-batch negatives to first get a set of candidate negative documents. Next, they used an approximate nearest neighbor (ANN) approach on the query and this set of negatives to find the negative closest to the input query. This negative was then used as the hard negative. In their experiments, they found that the optimal setting is at most two hard negatives per positive. Using more than two hard negatives had an adverse impact on the model quality. Denoised Hard NegativesTo solve the false negatives issue, RocketQA3 proposed to utilize a well-trained cross-encoder to remove false negatives from top-k ranked documents. Compared to a dual encoder, a cross-encoder model is inefficient to be used for inference in real-time, but it is also a highly effective and robust method. So a cross-encoder can be used on the top-k ranked documents from a retriever and only keep the documents that are predicted to be negatives with high confidence. The final (denoised) set of negatives is more reliable and can be used as hard negatives. ANCEWe saw earlier how Approximate nearest neighbor Negative Contrastive Learning (ANCE)7 can be used to sample negatives. The main drawback of this approach was that the inference is too expensive to compute per batch as it requires a forward pass on the entire corpus which is much bigger than the training batch. But it also means that the retrieved samples are global negatives from the entire corpus and can be considered hard negatives. They also show that the local, uninformative negative samples (with near zero gradients) can lead to diminishing gradient norms, large stochastic gradient variances, and slow learning convergence. Caution: Note that we do not want all of the negative training instances to be hard negatives. Using only hard negatives will change the representativeness of the retrieval task training data, which might impose a non-trivial bias on the learned embeddings. For example, the model may hardly learn to retrieve if the negatives are too weak; it may be optimized in the wrong direction if the negative samples are biased8. In Facebook Search2, the authors found that models trained simply using hard negatives cannot outperform models trained with random negatives. Their experiments showed that blending random and hard negatives in training is advantageous. Increasing the ratio of easy-to-hard negatives continued the model improvement up to a 100:1 easy-to-hard ratio. They also compared sampling from different rank positions and found that using the hardest examples is not the best strategy, and sampling between ranks 101-500 achieved the best model recall. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:4:1","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#in-batch-hard-negatives"},{"categories":["Information Retrieval"],"content":"\rHard Negative MiningThe training and inference processes for dual-encoder-based retrievers are usually inconsistent. During training, the retriever is given only a small candidate set for each question (usually 1 positive pair per question) to calculate the question-document similarity, while during inference it is required to identify a positive document for a given question out of millions of candidates. Using negative sampling methods, like cross-batch negatives, does help with reducing this vast discrepancy, but the model may still fail to learn effectively because the negatives might be too weak. Also, because fully annotating a large dataset can be prohibitive, there might also be a large number of unlabeled positives that might get picked as negative examples, leading to an increase in false positives by the model. Using reliable, hard negatives helps in alleviating such issues. Top-ranked NegativesA straightforward approach to picking hard negatives is to use the trained retriever to make inferences on negative samples and select the top-k ranked documents as negatives20. This approach can still suffer from false positives as a lot of negative samples could actually be unlabeled positive samples. In-batch Hard NegativesIn Facebook Search2, the authors used the in-batch negatives to first get a set of candidate negative documents. Next, they used an approximate nearest neighbor (ANN) approach on the query and this set of negatives to find the negative closest to the input query. This negative was then used as the hard negative. In their experiments, they found that the optimal setting is at most two hard negatives per positive. Using more than two hard negatives had an adverse impact on the model quality. Denoised Hard NegativesTo solve the false negatives issue, RocketQA3 proposed to utilize a well-trained cross-encoder to remove false negatives from top-k ranked documents. Compared to a dual encoder, a cross-encoder model is inefficient to be used for inference in real-time, but it is also a highly effective and robust method. So a cross-encoder can be used on the top-k ranked documents from a retriever and only keep the documents that are predicted to be negatives with high confidence. The final (denoised) set of negatives is more reliable and can be used as hard negatives. ANCEWe saw earlier how Approximate nearest neighbor Negative Contrastive Learning (ANCE)7 can be used to sample negatives. The main drawback of this approach was that the inference is too expensive to compute per batch as it requires a forward pass on the entire corpus which is much bigger than the training batch. But it also means that the retrieved samples are global negatives from the entire corpus and can be considered hard negatives. They also show that the local, uninformative negative samples (with near zero gradients) can lead to diminishing gradient norms, large stochastic gradient variances, and slow learning convergence. Caution: Note that we do not want all of the negative training instances to be hard negatives. Using only hard negatives will change the representativeness of the retrieval task training data, which might impose a non-trivial bias on the learned embeddings. For example, the model may hardly learn to retrieve if the negatives are too weak; it may be optimized in the wrong direction if the negative samples are biased8. In Facebook Search2, the authors found that models trained simply using hard negatives cannot outperform models trained with random negatives. Their experiments showed that blending random and hard negatives in training is advantageous. Increasing the ratio of easy-to-hard negatives continued the model improvement up to a 100:1 easy-to-hard ratio. They also compared sampling from different rank positions and found that using the hardest examples is not the best strategy, and sampling between ranks 101-500 achieved the best model recall. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:4:1","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#denoised-hard-negatives"},{"categories":["Information Retrieval"],"content":"\rHard Negative MiningThe training and inference processes for dual-encoder-based retrievers are usually inconsistent. During training, the retriever is given only a small candidate set for each question (usually 1 positive pair per question) to calculate the question-document similarity, while during inference it is required to identify a positive document for a given question out of millions of candidates. Using negative sampling methods, like cross-batch negatives, does help with reducing this vast discrepancy, but the model may still fail to learn effectively because the negatives might be too weak. Also, because fully annotating a large dataset can be prohibitive, there might also be a large number of unlabeled positives that might get picked as negative examples, leading to an increase in false positives by the model. Using reliable, hard negatives helps in alleviating such issues. Top-ranked NegativesA straightforward approach to picking hard negatives is to use the trained retriever to make inferences on negative samples and select the top-k ranked documents as negatives20. This approach can still suffer from false positives as a lot of negative samples could actually be unlabeled positive samples. In-batch Hard NegativesIn Facebook Search2, the authors used the in-batch negatives to first get a set of candidate negative documents. Next, they used an approximate nearest neighbor (ANN) approach on the query and this set of negatives to find the negative closest to the input query. This negative was then used as the hard negative. In their experiments, they found that the optimal setting is at most two hard negatives per positive. Using more than two hard negatives had an adverse impact on the model quality. Denoised Hard NegativesTo solve the false negatives issue, RocketQA3 proposed to utilize a well-trained cross-encoder to remove false negatives from top-k ranked documents. Compared to a dual encoder, a cross-encoder model is inefficient to be used for inference in real-time, but it is also a highly effective and robust method. So a cross-encoder can be used on the top-k ranked documents from a retriever and only keep the documents that are predicted to be negatives with high confidence. The final (denoised) set of negatives is more reliable and can be used as hard negatives. ANCEWe saw earlier how Approximate nearest neighbor Negative Contrastive Learning (ANCE)7 can be used to sample negatives. The main drawback of this approach was that the inference is too expensive to compute per batch as it requires a forward pass on the entire corpus which is much bigger than the training batch. But it also means that the retrieved samples are global negatives from the entire corpus and can be considered hard negatives. They also show that the local, uninformative negative samples (with near zero gradients) can lead to diminishing gradient norms, large stochastic gradient variances, and slow learning convergence. Caution: Note that we do not want all of the negative training instances to be hard negatives. Using only hard negatives will change the representativeness of the retrieval task training data, which might impose a non-trivial bias on the learned embeddings. For example, the model may hardly learn to retrieve if the negatives are too weak; it may be optimized in the wrong direction if the negative samples are biased8. In Facebook Search2, the authors found that models trained simply using hard negatives cannot outperform models trained with random negatives. Their experiments showed that blending random and hard negatives in training is advantageous. Increasing the ratio of easy-to-hard negatives continued the model improvement up to a 100:1 easy-to-hard ratio. They also compared sampling from different rank positions and found that using the hardest examples is not the best strategy, and sampling between ranks 101-500 achieved the best model recall. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:4:1","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#ance"},{"categories":["Information Retrieval"],"content":"\rHard Positive MiningWhile the majority of the literature focuses on bootstrapping (hard negative mining), some works also suggest mining hard positives. For example: In Facebook Search2 the authors mined potential target results for failed search sessions from searchers’ activity logs. They found that these hard positives improved the model’s effectiveness and these hard positives combined with other positive data (like clicks or impressions) can further improve the model recall. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:4:2","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#hard-positive-mining"},{"categories":["Information Retrieval"],"content":"\rOther Approaches","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:5:0","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#other-approaches"},{"categories":["Information Retrieval"],"content":"\rData AugmentationData Augmentation is another approach commonly used to create additional positive and negative pairs for training. For example, RocketQA3 trains a cross-encoder and uses it on unlabeled data to make predictions. The positive and negative documents with high confidence scores are used to augment the training. They selected the top retrieved documents with scores less than 0.1 as negatives and those with scores higher than 0.9 as positives. They also showed that the model performance increases as the size of the augmented data increases. In AugTriever21, the authors propose multiple unsupervised ways to extract salient spans from text documents to create pseudo-queries that can be used to create positive and negative pairs. For structured documents, they recommend utilizing structural heuristics (like title and anchor texts). For unstructured documents, they use approaches like BM25, dual encoder, and pre-trained language model to measure the salience between a document and various spans extracted from it. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:5:1","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#data-augmentation"},{"categories":["Information Retrieval"],"content":"\rMiscellaneousIn Learning To Retrieve (LTRe), Zhan et al.8 use a novel approach that doesn’t require any negative sampling. First, a pretrained document encoder to represent documents as embeddings, which are fixed throughout the training process. At each training step, a dense retrieval model outputs a batch of query representations. LTRe then uses them and performs full retrieval. Based on the retrieval results, it updates the model parameters such that queries are mapped close to the relevant documents and far from the irrelevant ones. The methods listed above are taken from a wide survey of the NLP domain. Parallel to this, there is also a lot of work done in the computer vision domain by researchers to build positive and negative pairs for effective representation learning. For example, two independent data augmentations to the same image provide two “views”, that could be considered as a positive pair. There are also dependent transformations designed to reduce the correlation between views. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:5:2","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#miscellaneous"},{"categories":["Information Retrieval"],"content":"\rSummaryIn this article, we looked at a wide range of methods to create positive and negative samples for representation learning. The article also explored the need for mining hard positive and negative samples. Through the literature survey, we learned that both easy and hard examples are important for training a retriever model. Some research work also shows that models that are optimized for recall can benefit from random negative sampling, while the models focused more on precision benefit from hard negatives2. These learnings can greatly help in designing search ranking systems that balance the effectiveness and efficiency tradeoffs. ","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:6:0","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#summary"},{"categories":["Information Retrieval"],"content":"\rReferences Chang, W., Yu, F.X., Chang, Y., Yang, Y., \u0026 Kumar, S. (2020). Pre-training Tasks for Embedding-based Large-scale Retrieval. ArXiv, abs/2002.03932. ↩︎ Huang, J., Sharma, A., Sun, S., Xia, L., Zhang, D., Pronin, P., Padmanabhan, J., Ottaviano, G., \u0026 Yang, L. (2020). Embedding-based Retrieval in Facebook Search. Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \u0026 Data Mining. ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ Qu, Y., Ding, Y., Liu, J., Liu, K., Ren, R., Zhao, X., Dong, D., Wu, H., \u0026 Wang, H. (2020). RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering. North American Chapter of the Association for Computational Linguistics. ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ Chen, T., Kornblith, S., Norouzi, M., \u0026 Hinton, G.E. (2020). A Simple Framework for Contrastive Learning of Visual Representations. ArXiv, abs/2002.05709. ↩︎ He, K., Fan, H., Wu, Y., Xie, S., \u0026 Girshick, R.B. (2019). Momentum Contrast for Unsupervised Visual Representation Learning. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 9726-9735. ↩︎ ↩︎ ↩︎ Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A., \u0026 Grave, E. (2021). Towards Unsupervised Dense Information Retrieval with Contrastive Learning. ArXiv, abs/2112.09118. ↩︎ ↩︎ ↩︎ Xiong, L., Xiong, C., Li, Y., Tang, K., Liu, J., Bennett, P., Ahmed, J., \u0026 Overwijk, A. (2020). Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval. ArXiv, abs/2007.00808. ↩︎ ↩︎ ↩︎ ↩︎ Zhan, J., Mao, J., Liu, Y., Zhang, M., \u0026 Ma, S. (2020). Learning To Retrieve: How to Train a Dense Retrieval Model Effectively and Efficiently. ArXiv, abs/2010.10469. ↩︎ ↩︎ ↩︎ ↩︎ Karpukhin, V., Oğuz, B., Min, S., Lewis, P., Wu, L.Y., Edunov, S., Chen, D., \u0026 Yih, W. (2020). Dense Passage Retrieval for Open-Domain Question Answering. ArXiv, abs/2004.04906. ↩︎ ↩︎ ↩︎ Chen, T., Kornblith, S., Norouzi, M., \u0026 Hinton, G.E. (2020). A Simple Framework for Contrastive Learning of Visual Representations. ArXiv, abs/2002.05709. ↩︎ Wu, Z., Xiong, Y., Yu, S.X., \u0026 Lin, D. (2018). Unsupervised Feature Learning via Non-parametric Instance Discrimination. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 3733-3742. ↩︎ Meng, R., Liu, Y., Yavuz, S., Agarwal, D., Tu, L., Yu, N., Zhang, J., Bhat, M., \u0026 Zhou, Y. (2022). AugTriever: Unsupervised Dense Retrieval by Scalable Data Augmentation. ArXiv. /abs/2212.08841 ↩︎ Luan, Y., Eisenstein, J., Toutanova, K., \u0026 Collins, M. (2020). Sparse, Dense, and Attentional Representations for Text Retrieval. Transactions of the Association for Computational Linguistics, 9, 329-345. ↩︎ Lee, K., Chang, M., \u0026 Toutanova, K. (2019). Latent Retrieval for Weakly Supervised Open Domain Question Answering. ArXiv, abs/1906.00300. ↩︎ ↩︎ Ram, O., Shachaf, G., Levy, O., Berant, J., \u0026 Globerson, A. (2021). Learning to Retrieve Passages without Supervision. ArXiv. /abs/2112.07708 ↩︎ Guu, K., Lee, K., Tung, Z., Pasupat, P., \u0026 Chang, M. (2020). Retrieval Augmented Language Model Pre-Training. International Conference on Machine Learning. ↩︎ Neelakantan, A., Xu, T.. Text and Code Embeddings by Contrastive Pre-Training. ArXiv. /abs/2201.10005 ↩︎ Chen, X., Lakhotia, K., Oğuz, B., Gupta, A., Lewis, P., Peshterliev, S., Mehdad, Y., Gupta, S., \u0026 Yih, W. (2021). Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One? Conference on Empirical Methods in Natural Language Processing. ↩︎ Shrivastava, A., Gupta, A.K., \u0026 Girshick, R.B. (2016). Training Region-Based Object Detectors with Online Hard Example Mining. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 761-769. ↩︎ Gillick, D., Kulkarni, S., Lansing, L., Presta, A., Baldridge, J., Ie, E., \u0026 Garcia-Olano, D. (2019). Learning Dense Representations for Entity Retrieval. ArXiv, abs/1909.10506. ↩︎ Meng, R., Liu, Y., Yavuz, S., Agarwal, D., Tu, L., Yu, N., Zhang, J., Bhat, M.M., \u0026 Zhou, Y. (2022). AugTriever: Unsupervi","date":"2023-03-22","objectID":"/posts/2023/03/pairing-for-representation/:7:0","series":null,"tags":["literature review","retrieval","ranking"],"title":"Positive and Negative Sampling Strategies for Representation Learning in Semantic Search","uri":"/posts/2023/03/pairing-for-representation/#references"},{"categories":["Information Retrieval"],"content":"\rIntroductionText retrieval and ranking simply refers to the task of finding a ranked list of the most relevant documents or passages out of a large text collection given a user query. Many Information Retrieval (IR) applications such as search ranking, open-domain question answering, fact verification, etc. use text retrievers to find the text that fulfills users’ information needs. This article gives a brief overview of a standard text ranking workflow and then introduces several recently proposed ideas to utilize large language models to enhance the text ranking task. ","date":"2023-03-16","objectID":"/posts/2023/03/llm-for-text-ranking/:1:0","series":null,"tags":["literature review","retrieval","ranking","LLM"],"title":"Zero and Few Shot Text Retrieval and Ranking Using Large Language Models","uri":"/posts/2023/03/llm-for-text-ranking/#introduction"},{"categories":["Information Retrieval"],"content":"\rSyntactic vs Semantic ApproachesSome of the most common methods used in these applications are based on keyword matching, sophisticated dense representations, or a hybrid of the two. BM25 is one simple term-matching-based scoring function that was proposed decades ago1, but is still immensely popular in the IR domain. BM25 only uses the terms common to both the query and the document and isn’t able to recognize synonyms and distinguish between ambiguous words2. Still, a lot of studies in the field have proven BM25 to be a really strong baseline3. Neural information retrieval, on the other hand, captures and compares the semantics of queries and documents. Dense representation-based neural retrieval models usually take the form of either a bi-encoder network (aka “dual encoder”4, “two tower network”, “Siamese network”, “DSSM”) or a cross-encoder network. Dense Retrievers. Figure adapted from ColBERT paper [^3]\rA bi-encoder network independently learns latent representations for query and document inputs and interacts them only at the final layer to calculate a similarity function, such as dot-product, cosine, MaxSim5, or Euclidian distance, on them. After offline training, the document encoder is often frozen and an indexing solution like FAISS is used to fetch document embeddings in real-time during inference. Their efficiency has made bi-encoder a really popular choice for production environments, even though the late interaction makes the model less effective. To read more about the Two-tower models’ usage in the recommender systems domain and some potential architectural extensions, please refer to this article. A cross-encoder network takes a query and a document vector as the input and calculates the relevance scores as the maximum inner product over it. Cross-Encoders achieve higher performance than bi-encoders due to rich interactions, however, they do not scale well for large datasets6. To evaluate these dense retrieval models, metrics like accuracy, mean rank, and mean reciprocal rank (MRR) are used if the relevance score is binary, otherwise, metrics like discounted cumulative gain (DCG) and normalized DCG (nDCG) are used if a graded relevance score is used. ","date":"2023-03-16","objectID":"/posts/2023/03/llm-for-text-ranking/:2:0","series":null,"tags":["literature review","retrieval","ranking","LLM"],"title":"Zero and Few Shot Text Retrieval and Ranking Using Large Language Models","uri":"/posts/2023/03/llm-for-text-ranking/#syntactic-vs-semantic-approaches"},{"categories":["Information Retrieval"],"content":"\rCascade Ranking PipelineWhile designing end-to-end retrieval systems, we often have to balance the tradeoff between effectiveness and efficiency. As mentioned earlier, while a cross-encoder can be highly effective, its time complexity can be prohibitive for most real-time production use cases that work with large-scale document collections. A bi-encoder model can be much more efficient but doesn’t usually have the same level of accuracy as the cross-encoders. To address this tradeoff, a cascading ranking pipeline is adopted where increasingly complex ranking functions progressively prune and refine the set of candidate documents to minimize retrieval latency and maximize result set quality7. In a semantic search pipeline, a relatively simpler algorithm like Elasticsearch, BM25, bi-encoder, or a combination may be used to retrieve the top-n (e.g. 100, 1000) candidate documents followed by a more complex algorithm like cross-encoder to re-rank these candidates. Often modern IR systems use multi-stage re-ranking, for example, by using a bi-encoder followed by a cheap cross-encoder, followed by a more expensive cross-encoder model for the final re-ranking of the top candidates8. As an example, open-domain question-answering (ODQA) workflow is usually implemented as a two-stage pipeline: 1) given a question, a context retriever selects relevant passages and 2) a question-answering model, also known as a reader, answers the given question based on the retrieved passages9. This decoupling also allows for independent advancements of the two models. ","date":"2023-03-16","objectID":"/posts/2023/03/llm-for-text-ranking/:3:0","series":null,"tags":["literature review","retrieval","ranking","LLM"],"title":"Zero and Few Shot Text Retrieval and Ranking Using Large Language Models","uri":"/posts/2023/03/llm-for-text-ranking/#cascade-ranking-pipeline"},{"categories":["Information Retrieval"],"content":"\rZero and Few Shot SettingsA significant challenge in developing neural retrievers is the lack of domain-specific training data. Low-resource target domains lack labeled training data. Manually constructing high-quality datasets is often costly and takes a lot of time. It could be especially difficult for retrieval applications as they require queries from real users. There are a few general-purpose datasets like MS MARCO and Natural Questions, but they do not always generalize well for out-of-domain uses and are often not available under a commercial license. Additionally, the original paper that introduced the BEIR IR benchmark showed that the performance of dense retrievers severely degrades under a domain shift (i.e. the shift in data distribution), often performing worse than traditional models such as BM25. This paper also showed that dense retrievers require a large amount of training data to work well10. Hence zero-shot and few-shot adaptions of effective retrieval and ranking models do not necessarily produce generalizations that are fully compatible with the target domain, leading to severe degradation when the source and target domains differ drastically9 11. To address this, a recent line of research work has started using generative large language models (LLMs) to do zero-shot or few-shot domain adaptions of retrieval and ranking models. One reason for the popularity of these LLMs has been their capability to produce better performance from smaller quantities of labeled data12. A lot of the research work focuses on prompting LLMs with instructions for the task and a few examples in natural language to generate synthetic examples to finetune task-specific models. In the next section, we will take a look at some of the most recent and prominent research proposals on using LLMs to help retrievers and rankers with target domain adaption. ","date":"2023-03-16","objectID":"/posts/2023/03/llm-for-text-ranking/:4:0","series":null,"tags":["literature review","retrieval","ranking","LLM"],"title":"Zero and Few Shot Text Retrieval and Ranking Using Large Language Models","uri":"/posts/2023/03/llm-for-text-ranking/#zero-and-few-shot-settings"},{"categories":["Information Retrieval"],"content":"\rUsing LLMs for Zero or Few Shot Domain Adaption","date":"2023-03-16","objectID":"/posts/2023/03/llm-for-text-ranking/:5:0","series":null,"tags":["literature review","retrieval","ranking","LLM"],"title":"Zero and Few Shot Text Retrieval and Ranking Using Large Language Models","uri":"/posts/2023/03/llm-for-text-ranking/#using-llms-for-zero-or-few-shot-domain-adaption"},{"categories":["Information Retrieval"],"content":"\rInParsIn Inquisitive Parrots for Search (InPars), Bonifacio et al.13 used LLMs to generate synthetic data for IR tasks in a few-shot manner under minimal supervision. This data is then used to finetune a neural reranker model that is used to rerank search results in a pipeline comprised of a BM25 retriever followed by a neural monoT5 reranker. For the reranker, they tried a monoT5 model with 220M and also one with 3B parameters. InPars Method\rThe training set consists of query, positive, and negative document triplets. Given a collection of documents, 100,000 documents are randomly sampled. Documents with less than 300 characters are discarded and a new document is sampled instead. GPT-3 Curie model is used as the LLM that generates one question corresponding to each of the sampled documents based on greedy decoding (temperature = 0). They experimented with two prompting strategies: Vanilla prompting, which uses 3 randomly chosen pairs of the document and relevant question from the MS MARCO dataset as shown on the left in the following diagram ({document_text} is replaced with the sampled document and the LLM generates a question one token at a time) Guided by Bad Questions (GBQ), that uses a strategy similar to Vanilla but the corresponding question from MS MARCO is marked as a “bad” question while a manually created example is marked a “good” question. This was done to encourage the model to produce more contextual-aware questions than the one from MS MARCO. Only the good questions are used to create the question-document positive pair. Negative examples were sampled from the top candidates returned by the BM25 method that weren’t relevant documents. Only top-k generated examples (sorted by log probability output of the LLM) were used to finetune the reranker model. InPars: Vanilla (left) vs GBQ (right) prompts\rThe Vanilla prompting strategy worked better for two out of the five tested datasets while GBQ performed better for the other three. On the target domain, the retrieval model finetuned solely on the synthetic examples outperformed BM25 and self-supervised dense method baselines. While models finetuned on both supervised and synthetic data achieved better results than models finetuned only on the supervised data. The code, model, and data for InPars are available on GitHub. ","date":"2023-03-16","objectID":"/posts/2023/03/llm-for-text-ranking/:5:1","series":null,"tags":["literature review","retrieval","ranking","LLM"],"title":"Zero and Few Shot Text Retrieval and Ranking Using Large Language Models","uri":"/posts/2023/03/llm-for-text-ranking/#inpars"},{"categories":["Information Retrieval"],"content":"\rPromptagator 🐊In Prompt-base Query Generation for Retriever (Promptagator 🐊), Dai et al.14 argued that different retrieval tasks have very different search intents (like retrieving entities, finding evidence, etc.). So they proposed a few-shot setting for dense retrievers where each task comes with a short description and a few annotated examples to clearly illustrate the search intents. Their proposed method “Promptagator” relies solely on a few (2 to 8) in-domain relevant query-document examples from the target tasks without using any query-document pairs from other tasks or datasets. The authors used the following instruction prompt: $$ (e_{doc}(d_{1}), e_{query}(q1), …, e_{doc}(d_{k}), e_{query}(qk), e_{doc}(d)) $$ where $e_{doc}(d)$ and $e_{query}(q)$ are one of the k-pairs of the task-specific document and query descriptions respectively, and $d$ is a new document. They ran the prompt on all documents from the corpus and created a large set of synthetic examples using a FLAN-137B LLM. During prompt engineering, they used 2 to 8 examples depending on the input length limit of FLAN and generated 8 questions per document using sampling decoding (temperature=0.7). The quality of generated queries was further improved by ensuring “round-trip consistency”, i.e. the query should retrieve its source passage. To do this consistency filtering, they first trained an initial retriever using only the synthetic query and document pairs. Then the kept query only if the corresponding document occurs among top-K (the used K=1) passages returned in the prediction by the same retriever. This seemingly counter-intuitive approach worked well in their experiments. Promptagator++ Training Pipeline\rFinally, they trained a retriever (a dual encoder) followed by a cross-attention reranker on the filtered data. The dual encoder was a GTR initialized from a T5-110M network that was pretrained on the C4 dataset using independent cropping (using two random crops from the same document as positive pairs and in-batch negatives) with cross-entropy loss. This dual encoder was then finetuned using synthetic data. After this training is done, the same model is used to perform the consistency filtering mentioned before. The reranker component was proposed in the Promptagator++ variant and was trained using negative data sampled from the retriever step and the positive synthetic data. They also tested a zero-shot approach for query generation where the following prompt was used irrespective of the task: '{d} Read the passage and generate a query.'. In their experiments, Promptagator outperformed ColBERTv2 and SPLADEv2 on all tested retrieval tasks. ","date":"2023-03-16","objectID":"/posts/2023/03/llm-for-text-ranking/:5:2","series":null,"tags":["literature review","retrieval","ranking","LLM"],"title":"Zero and Few Shot Text Retrieval and Ranking Using Large Language Models","uri":"/posts/2023/03/llm-for-text-ranking/#promptagator-"},{"categories":["Information Retrieval"],"content":"\rUPRIn Unsupervised Passage Re-ranker (UPR), Sachan et al.15 proposed a fully unsupervised pipeline consisting of a retriever and a reranker that can outperform supervised dense retrieval models (like DPR4) alone. They applied UPR to a zero-shot question generation task where given a question, the retriever fetches the most relevant passages and reranker reorders these passages such that a passage with the correct answer is ranked as highly as possible. UPR Pipeline\rThe retriever could be based on any unsupervised method like BM25, Contriever16, or MSS17, the only requirement is that the retriever provides the K most relevant passages. For the reranker, they experimented with off-the-shelf T5-lm-adapt, T0, and GPT-neo models. The rerankers were given the following prompt in a zero-shot manner: 'Passage: {p}. Please write a question based on this passage.'. The reranking score is computed as $p(z_{i}|q)$ for each passage $z_{i}$ and a query $q$. The paper shows that this relevancy score can be approximated by computing the average log-likelihood of the question conditioned on the passage, i.e. $log p(q|z)$. In their experiments, UPR is shown to improve both unsupervised and supervised retrieval tasks in terms of top-20 passage retrieval accuracy. However, due to the LLM usage in their pipeline, UPR also suffers from high latency issues with a complexity directly proportional to the product of question and passage tokens and the number of layers in LLM. The code, data, and model checkpoints for UPR are available on GitHub. ","date":"2023-03-16","objectID":"/posts/2023/03/llm-for-text-ranking/:5:3","series":null,"tags":["literature review","retrieval","ranking","LLM"],"title":"Zero and Few Shot Text Retrieval and Ranking Using Large Language Models","uri":"/posts/2023/03/llm-for-text-ranking/#upr"},{"categories":["Information Retrieval"],"content":"\rHyDEIn Hypothetical Document Embeddings (HyDE), Gao et al. 18 proposed a novel zero-shot dense retrieval method. Given a query, they first zero-shot instruct an instruction-following language model (InstructGPT) to generate a synthetic (“hypothetical”) document. Next, they use an unsupervised contrastively learned encoder (like a Contriever) to encode this hypothetical document into an embedding vector. Finally, they use a nearest-neighbor approach to fetch similar real documents based on vector similarity in corpus embedding space. The assumption here is that the bottleneck layer in the encoder filters out factual errors and incorrect details in the hypothetical document. In their experiments, HyDE outperformed the state-of-the-art unsupervised Contriever method and also performed comparably to finetuned retrievers on various tasks. The code for the HyDE method is available on GitHub. The HyDE Model\r","date":"2023-03-16","objectID":"/posts/2023/03/llm-for-text-ranking/:5:4","series":null,"tags":["literature review","retrieval","ranking","LLM"],"title":"Zero and Few Shot Text Retrieval and Ranking Using Large Language Models","uri":"/posts/2023/03/llm-for-text-ranking/#hyde"},{"categories":["Information Retrieval"],"content":"\rGenReadIn Generate-then-Read (GenRead), Yu et al.19 replace the retriever in the traditional retrieve-then-read QA pipeline with a LLM generator model to create a generate-then-read pipeline instead. Their approach does not require any external world or domain knowledge. They generate a synthetic document using an InstructGPT LLM given the input query and then use the reader model on the generated document to produce the final answer. Using multiple datasets, they show that the LLM-generated document is more likely to contain correct answers than the top retrieved document, which justifies the use of the generator in this context. Under a zero-shot setting, the generator uses a prompt like: Generate a background document to answer the given question. {question placeholder} and the reader does zero-shot reading comprehension with a prompt like: Refer to the passage below and answer the following question. Passage: {background placeholder} Question: {question placeholder}. A supervised setting with a reader model like FiD (Fusion-in-Decoder) was shown to provide better performance than a zero-shot setting. The code and generated documents for GenRead are available on GitHub. GenRead's clustering-based prompting method\rTo improve recall performance, the authors propose a clustering-based prompt method that introduces variance and diversity in generated documents. They do offline K-Means clustering on GPT-3 embeddings of a corpus of query-document pairs. At inference, a fixed number of documents are sampled from each of these clusters and given to the reader model. Their experiments showed that the GenRead model outperformed zero-shot retrieve-then-read pipeline models that used the Google search engine to get relevant contextual documents. A major limitation of this method is that the LLM model needs retraining to update the latest external world or domain knowledge. ","date":"2023-03-16","objectID":"/posts/2023/03/llm-for-text-ranking/:5:5","series":null,"tags":["literature review","retrieval","ranking","LLM"],"title":"Zero and Few Shot Text Retrieval and Ranking Using Large Language Models","uri":"/posts/2023/03/llm-for-text-ranking/#genread"},{"categories":["Information Retrieval"],"content":"\rInPars-v2The authors of InPars released an update, called InPars-v220, where they swapped GPT-3 LLM with the open-source GPT-J (6B) model. For prompting the LLM, they only used the GBQ strategy proposed in InPars-v1. Similar to the v1 proposal they sampled 100K documents from the corpus and generated one synthetic query per document. However, instead of filtering them to top-10K pairs with the highest log probabilities of generation like the v1 method, they used a relevancy score calculated by a monoT5 (3B) model finetuned on MS MARCO, to keep the top-10K pairs. Compared to InPars-v1, this model showed better performance on a majority of the tested datasets. The code, finetuned model, and synthetic data for InPars-v2 are available on GitHub. ","date":"2023-03-16","objectID":"/posts/2023/03/llm-for-text-ranking/:5:6","series":null,"tags":["literature review","retrieval","ranking","LLM"],"title":"Zero and Few Shot Text Retrieval and Ranking Using Large Language Models","uri":"/posts/2023/03/llm-for-text-ranking/#inpars-v2"},{"categories":["Information Retrieval"],"content":"\rInPars-LightIn InPars-Light Boytsov et al.21 did a reproducibility study of InPars and also proposed some cost-effective improvements. Instead of the proprietary GPT-3, they chose to use the open-source LLM models BLOOM and GPT-J, and instead of using a MonoT5 (220M/3B), they experimented with MiniLM (30M), ERNIEv2 (335M) and DeBERTAv3 (435M) reranker models. For prompting the LLM, they used the ‘vanilla’ strategy proposed in the InPars paper. For consistency checking they used the same approach as used in the Promptagator model but with K value set to 3. They also pretrained the reranker on all-domain data. Through experiments, they showed that for a good ranking output, they only needed to rerank 100 candidates as opposed to 1000 in InPars. ","date":"2023-03-16","objectID":"/posts/2023/03/llm-for-text-ranking/:5:7","series":null,"tags":["literature review","retrieval","ranking","LLM"],"title":"Zero and Few Shot Text Retrieval and Ranking Using Large Language Models","uri":"/posts/2023/03/llm-for-text-ranking/#inpars-light"},{"categories":["Information Retrieval"],"content":"\rUDAPDRIn Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers (UDAPDR), Falcon et al.22 used a two-stage LLM pipeline (one powerful and expensive LLM followed by a smaller and cheaper LLM) to generate synthetic queries in zero-shot settings. These queries are used to finetune a reranker model. This reranker is then distilled into a single efficient retriever. UDAPDR Approach\rThis approach requires access to in-domain passages (but no in-domain queries or labels are required). These passages and LLM-prompting is used to generate a large number of synthetic queries. These passages are first fed to a GPT-3 text-davinci-002 model, using five prompting strategies as shown below. Note that the first two prompts are the same as the InPars paper and the other three zero-short strategies are taken from another recent paper. UDAPDR prompting strategies\rThe generated synthetic query and document pairs are then used to populate the following prompt template. This prompt is used to generate a good query for a new passage through a smaller LLM (they used the FLAN-T5 XXL model). While the first LLM was given a few (X = 5 to 100) sampled passages (to generate 5X synthetic queries), this smaller LLM is given a much large sampled set (10K to 100K) of passages. UDAPDR second stage prompt template\rSimilar to earlier work, consistency filtering is applied. UDAPDR uses a zero-shot ColBERTv2 model for this purpose and keeps the synthetic query only if it returns its gold passage within the top-20 results. Finally, a DeBERTaV3-Large reranker trained using this filtered synthetic data is distilled into a ColBERTv2 retriever model. Their experiments showed good zero-shot results in long-tail domains. One drawback of this approach is that it requires a substantial number of passages from the target domain. The code and synthetic data for UDAPDR is available on GitHub. ","date":"2023-03-16","objectID":"/posts/2023/03/llm-for-text-ranking/:5:8","series":null,"tags":["literature review","retrieval","ranking","LLM"],"title":"Zero and Few Shot Text Retrieval and Ranking Using Large Language Models","uri":"/posts/2023/03/llm-for-text-ranking/#udapdr"},{"categories":["Information Retrieval"],"content":"\rDataGenIn DataGen, Dua et al.9 proposed a taxonomy for dataset shift and showed that zero-shot adaptions do not work well in cases where the target domain distribution is very far from the source domain. To fix this, they prompted a Pathways Language Model (PaLM) in few-shot settings to generate a query given a passage. They prompted the model with After reading the article, \u003c\u003ccontext\u003e\u003e the doctor said \u003c\u003csentence\u003e\u003e. for PubMed articles. They replaced “doctor” with engineer, journalist, and poster for StackOverflow, DailyMail, and Reddit target corpora respectively. They filtered out the questions that repeated the passage verbatim or had a 75% or more word overlap with it. Then they used both supervised and synthetic data to train their retriever model. ","date":"2023-03-16","objectID":"/posts/2023/03/llm-for-text-ranking/:5:9","series":null,"tags":["literature review","retrieval","ranking","LLM"],"title":"Zero and Few Shot Text Retrieval and Ranking Using Large Language Models","uri":"/posts/2023/03/llm-for-text-ranking/#datagen"},{"categories":["Information Retrieval"],"content":"\rSummarySimilar to a majority of NLP tasks, information retrieval has recently witnessed a revolution due to large pretrained transformer models. The ability of these models to understand task instructions specified in natural language and then perform well on tasks in a zero-shot or few-shot manner has unlocked a world of possibilities and exciting solutions. This article reviewed some very recent proposals from the research community to boost text retrieval and ranking tasks using LLMs. ","date":"2023-03-16","objectID":"/posts/2023/03/llm-for-text-ranking/:6:0","series":null,"tags":["literature review","retrieval","ranking","LLM"],"title":"Zero and Few Shot Text Retrieval and Ranking Using Large Language Models","uri":"/posts/2023/03/llm-for-text-ranking/#summary"},{"categories":["Information Retrieval"],"content":"\rReferences Robertson, Stephen. (2004). Understanding Inverse Document Frequency: On Theoretical Arguments for IDF. Journal of Documentation - J DOC. 60. 503-520. 10.1108/00220410410560582. ↩︎ Wang, K., Thakur, N., Reimers, N., \u0026 Gurevych, I. (2021). GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval. North American Chapter of the Association for Computational Linguistics. ↩︎ Ma, X., Sun, K., Pradeep, R., \u0026 Lin, J.J. (2021). A Replication Study of Dense Passage Retriever. ArXiv, abs/2104.05740. ↩︎ Karpukhin, Vladimir \u0026 Oğuz, Barlas \u0026 Min, Sewon \u0026 Wu, Ledell \u0026 Edunov, Sergey \u0026 Chen, Danqi \u0026 Yih, Wen-tau. (2020). Dense Passage Retrieval for Open-Domain Question Answering. ↩︎ ↩︎ Khattab, O., \u0026 Zaharia, M.A. (2020). ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. ↩︎ SBERT. Cross-Encoders. https://www.sbert.net/examples/applications/cross-encoder/README.html ↩︎ Wang, L., Lin, J.J., \u0026 Metzler, D. (2011). A cascade ranking model for efficient ranked retrieval. Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. ↩︎ Zhou, G., \u0026 Devlin, J. (2021). Multi-Vector Attention Models for Deep Re-ranking. Conference on Empirical Methods in Natural Language Processing. ↩︎ Dua, D., Strubell, E., Singh, S., \u0026 Verga, P. (2022). To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering. ArXiv, abs/2212.10381. ↩︎ ↩︎ ↩︎ Thakur, N., Reimers, N., Ruckl’e, A., Srivastava, A., \u0026 Gurevych, I. (2021). BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models. ArXiv, abs/2104.08663. ↩︎ Improving Zero-Shot Ranking with Vespa Hybrid Search. https://blog.vespa.ai/improving-zero-shot-ranking-with-vespa/ ↩︎ Scao, T.L. et al. (2022). BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. ArXiv, abs/2211.05100. ↩︎ Bonifacio, L.H., Abonizio, H.Q., Fadaee, M., \u0026 Nogueira, R. (2022). InPars: Data Augmentation for Information Retrieval using Large Language Models. ArXiv, abs/2202.05144. ↩︎ Dai, Z., Zhao, V., Ma, J., Luan, Y., Ni, J., Lu, J., Bakalov, A., Guu, K., Hall, K.B., \u0026 Chang, M. (2022). Promptagator: Few-shot Dense Retrieval From 8 Examples. ArXiv, abs/2209.11755. ↩︎ Sachan, D.S., Lewis, M., Joshi, M., Aghajanyan, A., Yih, W., Pineau, J., \u0026 Zettlemoyer, L. (2022). Improving Passage Retrieval with Zero-Shot Question Generation. Conference on Empirical Methods in Natural Language Processing. ↩︎ Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A., \u0026 Grave, E. (2021). Unsupervised Dense Information Retrieval with Contrastive Learning. ↩︎ Sachan, D.S., Reddy, S., Hamilton, W., Dyer, C., \u0026 Yogatama, D. (2021). End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering. ArXiv, abs/2106.05346. ↩︎ Gao, Luyu \u0026 Ma, Xueguang \u0026 Lin, Jimmy \u0026 Callan, Jamie. (2022). Precise Zero-Shot Dense Retrieval without Relevance Labels. 10.48550/arXiv.2212.10496. ↩︎ Yu, W., Iter, D., Wang, S., Xu, Y., Ju, M., Sanyal, S., Zhu, C., Zeng, M., \u0026 Jiang, M. (2022). Generate rather than Retrieve: Large Language Models are Strong Context Generators. ArXiv, abs/2209.10063. ↩︎ Jeronymo, V., Bonifacio, L.H., Abonizio, H.Q., Fadaee, M., Lotufo, R.D., Zavrel, J., \u0026 Nogueira, R. (2023). InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval. ArXiv, abs/2301.01820. ↩︎ Boytsov, L., Patel, P., Sourabh, V., Nisar, R., Kundu, S., Ramanathan, R., \u0026 Nyberg, E. (2023). InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers. ArXiv, abs/2301.02998. ↩︎ Saad-Falcon, J., Khattab, O., Santhanam, K., Florian, R., Franz, M., Roukos, S., Sil, A., Sultan, M., \u0026 Potts, C. (2023). UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers. Ar","date":"2023-03-16","objectID":"/posts/2023/03/llm-for-text-ranking/:7:0","series":null,"tags":["literature review","retrieval","ranking","LLM"],"title":"Zero and Few Shot Text Retrieval and Ranking Using Large Language Models","uri":"/posts/2023/03/llm-for-text-ranking/#references"},{"categories":["Recommender systems"],"content":"\rIntroductionIn the age of information overload, recommender systems have become an indispensable tool in our digital lives to help catalog online content. A lot of large-scale recommender systems today are deployed as a cascade ranking architecture 1. Under such “cloud-to-edge” frameworks, a mobile client initiates a paging request to a cloud server when a user triggers a recommendation scenario, for example by opening their mobile app. And recommendation models on the server side retrieve and return a ranked list of items to be displayed to the user. Often these recommendations are computed in advance on the cloud server. An exciting line of research has recently started exploring the potential of extending this recommendation computation to edge devices. This article is intended to help understand the need and advantage of computing real-time recommendations on mobile devices. We will also look at the system design and implementation of some of the commercial recommender systems that are built on this idea and are serving billions of users. ","date":"2023-03-09","objectID":"/posts/2023/03/reranking-on-edge/:1:0","series":null,"tags":["literature review","recsys","ranking"],"title":"Next Gen Recommender Systems: Real-time reranking on mobile devices","uri":"/posts/2023/03/reranking-on-edge/#introduction"},{"categories":["Recommender systems"],"content":"\rWhy should we compute real-time recommendations on edge devices?","date":"2023-03-09","objectID":"/posts/2023/03/reranking-on-edge/:2:0","series":null,"tags":["literature review","recsys","ranking"],"title":"Next Gen Recommender Systems: Real-time reranking on mobile devices","uri":"/posts/2023/03/reranking-on-edge/#why-should-we-compute-real-time-recommendations-on-edge-devices"},{"categories":["Recommender systems"],"content":"\rShortcomings of the existing systemDue to network bandwidth and latency constraints, the server usually sends a batch of items and a pagination mechanism is implemented on the client. The server only sends the next batch when a recommendation event is triggered on the client, for example, when the user scrolls to the end of the current page. This means that the system is not designed to respond in real time to changing user behavior and interests, and it leads to the following shortcomings. Delays in getting system feedback: If a user makes her interests known earlier on, for example, by positively interacting with the first few videos in the recommended feed, the recommender system on the server can’t respond to this feedback until the next batch of videos is requested by the client. Depending on the architecture, it may even take tens of seconds or minutes for the recommender system on the server to process and respond to the newly collected user feedback because the new signals can only be processed at the server end. Also, some of the real-time client-specific features such as network conditions are not available to the cloud model in real-time. Delays in adapting to user interests: If the user interacts with an item on the current page, for example, by leaving positive feedback on a video, it is impossible to adjust the order of the remaining content of the batch, even if there are some videos that closely match the new known user interest. ","date":"2023-03-09","objectID":"/posts/2023/03/reranking-on-edge/:2:1","series":null,"tags":["literature review","recsys","ranking"],"title":"Next Gen Recommender Systems: Real-time reranking on mobile devices","uri":"/posts/2023/03/reranking-on-edge/#shortcomings-of-the-existing-system"},{"categories":["Recommender systems"],"content":"\rFactors promoting device-side computationsSeveral other factors help push the idea of cost-effective computation of recommendations on edge devices as mentioned below. On short-video platforms like TikTok, Instagram Reels, and YouTube Shorts, the user usually watches a diverse set of short videos in a short period that belong to different topics. In such an environment, real-time user interest is constantly changing. Device-side modeling can help reduce the latency in responding quickly to perceived user interests. Computing power on mobile phones has increased exponentially in recent years. For instance, the TI-84 calculator developed by Texas Instruments in 2004 is 350 times faster than Apollo computers and had 32 times more RAM and 14,500 times more ROM 2. On-device engines like PyTorch Mobile, Google’s TFLite, and Apple’s CoreML for DNN model inference and even training have also started to gain popularity. Hardware-efficient neural network architectures like MobileNets, and model compression techniques based on network pruning or quantization have made it possible to optimize computation for on-device inference. Deploying models on the device helps to reduce server congestion. It is prohibitive to train the recommender algorithm at the server with embedded personalized behavior sequences for every user. Personalized client-side models help to unbias the server-side recommender algorithms from long-tailed users or items. It is common to have discrepancies between training and test data distributions that make the global model not optimal for each user. On-device training approaches like Federated Learning also help resolve some of the privacy concerns around using real-time client-specific signals. ","date":"2023-03-09","objectID":"/posts/2023/03/reranking-on-edge/:2:2","series":null,"tags":["literature review","recsys","ranking"],"title":"Next Gen Recommender Systems: Real-time reranking on mobile devices","uri":"/posts/2023/03/reranking-on-edge/#factors-promoting-device-side-computations"},{"categories":["Recommender systems"],"content":"\rChallenges Data for individual users is often insufficient and sparse to be able to train or fine-tune a model local to the edge device. Few-shot learning also has a higher generalization error and hence a higher likelihood of failing to realize the desired personalization. Learning from on-device data often suffers from local optimization in the long term and can lead to overfitting for individual users. To respect the mobile data plans and battery consumption on the user device, the models have to be kept small in size and simple to compute with. Updating an on-device model is often constrained by factors like Wi-Fi availability and current battery level as well. In the on-device learning section, we will look at some of the solutions to handle the first two challenges mentioned above. Whereas in the on-device inference section, we will see some model optimizations and versioning methods to deal with the last two challenges. ","date":"2023-03-09","objectID":"/posts/2023/03/reranking-on-edge/:2:3","series":null,"tags":["literature review","recsys","ranking"],"title":"Next Gen Recommender Systems: Real-time reranking on mobile devices","uri":"/posts/2023/03/reranking-on-edge/#challenges"},{"categories":["Recommender systems"],"content":"\rDevice-side computation paradigmsWe can think of the majority of the prior work done in on-edge recommender systems as belonging to one of the two categories below. On-device Inference: In this strategy, the server sends the pre-trained model(s) to the device, and the device uses this model to run inference. This method saves communication latency between client and server while being able to capture real-time user behavior and system feedback. On-device Learning: A lot of the work done in this category belongs to the Federated recommender systems. This strategy aggregates the on-device trained information from multiple devices and then trains a centralized model on the server. This method overcomes some of the privacy concerns regarding user data usage. Later this article will introduce some alternatives to the federated recommender systems paradigm that do on-device learning. Next, we will look at the design philosophy and architecture choices of some industrial large-scale examples of mobile recommender systems paradigms currently serving billions of users. ","date":"2023-03-09","objectID":"/posts/2023/03/reranking-on-edge/:3:0","series":null,"tags":["literature review","recsys","ranking"],"title":"Next Gen Recommender Systems: Real-time reranking on mobile devices","uri":"/posts/2023/03/reranking-on-edge/#device-side-computation-paradigms"},{"categories":["Recommender systems"],"content":"\rOn-Device Inference\rKuaishou’s Short Video Recommendation on Mobile DevicesChinese social network app, Kuaishou, released a paper describing an on-device inference framework used in their billion-user scale short video application 3. They developed an on-device ranking model that used users’ real-time feedback of watched videos and client-specific real-time features. Their system can react immediately to users’ implicit or explicit feedback to make appropriate ordering adjustments to remaining candidate videos. Kuaishou real-time short video recommender system architecture\rOn the cloud side, we have a traditional recommender system along with a Mobile Ranking Model Training module that uses the input features generated from the access logs from user devices and trains a ranking model. This trained model is exported to TFLite format periodically and sent to the user device for deployment. While responding to the initial paging request, the server also sends the item features for the recommended candidates along with the candidate videos. The client uses these features along with the client-specific features (watched videos and corresponding feedback along with real-time features like network signal) for inference using the on-device ranking model. What input features are used?The server-side model primarily focuses on modeling users’ long-term interests and uses a lot of ID features (user id, video id, crossing features), users’ watch history, etc. The following table shows the features used by the on-device reranking model. Apart from these, the model also used some engineered cross features such as time since last impression, pXTR, and impression position diffs between with preceding historical item. Note that the server-side features are not used in the on-device model because the offline experiments showed that their information has already been distilled into the predictions from the server model. When is the on-device reranking triggered?Kuaishou’s interface allows for only one video to occupy the current screen space. At each user swipe, the client system performs reranking with the remaining candidate videos and inserts the top-ranked candidate video at the next position. Model Architecture\rTheir model also considers interaction among candidate videos. The watch history sequence is modeled using a multi-head attention (MHA) mechanism with Q projected from candidate item features and K, V projected from watch history sequence features. Only one candidate video is considered at a time. Outputs of the two MHA modules are concatenated with target features and other features and passed to a Multi-gate Mixture of Experts (MMoE) to calculate probabilities of the user continuing to watch the next video, watching the video for longer than a threshold like 3 seconds, and liking the video. The loss function is defined as the sum of log losses of each target averaged by the number of training samples. They also proposed a novel beam search strategy to further optimize evaluating all candidates in the target list. ResultsKuaishou deployed this architecture in production to serve over a billion users, and improved effective view, like, and follow by 1.28%, 8.22%, and 13.6% respectively. Their studies also show that each inference had an average cost of 120 ms on the Android platform and 50 ms on the iOS platform. The increase in CPU and memory usage was about 2% each on Android and 0.5% and 1.5% respectively on the iOS platform. Taobao’s EdgeRecEdgeRec 4 was one of the earliest on-device inference-based recommender systems. It was fully deployed in Alibaba’s Taobao application and served billions of users. Through real-time computing on edge devices, EdgeRec can capture users’ diverse interests in real-time and adjust recommendation results accordingly without making any additional requests to the cloud server. EdgeRec system overview. The module on the left was deployed in mobile client and the one of the right was deployed on clou","date":"2023-03-09","objectID":"/posts/2023/03/reranking-on-edge/:3:1","series":null,"tags":["literature review","recsys","ranking"],"title":"Next Gen Recommender Systems: Real-time reranking on mobile devices","uri":"/posts/2023/03/reranking-on-edge/#on-device-inference"},{"categories":["Recommender systems"],"content":"\rOn-Device Inference\rKuaishou’s Short Video Recommendation on Mobile DevicesChinese social network app, Kuaishou, released a paper describing an on-device inference framework used in their billion-user scale short video application 3. They developed an on-device ranking model that used users’ real-time feedback of watched videos and client-specific real-time features. Their system can react immediately to users’ implicit or explicit feedback to make appropriate ordering adjustments to remaining candidate videos. Kuaishou real-time short video recommender system architecture\rOn the cloud side, we have a traditional recommender system along with a Mobile Ranking Model Training module that uses the input features generated from the access logs from user devices and trains a ranking model. This trained model is exported to TFLite format periodically and sent to the user device for deployment. While responding to the initial paging request, the server also sends the item features for the recommended candidates along with the candidate videos. The client uses these features along with the client-specific features (watched videos and corresponding feedback along with real-time features like network signal) for inference using the on-device ranking model. What input features are used?The server-side model primarily focuses on modeling users’ long-term interests and uses a lot of ID features (user id, video id, crossing features), users’ watch history, etc. The following table shows the features used by the on-device reranking model. Apart from these, the model also used some engineered cross features such as time since last impression, pXTR, and impression position diffs between with preceding historical item. Note that the server-side features are not used in the on-device model because the offline experiments showed that their information has already been distilled into the predictions from the server model. When is the on-device reranking triggered?Kuaishou’s interface allows for only one video to occupy the current screen space. At each user swipe, the client system performs reranking with the remaining candidate videos and inserts the top-ranked candidate video at the next position. Model Architecture\rTheir model also considers interaction among candidate videos. The watch history sequence is modeled using a multi-head attention (MHA) mechanism with Q projected from candidate item features and K, V projected from watch history sequence features. Only one candidate video is considered at a time. Outputs of the two MHA modules are concatenated with target features and other features and passed to a Multi-gate Mixture of Experts (MMoE) to calculate probabilities of the user continuing to watch the next video, watching the video for longer than a threshold like 3 seconds, and liking the video. The loss function is defined as the sum of log losses of each target averaged by the number of training samples. They also proposed a novel beam search strategy to further optimize evaluating all candidates in the target list. ResultsKuaishou deployed this architecture in production to serve over a billion users, and improved effective view, like, and follow by 1.28%, 8.22%, and 13.6% respectively. Their studies also show that each inference had an average cost of 120 ms on the Android platform and 50 ms on the iOS platform. The increase in CPU and memory usage was about 2% each on Android and 0.5% and 1.5% respectively on the iOS platform. Taobao’s EdgeRecEdgeRec 4 was one of the earliest on-device inference-based recommender systems. It was fully deployed in Alibaba’s Taobao application and served billions of users. Through real-time computing on edge devices, EdgeRec can capture users’ diverse interests in real-time and adjust recommendation results accordingly without making any additional requests to the cloud server. EdgeRec system overview. The module on the left was deployed in mobile client and the one of the right was deployed on clou","date":"2023-03-09","objectID":"/posts/2023/03/reranking-on-edge/:3:1","series":null,"tags":["literature review","recsys","ranking"],"title":"Next Gen Recommender Systems: Real-time reranking on mobile devices","uri":"/posts/2023/03/reranking-on-edge/#kuaishous-short-video-recommendation-on-mobile-devices"},{"categories":["Recommender systems"],"content":"\rOn-Device Inference\rKuaishou’s Short Video Recommendation on Mobile DevicesChinese social network app, Kuaishou, released a paper describing an on-device inference framework used in their billion-user scale short video application 3. They developed an on-device ranking model that used users’ real-time feedback of watched videos and client-specific real-time features. Their system can react immediately to users’ implicit or explicit feedback to make appropriate ordering adjustments to remaining candidate videos. Kuaishou real-time short video recommender system architecture\rOn the cloud side, we have a traditional recommender system along with a Mobile Ranking Model Training module that uses the input features generated from the access logs from user devices and trains a ranking model. This trained model is exported to TFLite format periodically and sent to the user device for deployment. While responding to the initial paging request, the server also sends the item features for the recommended candidates along with the candidate videos. The client uses these features along with the client-specific features (watched videos and corresponding feedback along with real-time features like network signal) for inference using the on-device ranking model. What input features are used?The server-side model primarily focuses on modeling users’ long-term interests and uses a lot of ID features (user id, video id, crossing features), users’ watch history, etc. The following table shows the features used by the on-device reranking model. Apart from these, the model also used some engineered cross features such as time since last impression, pXTR, and impression position diffs between with preceding historical item. Note that the server-side features are not used in the on-device model because the offline experiments showed that their information has already been distilled into the predictions from the server model. When is the on-device reranking triggered?Kuaishou’s interface allows for only one video to occupy the current screen space. At each user swipe, the client system performs reranking with the remaining candidate videos and inserts the top-ranked candidate video at the next position. Model Architecture\rTheir model also considers interaction among candidate videos. The watch history sequence is modeled using a multi-head attention (MHA) mechanism with Q projected from candidate item features and K, V projected from watch history sequence features. Only one candidate video is considered at a time. Outputs of the two MHA modules are concatenated with target features and other features and passed to a Multi-gate Mixture of Experts (MMoE) to calculate probabilities of the user continuing to watch the next video, watching the video for longer than a threshold like 3 seconds, and liking the video. The loss function is defined as the sum of log losses of each target averaged by the number of training samples. They also proposed a novel beam search strategy to further optimize evaluating all candidates in the target list. ResultsKuaishou deployed this architecture in production to serve over a billion users, and improved effective view, like, and follow by 1.28%, 8.22%, and 13.6% respectively. Their studies also show that each inference had an average cost of 120 ms on the Android platform and 50 ms on the iOS platform. The increase in CPU and memory usage was about 2% each on Android and 0.5% and 1.5% respectively on the iOS platform. Taobao’s EdgeRecEdgeRec 4 was one of the earliest on-device inference-based recommender systems. It was fully deployed in Alibaba’s Taobao application and served billions of users. Through real-time computing on edge devices, EdgeRec can capture users’ diverse interests in real-time and adjust recommendation results accordingly without making any additional requests to the cloud server. EdgeRec system overview. The module on the left was deployed in mobile client and the one of the right was deployed on clou","date":"2023-03-09","objectID":"/posts/2023/03/reranking-on-edge/:3:1","series":null,"tags":["literature review","recsys","ranking"],"title":"Next Gen Recommender Systems: Real-time reranking on mobile devices","uri":"/posts/2023/03/reranking-on-edge/#what-input-features-are-used"},{"categories":["Recommender systems"],"content":"\rOn-Device Inference\rKuaishou’s Short Video Recommendation on Mobile DevicesChinese social network app, Kuaishou, released a paper describing an on-device inference framework used in their billion-user scale short video application 3. They developed an on-device ranking model that used users’ real-time feedback of watched videos and client-specific real-time features. Their system can react immediately to users’ implicit or explicit feedback to make appropriate ordering adjustments to remaining candidate videos. Kuaishou real-time short video recommender system architecture\rOn the cloud side, we have a traditional recommender system along with a Mobile Ranking Model Training module that uses the input features generated from the access logs from user devices and trains a ranking model. This trained model is exported to TFLite format periodically and sent to the user device for deployment. While responding to the initial paging request, the server also sends the item features for the recommended candidates along with the candidate videos. The client uses these features along with the client-specific features (watched videos and corresponding feedback along with real-time features like network signal) for inference using the on-device ranking model. What input features are used?The server-side model primarily focuses on modeling users’ long-term interests and uses a lot of ID features (user id, video id, crossing features), users’ watch history, etc. The following table shows the features used by the on-device reranking model. Apart from these, the model also used some engineered cross features such as time since last impression, pXTR, and impression position diffs between with preceding historical item. Note that the server-side features are not used in the on-device model because the offline experiments showed that their information has already been distilled into the predictions from the server model. When is the on-device reranking triggered?Kuaishou’s interface allows for only one video to occupy the current screen space. At each user swipe, the client system performs reranking with the remaining candidate videos and inserts the top-ranked candidate video at the next position. Model Architecture\rTheir model also considers interaction among candidate videos. The watch history sequence is modeled using a multi-head attention (MHA) mechanism with Q projected from candidate item features and K, V projected from watch history sequence features. Only one candidate video is considered at a time. Outputs of the two MHA modules are concatenated with target features and other features and passed to a Multi-gate Mixture of Experts (MMoE) to calculate probabilities of the user continuing to watch the next video, watching the video for longer than a threshold like 3 seconds, and liking the video. The loss function is defined as the sum of log losses of each target averaged by the number of training samples. They also proposed a novel beam search strategy to further optimize evaluating all candidates in the target list. ResultsKuaishou deployed this architecture in production to serve over a billion users, and improved effective view, like, and follow by 1.28%, 8.22%, and 13.6% respectively. Their studies also show that each inference had an average cost of 120 ms on the Android platform and 50 ms on the iOS platform. The increase in CPU and memory usage was about 2% each on Android and 0.5% and 1.5% respectively on the iOS platform. Taobao’s EdgeRecEdgeRec 4 was one of the earliest on-device inference-based recommender systems. It was fully deployed in Alibaba’s Taobao application and served billions of users. Through real-time computing on edge devices, EdgeRec can capture users’ diverse interests in real-time and adjust recommendation results accordingly without making any additional requests to the cloud server. EdgeRec system overview. The module on the left was deployed in mobile client and the one of the right was deployed on clou","date":"2023-03-09","objectID":"/posts/2023/03/reranking-on-edge/:3:1","series":null,"tags":["literature review","recsys","ranking"],"title":"Next Gen Recommender Systems: Real-time reranking on mobile devices","uri":"/posts/2023/03/reranking-on-edge/#when-is-the-on-device-reranking-triggered"},{"categories":["Recommender systems"],"content":"\rOn-Device Inference\rKuaishou’s Short Video Recommendation on Mobile DevicesChinese social network app, Kuaishou, released a paper describing an on-device inference framework used in their billion-user scale short video application 3. They developed an on-device ranking model that used users’ real-time feedback of watched videos and client-specific real-time features. Their system can react immediately to users’ implicit or explicit feedback to make appropriate ordering adjustments to remaining candidate videos. Kuaishou real-time short video recommender system architecture\rOn the cloud side, we have a traditional recommender system along with a Mobile Ranking Model Training module that uses the input features generated from the access logs from user devices and trains a ranking model. This trained model is exported to TFLite format periodically and sent to the user device for deployment. While responding to the initial paging request, the server also sends the item features for the recommended candidates along with the candidate videos. The client uses these features along with the client-specific features (watched videos and corresponding feedback along with real-time features like network signal) for inference using the on-device ranking model. What input features are used?The server-side model primarily focuses on modeling users’ long-term interests and uses a lot of ID features (user id, video id, crossing features), users’ watch history, etc. The following table shows the features used by the on-device reranking model. Apart from these, the model also used some engineered cross features such as time since last impression, pXTR, and impression position diffs between with preceding historical item. Note that the server-side features are not used in the on-device model because the offline experiments showed that their information has already been distilled into the predictions from the server model. When is the on-device reranking triggered?Kuaishou’s interface allows for only one video to occupy the current screen space. At each user swipe, the client system performs reranking with the remaining candidate videos and inserts the top-ranked candidate video at the next position. Model Architecture\rTheir model also considers interaction among candidate videos. The watch history sequence is modeled using a multi-head attention (MHA) mechanism with Q projected from candidate item features and K, V projected from watch history sequence features. Only one candidate video is considered at a time. Outputs of the two MHA modules are concatenated with target features and other features and passed to a Multi-gate Mixture of Experts (MMoE) to calculate probabilities of the user continuing to watch the next video, watching the video for longer than a threshold like 3 seconds, and liking the video. The loss function is defined as the sum of log losses of each target averaged by the number of training samples. They also proposed a novel beam search strategy to further optimize evaluating all candidates in the target list. ResultsKuaishou deployed this architecture in production to serve over a billion users, and improved effective view, like, and follow by 1.28%, 8.22%, and 13.6% respectively. Their studies also show that each inference had an average cost of 120 ms on the Android platform and 50 ms on the iOS platform. The increase in CPU and memory usage was about 2% each on Android and 0.5% and 1.5% respectively on the iOS platform. Taobao’s EdgeRecEdgeRec 4 was one of the earliest on-device inference-based recommender systems. It was fully deployed in Alibaba’s Taobao application and served billions of users. Through real-time computing on edge devices, EdgeRec can capture users’ diverse interests in real-time and adjust recommendation results accordingly without making any additional requests to the cloud server. EdgeRec system overview. The module on the left was deployed in mobile client and the one of the right was deployed on clou","date":"2023-03-09","objectID":"/posts/2023/03/reranking-on-edge/:3:1","series":null,"tags":["literature review","recsys","ranking"],"title":"Next Gen Recommender Systems: Real-time reranking on mobile devices","uri":"/posts/2023/03/reranking-on-edge/#model-architecture"},{"categories":["Recommender systems"],"content":"\rOn-Device Inference\rKuaishou’s Short Video Recommendation on Mobile DevicesChinese social network app, Kuaishou, released a paper describing an on-device inference framework used in their billion-user scale short video application 3. They developed an on-device ranking model that used users’ real-time feedback of watched videos and client-specific real-time features. Their system can react immediately to users’ implicit or explicit feedback to make appropriate ordering adjustments to remaining candidate videos. Kuaishou real-time short video recommender system architecture\rOn the cloud side, we have a traditional recommender system along with a Mobile Ranking Model Training module that uses the input features generated from the access logs from user devices and trains a ranking model. This trained model is exported to TFLite format periodically and sent to the user device for deployment. While responding to the initial paging request, the server also sends the item features for the recommended candidates along with the candidate videos. The client uses these features along with the client-specific features (watched videos and corresponding feedback along with real-time features like network signal) for inference using the on-device ranking model. What input features are used?The server-side model primarily focuses on modeling users’ long-term interests and uses a lot of ID features (user id, video id, crossing features), users’ watch history, etc. The following table shows the features used by the on-device reranking model. Apart from these, the model also used some engineered cross features such as time since last impression, pXTR, and impression position diffs between with preceding historical item. Note that the server-side features are not used in the on-device model because the offline experiments showed that their information has already been distilled into the predictions from the server model. When is the on-device reranking triggered?Kuaishou’s interface allows for only one video to occupy the current screen space. At each user swipe, the client system performs reranking with the remaining candidate videos and inserts the top-ranked candidate video at the next position. Model Architecture\rTheir model also considers interaction among candidate videos. The watch history sequence is modeled using a multi-head attention (MHA) mechanism with Q projected from candidate item features and K, V projected from watch history sequence features. Only one candidate video is considered at a time. Outputs of the two MHA modules are concatenated with target features and other features and passed to a Multi-gate Mixture of Experts (MMoE) to calculate probabilities of the user continuing to watch the next video, watching the video for longer than a threshold like 3 seconds, and liking the video. The loss function is defined as the sum of log losses of each target averaged by the number of training samples. They also proposed a novel beam search strategy to further optimize evaluating all candidates in the target list. ResultsKuaishou deployed this architecture in production to serve over a billion users, and improved effective view, like, and follow by 1.28%, 8.22%, and 13.6% respectively. Their studies also show that each inference had an average cost of 120 ms on the Android platform and 50 ms on the iOS platform. The increase in CPU and memory usage was about 2% each on Android and 0.5% and 1.5% respectively on the iOS platform. Taobao’s EdgeRecEdgeRec 4 was one of the earliest on-device inference-based recommender systems. It was fully deployed in Alibaba’s Taobao application and served billions of users. Through real-time computing on edge devices, EdgeRec can capture users’ diverse interests in real-time and adjust recommendation results accordingly without making any additional requests to the cloud server. EdgeRec system overview. The module on the left was deployed in mobile client and the one of the right was deployed on clou","date":"2023-03-09","objectID":"/posts/2023/03/reranking-on-edge/:3:1","series":null,"tags":["literature review","recsys","ranking"],"title":"Next Gen Recommender Systems: Real-time reranking on mobile devices","uri":"/posts/2023/03/reranking-on-edge/#results"},{"categories":["Recommender systems"],"content":"\rOn-Device Inference\rKuaishou’s Short Video Recommendation on Mobile DevicesChinese social network app, Kuaishou, released a paper describing an on-device inference framework used in their billion-user scale short video application 3. They developed an on-device ranking model that used users’ real-time feedback of watched videos and client-specific real-time features. Their system can react immediately to users’ implicit or explicit feedback to make appropriate ordering adjustments to remaining candidate videos. Kuaishou real-time short video recommender system architecture\rOn the cloud side, we have a traditional recommender system along with a Mobile Ranking Model Training module that uses the input features generated from the access logs from user devices and trains a ranking model. This trained model is exported to TFLite format periodically and sent to the user device for deployment. While responding to the initial paging request, the server also sends the item features for the recommended candidates along with the candidate videos. The client uses these features along with the client-specific features (watched videos and corresponding feedback along with real-time features like network signal) for inference using the on-device ranking model. What input features are used?The server-side model primarily focuses on modeling users’ long-term interests and uses a lot of ID features (user id, video id, crossing features), users’ watch history, etc. The following table shows the features used by the on-device reranking model. Apart from these, the model also used some engineered cross features such as time since last impression, pXTR, and impression position diffs between with preceding historical item. Note that the server-side features are not used in the on-device model because the offline experiments showed that their information has already been distilled into the predictions from the server model. When is the on-device reranking triggered?Kuaishou’s interface allows for only one video to occupy the current screen space. At each user swipe, the client system performs reranking with the remaining candidate videos and inserts the top-ranked candidate video at the next position. Model Architecture\rTheir model also considers interaction among candidate videos. The watch history sequence is modeled using a multi-head attention (MHA) mechanism with Q projected from candidate item features and K, V projected from watch history sequence features. Only one candidate video is considered at a time. Outputs of the two MHA modules are concatenated with target features and other features and passed to a Multi-gate Mixture of Experts (MMoE) to calculate probabilities of the user continuing to watch the next video, watching the video for longer than a threshold like 3 seconds, and liking the video. The loss function is defined as the sum of log losses of each target averaged by the number of training samples. They also proposed a novel beam search strategy to further optimize evaluating all candidates in the target list. ResultsKuaishou deployed this architecture in production to serve over a billion users, and improved effective view, like, and follow by 1.28%, 8.22%, and 13.6% respectively. Their studies also show that each inference had an average cost of 120 ms on the Android platform and 50 ms on the iOS platform. The increase in CPU and memory usage was about 2% each on Android and 0.5% and 1.5% respectively on the iOS platform. Taobao’s EdgeRecEdgeRec 4 was one of the earliest on-device inference-based recommender systems. It was fully deployed in Alibaba’s Taobao application and served billions of users. Through real-time computing on edge devices, EdgeRec can capture users’ diverse interests in real-time and adjust recommendation results accordingly without making any additional requests to the cloud server. EdgeRec system overview. The module on the left was deployed in mobile client and the one of the right was deployed on clou","date":"2023-03-09","objectID":"/posts/2023/03/reranking-on-edge/:3:1","series":null,"tags":["literature review","recsys","ranking"],"title":"Next Gen Recommender Systems: Real-time reranking on mobile devices","uri":"/posts/2023/03/reranking-on-edge/#taobaos-edgerec"},{"categories":["Recommender systems"],"content":"\rOn-Device Inference\rKuaishou’s Short Video Recommendation on Mobile DevicesChinese social network app, Kuaishou, released a paper describing an on-device inference framework used in their billion-user scale short video application 3. They developed an on-device ranking model that used users’ real-time feedback of watched videos and client-specific real-time features. Their system can react immediately to users’ implicit or explicit feedback to make appropriate ordering adjustments to remaining candidate videos. Kuaishou real-time short video recommender system architecture\rOn the cloud side, we have a traditional recommender system along with a Mobile Ranking Model Training module that uses the input features generated from the access logs from user devices and trains a ranking model. This trained model is exported to TFLite format periodically and sent to the user device for deployment. While responding to the initial paging request, the server also sends the item features for the recommended candidates along with the candidate videos. The client uses these features along with the client-specific features (watched videos and corresponding feedback along with real-time features like network signal) for inference using the on-device ranking model. What input features are used?The server-side model primarily focuses on modeling users’ long-term interests and uses a lot of ID features (user id, video id, crossing features), users’ watch history, etc. The following table shows the features used by the on-device reranking model. Apart from these, the model also used some engineered cross features such as time since last impression, pXTR, and impression position diffs between with preceding historical item. Note that the server-side features are not used in the on-device model because the offline experiments showed that their information has already been distilled into the predictions from the server model. When is the on-device reranking triggered?Kuaishou’s interface allows for only one video to occupy the current screen space. At each user swipe, the client system performs reranking with the remaining candidate videos and inserts the top-ranked candidate video at the next position. Model Architecture\rTheir model also considers interaction among candidate videos. The watch history sequence is modeled using a multi-head attention (MHA) mechanism with Q projected from candidate item features and K, V projected from watch history sequence features. Only one candidate video is considered at a time. Outputs of the two MHA modules are concatenated with target features and other features and passed to a Multi-gate Mixture of Experts (MMoE) to calculate probabilities of the user continuing to watch the next video, watching the video for longer than a threshold like 3 seconds, and liking the video. The loss function is defined as the sum of log losses of each target averaged by the number of training samples. They also proposed a novel beam search strategy to further optimize evaluating all candidates in the target list. ResultsKuaishou deployed this architecture in production to serve over a billion users, and improved effective view, like, and follow by 1.28%, 8.22%, and 13.6% respectively. Their studies also show that each inference had an average cost of 120 ms on the Android platform and 50 ms on the iOS platform. The increase in CPU and memory usage was about 2% each on Android and 0.5% and 1.5% respectively on the iOS platform. Taobao’s EdgeRecEdgeRec 4 was one of the earliest on-device inference-based recommender systems. It was fully deployed in Alibaba’s Taobao application and served billions of users. Through real-time computing on edge devices, EdgeRec can capture users’ diverse interests in real-time and adjust recommendation results accordingly without making any additional requests to the cloud server. EdgeRec system overview. The module on the left was deployed in mobile client and the one of the right was deployed on clou","date":"2023-03-09","objectID":"/posts/2023/03/reranking-on-edge/:3:1","series":null,"tags":["literature review","recsys","ranking"],"title":"Next Gen Recommender Systems: Real-time reranking on mobile devices","uri":"/posts/2023/03/reranking-on-edge/#what-input-features-are-used-1"},{"categories":["Recommender systems"],"content":"\rOn-Device Inference\rKuaishou’s Short Video Recommendation on Mobile DevicesChinese social network app, Kuaishou, released a paper describing an on-device inference framework used in their billion-user scale short video application 3. They developed an on-device ranking model that used users’ real-time feedback of watched videos and client-specific real-time features. Their system can react immediately to users’ implicit or explicit feedback to make appropriate ordering adjustments to remaining candidate videos. Kuaishou real-time short video recommender system architecture\rOn the cloud side, we have a traditional recommender system along with a Mobile Ranking Model Training module that uses the input features generated from the access logs from user devices and trains a ranking model. This trained model is exported to TFLite format periodically and sent to the user device for deployment. While responding to the initial paging request, the server also sends the item features for the recommended candidates along with the candidate videos. The client uses these features along with the client-specific features (watched videos and corresponding feedback along with real-time features like network signal) for inference using the on-device ranking model. What input features are used?The server-side model primarily focuses on modeling users’ long-term interests and uses a lot of ID features (user id, video id, crossing features), users’ watch history, etc. The following table shows the features used by the on-device reranking model. Apart from these, the model also used some engineered cross features such as time since last impression, pXTR, and impression position diffs between with preceding historical item. Note that the server-side features are not used in the on-device model because the offline experiments showed that their information has already been distilled into the predictions from the server model. When is the on-device reranking triggered?Kuaishou’s interface allows for only one video to occupy the current screen space. At each user swipe, the client system performs reranking with the remaining candidate videos and inserts the top-ranked candidate video at the next position. Model Architecture\rTheir model also considers interaction among candidate videos. The watch history sequence is modeled using a multi-head attention (MHA) mechanism with Q projected from candidate item features and K, V projected from watch history sequence features. Only one candidate video is considered at a time. Outputs of the two MHA modules are concatenated with target features and other features and passed to a Multi-gate Mixture of Experts (MMoE) to calculate probabilities of the user continuing to watch the next video, watching the video for longer than a threshold like 3 seconds, and liking the video. The loss function is defined as the sum of log losses of each target averaged by the number of training samples. They also proposed a novel beam search strategy to further optimize evaluating all candidates in the target list. ResultsKuaishou deployed this architecture in production to serve over a billion users, and improved effective view, like, and follow by 1.28%, 8.22%, and 13.6% respectively. Their studies also show that each inference had an average cost of 120 ms on the Android platform and 50 ms on the iOS platform. The increase in CPU and memory usage was about 2% each on Android and 0.5% and 1.5% respectively on the iOS platform. Taobao’s EdgeRecEdgeRec 4 was one of the earliest on-device inference-based recommender systems. It was fully deployed in Alibaba’s Taobao application and served billions of users. Through real-time computing on edge devices, EdgeRec can capture users’ diverse interests in real-time and adjust recommendation results accordingly without making any additional requests to the cloud server. EdgeRec system overview. The module on the left was deployed in mobile client and the one of the right was deployed on clou","date":"2023-03-09","objectID":"/posts/2023/03/reranking-on-edge/:3:1","series":null,"tags":["literature review","recsys","ranking"],"title":"Next Gen Recommender Systems: Real-time reranking on mobile devices","uri":"/posts/2023/03/reranking-on-edge/#when-is-the-on-device-reranking-triggered-1"},{"categories":["Recommender systems"],"content":"\rOn-Device Inference\rKuaishou’s Short Video Recommendation on Mobile DevicesChinese social network app, Kuaishou, released a paper describing an on-device inference framework used in their billion-user scale short video application 3. They developed an on-device ranking model that used users’ real-time feedback of watched videos and client-specific real-time features. Their system can react immediately to users’ implicit or explicit feedback to make appropriate ordering adjustments to remaining candidate videos. Kuaishou real-time short video recommender system architecture\rOn the cloud side, we have a traditional recommender system along with a Mobile Ranking Model Training module that uses the input features generated from the access logs from user devices and trains a ranking model. This trained model is exported to TFLite format periodically and sent to the user device for deployment. While responding to the initial paging request, the server also sends the item features for the recommended candidates along with the candidate videos. The client uses these features along with the client-specific features (watched videos and corresponding feedback along with real-time features like network signal) for inference using the on-device ranking model. What input features are used?The server-side model primarily focuses on modeling users’ long-term interests and uses a lot of ID features (user id, video id, crossing features), users’ watch history, etc. The following table shows the features used by the on-device reranking model. Apart from these, the model also used some engineered cross features such as time since last impression, pXTR, and impression position diffs between with preceding historical item. Note that the server-side features are not used in the on-device model because the offline experiments showed that their information has already been distilled into the predictions from the server model. When is the on-device reranking triggered?Kuaishou’s interface allows for only one video to occupy the current screen space. At each user swipe, the client system performs reranking with the remaining candidate videos and inserts the top-ranked candidate video at the next position. Model Architecture\rTheir model also considers interaction among candidate videos. The watch history sequence is modeled using a multi-head attention (MHA) mechanism with Q projected from candidate item features and K, V projected from watch history sequence features. Only one candidate video is considered at a time. Outputs of the two MHA modules are concatenated with target features and other features and passed to a Multi-gate Mixture of Experts (MMoE) to calculate probabilities of the user continuing to watch the next video, watching the video for longer than a threshold like 3 seconds, and liking the video. The loss function is defined as the sum of log losses of each target averaged by the number of training samples. They also proposed a novel beam search strategy to further optimize evaluating all candidates in the target list. ResultsKuaishou deployed this architecture in production to serve over a billion users, and improved effective view, like, and follow by 1.28%, 8.22%, and 13.6% respectively. Their studies also show that each inference had an average cost of 120 ms on the Android platform and 50 ms on the iOS platform. The increase in CPU and memory usage was about 2% each on Android and 0.5% and 1.5% respectively on the iOS platform. Taobao’s EdgeRecEdgeRec 4 was one of the earliest on-device inference-based recommender systems. It was fully deployed in Alibaba’s Taobao application and served billions of users. Through real-time computing on edge devices, EdgeRec can capture users’ diverse interests in real-time and adjust recommendation results accordingly without making any additional requests to the cloud server. EdgeRec system overview. The module on the left was deployed in mobile client and the one of the right was deployed on clou","date":"2023-03-09","objectID":"/posts/2023/03/reranking-on-edge/:3:1","series":null,"tags":["literature review","recsys","ranking"],"title":"Next Gen Recommender Systems: Real-time reranking on mobile devices","uri":"/posts/2023/03/reranking-on-edge/#model-architecture-1"},{"categories":["Recommender systems"],"content":"\rOn-Device Inference\rKuaishou’s Short Video Recommendation on Mobile DevicesChinese social network app, Kuaishou, released a paper describing an on-device inference framework used in their billion-user scale short video application 3. They developed an on-device ranking model that used users’ real-time feedback of watched videos and client-specific real-time features. Their system can react immediately to users’ implicit or explicit feedback to make appropriate ordering adjustments to remaining candidate videos. Kuaishou real-time short video recommender system architecture\rOn the cloud side, we have a traditional recommender system along with a Mobile Ranking Model Training module that uses the input features generated from the access logs from user devices and trains a ranking model. This trained model is exported to TFLite format periodically and sent to the user device for deployment. While responding to the initial paging request, the server also sends the item features for the recommended candidates along with the candidate videos. The client uses these features along with the client-specific features (watched videos and corresponding feedback along with real-time features like network signal) for inference using the on-device ranking model. What input features are used?The server-side model primarily focuses on modeling users’ long-term interests and uses a lot of ID features (user id, video id, crossing features), users’ watch history, etc. The following table shows the features used by the on-device reranking model. Apart from these, the model also used some engineered cross features such as time since last impression, pXTR, and impression position diffs between with preceding historical item. Note that the server-side features are not used in the on-device model because the offline experiments showed that their information has already been distilled into the predictions from the server model. When is the on-device reranking triggered?Kuaishou’s interface allows for only one video to occupy the current screen space. At each user swipe, the client system performs reranking with the remaining candidate videos and inserts the top-ranked candidate video at the next position. Model Architecture\rTheir model also considers interaction among candidate videos. The watch history sequence is modeled using a multi-head attention (MHA) mechanism with Q projected from candidate item features and K, V projected from watch history sequence features. Only one candidate video is considered at a time. Outputs of the two MHA modules are concatenated with target features and other features and passed to a Multi-gate Mixture of Experts (MMoE) to calculate probabilities of the user continuing to watch the next video, watching the video for longer than a threshold like 3 seconds, and liking the video. The loss function is defined as the sum of log losses of each target averaged by the number of training samples. They also proposed a novel beam search strategy to further optimize evaluating all candidates in the target list. ResultsKuaishou deployed this architecture in production to serve over a billion users, and improved effective view, like, and follow by 1.28%, 8.22%, and 13.6% respectively. Their studies also show that each inference had an average cost of 120 ms on the Android platform and 50 ms on the iOS platform. The increase in CPU and memory usage was about 2% each on Android and 0.5% and 1.5% respectively on the iOS platform. Taobao’s EdgeRecEdgeRec 4 was one of the earliest on-device inference-based recommender systems. It was fully deployed in Alibaba’s Taobao application and served billions of users. Through real-time computing on edge devices, EdgeRec can capture users’ diverse interests in real-time and adjust recommendation results accordingly without making any additional requests to the cloud server. EdgeRec system overview. The module on the left was deployed in mobile client and the one of the right was deployed on clou","date":"2023-03-09","objectID":"/posts/2023/03/reranking-on-edge/:3:1","series":null,"tags":["literature review","recsys","ranking"],"title":"Next Gen Recommender Systems: Real-time reranking on mobile devices","uri":"/posts/2023/03/reranking-on-edge/#results-1"},{"categories":["Recommender systems"],"content":"\rOn-device LearningDue to data heterogeneity, cloud models trained over the global data are non-optimal to individual users’ local data distribution. To deal with model personalization, on-device learning is a great solution. However, as explained earlier in the “challenges” section, user data on local devices is often very small and sparse which leads to overfitting and loss of generalization. In this section, we will mainly focus on solutions that can help fix this problem. Device-Cloud Collaborative Learning (DCCL) frameworkAs mentioned earlier, a lot of work done under the on-device learning paradigm belongs to the Federated Recommender Systems. Under these systems, on-device learning is used to compute gradients over the local data which are then aggregated over multiple devices and used to update a central model’s parameters by approaches like federated averaging (FedAvg), etc. For example, Qi et al.5 by applying Federated Learning and differential privacy for news recommendation use case, and Lin et al.6 accounted for storage, power, and network bandwidth to distribute gradient computation among user devices and cloud servers. This centralized cloud model-based traditional approach follows the “model-over-data” paradigm. One limitation of federated learning is that it assumes the data never leaves the user device, and hence there is no concept of global data. Such a scenario is unlikely in modern recommender systems because most user data has already been uploaded to the cloud and the cloud server still does a majority of personalization computations. In DCCL Yao et al.7 proposed an alternative “model-over-models” paradigm to model the collaboration between the device and the cloud. Under this framework, the cloud model is frozen and the edge devices use patch learning method 8 to learn the parameters of a trainable patch function using sparse local samples. These patches are then sent to the server where the cloud model uses a distillation process with these patches as meaningful priors to optimize the cloud model. Collaborative learning with Data Augmentation (CoDA)Gu et al. proposed a device-cloud collaborative learning framework called CoDA where the edge model also receives extra samples from the cloud’s global pool of data to augment their local dataset for training the on-device model. The edge device also maintains a personalized sample classifier for further filtering out some of the samples received from the cloud. Global models are trained with all users’ data but do inferences over each user’s local data. CoDA attempts to address this data discrepancy so that the model can be made optimal for each user. In the CoDA framework, an on-device model is trained with both local data and the augmentation data received from the server. The cloud server uses a K nearest-neighbor (KNN) method on user feature vectors to find samples similar to the current user behavior. The server then desensitizes and anonymizes these samples before sending them to the end user’s device. The user device uses this augmented data and the local data for training and evaluating a personalized binary classifier. This classifier is trained to identify local samples from the combined data, and the augmentation samples with high scores (i.e. false positives) are considered close enough to be used for the on-device reranking model. The remaining augmentation data is dropped. Both classification and reranking inference tasks use models based on Deep Interest Network architecture 9. ","date":"2023-03-09","objectID":"/posts/2023/03/reranking-on-edge/:3:2","series":null,"tags":["literature review","recsys","ranking"],"title":"Next Gen Recommender Systems: Real-time reranking on mobile devices","uri":"/posts/2023/03/reranking-on-edge/#on-device-learning"},{"categories":["Recommender systems"],"content":"\rOn-device LearningDue to data heterogeneity, cloud models trained over the global data are non-optimal to individual users’ local data distribution. To deal with model personalization, on-device learning is a great solution. However, as explained earlier in the “challenges” section, user data on local devices is often very small and sparse which leads to overfitting and loss of generalization. In this section, we will mainly focus on solutions that can help fix this problem. Device-Cloud Collaborative Learning (DCCL) frameworkAs mentioned earlier, a lot of work done under the on-device learning paradigm belongs to the Federated Recommender Systems. Under these systems, on-device learning is used to compute gradients over the local data which are then aggregated over multiple devices and used to update a central model’s parameters by approaches like federated averaging (FedAvg), etc. For example, Qi et al.5 by applying Federated Learning and differential privacy for news recommendation use case, and Lin et al.6 accounted for storage, power, and network bandwidth to distribute gradient computation among user devices and cloud servers. This centralized cloud model-based traditional approach follows the “model-over-data” paradigm. One limitation of federated learning is that it assumes the data never leaves the user device, and hence there is no concept of global data. Such a scenario is unlikely in modern recommender systems because most user data has already been uploaded to the cloud and the cloud server still does a majority of personalization computations. In DCCL Yao et al.7 proposed an alternative “model-over-models” paradigm to model the collaboration between the device and the cloud. Under this framework, the cloud model is frozen and the edge devices use patch learning method 8 to learn the parameters of a trainable patch function using sparse local samples. These patches are then sent to the server where the cloud model uses a distillation process with these patches as meaningful priors to optimize the cloud model. Collaborative learning with Data Augmentation (CoDA)Gu et al. proposed a device-cloud collaborative learning framework called CoDA where the edge model also receives extra samples from the cloud’s global pool of data to augment their local dataset for training the on-device model. The edge device also maintains a personalized sample classifier for further filtering out some of the samples received from the cloud. Global models are trained with all users’ data but do inferences over each user’s local data. CoDA attempts to address this data discrepancy so that the model can be made optimal for each user. In the CoDA framework, an on-device model is trained with both local data and the augmentation data received from the server. The cloud server uses a K nearest-neighbor (KNN) method on user feature vectors to find samples similar to the current user behavior. The server then desensitizes and anonymizes these samples before sending them to the end user’s device. The user device uses this augmented data and the local data for training and evaluating a personalized binary classifier. This classifier is trained to identify local samples from the combined data, and the augmentation samples with high scores (i.e. false positives) are considered close enough to be used for the on-device reranking model. The remaining augmentation data is dropped. Both classification and reranking inference tasks use models based on Deep Interest Network architecture 9. ","date":"2023-03-09","objectID":"/posts/2023/03/reranking-on-edge/:3:2","series":null,"tags":["literature review","recsys","ranking"],"title":"Next Gen Recommender Systems: Real-time reranking on mobile devices","uri":"/posts/2023/03/reranking-on-edge/#device-cloud-collaborative-learning-dccl-framework"},{"categories":["Recommender systems"],"content":"\rOn-device LearningDue to data heterogeneity, cloud models trained over the global data are non-optimal to individual users’ local data distribution. To deal with model personalization, on-device learning is a great solution. However, as explained earlier in the “challenges” section, user data on local devices is often very small and sparse which leads to overfitting and loss of generalization. In this section, we will mainly focus on solutions that can help fix this problem. Device-Cloud Collaborative Learning (DCCL) frameworkAs mentioned earlier, a lot of work done under the on-device learning paradigm belongs to the Federated Recommender Systems. Under these systems, on-device learning is used to compute gradients over the local data which are then aggregated over multiple devices and used to update a central model’s parameters by approaches like federated averaging (FedAvg), etc. For example, Qi et al.5 by applying Federated Learning and differential privacy for news recommendation use case, and Lin et al.6 accounted for storage, power, and network bandwidth to distribute gradient computation among user devices and cloud servers. This centralized cloud model-based traditional approach follows the “model-over-data” paradigm. One limitation of federated learning is that it assumes the data never leaves the user device, and hence there is no concept of global data. Such a scenario is unlikely in modern recommender systems because most user data has already been uploaded to the cloud and the cloud server still does a majority of personalization computations. In DCCL Yao et al.7 proposed an alternative “model-over-models” paradigm to model the collaboration between the device and the cloud. Under this framework, the cloud model is frozen and the edge devices use patch learning method 8 to learn the parameters of a trainable patch function using sparse local samples. These patches are then sent to the server where the cloud model uses a distillation process with these patches as meaningful priors to optimize the cloud model. Collaborative learning with Data Augmentation (CoDA)Gu et al. proposed a device-cloud collaborative learning framework called CoDA where the edge model also receives extra samples from the cloud’s global pool of data to augment their local dataset for training the on-device model. The edge device also maintains a personalized sample classifier for further filtering out some of the samples received from the cloud. Global models are trained with all users’ data but do inferences over each user’s local data. CoDA attempts to address this data discrepancy so that the model can be made optimal for each user. In the CoDA framework, an on-device model is trained with both local data and the augmentation data received from the server. The cloud server uses a K nearest-neighbor (KNN) method on user feature vectors to find samples similar to the current user behavior. The server then desensitizes and anonymizes these samples before sending them to the end user’s device. The user device uses this augmented data and the local data for training and evaluating a personalized binary classifier. This classifier is trained to identify local samples from the combined data, and the augmentation samples with high scores (i.e. false positives) are considered close enough to be used for the on-device reranking model. The remaining augmentation data is dropped. Both classification and reranking inference tasks use models based on Deep Interest Network architecture 9. ","date":"2023-03-09","objectID":"/posts/2023/03/reranking-on-edge/:3:2","series":null,"tags":["literature review","recsys","ranking"],"title":"Next Gen Recommender Systems: Real-time reranking on mobile devices","uri":"/posts/2023/03/reranking-on-edge/#collaborative-learning-with-data-augmentation-coda"},{"categories":["Recommender systems"],"content":"\rSummaryThis article explained the shortcomings of a cloud server model trained on global data for serving personalized recommendations. It introduced on-device inference and on-device learning paradigms which aim to timely capture rich user behavior and respond to users’ changing demands in real time. We also looked at system design choices and implementation details of some exemplar systems from different industrial applications that have served recommendations to billions of users. ","date":"2023-03-09","objectID":"/posts/2023/03/reranking-on-edge/:4:0","series":null,"tags":["literature review","recsys","ranking"],"title":"Next Gen Recommender Systems: Real-time reranking on mobile devices","uri":"/posts/2023/03/reranking-on-edge/#summary"},{"categories":["Recommender systems"],"content":"\rReferences Two Tower Model Architecture: Current State and Promising Extensions. Cascade Ranking System: https://blog.reachsumit.com/posts/2023/03/two-tower-model/#cascade-ranking-system ↩︎ Your smartphone is millions of times more powerful than the Apollo 11 guidance computers. https://www.zmescience.com/science/news-science/smartphone-power-compared-to-apollo-432/ ↩︎ Gong, X., Feng, Q., Zhang, Y., Qin, J., Ding, W., Li, B., \u0026 Jiang, P. (2022). Real-time Short Video Recommendation on Mobile Devices. Proceedings of the 31st ACM International Conference on Information \u0026 Knowledge Management. ↩︎ Gong, Yu \u0026 Jiang, Ziwen \u0026 Zhao, Kaiqi \u0026 Liu, Qingwen \u0026 Ou, Wenwu. (2020). EdgeRec: Recommender System on Edge in Mobile Taobao. ↩︎ Qi, Tao \u0026 Wu, Fangzhao \u0026 Wu, Chuhan \u0026 Huang, Yongfeng \u0026 Xie, Xing. (2020). Privacy-Preserving News Recommendation Model Training via Federated Learning. ↩︎ Lin, Yujie \u0026 Ren, Pengjie \u0026 Chen, Zhumin \u0026 Ren, Zhaochun \u0026 Yu, Dongxiao \u0026 Ma, Jun \u0026 Rijke, Maarten \u0026 Cheng, Xiuzhen. (2020). Meta Matrix Factorization for Federated Rating Predictions. 981-990. 10.1145/3397271.3401081. ↩︎ Yao, Jiangchao \u0026 Wang, Feng \u0026 Jia, Kunyang \u0026 Han, Bo \u0026 Zhou, Jingren \u0026 Yang, Hongxia. (2021). Device-Cloud Collaborative Learning for Recommendation. ↩︎ Yuan, F., He, X., Karatzoglou, A., \u0026 Zhang, L. (2020). Parameter-Efficient Transfer from Sequential Behaviors for User Modeling and Recommendation. Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. ↩︎ Zhou, Guorui \u0026 Gai, Kun \u0026 Zhu, Xiaoqiang \u0026 Song, Chenru \u0026 Fan, Ying \u0026 Zhu, Han \u0026 Ma, Xiao \u0026 Yan, Yanghui \u0026 Jin, Junqi \u0026 Li, Han. (2018). Deep Interest Network for Click-Through Rate Prediction. 1059-1068. 10.1145/3219819.3219823. ↩︎ ","date":"2023-03-09","objectID":"/posts/2023/03/reranking-on-edge/:5:0","series":null,"tags":["literature review","recsys","ranking"],"title":"Next Gen Recommender Systems: Real-time reranking on mobile devices","uri":"/posts/2023/03/reranking-on-edge/#references"},{"categories":["Information Retrieval"],"content":"\rIntroductionTwo-tower model is widely adopted in industrial-scale retrieval and ranking workflows across a broad range of application domains, such as content recommendations, advertisement systems, and search engines. It is also the current go-to state-of-the-art solution for pre-ranking tasks. This article explores the history and current state of the Two Tower models and also highlights potential improvements proposed in some of the recently published literature. The goal here is to help understand what makes the Two Tower model an appropriate choice for a bunch of applications, and how it can be potentially extended from its current state. ","date":"2023-03-04","objectID":"/posts/2023/03/two-tower-model/:1:0","series":null,"tags":["literature review","recsys","ranking"],"title":"Two Tower Model Architecture: Current State and Promising Extensions","uri":"/posts/2023/03/two-tower-model/#introduction"},{"categories":["Information Retrieval"],"content":"\rCascade Ranking SystemLarge-scale information retrieval and item recommendation services often contain tens of millions of candidate items or documents. A search query on Google may have matching keywords in millions of documents (web pages), and Instagram may have thousands or even millions of candidate videos for generating a recommended feed for a user. Designing such systems often has to deal with the additional challenge of strict strict latency constraints. Studies have shown that even a 100ms increase in response time leads to degraded user experience and a measurable impact on revenue 1. A single, complex ranking algorithm cannot efficiently rank such large sets of candidates. Hence, a multi-stage ranking system is commonly adopted to balance efficiency and effectiveness. In this system, simpler and faster algorithms that focus on recall metrics are employed at earlier steps (Recall and Pre-Ranking). Whereas the large-scale deep neural networks are employed at the later steps (Ranking and Re-Ranking). Keeping latency expectations and accuracy tradeoffs in mind, an appropriate algorithm is used at each step. Each algorithm calculates some form of relevance or similarity value for every single candidate and passes the most relevant candidate set onto the next step. In an earlier blog post 2, I introduced several interaction-focused DNN algorithms that can be used for the ranking stage. I recommend going through that article for developing a good understanding of these algorithms. This current article will mainly talk about the Two Tower model in the context of the pre-ranking stage. ","date":"2023-03-04","objectID":"/posts/2023/03/two-tower-model/:2:0","series":null,"tags":["literature review","recsys","ranking"],"title":"Two Tower Model Architecture: Current State and Promising Extensions","uri":"/posts/2023/03/two-tower-model/#cascade-ranking-system"},{"categories":["Information Retrieval"],"content":"\rTwo Tower ModelThe pre-ranking stage does the initial filtering of the candidates received from the preceding recall or retrieval stage. Compared to the ranking stage, the pre-ranking model has to evaluate a larger number of candidates and therefore a faster approach is preferred here. The following figure shows the development history of pre-ranking systems from an ads recommendation modeling perspective 3. Here $x_{u}$, $x_{a}$, $x_{ua}$ are the raw user features, ad features, and cross features. The first generation calculated the pre-rank score in a non-personalized way by averaging the recent click-through rate of each ad. Logistic regression used in the second generation is also a lightweight algorithm that can be deployed in online learning and serving manner. The third generation commonly used a Two Tower model which is a vector-produced based DNN. In this method, user and ad features pass through embedding and DNN layers to generate a latent vector representation for both the user and the ad. Then an inner product of the two vectors is calculated to obtain the pre-ranking score. This Two Tower structure was originally developed in search systems as a means to map a query to the most relevant documents4. The reason why the Two Tower model rose to popularity was its accuracy as well as its inference efficiency-focused design. The two towers generate latent representations independently in parallel and interact only at the output layer (referred to as “late interaction”). Often the learned ad or item tower embeddings are also frozen after training and stored in an indexing service for a quicker operation at inference time. ","date":"2023-03-04","objectID":"/posts/2023/03/two-tower-model/:3:0","series":null,"tags":["literature review","recsys","ranking"],"title":"Two Tower Model Architecture: Current State and Promising Extensions","uri":"/posts/2023/03/two-tower-model/#two-tower-model"},{"categories":["Information Retrieval"],"content":"\rRelated DNN ParadigmsThe information retrieval domain has several other DNN paradigms related to the Two Tower model. The following figure shows some examples in the context of neural matching applications (query to document matching)5. As you can see, the Two Tower model is a representation-based ranker architecture, which independently computes embeddings for the query and documents and estimates their similarity via interaction between them at the output layer. The other paradigms represent more interaction-focused rankers. Models like DRMM, and KNRM model word- and phrase-level relationships across query and document using an interaction matrix and then feed it to a neural network like CNN or MLP. Similarly, models like BERT are much more powerful as they model interactions between words as well as across the query and documents at the same time. On the other hand, models like ColBERT (Contextualized later interactions over BERT) keep interactions within the query and document features while delaying the query-document interaction to the output layer. This allows the model to preserve the “query-document decoupling” (or “user-item decoupling”) architecture paradigm, and the document or item embeddings can be frozen after training and served through an index at the inference time as shown on the left in the figure below. Inference costs can be further reduced if the application can work with freezing both towers. For example, as shown on the right in the following figure, Alibaba creates indexes out of the learned embeddings from both towers and retrains the model offline on a daily frequency 3. ","date":"2023-03-04","objectID":"/posts/2023/03/two-tower-model/:4:0","series":null,"tags":["literature review","recsys","ranking"],"title":"Two Tower Model Architecture: Current State and Promising Extensions","uri":"/posts/2023/03/two-tower-model/#related-dnn-paradigms"},{"categories":["Information Retrieval"],"content":"\rEnhancing Two Tower ModelIn this section, we look at some of the proposals from different researchers for extending the Two Tower model. One common problem with the Two Tower model that this research works focus on is the lack of information interaction between the two towers. As we saw earlier, the Two Tower model trains the latent embeddings in both towers independently and without using any enriching information from the other tower. This limitation hinders the model’s performance. ","date":"2023-03-04","objectID":"/posts/2023/03/two-tower-model/:5:0","series":null,"tags":["literature review","recsys","ranking"],"title":"Two Tower Model Architecture: Current State and Promising Extensions","uri":"/posts/2023/03/two-tower-model/#enhancing-two-tower-model"},{"categories":["Information Retrieval"],"content":"\rDual Augmented Two-Tower Model (DAT)To model feature interactions between two towers, Yu et al.6 proposed augmenting the embedding input of each tower with a vector ($a_{u}$ and $a_{v}$ in the figure below) that captures historical positive interaction information from the other tower. $a_{u}$ and $a_{v}$ vectors get updated during the training process and are used to model the information interaction between the two towers by regarding them as the input feature of the two towers. This paper also proposes a new category alignment loss to handle the imbalance among different categories of items by transferring the knowledge learned from the category with a large amount of data to other categories. Later research showed that the gains achieved by the DAT model are still limited. ","date":"2023-03-04","objectID":"/posts/2023/03/two-tower-model/:5:1","series":null,"tags":["literature review","recsys","ranking"],"title":"Two Tower Model Architecture: Current State and Promising Extensions","uri":"/posts/2023/03/two-tower-model/#dual-augmented-two-tower-model-dat"},{"categories":["Information Retrieval"],"content":"\rInteraction Enhanced Two Tower Model (IntTower)The authors of this research design a two-tower model that emphasizes both information interactions and inference efficiency 7. Their proposed model consists of three new blocks, as mentioned below. Light-SE BlockThis module is used to identify the importance of different features and obtain refined feature representations in each tower. The design of this module is based on the SENET model proposed in the “Squeeze-and-Excitation Networks” paper 8. SENET is used in the computer vision domain to explicitly model the interdependencies between the image channels through feature recalibration, i.e. selectively emphasizing informative features and suppressing less useful ones. The basic structure of the SE building block is shown in the next figure. First, a squeeze operation aggregates the feature maps across spatial dimensions to produce a channel descriptor. An excitation operation then excites informative features by a self-gating mechanism based on channel dependence (similar to calculating attention weights). The final output of the block is obtained through a scaling step by rescaling the transformation output with the learned activations. The authors of the IntTower paper adopt a more lightweight approach with a single FC layer to obtain the feature importance. The following figure shows the SENET and Light-SE blocks side-by-side. FE-BlockThe purposed FE (Fine-grained and Early Feature Interaction) Block is inspired by the later interaction style of ColBERT. It performs fine-grained early feature interaction between multi-layer user representations and the last layer of item representation. CIR ModuleA Contrastive Interaction Regularization (CIR) module was also purposed to shorten the distance between a user and positive items using InfoNCE loss function 9. During training, this loss value is combined with the logloss between the model prediction scores and the true labels. IntTower Model Architecture\rAs shown in the figure above, the IntTower model architecture combines the three blocks described earlier. Through experimentation, the authors show that the IntTower model outperforms other pre-ranking algorithms (Logistic Regression, Two-Tower, DAT, COLD) and even performs comparably to some ranking algorithms (Wide\u0026Deep, DeepFM, DCN, AutoInt). Compared with the Two Tower model, the increased parameters count and training time of IntTower are negligible and the serving latency is also acceptable. The authors also investigate the user and item representations generated by Two Tower and IntTower models by projecting them into 2-dimensions using t-SNE. As shown in the next figure, the IntTower representations have the user and positive items in the same cluster while negative items are far away from the user. ","date":"2023-03-04","objectID":"/posts/2023/03/two-tower-model/:5:2","series":null,"tags":["literature review","recsys","ranking"],"title":"Two Tower Model Architecture: Current State and Promising Extensions","uri":"/posts/2023/03/two-tower-model/#interaction-enhanced-two-tower-model-inttower"},{"categories":["Information Retrieval"],"content":"\rInteraction Enhanced Two Tower Model (IntTower)The authors of this research design a two-tower model that emphasizes both information interactions and inference efficiency 7. Their proposed model consists of three new blocks, as mentioned below. Light-SE BlockThis module is used to identify the importance of different features and obtain refined feature representations in each tower. The design of this module is based on the SENET model proposed in the “Squeeze-and-Excitation Networks” paper 8. SENET is used in the computer vision domain to explicitly model the interdependencies between the image channels through feature recalibration, i.e. selectively emphasizing informative features and suppressing less useful ones. The basic structure of the SE building block is shown in the next figure. First, a squeeze operation aggregates the feature maps across spatial dimensions to produce a channel descriptor. An excitation operation then excites informative features by a self-gating mechanism based on channel dependence (similar to calculating attention weights). The final output of the block is obtained through a scaling step by rescaling the transformation output with the learned activations. The authors of the IntTower paper adopt a more lightweight approach with a single FC layer to obtain the feature importance. The following figure shows the SENET and Light-SE blocks side-by-side. FE-BlockThe purposed FE (Fine-grained and Early Feature Interaction) Block is inspired by the later interaction style of ColBERT. It performs fine-grained early feature interaction between multi-layer user representations and the last layer of item representation. CIR ModuleA Contrastive Interaction Regularization (CIR) module was also purposed to shorten the distance between a user and positive items using InfoNCE loss function 9. During training, this loss value is combined with the logloss between the model prediction scores and the true labels. IntTower Model Architecture\rAs shown in the figure above, the IntTower model architecture combines the three blocks described earlier. Through experimentation, the authors show that the IntTower model outperforms other pre-ranking algorithms (Logistic Regression, Two-Tower, DAT, COLD) and even performs comparably to some ranking algorithms (Wide\u0026Deep, DeepFM, DCN, AutoInt). Compared with the Two Tower model, the increased parameters count and training time of IntTower are negligible and the serving latency is also acceptable. The authors also investigate the user and item representations generated by Two Tower and IntTower models by projecting them into 2-dimensions using t-SNE. As shown in the next figure, the IntTower representations have the user and positive items in the same cluster while negative items are far away from the user. ","date":"2023-03-04","objectID":"/posts/2023/03/two-tower-model/:5:2","series":null,"tags":["literature review","recsys","ranking"],"title":"Two Tower Model Architecture: Current State and Promising Extensions","uri":"/posts/2023/03/two-tower-model/#light-se-block"},{"categories":["Information Retrieval"],"content":"\rInteraction Enhanced Two Tower Model (IntTower)The authors of this research design a two-tower model that emphasizes both information interactions and inference efficiency 7. Their proposed model consists of three new blocks, as mentioned below. Light-SE BlockThis module is used to identify the importance of different features and obtain refined feature representations in each tower. The design of this module is based on the SENET model proposed in the “Squeeze-and-Excitation Networks” paper 8. SENET is used in the computer vision domain to explicitly model the interdependencies between the image channels through feature recalibration, i.e. selectively emphasizing informative features and suppressing less useful ones. The basic structure of the SE building block is shown in the next figure. First, a squeeze operation aggregates the feature maps across spatial dimensions to produce a channel descriptor. An excitation operation then excites informative features by a self-gating mechanism based on channel dependence (similar to calculating attention weights). The final output of the block is obtained through a scaling step by rescaling the transformation output with the learned activations. The authors of the IntTower paper adopt a more lightweight approach with a single FC layer to obtain the feature importance. The following figure shows the SENET and Light-SE blocks side-by-side. FE-BlockThe purposed FE (Fine-grained and Early Feature Interaction) Block is inspired by the later interaction style of ColBERT. It performs fine-grained early feature interaction between multi-layer user representations and the last layer of item representation. CIR ModuleA Contrastive Interaction Regularization (CIR) module was also purposed to shorten the distance between a user and positive items using InfoNCE loss function 9. During training, this loss value is combined with the logloss between the model prediction scores and the true labels. IntTower Model Architecture\rAs shown in the figure above, the IntTower model architecture combines the three blocks described earlier. Through experimentation, the authors show that the IntTower model outperforms other pre-ranking algorithms (Logistic Regression, Two-Tower, DAT, COLD) and even performs comparably to some ranking algorithms (Wide\u0026Deep, DeepFM, DCN, AutoInt). Compared with the Two Tower model, the increased parameters count and training time of IntTower are negligible and the serving latency is also acceptable. The authors also investigate the user and item representations generated by Two Tower and IntTower models by projecting them into 2-dimensions using t-SNE. As shown in the next figure, the IntTower representations have the user and positive items in the same cluster while negative items are far away from the user. ","date":"2023-03-04","objectID":"/posts/2023/03/two-tower-model/:5:2","series":null,"tags":["literature review","recsys","ranking"],"title":"Two Tower Model Architecture: Current State and Promising Extensions","uri":"/posts/2023/03/two-tower-model/#fe-block"},{"categories":["Information Retrieval"],"content":"\rInteraction Enhanced Two Tower Model (IntTower)The authors of this research design a two-tower model that emphasizes both information interactions and inference efficiency 7. Their proposed model consists of three new blocks, as mentioned below. Light-SE BlockThis module is used to identify the importance of different features and obtain refined feature representations in each tower. The design of this module is based on the SENET model proposed in the “Squeeze-and-Excitation Networks” paper 8. SENET is used in the computer vision domain to explicitly model the interdependencies between the image channels through feature recalibration, i.e. selectively emphasizing informative features and suppressing less useful ones. The basic structure of the SE building block is shown in the next figure. First, a squeeze operation aggregates the feature maps across spatial dimensions to produce a channel descriptor. An excitation operation then excites informative features by a self-gating mechanism based on channel dependence (similar to calculating attention weights). The final output of the block is obtained through a scaling step by rescaling the transformation output with the learned activations. The authors of the IntTower paper adopt a more lightweight approach with a single FC layer to obtain the feature importance. The following figure shows the SENET and Light-SE blocks side-by-side. FE-BlockThe purposed FE (Fine-grained and Early Feature Interaction) Block is inspired by the later interaction style of ColBERT. It performs fine-grained early feature interaction between multi-layer user representations and the last layer of item representation. CIR ModuleA Contrastive Interaction Regularization (CIR) module was also purposed to shorten the distance between a user and positive items using InfoNCE loss function 9. During training, this loss value is combined with the logloss between the model prediction scores and the true labels. IntTower Model Architecture\rAs shown in the figure above, the IntTower model architecture combines the three blocks described earlier. Through experimentation, the authors show that the IntTower model outperforms other pre-ranking algorithms (Logistic Regression, Two-Tower, DAT, COLD) and even performs comparably to some ranking algorithms (Wide\u0026Deep, DeepFM, DCN, AutoInt). Compared with the Two Tower model, the increased parameters count and training time of IntTower are negligible and the serving latency is also acceptable. The authors also investigate the user and item representations generated by Two Tower and IntTower models by projecting them into 2-dimensions using t-SNE. As shown in the next figure, the IntTower representations have the user and positive items in the same cluster while negative items are far away from the user. ","date":"2023-03-04","objectID":"/posts/2023/03/two-tower-model/:5:2","series":null,"tags":["literature review","recsys","ranking"],"title":"Two Tower Model Architecture: Current State and Promising Extensions","uri":"/posts/2023/03/two-tower-model/#cir-module"},{"categories":["Information Retrieval"],"content":"\rInteraction Enhanced Two Tower Model (IntTower)The authors of this research design a two-tower model that emphasizes both information interactions and inference efficiency 7. Their proposed model consists of three new blocks, as mentioned below. Light-SE BlockThis module is used to identify the importance of different features and obtain refined feature representations in each tower. The design of this module is based on the SENET model proposed in the “Squeeze-and-Excitation Networks” paper 8. SENET is used in the computer vision domain to explicitly model the interdependencies between the image channels through feature recalibration, i.e. selectively emphasizing informative features and suppressing less useful ones. The basic structure of the SE building block is shown in the next figure. First, a squeeze operation aggregates the feature maps across spatial dimensions to produce a channel descriptor. An excitation operation then excites informative features by a self-gating mechanism based on channel dependence (similar to calculating attention weights). The final output of the block is obtained through a scaling step by rescaling the transformation output with the learned activations. The authors of the IntTower paper adopt a more lightweight approach with a single FC layer to obtain the feature importance. The following figure shows the SENET and Light-SE blocks side-by-side. FE-BlockThe purposed FE (Fine-grained and Early Feature Interaction) Block is inspired by the later interaction style of ColBERT. It performs fine-grained early feature interaction between multi-layer user representations and the last layer of item representation. CIR ModuleA Contrastive Interaction Regularization (CIR) module was also purposed to shorten the distance between a user and positive items using InfoNCE loss function 9. During training, this loss value is combined with the logloss between the model prediction scores and the true labels. IntTower Model Architecture\rAs shown in the figure above, the IntTower model architecture combines the three blocks described earlier. Through experimentation, the authors show that the IntTower model outperforms other pre-ranking algorithms (Logistic Regression, Two-Tower, DAT, COLD) and even performs comparably to some ranking algorithms (Wide\u0026Deep, DeepFM, DCN, AutoInt). Compared with the Two Tower model, the increased parameters count and training time of IntTower are negligible and the serving latency is also acceptable. The authors also investigate the user and item representations generated by Two Tower and IntTower models by projecting them into 2-dimensions using t-SNE. As shown in the next figure, the IntTower representations have the user and positive items in the same cluster while negative items are far away from the user. ","date":"2023-03-04","objectID":"/posts/2023/03/two-tower-model/:5:2","series":null,"tags":["literature review","recsys","ranking"],"title":"Two Tower Model Architecture: Current State and Promising Extensions","uri":"/posts/2023/03/two-tower-model/#inttower-model-architecture"},{"categories":["Information Retrieval"],"content":"\rAlternatives to Two Tower ModelApart from Two Tower models, there has been some research work with single-tower structures to fully model feature interactions and further improve prediction accuracy. However, due to the lack of a “user-item decoupling architecture” paradigm, these models have to employ several optimization tricks to alleviate efficiency degradations. For example, the COLD (Computing power cost-aware Online and Lightweight Deep pre-ranking system) model 3 performs offline feature selection experiments to find out a set of important features for the ranking algorithm that are optimal concerning metrics such as QPS (queries per second) and RT (return time). Similarly, FSCD (Feature Selection method based on feature Complexity and variational Dropout) model 10 uses learnable dropout parameters to perform feature-wise regularization such that the pre-ranking model is effectively inherited from that of the ranking model. ","date":"2023-03-04","objectID":"/posts/2023/03/two-tower-model/:6:0","series":null,"tags":["literature review","recsys","ranking"],"title":"Two Tower Model Architecture: Current State and Promising Extensions","uri":"/posts/2023/03/two-tower-model/#alternatives-to-two-tower-model"},{"categories":["Information Retrieval"],"content":"\rConclusionThis article introduced the historical evolution of pre-ranking approaches in the context of cascade ranking-based systems. We learned how the balance of efficiency and effectiveness makes Two Tower models an excellent choice for several information retrieval use cases. We also explored several recent research proposals that combine ideas from related domains to enhance the Two Tower model. We wrapped up the article with a look at some of the available alternatives to the Two Tower model architecture. ","date":"2023-03-04","objectID":"/posts/2023/03/two-tower-model/:7:0","series":null,"tags":["literature review","recsys","ranking"],"title":"Two Tower Model Architecture: Current State and Promising Extensions","uri":"/posts/2023/03/two-tower-model/#conclusion"},{"categories":["Information Retrieval"],"content":"\rReferences Kohavi, R., Deng, A., Frasca, B., Walker, T., Xu, Y., \u0026 Pohlmann, N. (2013). Online controlled experiments at large scale. Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining. ↩︎ Sumit Kumar. Recommender Systems for Modeling Feature Interactions under Sparse Settings. https://blog.reachsumit.com/posts/2022/11/sparse-recsys/ ↩︎ Wang, Zhe \u0026 Zhao, Liqin \u0026 Jiang, Biye \u0026 Zhou, Guorui \u0026 Zhu, Xiaoqiang \u0026 Gai, Kun. (2020). COLD: Towards the Next Generation of Pre-Ranking System. ↩︎ ↩︎ ↩︎ Huang, P., He, X., Gao, J., Deng, L., Acero, A., \u0026 Heck, L. (2013). Learning deep structured semantic models for web search using clickthrough data. Proceedings of the 22nd ACM international conference on Information \u0026 Knowledge Management. ↩︎ Khattab, O., \u0026 Zaharia, M.A. (2020). ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. ↩︎ Yu, Y., Wang, W., Feng, Z., Xue, D., Meituan, \u0026 Beijing (2021). A Dual Augmented Two-tower Model for Online Large-scale Recommendation. ↩︎ Li, X., Chen, B., Guo, H., Li, J., Zhu, C., Long, X., Li, S., Wang, Y., Guo, W., Mao, L., Liu, J., Dong, Z., \u0026 Tang, R. (2022). IntTower: The Next Generation of Two-Tower Model for Pre-Ranking System. Proceedings of the 31st ACM International Conference on Information \u0026 Knowledge Management. ↩︎ Hu, J., Shen, L., Albanie, S., Sun, G., \u0026 Wu, E. (2017). Squeeze-and-Excitation Networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42, 2011-2023. ↩︎ https://lilianweng.github.io/posts/2021-05-31-contrastive/#infonce ↩︎ Ma, X., Wang, P., Zhao, H., Liu, S., Zhao, C., Lin, W., Lee, K., Xu, J., \u0026 Zheng, B. (2021). Towards a Better Tradeoff between Effectiveness and Efficiency in Pre-Ranking: A Learnable Feature Selection based Approach. Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. ↩︎ ","date":"2023-03-04","objectID":"/posts/2023/03/two-tower-model/:8:0","series":null,"tags":["literature review","recsys","ranking"],"title":"Two Tower Model Architecture: Current State and Promising Extensions","uri":"/posts/2023/03/two-tower-model/#references"},{"categories":["Time Series"],"content":"\rIntroduction","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:1:0","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#introduction"},{"categories":["Time Series"],"content":"\rWhy Should We Use Deep Learning for Forecasting?Statistical algorithms have long been widely used for making forecasts with time series data. These classical algorithms, like Exponential Smoothing, and ARIMA models, prescribe the data generation process and require manual selections to account for factors like the trend, seasonality, and auto-correlation. However, modern data applications often deal with hundreds or millions of related time series. For example, a demand forecasting algorithm at Amazon may have to consider sales data from millions of products, and an engagement forecasting algorithm at Instagram may have to model metrics from millions of posts. Traditional forecasting methods learn characteristics of individual time series, and hence do not scale well because they fit a model for each time series and do not share parameters among them. Deep learning provides a data-driven approach that makes a minimal set of assumptions to learn from multiple related time series. In the previous article, I did a detailed literature review on the state of statistical vs machine learning vs deep learning approaches for time series forecasting1. It is important to note that deep learning methods are not necessarily free of inductive biases. While DL models make very few assumptions about the features, inductive biases creep into the modeling process in the form of the architectural design of the DL model. This is why we see certain models perform better than others on certain tasks. For example, Convolutional Neural Networks work well with images due to their spatial inductive biases and translational equivariance. Hence a careful design of the DL model based on the application domain is critical. Over the past few years, several new DL models have been developed for forecasting applications. In this article, we will go over some of the most popular DL models, understand their inductive biases, implement them in PyTorch and compare their results on a dataset with multiple time series. ","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:1:1","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#why-should-we-use-deep-learning-for-forecasting"},{"categories":["Time Series"],"content":"\rBasic ConceptsBefore going any further, let’s look at some fundamental concepts that will help develop a better understanding of the models. ","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:2:0","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#basic-concepts"},{"categories":["Time Series"],"content":"\rTypes of ForecastingWe will focus on the following two types of forecasting applications: Point forecasting: Our goal is to predict a single value, which is often the conditional mean or median of future observations. Probabilistic forecasting: Our goal here will be to predict the full distribution. Probabilistic forecasting captures the uncertainty of the future and hence plays a key role in automating and optimizing operational business processes. For example: in retail, it helps to optimize procurement, and inventory planning; in logistics, it enables efficient labor resource planning, delivery vehicle deployments, etc. We further divide probabilistic forecasting into two categories: Parametric probabilistic forecasting: We directly predict the parameters of the hypothetical target distribution, for example, we predict the mean and standard deviation under the gaussian distribution assumption. Maximum likelihood estimation is commonly applied to estimate the corresponding parameters in this setting. Non-Parametric probabilistic forecasting: We predict a set of values corresponding to each quantile point of interest. Such models are commonly trained to minimize quantile loss. ","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:2:1","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#types-of-forecasting"},{"categories":["Time Series"],"content":"\rCovariatesAlong with the time series data, a lot of models also incorporate covariates. Covariates provide additional information about an item being modeled or the time point corresponding to the item’s observed values. For example, covariates ensure that the information about the absolute or relative time is available to the model when we use the windowing approach during training. Item-dependent covariates, can be product id, product brand, category, product image, product text, etc. One common way to incorporate them into the modeling process is by using embeddings. Some numeric covariates can also be repeated along the time dimensions to be used together with the time series input. Time-depended covariates, can be product price, weekend, holidays, day-of-the-week, etc, or binary features like price adjustment, censoring, etc. This information can usually be modeled using the corresponding numeric values. Covariates can also be both time and item dependent, for example, product-specific promotions. ","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:2:2","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#covariates"},{"categories":["Time Series"],"content":"\rModels","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:3:0","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#models"},{"categories":["Time Series"],"content":"\r1. DeepARDeepAR is an RNN-based probabilistic forecasting model proposed by Amazon that trains on a large number of related time series 2. Even though it trains on all time series, it is still a univariate model, i.e. it predicts a univariate target. It learns seasonal behavior and dependencies without any manual feature engineering. It can also incorporate cold items with limited historical data. Assuming $t_{0}$ as the forecast creation time, i.e. the time step at which the forecast for the future horizon must be generated, our goal is to model the following conditional distribution: Using the autoregressive recurrent network, we can further represent our model distribution as the product of likelihood factors parameterized by the output $h_{i,t}$ of an autoregressive recurrent network. Handling Unique Data ChallengesAmazon’s website catalogs millions of products with very skewed sales velocity. The magnitudes of sales numbers among series also have a large variance which makes common normalization techniques like input standardization or batch normalization less effective. Also, the erratic, intermittent and bursty nature of data in such demand forecasting violates the core assumptions of many classical techniques, such as gaussian errors, stationarity, or homoscedasticity of the time series. The authors propose two solutions: Scaling input time series: An average-based heuristic is used to scale the inputs. The autoregressive inputs $z_{i,t}$ are divided by the following average value $v_{i}$ and at the end, likelihood parameters are multiplied by the same value. Velocity-based sampling: Large magnitude variance can also lead to suboptimal results because an optimization procedure that picks training instances uniformly at random will visit the small number time series with a large scale very infrequently, which results in underfitting those time series. To handle this, a weighted sampling scheme is adopted where the probability of selecting a sample with scale $v_{i}$ is proportional to $v_{i}$ (see WeightedSampler class in code). The paper also recommends using the following covariates: age (distance to the first observation in that time series), day-of-the-week and hour-of-the-week for hourly data, week-of-the-year for weekly data, month-of-the-year for monthly data, and a time series id (representing the product category) as embedding. All covariates were standardized to zero mean and unit variance. Likelihood modelDeepAR maximizes the log-likelihood but does not limit itself to assuming Gaussian noise. Any noise model can be chosen as long as the log-likelihood and its gradients wrt the parameters can be evaluated. The authors recommend using Gaussian distribution (parameterized by mean and standard deviation) for real-valued data, and negative-binomial likelihood (parameterized by mean and shape) for unbound positive count data and long-tail data. Other possibilities include Beta distribution for unit interval, Bernoulli for binary data, and mixtures for complex marginal distributions. The paper also recommends using the SoftPlus activation function for parameters, like standard deviation, that are always positive. Model Architecture\rDeepAR model uses an encoder-decoder design but uses the same architecture for the encoder and decoder components. During training (left), at each time step t, the model takes the target value at the previous time step $z_{i,t-1}$, covariates $x_{i,t}$, as well as the previous network output $h_{i,t-1}$. The model is using teacher forcing approach, which has been shown to pose a few problems for tasks like NLP but hasn’t had any known adverse effect in the forecasting setting. For prediction, the history of the time series is first fed for all timestamps before the forecast creation time (left), and a sample is drawn (also called ancestral sampling) and fed back for the next point until the end of the prediction range (right). DeepAR model has been extended in several other research wor","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:3:1","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#1-deepar"},{"categories":["Time Series"],"content":"\r1. DeepARDeepAR is an RNN-based probabilistic forecasting model proposed by Amazon that trains on a large number of related time series 2. Even though it trains on all time series, it is still a univariate model, i.e. it predicts a univariate target. It learns seasonal behavior and dependencies without any manual feature engineering. It can also incorporate cold items with limited historical data. Assuming $t_{0}$ as the forecast creation time, i.e. the time step at which the forecast for the future horizon must be generated, our goal is to model the following conditional distribution: Using the autoregressive recurrent network, we can further represent our model distribution as the product of likelihood factors parameterized by the output $h_{i,t}$ of an autoregressive recurrent network. Handling Unique Data ChallengesAmazon’s website catalogs millions of products with very skewed sales velocity. The magnitudes of sales numbers among series also have a large variance which makes common normalization techniques like input standardization or batch normalization less effective. Also, the erratic, intermittent and bursty nature of data in such demand forecasting violates the core assumptions of many classical techniques, such as gaussian errors, stationarity, or homoscedasticity of the time series. The authors propose two solutions: Scaling input time series: An average-based heuristic is used to scale the inputs. The autoregressive inputs $z_{i,t}$ are divided by the following average value $v_{i}$ and at the end, likelihood parameters are multiplied by the same value. Velocity-based sampling: Large magnitude variance can also lead to suboptimal results because an optimization procedure that picks training instances uniformly at random will visit the small number time series with a large scale very infrequently, which results in underfitting those time series. To handle this, a weighted sampling scheme is adopted where the probability of selecting a sample with scale $v_{i}$ is proportional to $v_{i}$ (see WeightedSampler class in code). The paper also recommends using the following covariates: age (distance to the first observation in that time series), day-of-the-week and hour-of-the-week for hourly data, week-of-the-year for weekly data, month-of-the-year for monthly data, and a time series id (representing the product category) as embedding. All covariates were standardized to zero mean and unit variance. Likelihood modelDeepAR maximizes the log-likelihood but does not limit itself to assuming Gaussian noise. Any noise model can be chosen as long as the log-likelihood and its gradients wrt the parameters can be evaluated. The authors recommend using Gaussian distribution (parameterized by mean and standard deviation) for real-valued data, and negative-binomial likelihood (parameterized by mean and shape) for unbound positive count data and long-tail data. Other possibilities include Beta distribution for unit interval, Bernoulli for binary data, and mixtures for complex marginal distributions. The paper also recommends using the SoftPlus activation function for parameters, like standard deviation, that are always positive. Model Architecture\rDeepAR model uses an encoder-decoder design but uses the same architecture for the encoder and decoder components. During training (left), at each time step t, the model takes the target value at the previous time step $z_{i,t-1}$, covariates $x_{i,t}$, as well as the previous network output $h_{i,t-1}$. The model is using teacher forcing approach, which has been shown to pose a few problems for tasks like NLP but hasn’t had any known adverse effect in the forecasting setting. For prediction, the history of the time series is first fed for all timestamps before the forecast creation time (left), and a sample is drawn (also called ancestral sampling) and fed back for the next point until the end of the prediction range (right). DeepAR model has been extended in several other research wor","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:3:1","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#handling-unique-data-challenges"},{"categories":["Time Series"],"content":"\r1. DeepARDeepAR is an RNN-based probabilistic forecasting model proposed by Amazon that trains on a large number of related time series 2. Even though it trains on all time series, it is still a univariate model, i.e. it predicts a univariate target. It learns seasonal behavior and dependencies without any manual feature engineering. It can also incorporate cold items with limited historical data. Assuming $t_{0}$ as the forecast creation time, i.e. the time step at which the forecast for the future horizon must be generated, our goal is to model the following conditional distribution: Using the autoregressive recurrent network, we can further represent our model distribution as the product of likelihood factors parameterized by the output $h_{i,t}$ of an autoregressive recurrent network. Handling Unique Data ChallengesAmazon’s website catalogs millions of products with very skewed sales velocity. The magnitudes of sales numbers among series also have a large variance which makes common normalization techniques like input standardization or batch normalization less effective. Also, the erratic, intermittent and bursty nature of data in such demand forecasting violates the core assumptions of many classical techniques, such as gaussian errors, stationarity, or homoscedasticity of the time series. The authors propose two solutions: Scaling input time series: An average-based heuristic is used to scale the inputs. The autoregressive inputs $z_{i,t}$ are divided by the following average value $v_{i}$ and at the end, likelihood parameters are multiplied by the same value. Velocity-based sampling: Large magnitude variance can also lead to suboptimal results because an optimization procedure that picks training instances uniformly at random will visit the small number time series with a large scale very infrequently, which results in underfitting those time series. To handle this, a weighted sampling scheme is adopted where the probability of selecting a sample with scale $v_{i}$ is proportional to $v_{i}$ (see WeightedSampler class in code). The paper also recommends using the following covariates: age (distance to the first observation in that time series), day-of-the-week and hour-of-the-week for hourly data, week-of-the-year for weekly data, month-of-the-year for monthly data, and a time series id (representing the product category) as embedding. All covariates were standardized to zero mean and unit variance. Likelihood modelDeepAR maximizes the log-likelihood but does not limit itself to assuming Gaussian noise. Any noise model can be chosen as long as the log-likelihood and its gradients wrt the parameters can be evaluated. The authors recommend using Gaussian distribution (parameterized by mean and standard deviation) for real-valued data, and negative-binomial likelihood (parameterized by mean and shape) for unbound positive count data and long-tail data. Other possibilities include Beta distribution for unit interval, Bernoulli for binary data, and mixtures for complex marginal distributions. The paper also recommends using the SoftPlus activation function for parameters, like standard deviation, that are always positive. Model Architecture\rDeepAR model uses an encoder-decoder design but uses the same architecture for the encoder and decoder components. During training (left), at each time step t, the model takes the target value at the previous time step $z_{i,t-1}$, covariates $x_{i,t}$, as well as the previous network output $h_{i,t-1}$. The model is using teacher forcing approach, which has been shown to pose a few problems for tasks like NLP but hasn’t had any known adverse effect in the forecasting setting. For prediction, the history of the time series is first fed for all timestamps before the forecast creation time (left), and a sample is drawn (also called ancestral sampling) and fed back for the next point until the end of the prediction range (right). DeepAR model has been extended in several other research wor","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:3:1","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#likelihood-model"},{"categories":["Time Series"],"content":"\r1. DeepARDeepAR is an RNN-based probabilistic forecasting model proposed by Amazon that trains on a large number of related time series 2. Even though it trains on all time series, it is still a univariate model, i.e. it predicts a univariate target. It learns seasonal behavior and dependencies without any manual feature engineering. It can also incorporate cold items with limited historical data. Assuming $t_{0}$ as the forecast creation time, i.e. the time step at which the forecast for the future horizon must be generated, our goal is to model the following conditional distribution: Using the autoregressive recurrent network, we can further represent our model distribution as the product of likelihood factors parameterized by the output $h_{i,t}$ of an autoregressive recurrent network. Handling Unique Data ChallengesAmazon’s website catalogs millions of products with very skewed sales velocity. The magnitudes of sales numbers among series also have a large variance which makes common normalization techniques like input standardization or batch normalization less effective. Also, the erratic, intermittent and bursty nature of data in such demand forecasting violates the core assumptions of many classical techniques, such as gaussian errors, stationarity, or homoscedasticity of the time series. The authors propose two solutions: Scaling input time series: An average-based heuristic is used to scale the inputs. The autoregressive inputs $z_{i,t}$ are divided by the following average value $v_{i}$ and at the end, likelihood parameters are multiplied by the same value. Velocity-based sampling: Large magnitude variance can also lead to suboptimal results because an optimization procedure that picks training instances uniformly at random will visit the small number time series with a large scale very infrequently, which results in underfitting those time series. To handle this, a weighted sampling scheme is adopted where the probability of selecting a sample with scale $v_{i}$ is proportional to $v_{i}$ (see WeightedSampler class in code). The paper also recommends using the following covariates: age (distance to the first observation in that time series), day-of-the-week and hour-of-the-week for hourly data, week-of-the-year for weekly data, month-of-the-year for monthly data, and a time series id (representing the product category) as embedding. All covariates were standardized to zero mean and unit variance. Likelihood modelDeepAR maximizes the log-likelihood but does not limit itself to assuming Gaussian noise. Any noise model can be chosen as long as the log-likelihood and its gradients wrt the parameters can be evaluated. The authors recommend using Gaussian distribution (parameterized by mean and standard deviation) for real-valued data, and negative-binomial likelihood (parameterized by mean and shape) for unbound positive count data and long-tail data. Other possibilities include Beta distribution for unit interval, Bernoulli for binary data, and mixtures for complex marginal distributions. The paper also recommends using the SoftPlus activation function for parameters, like standard deviation, that are always positive. Model Architecture\rDeepAR model uses an encoder-decoder design but uses the same architecture for the encoder and decoder components. During training (left), at each time step t, the model takes the target value at the previous time step $z_{i,t-1}$, covariates $x_{i,t}$, as well as the previous network output $h_{i,t-1}$. The model is using teacher forcing approach, which has been shown to pose a few problems for tasks like NLP but hasn’t had any known adverse effect in the forecasting setting. For prediction, the history of the time series is first fed for all timestamps before the forecast creation time (left), and a sample is drawn (also called ancestral sampling) and fed back for the next point until the end of the prediction range (right). DeepAR model has been extended in several other research wor","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:3:1","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#model-architecture"},{"categories":["Time Series"],"content":"\r1. DeepARDeepAR is an RNN-based probabilistic forecasting model proposed by Amazon that trains on a large number of related time series 2. Even though it trains on all time series, it is still a univariate model, i.e. it predicts a univariate target. It learns seasonal behavior and dependencies without any manual feature engineering. It can also incorporate cold items with limited historical data. Assuming $t_{0}$ as the forecast creation time, i.e. the time step at which the forecast for the future horizon must be generated, our goal is to model the following conditional distribution: Using the autoregressive recurrent network, we can further represent our model distribution as the product of likelihood factors parameterized by the output $h_{i,t}$ of an autoregressive recurrent network. Handling Unique Data ChallengesAmazon’s website catalogs millions of products with very skewed sales velocity. The magnitudes of sales numbers among series also have a large variance which makes common normalization techniques like input standardization or batch normalization less effective. Also, the erratic, intermittent and bursty nature of data in such demand forecasting violates the core assumptions of many classical techniques, such as gaussian errors, stationarity, or homoscedasticity of the time series. The authors propose two solutions: Scaling input time series: An average-based heuristic is used to scale the inputs. The autoregressive inputs $z_{i,t}$ are divided by the following average value $v_{i}$ and at the end, likelihood parameters are multiplied by the same value. Velocity-based sampling: Large magnitude variance can also lead to suboptimal results because an optimization procedure that picks training instances uniformly at random will visit the small number time series with a large scale very infrequently, which results in underfitting those time series. To handle this, a weighted sampling scheme is adopted where the probability of selecting a sample with scale $v_{i}$ is proportional to $v_{i}$ (see WeightedSampler class in code). The paper also recommends using the following covariates: age (distance to the first observation in that time series), day-of-the-week and hour-of-the-week for hourly data, week-of-the-year for weekly data, month-of-the-year for monthly data, and a time series id (representing the product category) as embedding. All covariates were standardized to zero mean and unit variance. Likelihood modelDeepAR maximizes the log-likelihood but does not limit itself to assuming Gaussian noise. Any noise model can be chosen as long as the log-likelihood and its gradients wrt the parameters can be evaluated. The authors recommend using Gaussian distribution (parameterized by mean and standard deviation) for real-valued data, and negative-binomial likelihood (parameterized by mean and shape) for unbound positive count data and long-tail data. Other possibilities include Beta distribution for unit interval, Bernoulli for binary data, and mixtures for complex marginal distributions. The paper also recommends using the SoftPlus activation function for parameters, like standard deviation, that are always positive. Model Architecture\rDeepAR model uses an encoder-decoder design but uses the same architecture for the encoder and decoder components. During training (left), at each time step t, the model takes the target value at the previous time step $z_{i,t-1}$, covariates $x_{i,t}$, as well as the previous network output $h_{i,t-1}$. The model is using teacher forcing approach, which has been shown to pose a few problems for tasks like NLP but hasn’t had any known adverse effect in the forecasting setting. For prediction, the history of the time series is first fed for all timestamps before the forecast creation time (left), and a sample is drawn (also called ancestral sampling) and fed back for the next point until the end of the prediction range (right). DeepAR model has been extended in several other research wor","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:3:1","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#pytorch-code"},{"categories":["Time Series"],"content":"\r2. MQRNNMulti-horizon Quantile Recurrent Neural Network (MQRNN) is an RNN-based non-parametric probabilistic forecasting model proposed by Amazon 6. It uses quantile loss to predict values for each desired quantile for every time step in the forecast horizon. One problem with recursive forecast generators like DeepAR, is that they tend to accumulate errors from previous steps during recursive forecast generation. Some empirical search also suggests that directly forecasting values for the full forecast horizon are less biased, more stable, and retrains the efficiency of the parameter sharing 7. MQRNN builds on this idea and uses direct multi-horizon forecasting instead of a one-step-ahead recursive forecasting approach. It also incorporates both static and temporal covariates and solves the following large-scale time series regression problem: Incorporating Future Covariate ValuesThis paper suggests that distant future information can have a retrospective impact on near-future horizons. For example, if a festival or black-Friday sales event is coming up in the future, the anticipation of it can affect a customer’s buying decisions. As explained later, this future information is supplied to the two decoder MLP components. Decoder DesignThe model adopts the encoder-decoder framework. The encoder is a vanilla LSTM network that takes historical time series and covariates values. For the decoder structure, the model uses two MLP branches instead of a recursive decoder. As stated earlier, the model focuses on directly producing output for the full horizon at once. The MLP branches are used to achieve this goal. The design philosophy for the two decoders is as follows: Global Decoder: The global MLP takes encoder output and future covariates as input, and generates two context vectors as output: horizon-specific context, and horizon-agnostic context. The idea here behind horizon-specific context is that it carries the structural awareness of the temporal distance between a forecast creation time point and a specific horizon. This is essential to aspects like seasonality mapping. Whereas the horizon-agnostic context is based on the idea that not all relevant information is time-sensitive. Note that in the standard encoder-decoder architecture, only horizon-agnostic context exists, and horizon awareness is indirectly enforced by recursively feeding predictions not the RNN cell for the next time step. Local Decoder: The local MLP takes the global decoder’s outputs and also future covariates as input, and generates all required quantiles for each specific horizon. The local MLP is key to aligning future seasonality and event, and the capability to generate sharp, spiky forecasts. In cases where there is no meaningful future information, or sharp, spiky forecasts are not desired, the local MLP can be removed. Model Architecture\rThe loss function for MQRNN is the sum of individual quantile loss, and the output is all the quantile forecasts. At 0.5 quantile, the quantile loss is simply the Mean Absolute Error and its minimizer is the median of the predictive distribution. MQRNN generates multi-horizon forecasts by placing a series of decoders, with shared parameters, at each recurrent layer (time point) in the encoder, and computes the loss against the corresponding targets. Each time series of arbitrary lengths can serve as a single sample in the model training, hence allowing cold content with limited history to also be used in the model. It also uses static information by replicating it across time. The authors also recommend trying different encoder structures for processing sequential input, like dilated 1D causal convolution layers (MQCNN), NARX-like LSTM, or WaveNet. PyTorch Code class MQRNN(nn.Module): def __init__(self, horizon_size:int=horizon_size, hidden_size:int=100, quantiles:list=desired_quantiles, dropout:float=0.3, layer_size:int=3, context_size:int=50, covariate_size:int=num_covariates, bidirectional=False, device=torch.d","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:3:2","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#2-mqrnn"},{"categories":["Time Series"],"content":"\r2. MQRNNMulti-horizon Quantile Recurrent Neural Network (MQRNN) is an RNN-based non-parametric probabilistic forecasting model proposed by Amazon 6. It uses quantile loss to predict values for each desired quantile for every time step in the forecast horizon. One problem with recursive forecast generators like DeepAR, is that they tend to accumulate errors from previous steps during recursive forecast generation. Some empirical search also suggests that directly forecasting values for the full forecast horizon are less biased, more stable, and retrains the efficiency of the parameter sharing 7. MQRNN builds on this idea and uses direct multi-horizon forecasting instead of a one-step-ahead recursive forecasting approach. It also incorporates both static and temporal covariates and solves the following large-scale time series regression problem: Incorporating Future Covariate ValuesThis paper suggests that distant future information can have a retrospective impact on near-future horizons. For example, if a festival or black-Friday sales event is coming up in the future, the anticipation of it can affect a customer’s buying decisions. As explained later, this future information is supplied to the two decoder MLP components. Decoder DesignThe model adopts the encoder-decoder framework. The encoder is a vanilla LSTM network that takes historical time series and covariates values. For the decoder structure, the model uses two MLP branches instead of a recursive decoder. As stated earlier, the model focuses on directly producing output for the full horizon at once. The MLP branches are used to achieve this goal. The design philosophy for the two decoders is as follows: Global Decoder: The global MLP takes encoder output and future covariates as input, and generates two context vectors as output: horizon-specific context, and horizon-agnostic context. The idea here behind horizon-specific context is that it carries the structural awareness of the temporal distance between a forecast creation time point and a specific horizon. This is essential to aspects like seasonality mapping. Whereas the horizon-agnostic context is based on the idea that not all relevant information is time-sensitive. Note that in the standard encoder-decoder architecture, only horizon-agnostic context exists, and horizon awareness is indirectly enforced by recursively feeding predictions not the RNN cell for the next time step. Local Decoder: The local MLP takes the global decoder’s outputs and also future covariates as input, and generates all required quantiles for each specific horizon. The local MLP is key to aligning future seasonality and event, and the capability to generate sharp, spiky forecasts. In cases where there is no meaningful future information, or sharp, spiky forecasts are not desired, the local MLP can be removed. Model Architecture\rThe loss function for MQRNN is the sum of individual quantile loss, and the output is all the quantile forecasts. At 0.5 quantile, the quantile loss is simply the Mean Absolute Error and its minimizer is the median of the predictive distribution. MQRNN generates multi-horizon forecasts by placing a series of decoders, with shared parameters, at each recurrent layer (time point) in the encoder, and computes the loss against the corresponding targets. Each time series of arbitrary lengths can serve as a single sample in the model training, hence allowing cold content with limited history to also be used in the model. It also uses static information by replicating it across time. The authors also recommend trying different encoder structures for processing sequential input, like dilated 1D causal convolution layers (MQCNN), NARX-like LSTM, or WaveNet. PyTorch Code class MQRNN(nn.Module): def __init__(self, horizon_size:int=horizon_size, hidden_size:int=100, quantiles:list=desired_quantiles, dropout:float=0.3, layer_size:int=3, context_size:int=50, covariate_size:int=num_covariates, bidirectional=False, device=torch.d","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:3:2","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#incorporating-future-covariate-values"},{"categories":["Time Series"],"content":"\r2. MQRNNMulti-horizon Quantile Recurrent Neural Network (MQRNN) is an RNN-based non-parametric probabilistic forecasting model proposed by Amazon 6. It uses quantile loss to predict values for each desired quantile for every time step in the forecast horizon. One problem with recursive forecast generators like DeepAR, is that they tend to accumulate errors from previous steps during recursive forecast generation. Some empirical search also suggests that directly forecasting values for the full forecast horizon are less biased, more stable, and retrains the efficiency of the parameter sharing 7. MQRNN builds on this idea and uses direct multi-horizon forecasting instead of a one-step-ahead recursive forecasting approach. It also incorporates both static and temporal covariates and solves the following large-scale time series regression problem: Incorporating Future Covariate ValuesThis paper suggests that distant future information can have a retrospective impact on near-future horizons. For example, if a festival or black-Friday sales event is coming up in the future, the anticipation of it can affect a customer’s buying decisions. As explained later, this future information is supplied to the two decoder MLP components. Decoder DesignThe model adopts the encoder-decoder framework. The encoder is a vanilla LSTM network that takes historical time series and covariates values. For the decoder structure, the model uses two MLP branches instead of a recursive decoder. As stated earlier, the model focuses on directly producing output for the full horizon at once. The MLP branches are used to achieve this goal. The design philosophy for the two decoders is as follows: Global Decoder: The global MLP takes encoder output and future covariates as input, and generates two context vectors as output: horizon-specific context, and horizon-agnostic context. The idea here behind horizon-specific context is that it carries the structural awareness of the temporal distance between a forecast creation time point and a specific horizon. This is essential to aspects like seasonality mapping. Whereas the horizon-agnostic context is based on the idea that not all relevant information is time-sensitive. Note that in the standard encoder-decoder architecture, only horizon-agnostic context exists, and horizon awareness is indirectly enforced by recursively feeding predictions not the RNN cell for the next time step. Local Decoder: The local MLP takes the global decoder’s outputs and also future covariates as input, and generates all required quantiles for each specific horizon. The local MLP is key to aligning future seasonality and event, and the capability to generate sharp, spiky forecasts. In cases where there is no meaningful future information, or sharp, spiky forecasts are not desired, the local MLP can be removed. Model Architecture\rThe loss function for MQRNN is the sum of individual quantile loss, and the output is all the quantile forecasts. At 0.5 quantile, the quantile loss is simply the Mean Absolute Error and its minimizer is the median of the predictive distribution. MQRNN generates multi-horizon forecasts by placing a series of decoders, with shared parameters, at each recurrent layer (time point) in the encoder, and computes the loss against the corresponding targets. Each time series of arbitrary lengths can serve as a single sample in the model training, hence allowing cold content with limited history to also be used in the model. It also uses static information by replicating it across time. The authors also recommend trying different encoder structures for processing sequential input, like dilated 1D causal convolution layers (MQCNN), NARX-like LSTM, or WaveNet. PyTorch Code class MQRNN(nn.Module): def __init__(self, horizon_size:int=horizon_size, hidden_size:int=100, quantiles:list=desired_quantiles, dropout:float=0.3, layer_size:int=3, context_size:int=50, covariate_size:int=num_covariates, bidirectional=False, device=torch.d","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:3:2","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#decoder-design"},{"categories":["Time Series"],"content":"\r2. MQRNNMulti-horizon Quantile Recurrent Neural Network (MQRNN) is an RNN-based non-parametric probabilistic forecasting model proposed by Amazon 6. It uses quantile loss to predict values for each desired quantile for every time step in the forecast horizon. One problem with recursive forecast generators like DeepAR, is that they tend to accumulate errors from previous steps during recursive forecast generation. Some empirical search also suggests that directly forecasting values for the full forecast horizon are less biased, more stable, and retrains the efficiency of the parameter sharing 7. MQRNN builds on this idea and uses direct multi-horizon forecasting instead of a one-step-ahead recursive forecasting approach. It also incorporates both static and temporal covariates and solves the following large-scale time series regression problem: Incorporating Future Covariate ValuesThis paper suggests that distant future information can have a retrospective impact on near-future horizons. For example, if a festival or black-Friday sales event is coming up in the future, the anticipation of it can affect a customer’s buying decisions. As explained later, this future information is supplied to the two decoder MLP components. Decoder DesignThe model adopts the encoder-decoder framework. The encoder is a vanilla LSTM network that takes historical time series and covariates values. For the decoder structure, the model uses two MLP branches instead of a recursive decoder. As stated earlier, the model focuses on directly producing output for the full horizon at once. The MLP branches are used to achieve this goal. The design philosophy for the two decoders is as follows: Global Decoder: The global MLP takes encoder output and future covariates as input, and generates two context vectors as output: horizon-specific context, and horizon-agnostic context. The idea here behind horizon-specific context is that it carries the structural awareness of the temporal distance between a forecast creation time point and a specific horizon. This is essential to aspects like seasonality mapping. Whereas the horizon-agnostic context is based on the idea that not all relevant information is time-sensitive. Note that in the standard encoder-decoder architecture, only horizon-agnostic context exists, and horizon awareness is indirectly enforced by recursively feeding predictions not the RNN cell for the next time step. Local Decoder: The local MLP takes the global decoder’s outputs and also future covariates as input, and generates all required quantiles for each specific horizon. The local MLP is key to aligning future seasonality and event, and the capability to generate sharp, spiky forecasts. In cases where there is no meaningful future information, or sharp, spiky forecasts are not desired, the local MLP can be removed. Model Architecture\rThe loss function for MQRNN is the sum of individual quantile loss, and the output is all the quantile forecasts. At 0.5 quantile, the quantile loss is simply the Mean Absolute Error and its minimizer is the median of the predictive distribution. MQRNN generates multi-horizon forecasts by placing a series of decoders, with shared parameters, at each recurrent layer (time point) in the encoder, and computes the loss against the corresponding targets. Each time series of arbitrary lengths can serve as a single sample in the model training, hence allowing cold content with limited history to also be used in the model. It also uses static information by replicating it across time. The authors also recommend trying different encoder structures for processing sequential input, like dilated 1D causal convolution layers (MQCNN), NARX-like LSTM, or WaveNet. PyTorch Code class MQRNN(nn.Module): def __init__(self, horizon_size:int=horizon_size, hidden_size:int=100, quantiles:list=desired_quantiles, dropout:float=0.3, layer_size:int=3, context_size:int=50, covariate_size:int=num_covariates, bidirectional=False, device=torch.d","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:3:2","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#model-architecture-1"},{"categories":["Time Series"],"content":"\r2. MQRNNMulti-horizon Quantile Recurrent Neural Network (MQRNN) is an RNN-based non-parametric probabilistic forecasting model proposed by Amazon 6. It uses quantile loss to predict values for each desired quantile for every time step in the forecast horizon. One problem with recursive forecast generators like DeepAR, is that they tend to accumulate errors from previous steps during recursive forecast generation. Some empirical search also suggests that directly forecasting values for the full forecast horizon are less biased, more stable, and retrains the efficiency of the parameter sharing 7. MQRNN builds on this idea and uses direct multi-horizon forecasting instead of a one-step-ahead recursive forecasting approach. It also incorporates both static and temporal covariates and solves the following large-scale time series regression problem: Incorporating Future Covariate ValuesThis paper suggests that distant future information can have a retrospective impact on near-future horizons. For example, if a festival or black-Friday sales event is coming up in the future, the anticipation of it can affect a customer’s buying decisions. As explained later, this future information is supplied to the two decoder MLP components. Decoder DesignThe model adopts the encoder-decoder framework. The encoder is a vanilla LSTM network that takes historical time series and covariates values. For the decoder structure, the model uses two MLP branches instead of a recursive decoder. As stated earlier, the model focuses on directly producing output for the full horizon at once. The MLP branches are used to achieve this goal. The design philosophy for the two decoders is as follows: Global Decoder: The global MLP takes encoder output and future covariates as input, and generates two context vectors as output: horizon-specific context, and horizon-agnostic context. The idea here behind horizon-specific context is that it carries the structural awareness of the temporal distance between a forecast creation time point and a specific horizon. This is essential to aspects like seasonality mapping. Whereas the horizon-agnostic context is based on the idea that not all relevant information is time-sensitive. Note that in the standard encoder-decoder architecture, only horizon-agnostic context exists, and horizon awareness is indirectly enforced by recursively feeding predictions not the RNN cell for the next time step. Local Decoder: The local MLP takes the global decoder’s outputs and also future covariates as input, and generates all required quantiles for each specific horizon. The local MLP is key to aligning future seasonality and event, and the capability to generate sharp, spiky forecasts. In cases where there is no meaningful future information, or sharp, spiky forecasts are not desired, the local MLP can be removed. Model Architecture\rThe loss function for MQRNN is the sum of individual quantile loss, and the output is all the quantile forecasts. At 0.5 quantile, the quantile loss is simply the Mean Absolute Error and its minimizer is the median of the predictive distribution. MQRNN generates multi-horizon forecasts by placing a series of decoders, with shared parameters, at each recurrent layer (time point) in the encoder, and computes the loss against the corresponding targets. Each time series of arbitrary lengths can serve as a single sample in the model training, hence allowing cold content with limited history to also be used in the model. It also uses static information by replicating it across time. The authors also recommend trying different encoder structures for processing sequential input, like dilated 1D causal convolution layers (MQCNN), NARX-like LSTM, or WaveNet. PyTorch Code class MQRNN(nn.Module): def __init__(self, horizon_size:int=horizon_size, hidden_size:int=100, quantiles:list=desired_quantiles, dropout:float=0.3, layer_size:int=3, context_size:int=50, covariate_size:int=num_covariates, bidirectional=False, device=torch.d","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:3:2","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#pytorch-code-1"},{"categories":["Time Series"],"content":"\r3. Bayesian LSTMAnother clever usage of the encoder-decoder framework for time-series forecasting was proposed by Uber 8. The main objective of this proposal was to create a solution for quantifying the forecasting uncertainty using Bayesian Neural Networks. This approach can also be used for anomaly detection at scale. The bayesian perspective introduces uncertainty measurement to deep learning models. A prior (often Gaussian) is introduced for the weight parameters, and the model aims to fit the optimal posterior distribution. Many methods, such as stochastic search, variational Bayes, and probabilistic backpropagation, have been proposed to approximate inference methods for Bayesian networks at scale. This paper adopts a Monte Carlo Dropout based approach suggested by prior research 9. Specifically, stochastic dropouts are applied after each hidden layer, and the model output can be approximately viewed as a random sample generated from posterior prediction distribution. Hence the uncertainty in model predictions can be estimated by the sample variance of the model predictions in a few repetitions. Quantifying UncertaintyThis paper decomposes prediction uncertainty into 3 types: Uncertainty due to model (ignorance of model parameters). This uncertainty can be reduced by collecting more samples or by using MC dropouts. Uncertainty due to data (train and test samples are from a different population). This can be accounted for by fitting a latent embedding space for all training time series using an encoder-decoder model and calculating variance using only the encoder representations. Inherent noise (irreducible). This noise can be estimated through the residual sum of squares evaluated on an independent help-out validation set. Model Architecture\rThe encoder-decoder is a standard RNN-based framework that captures the inherent pattern in the time series during the pre-training step. After the encoder-decoder is pre-trained, the encoder is treated as an intelligent feature-extraction blackbox. A prediction network learns to take input from both the learned encoder, as well as any potential external features to generate the final prediction. Before training, the raw data are log-transformed to remove exponential effects. Within each input sliding window, the first day is subtracted from all values, so that trends are removed and the neural network is trained for the increment value. Later, these transformations are reverted to obtain predictions at the original scale. In the paper, learned embeddings (i.e. the last LSTM cell states in the encoder) are projected to lower dimensions using PCA for interpreting extracted features. PyTorch Code class VariationalDropout(nn.Module): def __init__(self, dropout): super().__init__() self.dropout = dropout def forward(self, x: torch.Tensor) -\u003e torch.Tensor: if not self.training: return x max_batch_size = x.size(1) m = x.new_empty(1, max_batch_size, x.size(2), requires_grad=False).bernoulli_(1 - self.dropout) x = x.masked_fill(m == 0, 0) / (1 - self.dropout) return x class LSTM(nn.LSTM): def __init__(self, *args, dropouti=0., dropoutw=0., dropouto=0., unit_forget_bias=True, **kwargs): super().__init__(*args, **kwargs) self.unit_forget_bias = unit_forget_bias self.dropoutw = dropoutw self.input_drop = VariationalDropout(dropouti) self.output_drop = VariationalDropout(dropouto) def forward(self, input): input = self.input_drop(input) seq, state = super().forward(input) return self.output_drop(seq), state class VDEncoder(nn.Module): def __init__(self, in_features, out_features, p): super(VDEncoder, self).__init__() self.model = nn.ModuleDict({ 'lstm1': LSTM(in_features, 32,dropouto=p), 'lstm2': LSTM(32, 8, dropouto=p), 'lstm3': LSTM(8, out_features, dropouto=p) }) def forward(self, x): out, _ = self.model['lstm1'](x) out, _ = self.model['lstm2'](out) out, _ = self.model['lstm3'](out) return out class VDDecoder(nn.Module): def __init__(self, p): super(VDDecoder, self).__init__() self.model = n","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:3:3","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#3-bayesian-lstm"},{"categories":["Time Series"],"content":"\r3. Bayesian LSTMAnother clever usage of the encoder-decoder framework for time-series forecasting was proposed by Uber 8. The main objective of this proposal was to create a solution for quantifying the forecasting uncertainty using Bayesian Neural Networks. This approach can also be used for anomaly detection at scale. The bayesian perspective introduces uncertainty measurement to deep learning models. A prior (often Gaussian) is introduced for the weight parameters, and the model aims to fit the optimal posterior distribution. Many methods, such as stochastic search, variational Bayes, and probabilistic backpropagation, have been proposed to approximate inference methods for Bayesian networks at scale. This paper adopts a Monte Carlo Dropout based approach suggested by prior research 9. Specifically, stochastic dropouts are applied after each hidden layer, and the model output can be approximately viewed as a random sample generated from posterior prediction distribution. Hence the uncertainty in model predictions can be estimated by the sample variance of the model predictions in a few repetitions. Quantifying UncertaintyThis paper decomposes prediction uncertainty into 3 types: Uncertainty due to model (ignorance of model parameters). This uncertainty can be reduced by collecting more samples or by using MC dropouts. Uncertainty due to data (train and test samples are from a different population). This can be accounted for by fitting a latent embedding space for all training time series using an encoder-decoder model and calculating variance using only the encoder representations. Inherent noise (irreducible). This noise can be estimated through the residual sum of squares evaluated on an independent help-out validation set. Model Architecture\rThe encoder-decoder is a standard RNN-based framework that captures the inherent pattern in the time series during the pre-training step. After the encoder-decoder is pre-trained, the encoder is treated as an intelligent feature-extraction blackbox. A prediction network learns to take input from both the learned encoder, as well as any potential external features to generate the final prediction. Before training, the raw data are log-transformed to remove exponential effects. Within each input sliding window, the first day is subtracted from all values, so that trends are removed and the neural network is trained for the increment value. Later, these transformations are reverted to obtain predictions at the original scale. In the paper, learned embeddings (i.e. the last LSTM cell states in the encoder) are projected to lower dimensions using PCA for interpreting extracted features. PyTorch Code class VariationalDropout(nn.Module): def __init__(self, dropout): super().__init__() self.dropout = dropout def forward(self, x: torch.Tensor) -\u003e torch.Tensor: if not self.training: return x max_batch_size = x.size(1) m = x.new_empty(1, max_batch_size, x.size(2), requires_grad=False).bernoulli_(1 - self.dropout) x = x.masked_fill(m == 0, 0) / (1 - self.dropout) return x class LSTM(nn.LSTM): def __init__(self, *args, dropouti=0., dropoutw=0., dropouto=0., unit_forget_bias=True, **kwargs): super().__init__(*args, **kwargs) self.unit_forget_bias = unit_forget_bias self.dropoutw = dropoutw self.input_drop = VariationalDropout(dropouti) self.output_drop = VariationalDropout(dropouto) def forward(self, input): input = self.input_drop(input) seq, state = super().forward(input) return self.output_drop(seq), state class VDEncoder(nn.Module): def __init__(self, in_features, out_features, p): super(VDEncoder, self).__init__() self.model = nn.ModuleDict({ 'lstm1': LSTM(in_features, 32,dropouto=p), 'lstm2': LSTM(32, 8, dropouto=p), 'lstm3': LSTM(8, out_features, dropouto=p) }) def forward(self, x): out, _ = self.model['lstm1'](x) out, _ = self.model['lstm2'](out) out, _ = self.model['lstm3'](out) return out class VDDecoder(nn.Module): def __init__(self, p): super(VDDecoder, self).__init__() self.model = n","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:3:3","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#quantifying-uncertainty"},{"categories":["Time Series"],"content":"\r3. Bayesian LSTMAnother clever usage of the encoder-decoder framework for time-series forecasting was proposed by Uber 8. The main objective of this proposal was to create a solution for quantifying the forecasting uncertainty using Bayesian Neural Networks. This approach can also be used for anomaly detection at scale. The bayesian perspective introduces uncertainty measurement to deep learning models. A prior (often Gaussian) is introduced for the weight parameters, and the model aims to fit the optimal posterior distribution. Many methods, such as stochastic search, variational Bayes, and probabilistic backpropagation, have been proposed to approximate inference methods for Bayesian networks at scale. This paper adopts a Monte Carlo Dropout based approach suggested by prior research 9. Specifically, stochastic dropouts are applied after each hidden layer, and the model output can be approximately viewed as a random sample generated from posterior prediction distribution. Hence the uncertainty in model predictions can be estimated by the sample variance of the model predictions in a few repetitions. Quantifying UncertaintyThis paper decomposes prediction uncertainty into 3 types: Uncertainty due to model (ignorance of model parameters). This uncertainty can be reduced by collecting more samples or by using MC dropouts. Uncertainty due to data (train and test samples are from a different population). This can be accounted for by fitting a latent embedding space for all training time series using an encoder-decoder model and calculating variance using only the encoder representations. Inherent noise (irreducible). This noise can be estimated through the residual sum of squares evaluated on an independent help-out validation set. Model Architecture\rThe encoder-decoder is a standard RNN-based framework that captures the inherent pattern in the time series during the pre-training step. After the encoder-decoder is pre-trained, the encoder is treated as an intelligent feature-extraction blackbox. A prediction network learns to take input from both the learned encoder, as well as any potential external features to generate the final prediction. Before training, the raw data are log-transformed to remove exponential effects. Within each input sliding window, the first day is subtracted from all values, so that trends are removed and the neural network is trained for the increment value. Later, these transformations are reverted to obtain predictions at the original scale. In the paper, learned embeddings (i.e. the last LSTM cell states in the encoder) are projected to lower dimensions using PCA for interpreting extracted features. PyTorch Code class VariationalDropout(nn.Module): def __init__(self, dropout): super().__init__() self.dropout = dropout def forward(self, x: torch.Tensor) -\u003e torch.Tensor: if not self.training: return x max_batch_size = x.size(1) m = x.new_empty(1, max_batch_size, x.size(2), requires_grad=False).bernoulli_(1 - self.dropout) x = x.masked_fill(m == 0, 0) / (1 - self.dropout) return x class LSTM(nn.LSTM): def __init__(self, *args, dropouti=0., dropoutw=0., dropouto=0., unit_forget_bias=True, **kwargs): super().__init__(*args, **kwargs) self.unit_forget_bias = unit_forget_bias self.dropoutw = dropoutw self.input_drop = VariationalDropout(dropouti) self.output_drop = VariationalDropout(dropouto) def forward(self, input): input = self.input_drop(input) seq, state = super().forward(input) return self.output_drop(seq), state class VDEncoder(nn.Module): def __init__(self, in_features, out_features, p): super(VDEncoder, self).__init__() self.model = nn.ModuleDict({ 'lstm1': LSTM(in_features, 32,dropouto=p), 'lstm2': LSTM(32, 8, dropouto=p), 'lstm3': LSTM(8, out_features, dropouto=p) }) def forward(self, x): out, _ = self.model['lstm1'](x) out, _ = self.model['lstm2'](out) out, _ = self.model['lstm3'](out) return out class VDDecoder(nn.Module): def __init__(self, p): super(VDDecoder, self).__init__() self.model = n","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:3:3","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#model-architecture-2"},{"categories":["Time Series"],"content":"\r3. Bayesian LSTMAnother clever usage of the encoder-decoder framework for time-series forecasting was proposed by Uber 8. The main objective of this proposal was to create a solution for quantifying the forecasting uncertainty using Bayesian Neural Networks. This approach can also be used for anomaly detection at scale. The bayesian perspective introduces uncertainty measurement to deep learning models. A prior (often Gaussian) is introduced for the weight parameters, and the model aims to fit the optimal posterior distribution. Many methods, such as stochastic search, variational Bayes, and probabilistic backpropagation, have been proposed to approximate inference methods for Bayesian networks at scale. This paper adopts a Monte Carlo Dropout based approach suggested by prior research 9. Specifically, stochastic dropouts are applied after each hidden layer, and the model output can be approximately viewed as a random sample generated from posterior prediction distribution. Hence the uncertainty in model predictions can be estimated by the sample variance of the model predictions in a few repetitions. Quantifying UncertaintyThis paper decomposes prediction uncertainty into 3 types: Uncertainty due to model (ignorance of model parameters). This uncertainty can be reduced by collecting more samples or by using MC dropouts. Uncertainty due to data (train and test samples are from a different population). This can be accounted for by fitting a latent embedding space for all training time series using an encoder-decoder model and calculating variance using only the encoder representations. Inherent noise (irreducible). This noise can be estimated through the residual sum of squares evaluated on an independent help-out validation set. Model Architecture\rThe encoder-decoder is a standard RNN-based framework that captures the inherent pattern in the time series during the pre-training step. After the encoder-decoder is pre-trained, the encoder is treated as an intelligent feature-extraction blackbox. A prediction network learns to take input from both the learned encoder, as well as any potential external features to generate the final prediction. Before training, the raw data are log-transformed to remove exponential effects. Within each input sliding window, the first day is subtracted from all values, so that trends are removed and the neural network is trained for the increment value. Later, these transformations are reverted to obtain predictions at the original scale. In the paper, learned embeddings (i.e. the last LSTM cell states in the encoder) are projected to lower dimensions using PCA for interpreting extracted features. PyTorch Code class VariationalDropout(nn.Module): def __init__(self, dropout): super().__init__() self.dropout = dropout def forward(self, x: torch.Tensor) -\u003e torch.Tensor: if not self.training: return x max_batch_size = x.size(1) m = x.new_empty(1, max_batch_size, x.size(2), requires_grad=False).bernoulli_(1 - self.dropout) x = x.masked_fill(m == 0, 0) / (1 - self.dropout) return x class LSTM(nn.LSTM): def __init__(self, *args, dropouti=0., dropoutw=0., dropouto=0., unit_forget_bias=True, **kwargs): super().__init__(*args, **kwargs) self.unit_forget_bias = unit_forget_bias self.dropoutw = dropoutw self.input_drop = VariationalDropout(dropouti) self.output_drop = VariationalDropout(dropouto) def forward(self, input): input = self.input_drop(input) seq, state = super().forward(input) return self.output_drop(seq), state class VDEncoder(nn.Module): def __init__(self, in_features, out_features, p): super(VDEncoder, self).__init__() self.model = nn.ModuleDict({ 'lstm1': LSTM(in_features, 32,dropouto=p), 'lstm2': LSTM(32, 8, dropouto=p), 'lstm3': LSTM(8, out_features, dropouto=p) }) def forward(self, x): out, _ = self.model['lstm1'](x) out, _ = self.model['lstm2'](out) out, _ = self.model['lstm3'](out) return out class VDDecoder(nn.Module): def __init__(self, p): super(VDDecoder, self).__init__() self.model = n","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:3:3","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#pytorch-code-2"},{"categories":["Time Series"],"content":"\r4. DeepTCNThe Deep Temporal Convolutional Network (DeepTCN) was proposed by Bigo Research 10. Instead of using RNNs, DeepTCN utilizes stacked layers of dilated causal convolutional nets to capture the long-term correlations in time series data. RNNs can be remarkably difficult to train due to exploding or vanishing gradients 11, and backpropagation through time (BPTT) often hampers efficient computation 12. On the other hand, Temporal Convolutional Network (TCN) is more robust to error accumulation due to its non-autoregressive nature, and it can also be efficiently trained in parallel. Notes on TCNA TCN is a 1D FCN with causal convolutions. Casual means that there is no information leakage from future to past, i.e. the output at time t can only be obtained from the inputs that are no later than t. Dilated convolutions enable exponentially large receptive fields (as opposed to linear to the depth of the network). Dilation allows the filter to be applied over an area larger than its length by skipping the input values by a certain step. Recent studies have empirically shown that TCNs outperform RNNs across a broad range of sequence modeling tasks. These studies also recommend using TCNs, instead of RNNs, as the default starting point for sequence modeling tasks 13. Data PreparationThis research used time-independent covariates like product_id to incorporate series-level information that helps in capturing the scale level and seasonality of each specific series. Other covariates included hour-of-the-day, day-of-the-week, and day-of-the-month for hourly data, day-of-the-year for daily data, and month-of-the-year for monthly data. One of the major concerns for this study was to effectively handle complex seasonal patterns, like holiday effects. This is done by using a hand-crafted exogenous variable (covariates) such as holiday indicators, weather forecasts, etc. New products with little historical information used zero-padding to ensure the desired input sequence length. Model Architecture\rDeepTCN uses an encoder-decoder framework. The encoder is composed of stacked residual blocks based on dilated causal convolutional nets to capture temporal dependencies. The diagram above shows an encoder on the left with {1, 2, 4, 8} as the dilation factors, filter size of 2, and receptive field of 16. More specifically, the encoder architecture is stacked dilated causal convolutions that model historical observations and covariates. The diagram below shows the original DeepTCN encoder on the left, and the one I adapted from another research13 for my implementation. For the decoder, the paper proposed a modified resnet-v block that takes two inputs (encoder output and future covariates). The decoder architecture is shown in the figure below. PyTorch Code class ResidualBlock(nn.Module): def __init__(self, input_dim, d, stride=1, num_filters=35, p=0.2, k=2, weight_norm=True): super(ResidualBlock, self).__init__() self.k, self.d, self.dropout_fn = k, d, nn.Dropout(p) self.conv1 = nn.Conv1d(input_dim, num_filters, kernel_size=k, dilation=d) self.conv2 = nn.Conv1d(num_filters, num_filters, kernel_size=k, dilation=d) if weight_norm: self.conv1, self.conv2 = nn.utils.weight_norm(self.conv1), nn.utils.weight_norm(self.conv2) self.downsample = nn.Conv1d(input_dim, num_filters, 1) if input_dim != num_filters else None def forward(self, x): out = self.dropout_fn(F.relu(self.conv1(x.float()))) out = self.dropout_fn(F.relu(self.conv2(out))) residual = x if self.downsample is None else self.downsample(x) return F.relu(out + residual[:,:,-out.shape[2]:]) class FutureResidual(nn.Module): def __init__(self, in_features): super(FutureResidual, self).__init__() self.net = nn.Sequential(nn.Linear(in_features=in_features, out_features=in_features), nn.BatchNorm1d(in_features), nn.ReLU(), nn.Linear(in_features=in_features, out_features=in_features), nn.BatchNorm1d(in_features),) def forward(self, lag_x, x): out = self.net(x.squeeze()) return F.relu(torch.cat((lag","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:3:4","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#4-deeptcn"},{"categories":["Time Series"],"content":"\r4. DeepTCNThe Deep Temporal Convolutional Network (DeepTCN) was proposed by Bigo Research 10. Instead of using RNNs, DeepTCN utilizes stacked layers of dilated causal convolutional nets to capture the long-term correlations in time series data. RNNs can be remarkably difficult to train due to exploding or vanishing gradients 11, and backpropagation through time (BPTT) often hampers efficient computation 12. On the other hand, Temporal Convolutional Network (TCN) is more robust to error accumulation due to its non-autoregressive nature, and it can also be efficiently trained in parallel. Notes on TCNA TCN is a 1D FCN with causal convolutions. Casual means that there is no information leakage from future to past, i.e. the output at time t can only be obtained from the inputs that are no later than t. Dilated convolutions enable exponentially large receptive fields (as opposed to linear to the depth of the network). Dilation allows the filter to be applied over an area larger than its length by skipping the input values by a certain step. Recent studies have empirically shown that TCNs outperform RNNs across a broad range of sequence modeling tasks. These studies also recommend using TCNs, instead of RNNs, as the default starting point for sequence modeling tasks 13. Data PreparationThis research used time-independent covariates like product_id to incorporate series-level information that helps in capturing the scale level and seasonality of each specific series. Other covariates included hour-of-the-day, day-of-the-week, and day-of-the-month for hourly data, day-of-the-year for daily data, and month-of-the-year for monthly data. One of the major concerns for this study was to effectively handle complex seasonal patterns, like holiday effects. This is done by using a hand-crafted exogenous variable (covariates) such as holiday indicators, weather forecasts, etc. New products with little historical information used zero-padding to ensure the desired input sequence length. Model Architecture\rDeepTCN uses an encoder-decoder framework. The encoder is composed of stacked residual blocks based on dilated causal convolutional nets to capture temporal dependencies. The diagram above shows an encoder on the left with {1, 2, 4, 8} as the dilation factors, filter size of 2, and receptive field of 16. More specifically, the encoder architecture is stacked dilated causal convolutions that model historical observations and covariates. The diagram below shows the original DeepTCN encoder on the left, and the one I adapted from another research13 for my implementation. For the decoder, the paper proposed a modified resnet-v block that takes two inputs (encoder output and future covariates). The decoder architecture is shown in the figure below. PyTorch Code class ResidualBlock(nn.Module): def __init__(self, input_dim, d, stride=1, num_filters=35, p=0.2, k=2, weight_norm=True): super(ResidualBlock, self).__init__() self.k, self.d, self.dropout_fn = k, d, nn.Dropout(p) self.conv1 = nn.Conv1d(input_dim, num_filters, kernel_size=k, dilation=d) self.conv2 = nn.Conv1d(num_filters, num_filters, kernel_size=k, dilation=d) if weight_norm: self.conv1, self.conv2 = nn.utils.weight_norm(self.conv1), nn.utils.weight_norm(self.conv2) self.downsample = nn.Conv1d(input_dim, num_filters, 1) if input_dim != num_filters else None def forward(self, x): out = self.dropout_fn(F.relu(self.conv1(x.float()))) out = self.dropout_fn(F.relu(self.conv2(out))) residual = x if self.downsample is None else self.downsample(x) return F.relu(out + residual[:,:,-out.shape[2]:]) class FutureResidual(nn.Module): def __init__(self, in_features): super(FutureResidual, self).__init__() self.net = nn.Sequential(nn.Linear(in_features=in_features, out_features=in_features), nn.BatchNorm1d(in_features), nn.ReLU(), nn.Linear(in_features=in_features, out_features=in_features), nn.BatchNorm1d(in_features),) def forward(self, lag_x, x): out = self.net(x.squeeze()) return F.relu(torch.cat((lag","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:3:4","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#notes-on-tcn"},{"categories":["Time Series"],"content":"\r4. DeepTCNThe Deep Temporal Convolutional Network (DeepTCN) was proposed by Bigo Research 10. Instead of using RNNs, DeepTCN utilizes stacked layers of dilated causal convolutional nets to capture the long-term correlations in time series data. RNNs can be remarkably difficult to train due to exploding or vanishing gradients 11, and backpropagation through time (BPTT) often hampers efficient computation 12. On the other hand, Temporal Convolutional Network (TCN) is more robust to error accumulation due to its non-autoregressive nature, and it can also be efficiently trained in parallel. Notes on TCNA TCN is a 1D FCN with causal convolutions. Casual means that there is no information leakage from future to past, i.e. the output at time t can only be obtained from the inputs that are no later than t. Dilated convolutions enable exponentially large receptive fields (as opposed to linear to the depth of the network). Dilation allows the filter to be applied over an area larger than its length by skipping the input values by a certain step. Recent studies have empirically shown that TCNs outperform RNNs across a broad range of sequence modeling tasks. These studies also recommend using TCNs, instead of RNNs, as the default starting point for sequence modeling tasks 13. Data PreparationThis research used time-independent covariates like product_id to incorporate series-level information that helps in capturing the scale level and seasonality of each specific series. Other covariates included hour-of-the-day, day-of-the-week, and day-of-the-month for hourly data, day-of-the-year for daily data, and month-of-the-year for monthly data. One of the major concerns for this study was to effectively handle complex seasonal patterns, like holiday effects. This is done by using a hand-crafted exogenous variable (covariates) such as holiday indicators, weather forecasts, etc. New products with little historical information used zero-padding to ensure the desired input sequence length. Model Architecture\rDeepTCN uses an encoder-decoder framework. The encoder is composed of stacked residual blocks based on dilated causal convolutional nets to capture temporal dependencies. The diagram above shows an encoder on the left with {1, 2, 4, 8} as the dilation factors, filter size of 2, and receptive field of 16. More specifically, the encoder architecture is stacked dilated causal convolutions that model historical observations and covariates. The diagram below shows the original DeepTCN encoder on the left, and the one I adapted from another research13 for my implementation. For the decoder, the paper proposed a modified resnet-v block that takes two inputs (encoder output and future covariates). The decoder architecture is shown in the figure below. PyTorch Code class ResidualBlock(nn.Module): def __init__(self, input_dim, d, stride=1, num_filters=35, p=0.2, k=2, weight_norm=True): super(ResidualBlock, self).__init__() self.k, self.d, self.dropout_fn = k, d, nn.Dropout(p) self.conv1 = nn.Conv1d(input_dim, num_filters, kernel_size=k, dilation=d) self.conv2 = nn.Conv1d(num_filters, num_filters, kernel_size=k, dilation=d) if weight_norm: self.conv1, self.conv2 = nn.utils.weight_norm(self.conv1), nn.utils.weight_norm(self.conv2) self.downsample = nn.Conv1d(input_dim, num_filters, 1) if input_dim != num_filters else None def forward(self, x): out = self.dropout_fn(F.relu(self.conv1(x.float()))) out = self.dropout_fn(F.relu(self.conv2(out))) residual = x if self.downsample is None else self.downsample(x) return F.relu(out + residual[:,:,-out.shape[2]:]) class FutureResidual(nn.Module): def __init__(self, in_features): super(FutureResidual, self).__init__() self.net = nn.Sequential(nn.Linear(in_features=in_features, out_features=in_features), nn.BatchNorm1d(in_features), nn.ReLU(), nn.Linear(in_features=in_features, out_features=in_features), nn.BatchNorm1d(in_features),) def forward(self, lag_x, x): out = self.net(x.squeeze()) return F.relu(torch.cat((lag","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:3:4","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#data-preparation"},{"categories":["Time Series"],"content":"\r4. DeepTCNThe Deep Temporal Convolutional Network (DeepTCN) was proposed by Bigo Research 10. Instead of using RNNs, DeepTCN utilizes stacked layers of dilated causal convolutional nets to capture the long-term correlations in time series data. RNNs can be remarkably difficult to train due to exploding or vanishing gradients 11, and backpropagation through time (BPTT) often hampers efficient computation 12. On the other hand, Temporal Convolutional Network (TCN) is more robust to error accumulation due to its non-autoregressive nature, and it can also be efficiently trained in parallel. Notes on TCNA TCN is a 1D FCN with causal convolutions. Casual means that there is no information leakage from future to past, i.e. the output at time t can only be obtained from the inputs that are no later than t. Dilated convolutions enable exponentially large receptive fields (as opposed to linear to the depth of the network). Dilation allows the filter to be applied over an area larger than its length by skipping the input values by a certain step. Recent studies have empirically shown that TCNs outperform RNNs across a broad range of sequence modeling tasks. These studies also recommend using TCNs, instead of RNNs, as the default starting point for sequence modeling tasks 13. Data PreparationThis research used time-independent covariates like product_id to incorporate series-level information that helps in capturing the scale level and seasonality of each specific series. Other covariates included hour-of-the-day, day-of-the-week, and day-of-the-month for hourly data, day-of-the-year for daily data, and month-of-the-year for monthly data. One of the major concerns for this study was to effectively handle complex seasonal patterns, like holiday effects. This is done by using a hand-crafted exogenous variable (covariates) such as holiday indicators, weather forecasts, etc. New products with little historical information used zero-padding to ensure the desired input sequence length. Model Architecture\rDeepTCN uses an encoder-decoder framework. The encoder is composed of stacked residual blocks based on dilated causal convolutional nets to capture temporal dependencies. The diagram above shows an encoder on the left with {1, 2, 4, 8} as the dilation factors, filter size of 2, and receptive field of 16. More specifically, the encoder architecture is stacked dilated causal convolutions that model historical observations and covariates. The diagram below shows the original DeepTCN encoder on the left, and the one I adapted from another research13 for my implementation. For the decoder, the paper proposed a modified resnet-v block that takes two inputs (encoder output and future covariates). The decoder architecture is shown in the figure below. PyTorch Code class ResidualBlock(nn.Module): def __init__(self, input_dim, d, stride=1, num_filters=35, p=0.2, k=2, weight_norm=True): super(ResidualBlock, self).__init__() self.k, self.d, self.dropout_fn = k, d, nn.Dropout(p) self.conv1 = nn.Conv1d(input_dim, num_filters, kernel_size=k, dilation=d) self.conv2 = nn.Conv1d(num_filters, num_filters, kernel_size=k, dilation=d) if weight_norm: self.conv1, self.conv2 = nn.utils.weight_norm(self.conv1), nn.utils.weight_norm(self.conv2) self.downsample = nn.Conv1d(input_dim, num_filters, 1) if input_dim != num_filters else None def forward(self, x): out = self.dropout_fn(F.relu(self.conv1(x.float()))) out = self.dropout_fn(F.relu(self.conv2(out))) residual = x if self.downsample is None else self.downsample(x) return F.relu(out + residual[:,:,-out.shape[2]:]) class FutureResidual(nn.Module): def __init__(self, in_features): super(FutureResidual, self).__init__() self.net = nn.Sequential(nn.Linear(in_features=in_features, out_features=in_features), nn.BatchNorm1d(in_features), nn.ReLU(), nn.Linear(in_features=in_features, out_features=in_features), nn.BatchNorm1d(in_features),) def forward(self, lag_x, x): out = self.net(x.squeeze()) return F.relu(torch.cat((lag","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:3:4","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#model-architecture-3"},{"categories":["Time Series"],"content":"\r4. DeepTCNThe Deep Temporal Convolutional Network (DeepTCN) was proposed by Bigo Research 10. Instead of using RNNs, DeepTCN utilizes stacked layers of dilated causal convolutional nets to capture the long-term correlations in time series data. RNNs can be remarkably difficult to train due to exploding or vanishing gradients 11, and backpropagation through time (BPTT) often hampers efficient computation 12. On the other hand, Temporal Convolutional Network (TCN) is more robust to error accumulation due to its non-autoregressive nature, and it can also be efficiently trained in parallel. Notes on TCNA TCN is a 1D FCN with causal convolutions. Casual means that there is no information leakage from future to past, i.e. the output at time t can only be obtained from the inputs that are no later than t. Dilated convolutions enable exponentially large receptive fields (as opposed to linear to the depth of the network). Dilation allows the filter to be applied over an area larger than its length by skipping the input values by a certain step. Recent studies have empirically shown that TCNs outperform RNNs across a broad range of sequence modeling tasks. These studies also recommend using TCNs, instead of RNNs, as the default starting point for sequence modeling tasks 13. Data PreparationThis research used time-independent covariates like product_id to incorporate series-level information that helps in capturing the scale level and seasonality of each specific series. Other covariates included hour-of-the-day, day-of-the-week, and day-of-the-month for hourly data, day-of-the-year for daily data, and month-of-the-year for monthly data. One of the major concerns for this study was to effectively handle complex seasonal patterns, like holiday effects. This is done by using a hand-crafted exogenous variable (covariates) such as holiday indicators, weather forecasts, etc. New products with little historical information used zero-padding to ensure the desired input sequence length. Model Architecture\rDeepTCN uses an encoder-decoder framework. The encoder is composed of stacked residual blocks based on dilated causal convolutional nets to capture temporal dependencies. The diagram above shows an encoder on the left with {1, 2, 4, 8} as the dilation factors, filter size of 2, and receptive field of 16. More specifically, the encoder architecture is stacked dilated causal convolutions that model historical observations and covariates. The diagram below shows the original DeepTCN encoder on the left, and the one I adapted from another research13 for my implementation. For the decoder, the paper proposed a modified resnet-v block that takes two inputs (encoder output and future covariates). The decoder architecture is shown in the figure below. PyTorch Code class ResidualBlock(nn.Module): def __init__(self, input_dim, d, stride=1, num_filters=35, p=0.2, k=2, weight_norm=True): super(ResidualBlock, self).__init__() self.k, self.d, self.dropout_fn = k, d, nn.Dropout(p) self.conv1 = nn.Conv1d(input_dim, num_filters, kernel_size=k, dilation=d) self.conv2 = nn.Conv1d(num_filters, num_filters, kernel_size=k, dilation=d) if weight_norm: self.conv1, self.conv2 = nn.utils.weight_norm(self.conv1), nn.utils.weight_norm(self.conv2) self.downsample = nn.Conv1d(input_dim, num_filters, 1) if input_dim != num_filters else None def forward(self, x): out = self.dropout_fn(F.relu(self.conv1(x.float()))) out = self.dropout_fn(F.relu(self.conv2(out))) residual = x if self.downsample is None else self.downsample(x) return F.relu(out + residual[:,:,-out.shape[2]:]) class FutureResidual(nn.Module): def __init__(self, in_features): super(FutureResidual, self).__init__() self.net = nn.Sequential(nn.Linear(in_features=in_features, out_features=in_features), nn.BatchNorm1d(in_features), nn.ReLU(), nn.Linear(in_features=in_features, out_features=in_features), nn.BatchNorm1d(in_features),) def forward(self, lag_x, x): out = self.net(x.squeeze()) return F.relu(torch.cat((lag","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:3:4","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#pytorch-code-3"},{"categories":["Time Series"],"content":"\r5. N-BEATSN-BEATS is a univariate time series forecasting model proposed by Element AI (acquired by ServiceNow) 14. This research work came out at the time when, ES-RNN, a hybrid of Exponential Smoothing and RNN model won the M4 competition in 2018. A popular narrative at that time suggested that the hybrid of statistical and deep learning methods could be the way forward. But this paper challenged this notion by developing a pure-DL architecture for time series forecasting that was inspired by the signal processing domain. Their architecture also allowed for interpretable outputs by carefully injecting a suitable inductive bias into the model. However, there is no provision to include covariates in the model. N-BEATS was later extended by N-BEATSx 15 to incorporate exogenous variables. Another work, N-HiTS 16 altered the architecture and achieved accuracy improvements along with drastically cutting long-forecasting compute costs. Model Architecture\rThe above diagram shows the N-BEATS architecture from the most granular view on the left to the high-level view on the right. N-BEATS used the residual principle to stack many basic blocks and the paper has shown that we can stack up to 150 layers and still facilitate efficient learning. Let’s look at each of the above 3 columns individually. [Left] Block: Each block takes the lookback period data as input and generates two outputs: a backcast and a forecast. The backcast is the block’s own best prediction of the lookback period. The input to the block is first processed by four standard fully connected layers (with bias and activation) and output from this FC stack is transformed by two separate linear layers (no bias or activation) to something the paper calls expansion coefficients for the backcast and forecast, $\\theta^{b}$, and $\\theta^{f}$, respectively. These expansion coefficients are then mapped to output using a set of basis layers ($g^{b}$ and $g^{f}$). [Middle] Stacks: Different blocks are arranged in a stack. All the blocks in a stack share the same kind of basis layer. The blocks are arranged in a residual manner, such that the input to a block $l$ is $x_{l} = x_{l-1} - \\hat{x}_{l-1}$, i.e. at each step, the backcast generated by the block is subtracted from the input to that block before it is passed on to the next layer. And all the forecast outputs of all the blocks in a stack are added up to make the stack forecast. The residual backcast from the last block in a stack is the stack residual [Right] Overall Architecture: On the right, we see the top-level view of the architecture. Each stack is chained together so that for any stack, the stack residual output of the previous stack is the input and the stack generates two outputs: the stack forecast and the stack residual. Finally, the N-BEATS forecast is the additive sum of all the stack forecasts. Basis Layer A basis layer puts a constraint on the functional space and thereby limits the output representation constrained by the chosen basis function. We can have the basis of any arbitrary function, which gives us a lot of flexibility. In the paper, N-BEATS operates in two modes: generic and interpretable. The generic mode doesn’t have any basis function constraining the function search (we can think of it as having an identity function). So, in this mode, we are leaving the function completely learned by the model through a linear projection of the basis coefficients. Fixing the basis function allows for human interpretability about what each stack signifies. The authors propose two specific basis functions that capture trend (polynomial function) and seasonality (Fourier basis). So we can say that the forecast output of the stack represents trend or seasonality based on the corresponding chosen basis function. PyTorch Code class GenericBasis(nn.Module): def __init__(self, backcast_size, forecast_size): super().__init__() self.backcast_size, self.forecast_size = backcast_size, forecast_size def forward(self, theta)","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:3:5","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#5-n-beats"},{"categories":["Time Series"],"content":"\r5. N-BEATSN-BEATS is a univariate time series forecasting model proposed by Element AI (acquired by ServiceNow) 14. This research work came out at the time when, ES-RNN, a hybrid of Exponential Smoothing and RNN model won the M4 competition in 2018. A popular narrative at that time suggested that the hybrid of statistical and deep learning methods could be the way forward. But this paper challenged this notion by developing a pure-DL architecture for time series forecasting that was inspired by the signal processing domain. Their architecture also allowed for interpretable outputs by carefully injecting a suitable inductive bias into the model. However, there is no provision to include covariates in the model. N-BEATS was later extended by N-BEATSx 15 to incorporate exogenous variables. Another work, N-HiTS 16 altered the architecture and achieved accuracy improvements along with drastically cutting long-forecasting compute costs. Model Architecture\rThe above diagram shows the N-BEATS architecture from the most granular view on the left to the high-level view on the right. N-BEATS used the residual principle to stack many basic blocks and the paper has shown that we can stack up to 150 layers and still facilitate efficient learning. Let’s look at each of the above 3 columns individually. [Left] Block: Each block takes the lookback period data as input and generates two outputs: a backcast and a forecast. The backcast is the block’s own best prediction of the lookback period. The input to the block is first processed by four standard fully connected layers (with bias and activation) and output from this FC stack is transformed by two separate linear layers (no bias or activation) to something the paper calls expansion coefficients for the backcast and forecast, $\\theta^{b}$, and $\\theta^{f}$, respectively. These expansion coefficients are then mapped to output using a set of basis layers ($g^{b}$ and $g^{f}$). [Middle] Stacks: Different blocks are arranged in a stack. All the blocks in a stack share the same kind of basis layer. The blocks are arranged in a residual manner, such that the input to a block $l$ is $x_{l} = x_{l-1} - \\hat{x}_{l-1}$, i.e. at each step, the backcast generated by the block is subtracted from the input to that block before it is passed on to the next layer. And all the forecast outputs of all the blocks in a stack are added up to make the stack forecast. The residual backcast from the last block in a stack is the stack residual [Right] Overall Architecture: On the right, we see the top-level view of the architecture. Each stack is chained together so that for any stack, the stack residual output of the previous stack is the input and the stack generates two outputs: the stack forecast and the stack residual. Finally, the N-BEATS forecast is the additive sum of all the stack forecasts. Basis Layer A basis layer puts a constraint on the functional space and thereby limits the output representation constrained by the chosen basis function. We can have the basis of any arbitrary function, which gives us a lot of flexibility. In the paper, N-BEATS operates in two modes: generic and interpretable. The generic mode doesn’t have any basis function constraining the function search (we can think of it as having an identity function). So, in this mode, we are leaving the function completely learned by the model through a linear projection of the basis coefficients. Fixing the basis function allows for human interpretability about what each stack signifies. The authors propose two specific basis functions that capture trend (polynomial function) and seasonality (Fourier basis). So we can say that the forecast output of the stack represents trend or seasonality based on the corresponding chosen basis function. PyTorch Code class GenericBasis(nn.Module): def __init__(self, backcast_size, forecast_size): super().__init__() self.backcast_size, self.forecast_size = backcast_size, forecast_size def forward(self, theta)","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:3:5","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#model-architecture-4"},{"categories":["Time Series"],"content":"\r5. N-BEATSN-BEATS is a univariate time series forecasting model proposed by Element AI (acquired by ServiceNow) 14. This research work came out at the time when, ES-RNN, a hybrid of Exponential Smoothing and RNN model won the M4 competition in 2018. A popular narrative at that time suggested that the hybrid of statistical and deep learning methods could be the way forward. But this paper challenged this notion by developing a pure-DL architecture for time series forecasting that was inspired by the signal processing domain. Their architecture also allowed for interpretable outputs by carefully injecting a suitable inductive bias into the model. However, there is no provision to include covariates in the model. N-BEATS was later extended by N-BEATSx 15 to incorporate exogenous variables. Another work, N-HiTS 16 altered the architecture and achieved accuracy improvements along with drastically cutting long-forecasting compute costs. Model Architecture\rThe above diagram shows the N-BEATS architecture from the most granular view on the left to the high-level view on the right. N-BEATS used the residual principle to stack many basic blocks and the paper has shown that we can stack up to 150 layers and still facilitate efficient learning. Let’s look at each of the above 3 columns individually. [Left] Block: Each block takes the lookback period data as input and generates two outputs: a backcast and a forecast. The backcast is the block’s own best prediction of the lookback period. The input to the block is first processed by four standard fully connected layers (with bias and activation) and output from this FC stack is transformed by two separate linear layers (no bias or activation) to something the paper calls expansion coefficients for the backcast and forecast, $\\theta^{b}$, and $\\theta^{f}$, respectively. These expansion coefficients are then mapped to output using a set of basis layers ($g^{b}$ and $g^{f}$). [Middle] Stacks: Different blocks are arranged in a stack. All the blocks in a stack share the same kind of basis layer. The blocks are arranged in a residual manner, such that the input to a block $l$ is $x_{l} = x_{l-1} - \\hat{x}_{l-1}$, i.e. at each step, the backcast generated by the block is subtracted from the input to that block before it is passed on to the next layer. And all the forecast outputs of all the blocks in a stack are added up to make the stack forecast. The residual backcast from the last block in a stack is the stack residual [Right] Overall Architecture: On the right, we see the top-level view of the architecture. Each stack is chained together so that for any stack, the stack residual output of the previous stack is the input and the stack generates two outputs: the stack forecast and the stack residual. Finally, the N-BEATS forecast is the additive sum of all the stack forecasts. Basis Layer A basis layer puts a constraint on the functional space and thereby limits the output representation constrained by the chosen basis function. We can have the basis of any arbitrary function, which gives us a lot of flexibility. In the paper, N-BEATS operates in two modes: generic and interpretable. The generic mode doesn’t have any basis function constraining the function search (we can think of it as having an identity function). So, in this mode, we are leaving the function completely learned by the model through a linear projection of the basis coefficients. Fixing the basis function allows for human interpretability about what each stack signifies. The authors propose two specific basis functions that capture trend (polynomial function) and seasonality (Fourier basis). So we can say that the forecast output of the stack represents trend or seasonality based on the corresponding chosen basis function. PyTorch Code class GenericBasis(nn.Module): def __init__(self, backcast_size, forecast_size): super().__init__() self.backcast_size, self.forecast_size = backcast_size, forecast_size def forward(self, theta)","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:3:5","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#pytorch-code-4"},{"categories":["Time Series"],"content":"\rExperiment ResultsI used UCI’s ElectricityLoadDiagrams20112014 dataset17 to run a quick experiment with minimal PyTorch implementations of these models. The dataset contains 370 time series sampled at 15 mins with a total of 140K observations for each series. The data were resampled to 1 hour. Three covariates (weekday, hour, month) and one time series id was used wherever allowed by the model architecture. Refer to the notebooks linked in respective sections to see the code for each experiment. The following chart shows RMSE values for the test set for each of the models. Results are sorted from left to right by the best to the worst. ","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:4:0","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#experiment-results"},{"categories":["Time Series"],"content":"\rSummaryIn this article, we defined the need for using deep learning for modern time series forecasting and then looked at some of the most popular deep learning algorithms designed for time series forecasting with different inductive biases in their model architecture. We implemented all of the algorithms in Python and compared their results on a toy dataset. ","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:5:0","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#summary"},{"categories":["Time Series"],"content":"\rReferences “Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting”, https://blog.reachsumit.com/posts/2022/12/stats-vs-ml-for-ts/ ↩︎ Flunkert, Valentin \u0026 Salinas, David \u0026 Gasthaus, Jan. (2017). DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks. International Journal of Forecasting. 36. 10.1016/j.ijforecast.2019.07.001. ↩︎ Jan Gasthaus, Konstantinos Benidis, Yuyang Wang, Syama Sundar Rangapuram, David Salinas, Valentin Flunkert, and Tim Januschowski. Probabilistic forecasting with spline quantile function RNNs. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1901–1910, 2019. ↩︎ oord, Aaron \u0026 Dieleman, Sander \u0026 Zen, Heiga \u0026 Simonyan, Karen \u0026 Vinyals, Oriol \u0026 Graves, Alex \u0026 Kalchbrenner, Nal \u0026 Senior, Andrew \u0026 Kavukcuoglu, Koray. (2016). WaveNet: A Generative Model for Raw Audio. ↩︎ Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting. In International Conference on Machine Learning, pages 8857–8868. PMLR, 2021. ↩︎ Wen, Ruofeng \u0026 Torkkola, Kari \u0026 Narayanaswamy, Balakrishnan. (2017). A Multi-Horizon Quantile Recurrent Forecaster. ↩︎ Ben Taieb, Souhaib \u0026 Atiya, Amir. (2015). A Bias and Variance Analysis for Multistep-Ahead Time Series Forecasting. IEEE transactions on neural networks and learning systems. 27. 10.1109/TNNLS.2015.2411629. ↩︎ Zhu, Lingxue \u0026 Laptev, Nikolay. (2017). Deep and Confident Prediction for Time Series at Uber. ↩︎ Gal, Yarin \u0026 Ghahramani, Zoubin. (2015). Dropout as a Bayesian Approximation: Appendix. ↩︎ Chen, Yitian \u0026 Kang, Yanfei \u0026 Chen, Yixiong \u0026 Wang, Zizhuo. (2019). Probabilistic Forecasting with Temporal Convolutional Neural Network. ↩︎ Pascanu, Razvan \u0026 Mikolov, Tomas \u0026 Bengio, Y.. (2012). On the difficulty of training Recurrent Neural Networks. 30th International Conference on Machine Learning, ICML 2013. ↩︎ Werbos, Paul. (1990). Backpropagation through time: what it does and how to do it. Proceedings of the IEEE. 78. 1550 - 1560. 10.1109/5.58337. ↩︎ Bai, Shaojie \u0026 Kolter, J. \u0026 Koltun, Vladlen. (2018). An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling. ↩︎ ↩︎ Oreshkin, Boris \u0026 Carpo, Dmitri \u0026 Chapados, Nicolas \u0026 Bengio, Yoshua. (2019). N-BEATS: Neural basis expansion analysis for interpretable time series forecasting. ↩︎ Gutierrez, Kin \u0026 Challu, Cristian \u0026 Marcjasz, Grzegorz \u0026 Weron, Rafał \u0026 Dubrawski, Artur. (2022). Neural basis expansion analysis with exogenous variables: Forecasting electricity prices with NBEATSx. International Journal of Forecasting. 10.1016/j.ijforecast.2022.03.001. ↩︎ Challu, Cristian \u0026 Gutierrez, Kin \u0026 Oreshkin, Boris \u0026 Garza, Federico \u0026 Mergenthaler, Max \u0026 Dubrawski, Artur. (2022). N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting. ↩︎ https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014 ↩︎ ","date":"2023-01-21","objectID":"/posts/2023/01/dl-for-forecasting/:6:0","series":null,"tags":["forecasting","literature review"],"title":"Specialized Deep Learning Architectures for Time Series Forecasting","uri":"/posts/2023/01/dl-for-forecasting/#references"},{"categories":["Time Series"],"content":"Over recent decades, Machine Learning (ML) and, its subdomain, Deep Learning (DL) based algorithms have achieved remarkable success in various areas, such as image processing, natural language understanding, and speech recognition. However, ML algorithms haven’t quite had the same widely known and unquestionable superiority when it comes to forecasting applications in the time series domain. While ML algorithms have to overcome the challenges in modeling the non-i.i.d. nature of data inherent in the time series domain, statistical models excel in this setting and provide explicit means to model time series structural elements, such as trend and seasonality. In this article, we will do a deep dive into literature and recent time series competitions to do a multifaceted comparison between Statistical, Machine Learning, and Deep Learning methods for time series forecasting. Note: This is a long-form article. If you need a TL;DR, feel free to skip to the last section named Takeaways. ","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:0:0","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#"},{"categories":["Time Series"],"content":"\rWhat’s the difference between Statistical and Machine Learning forecasting models?Before continuing, we should define what “statistical” and “machine learning” models are in the context of forecasting. This distinction is not always trivial because a majority of machine learning algorithms are maximum likelihood estimators meaning that they are statistical in nature. Similarly, traditional statistical models, such as autoregressive models, can be specified as linear regression on lags of the series, and linear regression is mostly considered a machine learning algorithm. So to distinguish between the two, we will use the following definition proposed by various researchers 12. A statistical model prescribes the data-generating process, for example, an autoregressive (AR) model only looks at the relationship between lags of a series and its future values. Whereas a machine learning model learns this relationship thus being more generic, for example, a neural network creates its own features using non-linear combinations of its inputs. ML methods, in contrast to statistical ones, are also more data-hungry. While the completely non-linear and assumption-free estimation of ML forecasts (e.g., in terms of trend and seasonality) let the data speak for themselves, it also demands a large volume of data for effectively capturing the dynamics and interconnections of the data. Moreover, many statistical methods have developed excellent heuristics that allow even novices to create reasonable models with a few function calls, determining a good approach for ML modeling is more experimental3. ","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:1:0","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#whats-the-difference-between-statistical-and-machine-learning-forecasting-models"},{"categories":["Time Series"],"content":"\rComparisons based on the forecasting literature","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:2:0","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#comparisons-based-on-the-forecasting-literature"},{"categories":["Time Series"],"content":"\r“Statistical and Machine Learning forecasting methods: Concerns and ways forward” by Makridakis et al.One of the earliest and most popular empirical studies that compared Statistical and ML methods was done by Makridakis et al 4. In their 2018 paper, they used a set of 1045 monthly (univariate) time series sampled from the dataset used in the M3 competition to evaluate performances of 8 traditional statistical (Seasonal Random Walk, SES, Holt \u0026 Damped Exponential Smoothing, Comb: average of SES, Holt, and Damped, Theta, ARIMA, and ETS) and 10 ML methods (MLP, BNN, RBF, GRNN, KNN, CART, SVR, GP, RNN, and LSTM) on multi-step ahead forecasting task. To generate multi-horizon forecasts from ML models, they considered three approaches: iterative, multi-node output, multi-network forecasting, and for preprocessing the input, they considered detrending, deseasonalizing, box-cox power transformation, a combination of the three, as well as directly using the original data. They also considered using mean, median, or mode over the outputs of multiple models trained with different hyperparameter initializations and reported the results from the most appropriate alternatives. The following figure shows the overall sMAPE (symmetric Mean Absolute Percentage Error) values averaged over 18 one-step-ahead forecasts for all the models they considered. Similar conclusions were drawn for the case of multi-step-ahead forecasts. SummaryAs seen above, the six most accurate methods were statistical. Even the Naïve 2 (seasonal random walk) baseline was shown to be more accurate than half of the ML methods. Through additional experiments, they also showed that the ML models, specially LSTMs, overfitted the data during training but had a worse performance during the testing phase. Additional Notes ML methods are computationally more demanding, and methods like deseasonalizing the data, utilizing simpler models, and using principled approaches to turning hyperparameters can help in reducing this cost. Using a sliding window approach gives as much information as possible about future values and also reduces the uncertainty in forecasting. ","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:2:1","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#statistical-and-machine-learning-forecasting-methods-concerns-and-ways-forward-by-makridakis-et-al"},{"categories":["Time Series"],"content":"\r“Statistical and Machine Learning forecasting methods: Concerns and ways forward” by Makridakis et al.One of the earliest and most popular empirical studies that compared Statistical and ML methods was done by Makridakis et al 4. In their 2018 paper, they used a set of 1045 monthly (univariate) time series sampled from the dataset used in the M3 competition to evaluate performances of 8 traditional statistical (Seasonal Random Walk, SES, Holt \u0026 Damped Exponential Smoothing, Comb: average of SES, Holt, and Damped, Theta, ARIMA, and ETS) and 10 ML methods (MLP, BNN, RBF, GRNN, KNN, CART, SVR, GP, RNN, and LSTM) on multi-step ahead forecasting task. To generate multi-horizon forecasts from ML models, they considered three approaches: iterative, multi-node output, multi-network forecasting, and for preprocessing the input, they considered detrending, deseasonalizing, box-cox power transformation, a combination of the three, as well as directly using the original data. They also considered using mean, median, or mode over the outputs of multiple models trained with different hyperparameter initializations and reported the results from the most appropriate alternatives. The following figure shows the overall sMAPE (symmetric Mean Absolute Percentage Error) values averaged over 18 one-step-ahead forecasts for all the models they considered. Similar conclusions were drawn for the case of multi-step-ahead forecasts. SummaryAs seen above, the six most accurate methods were statistical. Even the Naïve 2 (seasonal random walk) baseline was shown to be more accurate than half of the ML methods. Through additional experiments, they also showed that the ML models, specially LSTMs, overfitted the data during training but had a worse performance during the testing phase. Additional Notes ML methods are computationally more demanding, and methods like deseasonalizing the data, utilizing simpler models, and using principled approaches to turning hyperparameters can help in reducing this cost. Using a sliding window approach gives as much information as possible about future values and also reduces the uncertainty in forecasting. ","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:2:1","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#summary"},{"categories":["Time Series"],"content":"\r“Statistical and Machine Learning forecasting methods: Concerns and ways forward” by Makridakis et al.One of the earliest and most popular empirical studies that compared Statistical and ML methods was done by Makridakis et al 4. In their 2018 paper, they used a set of 1045 monthly (univariate) time series sampled from the dataset used in the M3 competition to evaluate performances of 8 traditional statistical (Seasonal Random Walk, SES, Holt \u0026 Damped Exponential Smoothing, Comb: average of SES, Holt, and Damped, Theta, ARIMA, and ETS) and 10 ML methods (MLP, BNN, RBF, GRNN, KNN, CART, SVR, GP, RNN, and LSTM) on multi-step ahead forecasting task. To generate multi-horizon forecasts from ML models, they considered three approaches: iterative, multi-node output, multi-network forecasting, and for preprocessing the input, they considered detrending, deseasonalizing, box-cox power transformation, a combination of the three, as well as directly using the original data. They also considered using mean, median, or mode over the outputs of multiple models trained with different hyperparameter initializations and reported the results from the most appropriate alternatives. The following figure shows the overall sMAPE (symmetric Mean Absolute Percentage Error) values averaged over 18 one-step-ahead forecasts for all the models they considered. Similar conclusions were drawn for the case of multi-step-ahead forecasts. SummaryAs seen above, the six most accurate methods were statistical. Even the Naïve 2 (seasonal random walk) baseline was shown to be more accurate than half of the ML methods. Through additional experiments, they also showed that the ML models, specially LSTMs, overfitted the data during training but had a worse performance during the testing phase. Additional Notes ML methods are computationally more demanding, and methods like deseasonalizing the data, utilizing simpler models, and using principled approaches to turning hyperparameters can help in reducing this cost. Using a sliding window approach gives as much information as possible about future values and also reduces the uncertainty in forecasting. ","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:2:1","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#additional-notes"},{"categories":["Time Series"],"content":"\r“Statistical, machine learning and deep learning forecasting methods: Comparisons and ways forward” by Makridakis et al.In 2022, the authors of the above paper followed up with another study and added more modern deep learning networks in this comparison 5. For statistical models, they used the top-2 performing algorithms from the earlier study: ETS and ARIMA, and an ensemble of the two: Ensemble-S. Similarly, they used top-2 performing ML algorithms: MLP and BNN. Next, they used four modern deep learning algorithms: DeepAR, Feed-forward, Transformer, WaveNet, and an ensemble of the four: Ensemble-DL. The following table shows the sMAPE and MASE accuracy metrics for all of these methods tested on the same 1045 M3 series data using the respective optimal hyperparameters found during experimentation. The computational time (CT), i.e., the time (measured in minutes) required by the methods for training and predicting, as well as the relative computational complexity (RCC) of the methods, i.e., the number of floating point operations required by the methods for inference when compared to the Naive 2, is also reported. Summary of results at different forecasting horizonsTheir experiment shows that ETS and ARIMA are still more accurate on average than the ML methods and many of the individual DL methods across all forecasting horizons (short: 1-6, medium: 7-12, long: 13-18). Both ensembles are more accurate than any of the individual models. Ensemble-DL performed 2-5% better than Ensemble-S overall, but Ensemble-S performed best on MASE on short-term forecasts. ML methods also performed better than DL methods and their ensemble on short-term forecast periods. Long-term forecasts tend to be less accurate than short-term ones for all methods. Through a separate set of methods, the authors show that when models are optimized based on their one-step-ahead forecasting accuracy, their forecasts will be relatively more accurate for short-term forecasts compared to models that are optimized based on their multi-step-ahead forecasting accuracy, and vice versa. Typically, statistical forecasting methods are optimized in terms of parameters so that the one-step-ahead forecast error is minimized, for example, ETS and ARIMA are parameterized with the objective to minimize the in-sample mean squared error of their forecasts. Similarly, MLP and BNN are parameterized with the objective to produce accurate multi-step-ahead forecasts in a recursive fashion. Extending to all 3,003 time series of the M3 datasetFurther expanding their work, the authors considered all of the 3,003 time series of the M3 dataset for the next experiment. The following table shows the resulting sMAPE accuracy both per data frequency and total. The table also shows the reported results from a variety of forecasting methods originally submitted in the M3 competition. Ensemble-DL continues to be the most accurate forecasting approach overall. DL ensemble was outperformed by a small margin by the LGT model (discussed later) on yearly data, and DeepAR performed better on the data labeled as “Other” (no frequency information available). The Ensemble-DL improvement over Ensemble-S decreases as the data frequency is increased. Ensemble-DL performance is very similar to the N-BEATS method with N-BEATS performing better than Ensemble-DL for the “Other” series. Balancing the comparison among ensemblesNote that in the above discussion, Ensemble-S only used 2 models while Ensemble-DL had 4. To make a fair comparison, the authors also evaluated the DL models in pairs and found that all of the pairwise DL models performed better than Ensemble-S from 0.4-6.1%. Interestingly, ensembles that involve DeepAR, i.e., the most accurate individual DL model, consistently outperform the rest, while ensembles that involve WaveNet, i.e., the least accurate individual DL model, sometimes deteriorate forecasting performance. Also, the accuracy of Ensemble-DL is comparable to that of the best pairwise en","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:2:2","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#statistical-machine-learning-and-deep-learning-forecasting-methods-comparisons-and-ways-forward-by-makridakis-et-al"},{"categories":["Time Series"],"content":"\r“Statistical, machine learning and deep learning forecasting methods: Comparisons and ways forward” by Makridakis et al.In 2022, the authors of the above paper followed up with another study and added more modern deep learning networks in this comparison 5. For statistical models, they used the top-2 performing algorithms from the earlier study: ETS and ARIMA, and an ensemble of the two: Ensemble-S. Similarly, they used top-2 performing ML algorithms: MLP and BNN. Next, they used four modern deep learning algorithms: DeepAR, Feed-forward, Transformer, WaveNet, and an ensemble of the four: Ensemble-DL. The following table shows the sMAPE and MASE accuracy metrics for all of these methods tested on the same 1045 M3 series data using the respective optimal hyperparameters found during experimentation. The computational time (CT), i.e., the time (measured in minutes) required by the methods for training and predicting, as well as the relative computational complexity (RCC) of the methods, i.e., the number of floating point operations required by the methods for inference when compared to the Naive 2, is also reported. Summary of results at different forecasting horizonsTheir experiment shows that ETS and ARIMA are still more accurate on average than the ML methods and many of the individual DL methods across all forecasting horizons (short: 1-6, medium: 7-12, long: 13-18). Both ensembles are more accurate than any of the individual models. Ensemble-DL performed 2-5% better than Ensemble-S overall, but Ensemble-S performed best on MASE on short-term forecasts. ML methods also performed better than DL methods and their ensemble on short-term forecast periods. Long-term forecasts tend to be less accurate than short-term ones for all methods. Through a separate set of methods, the authors show that when models are optimized based on their one-step-ahead forecasting accuracy, their forecasts will be relatively more accurate for short-term forecasts compared to models that are optimized based on their multi-step-ahead forecasting accuracy, and vice versa. Typically, statistical forecasting methods are optimized in terms of parameters so that the one-step-ahead forecast error is minimized, for example, ETS and ARIMA are parameterized with the objective to minimize the in-sample mean squared error of their forecasts. Similarly, MLP and BNN are parameterized with the objective to produce accurate multi-step-ahead forecasts in a recursive fashion. Extending to all 3,003 time series of the M3 datasetFurther expanding their work, the authors considered all of the 3,003 time series of the M3 dataset for the next experiment. The following table shows the resulting sMAPE accuracy both per data frequency and total. The table also shows the reported results from a variety of forecasting methods originally submitted in the M3 competition. Ensemble-DL continues to be the most accurate forecasting approach overall. DL ensemble was outperformed by a small margin by the LGT model (discussed later) on yearly data, and DeepAR performed better on the data labeled as “Other” (no frequency information available). The Ensemble-DL improvement over Ensemble-S decreases as the data frequency is increased. Ensemble-DL performance is very similar to the N-BEATS method with N-BEATS performing better than Ensemble-DL for the “Other” series. Balancing the comparison among ensemblesNote that in the above discussion, Ensemble-S only used 2 models while Ensemble-DL had 4. To make a fair comparison, the authors also evaluated the DL models in pairs and found that all of the pairwise DL models performed better than Ensemble-S from 0.4-6.1%. Interestingly, ensembles that involve DeepAR, i.e., the most accurate individual DL model, consistently outperform the rest, while ensembles that involve WaveNet, i.e., the least accurate individual DL model, sometimes deteriorate forecasting performance. Also, the accuracy of Ensemble-DL is comparable to that of the best pairwise en","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:2:2","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#summary-of-results-at-different-forecasting-horizons"},{"categories":["Time Series"],"content":"\r“Statistical, machine learning and deep learning forecasting methods: Comparisons and ways forward” by Makridakis et al.In 2022, the authors of the above paper followed up with another study and added more modern deep learning networks in this comparison 5. For statistical models, they used the top-2 performing algorithms from the earlier study: ETS and ARIMA, and an ensemble of the two: Ensemble-S. Similarly, they used top-2 performing ML algorithms: MLP and BNN. Next, they used four modern deep learning algorithms: DeepAR, Feed-forward, Transformer, WaveNet, and an ensemble of the four: Ensemble-DL. The following table shows the sMAPE and MASE accuracy metrics for all of these methods tested on the same 1045 M3 series data using the respective optimal hyperparameters found during experimentation. The computational time (CT), i.e., the time (measured in minutes) required by the methods for training and predicting, as well as the relative computational complexity (RCC) of the methods, i.e., the number of floating point operations required by the methods for inference when compared to the Naive 2, is also reported. Summary of results at different forecasting horizonsTheir experiment shows that ETS and ARIMA are still more accurate on average than the ML methods and many of the individual DL methods across all forecasting horizons (short: 1-6, medium: 7-12, long: 13-18). Both ensembles are more accurate than any of the individual models. Ensemble-DL performed 2-5% better than Ensemble-S overall, but Ensemble-S performed best on MASE on short-term forecasts. ML methods also performed better than DL methods and their ensemble on short-term forecast periods. Long-term forecasts tend to be less accurate than short-term ones for all methods. Through a separate set of methods, the authors show that when models are optimized based on their one-step-ahead forecasting accuracy, their forecasts will be relatively more accurate for short-term forecasts compared to models that are optimized based on their multi-step-ahead forecasting accuracy, and vice versa. Typically, statistical forecasting methods are optimized in terms of parameters so that the one-step-ahead forecast error is minimized, for example, ETS and ARIMA are parameterized with the objective to minimize the in-sample mean squared error of their forecasts. Similarly, MLP and BNN are parameterized with the objective to produce accurate multi-step-ahead forecasts in a recursive fashion. Extending to all 3,003 time series of the M3 datasetFurther expanding their work, the authors considered all of the 3,003 time series of the M3 dataset for the next experiment. The following table shows the resulting sMAPE accuracy both per data frequency and total. The table also shows the reported results from a variety of forecasting methods originally submitted in the M3 competition. Ensemble-DL continues to be the most accurate forecasting approach overall. DL ensemble was outperformed by a small margin by the LGT model (discussed later) on yearly data, and DeepAR performed better on the data labeled as “Other” (no frequency information available). The Ensemble-DL improvement over Ensemble-S decreases as the data frequency is increased. Ensemble-DL performance is very similar to the N-BEATS method with N-BEATS performing better than Ensemble-DL for the “Other” series. Balancing the comparison among ensemblesNote that in the above discussion, Ensemble-S only used 2 models while Ensemble-DL had 4. To make a fair comparison, the authors also evaluated the DL models in pairs and found that all of the pairwise DL models performed better than Ensemble-S from 0.4-6.1%. Interestingly, ensembles that involve DeepAR, i.e., the most accurate individual DL model, consistently outperform the rest, while ensembles that involve WaveNet, i.e., the least accurate individual DL model, sometimes deteriorate forecasting performance. Also, the accuracy of Ensemble-DL is comparable to that of the best pairwise en","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:2:2","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#extending-to-all-3003-time-series-of-the-m3-dataset"},{"categories":["Time Series"],"content":"\r“Statistical, machine learning and deep learning forecasting methods: Comparisons and ways forward” by Makridakis et al.In 2022, the authors of the above paper followed up with another study and added more modern deep learning networks in this comparison 5. For statistical models, they used the top-2 performing algorithms from the earlier study: ETS and ARIMA, and an ensemble of the two: Ensemble-S. Similarly, they used top-2 performing ML algorithms: MLP and BNN. Next, they used four modern deep learning algorithms: DeepAR, Feed-forward, Transformer, WaveNet, and an ensemble of the four: Ensemble-DL. The following table shows the sMAPE and MASE accuracy metrics for all of these methods tested on the same 1045 M3 series data using the respective optimal hyperparameters found during experimentation. The computational time (CT), i.e., the time (measured in minutes) required by the methods for training and predicting, as well as the relative computational complexity (RCC) of the methods, i.e., the number of floating point operations required by the methods for inference when compared to the Naive 2, is also reported. Summary of results at different forecasting horizonsTheir experiment shows that ETS and ARIMA are still more accurate on average than the ML methods and many of the individual DL methods across all forecasting horizons (short: 1-6, medium: 7-12, long: 13-18). Both ensembles are more accurate than any of the individual models. Ensemble-DL performed 2-5% better than Ensemble-S overall, but Ensemble-S performed best on MASE on short-term forecasts. ML methods also performed better than DL methods and their ensemble on short-term forecast periods. Long-term forecasts tend to be less accurate than short-term ones for all methods. Through a separate set of methods, the authors show that when models are optimized based on their one-step-ahead forecasting accuracy, their forecasts will be relatively more accurate for short-term forecasts compared to models that are optimized based on their multi-step-ahead forecasting accuracy, and vice versa. Typically, statistical forecasting methods are optimized in terms of parameters so that the one-step-ahead forecast error is minimized, for example, ETS and ARIMA are parameterized with the objective to minimize the in-sample mean squared error of their forecasts. Similarly, MLP and BNN are parameterized with the objective to produce accurate multi-step-ahead forecasts in a recursive fashion. Extending to all 3,003 time series of the M3 datasetFurther expanding their work, the authors considered all of the 3,003 time series of the M3 dataset for the next experiment. The following table shows the resulting sMAPE accuracy both per data frequency and total. The table also shows the reported results from a variety of forecasting methods originally submitted in the M3 competition. Ensemble-DL continues to be the most accurate forecasting approach overall. DL ensemble was outperformed by a small margin by the LGT model (discussed later) on yearly data, and DeepAR performed better on the data labeled as “Other” (no frequency information available). The Ensemble-DL improvement over Ensemble-S decreases as the data frequency is increased. Ensemble-DL performance is very similar to the N-BEATS method with N-BEATS performing better than Ensemble-DL for the “Other” series. Balancing the comparison among ensemblesNote that in the above discussion, Ensemble-S only used 2 models while Ensemble-DL had 4. To make a fair comparison, the authors also evaluated the DL models in pairs and found that all of the pairwise DL models performed better than Ensemble-S from 0.4-6.1%. Interestingly, ensembles that involve DeepAR, i.e., the most accurate individual DL model, consistently outperform the rest, while ensembles that involve WaveNet, i.e., the least accurate individual DL model, sometimes deteriorate forecasting performance. Also, the accuracy of Ensemble-DL is comparable to that of the best pairwise en","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:2:2","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#balancing-the-comparison-among-ensembles"},{"categories":["Time Series"],"content":"\r“Statistical, machine learning and deep learning forecasting methods: Comparisons and ways forward” by Makridakis et al.In 2022, the authors of the above paper followed up with another study and added more modern deep learning networks in this comparison 5. For statistical models, they used the top-2 performing algorithms from the earlier study: ETS and ARIMA, and an ensemble of the two: Ensemble-S. Similarly, they used top-2 performing ML algorithms: MLP and BNN. Next, they used four modern deep learning algorithms: DeepAR, Feed-forward, Transformer, WaveNet, and an ensemble of the four: Ensemble-DL. The following table shows the sMAPE and MASE accuracy metrics for all of these methods tested on the same 1045 M3 series data using the respective optimal hyperparameters found during experimentation. The computational time (CT), i.e., the time (measured in minutes) required by the methods for training and predicting, as well as the relative computational complexity (RCC) of the methods, i.e., the number of floating point operations required by the methods for inference when compared to the Naive 2, is also reported. Summary of results at different forecasting horizonsTheir experiment shows that ETS and ARIMA are still more accurate on average than the ML methods and many of the individual DL methods across all forecasting horizons (short: 1-6, medium: 7-12, long: 13-18). Both ensembles are more accurate than any of the individual models. Ensemble-DL performed 2-5% better than Ensemble-S overall, but Ensemble-S performed best on MASE on short-term forecasts. ML methods also performed better than DL methods and their ensemble on short-term forecast periods. Long-term forecasts tend to be less accurate than short-term ones for all methods. Through a separate set of methods, the authors show that when models are optimized based on their one-step-ahead forecasting accuracy, their forecasts will be relatively more accurate for short-term forecasts compared to models that are optimized based on their multi-step-ahead forecasting accuracy, and vice versa. Typically, statistical forecasting methods are optimized in terms of parameters so that the one-step-ahead forecast error is minimized, for example, ETS and ARIMA are parameterized with the objective to minimize the in-sample mean squared error of their forecasts. Similarly, MLP and BNN are parameterized with the objective to produce accurate multi-step-ahead forecasts in a recursive fashion. Extending to all 3,003 time series of the M3 datasetFurther expanding their work, the authors considered all of the 3,003 time series of the M3 dataset for the next experiment. The following table shows the resulting sMAPE accuracy both per data frequency and total. The table also shows the reported results from a variety of forecasting methods originally submitted in the M3 competition. Ensemble-DL continues to be the most accurate forecasting approach overall. DL ensemble was outperformed by a small margin by the LGT model (discussed later) on yearly data, and DeepAR performed better on the data labeled as “Other” (no frequency information available). The Ensemble-DL improvement over Ensemble-S decreases as the data frequency is increased. Ensemble-DL performance is very similar to the N-BEATS method with N-BEATS performing better than Ensemble-DL for the “Other” series. Balancing the comparison among ensemblesNote that in the above discussion, Ensemble-S only used 2 models while Ensemble-DL had 4. To make a fair comparison, the authors also evaluated the DL models in pairs and found that all of the pairwise DL models performed better than Ensemble-S from 0.4-6.1%. Interestingly, ensembles that involve DeepAR, i.e., the most accurate individual DL model, consistently outperform the rest, while ensembles that involve WaveNet, i.e., the least accurate individual DL model, sometimes deteriorate forecasting performance. Also, the accuracy of Ensemble-DL is comparable to that of the best pairwise en","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:2:2","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#results-at-different-forecasting-horizons-using-all-of-the-m3-data"},{"categories":["Time Series"],"content":"\r“Statistical, machine learning and deep learning forecasting methods: Comparisons and ways forward” by Makridakis et al.In 2022, the authors of the above paper followed up with another study and added more modern deep learning networks in this comparison 5. For statistical models, they used the top-2 performing algorithms from the earlier study: ETS and ARIMA, and an ensemble of the two: Ensemble-S. Similarly, they used top-2 performing ML algorithms: MLP and BNN. Next, they used four modern deep learning algorithms: DeepAR, Feed-forward, Transformer, WaveNet, and an ensemble of the four: Ensemble-DL. The following table shows the sMAPE and MASE accuracy metrics for all of these methods tested on the same 1045 M3 series data using the respective optimal hyperparameters found during experimentation. The computational time (CT), i.e., the time (measured in minutes) required by the methods for training and predicting, as well as the relative computational complexity (RCC) of the methods, i.e., the number of floating point operations required by the methods for inference when compared to the Naive 2, is also reported. Summary of results at different forecasting horizonsTheir experiment shows that ETS and ARIMA are still more accurate on average than the ML methods and many of the individual DL methods across all forecasting horizons (short: 1-6, medium: 7-12, long: 13-18). Both ensembles are more accurate than any of the individual models. Ensemble-DL performed 2-5% better than Ensemble-S overall, but Ensemble-S performed best on MASE on short-term forecasts. ML methods also performed better than DL methods and their ensemble on short-term forecast periods. Long-term forecasts tend to be less accurate than short-term ones for all methods. Through a separate set of methods, the authors show that when models are optimized based on their one-step-ahead forecasting accuracy, their forecasts will be relatively more accurate for short-term forecasts compared to models that are optimized based on their multi-step-ahead forecasting accuracy, and vice versa. Typically, statistical forecasting methods are optimized in terms of parameters so that the one-step-ahead forecast error is minimized, for example, ETS and ARIMA are parameterized with the objective to minimize the in-sample mean squared error of their forecasts. Similarly, MLP and BNN are parameterized with the objective to produce accurate multi-step-ahead forecasts in a recursive fashion. Extending to all 3,003 time series of the M3 datasetFurther expanding their work, the authors considered all of the 3,003 time series of the M3 dataset for the next experiment. The following table shows the resulting sMAPE accuracy both per data frequency and total. The table also shows the reported results from a variety of forecasting methods originally submitted in the M3 competition. Ensemble-DL continues to be the most accurate forecasting approach overall. DL ensemble was outperformed by a small margin by the LGT model (discussed later) on yearly data, and DeepAR performed better on the data labeled as “Other” (no frequency information available). The Ensemble-DL improvement over Ensemble-S decreases as the data frequency is increased. Ensemble-DL performance is very similar to the N-BEATS method with N-BEATS performing better than Ensemble-DL for the “Other” series. Balancing the comparison among ensemblesNote that in the above discussion, Ensemble-S only used 2 models while Ensemble-DL had 4. To make a fair comparison, the authors also evaluated the DL models in pairs and found that all of the pairwise DL models performed better than Ensemble-S from 0.4-6.1%. Interestingly, ensembles that involve DeepAR, i.e., the most accurate individual DL model, consistently outperform the rest, while ensembles that involve WaveNet, i.e., the least accurate individual DL model, sometimes deteriorate forecasting performance. Also, the accuracy of Ensemble-DL is comparable to that of the best pairwise en","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:2:2","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#comparison-using-the-mcb-method"},{"categories":["Time Series"],"content":"\r“Statistical, machine learning and deep learning forecasting methods: Comparisons and ways forward” by Makridakis et al.In 2022, the authors of the above paper followed up with another study and added more modern deep learning networks in this comparison 5. For statistical models, they used the top-2 performing algorithms from the earlier study: ETS and ARIMA, and an ensemble of the two: Ensemble-S. Similarly, they used top-2 performing ML algorithms: MLP and BNN. Next, they used four modern deep learning algorithms: DeepAR, Feed-forward, Transformer, WaveNet, and an ensemble of the four: Ensemble-DL. The following table shows the sMAPE and MASE accuracy metrics for all of these methods tested on the same 1045 M3 series data using the respective optimal hyperparameters found during experimentation. The computational time (CT), i.e., the time (measured in minutes) required by the methods for training and predicting, as well as the relative computational complexity (RCC) of the methods, i.e., the number of floating point operations required by the methods for inference when compared to the Naive 2, is also reported. Summary of results at different forecasting horizonsTheir experiment shows that ETS and ARIMA are still more accurate on average than the ML methods and many of the individual DL methods across all forecasting horizons (short: 1-6, medium: 7-12, long: 13-18). Both ensembles are more accurate than any of the individual models. Ensemble-DL performed 2-5% better than Ensemble-S overall, but Ensemble-S performed best on MASE on short-term forecasts. ML methods also performed better than DL methods and their ensemble on short-term forecast periods. Long-term forecasts tend to be less accurate than short-term ones for all methods. Through a separate set of methods, the authors show that when models are optimized based on their one-step-ahead forecasting accuracy, their forecasts will be relatively more accurate for short-term forecasts compared to models that are optimized based on their multi-step-ahead forecasting accuracy, and vice versa. Typically, statistical forecasting methods are optimized in terms of parameters so that the one-step-ahead forecast error is minimized, for example, ETS and ARIMA are parameterized with the objective to minimize the in-sample mean squared error of their forecasts. Similarly, MLP and BNN are parameterized with the objective to produce accurate multi-step-ahead forecasts in a recursive fashion. Extending to all 3,003 time series of the M3 datasetFurther expanding their work, the authors considered all of the 3,003 time series of the M3 dataset for the next experiment. The following table shows the resulting sMAPE accuracy both per data frequency and total. The table also shows the reported results from a variety of forecasting methods originally submitted in the M3 competition. Ensemble-DL continues to be the most accurate forecasting approach overall. DL ensemble was outperformed by a small margin by the LGT model (discussed later) on yearly data, and DeepAR performed better on the data labeled as “Other” (no frequency information available). The Ensemble-DL improvement over Ensemble-S decreases as the data frequency is increased. Ensemble-DL performance is very similar to the N-BEATS method with N-BEATS performing better than Ensemble-DL for the “Other” series. Balancing the comparison among ensemblesNote that in the above discussion, Ensemble-S only used 2 models while Ensemble-DL had 4. To make a fair comparison, the authors also evaluated the DL models in pairs and found that all of the pairwise DL models performed better than Ensemble-S from 0.4-6.1%. Interestingly, ensembles that involve DeepAR, i.e., the most accurate individual DL model, consistently outperform the rest, while ensembles that involve WaveNet, i.e., the least accurate individual DL model, sometimes deteriorate forecasting performance. Also, the accuracy of Ensemble-DL is comparable to that of the best pairwise en","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:2:2","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#summary-1"},{"categories":["Time Series"],"content":"\r“Statistical, machine learning and deep learning forecasting methods: Comparisons and ways forward” by Makridakis et al.In 2022, the authors of the above paper followed up with another study and added more modern deep learning networks in this comparison 5. For statistical models, they used the top-2 performing algorithms from the earlier study: ETS and ARIMA, and an ensemble of the two: Ensemble-S. Similarly, they used top-2 performing ML algorithms: MLP and BNN. Next, they used four modern deep learning algorithms: DeepAR, Feed-forward, Transformer, WaveNet, and an ensemble of the four: Ensemble-DL. The following table shows the sMAPE and MASE accuracy metrics for all of these methods tested on the same 1045 M3 series data using the respective optimal hyperparameters found during experimentation. The computational time (CT), i.e., the time (measured in minutes) required by the methods for training and predicting, as well as the relative computational complexity (RCC) of the methods, i.e., the number of floating point operations required by the methods for inference when compared to the Naive 2, is also reported. Summary of results at different forecasting horizonsTheir experiment shows that ETS and ARIMA are still more accurate on average than the ML methods and many of the individual DL methods across all forecasting horizons (short: 1-6, medium: 7-12, long: 13-18). Both ensembles are more accurate than any of the individual models. Ensemble-DL performed 2-5% better than Ensemble-S overall, but Ensemble-S performed best on MASE on short-term forecasts. ML methods also performed better than DL methods and their ensemble on short-term forecast periods. Long-term forecasts tend to be less accurate than short-term ones for all methods. Through a separate set of methods, the authors show that when models are optimized based on their one-step-ahead forecasting accuracy, their forecasts will be relatively more accurate for short-term forecasts compared to models that are optimized based on their multi-step-ahead forecasting accuracy, and vice versa. Typically, statistical forecasting methods are optimized in terms of parameters so that the one-step-ahead forecast error is minimized, for example, ETS and ARIMA are parameterized with the objective to minimize the in-sample mean squared error of their forecasts. Similarly, MLP and BNN are parameterized with the objective to produce accurate multi-step-ahead forecasts in a recursive fashion. Extending to all 3,003 time series of the M3 datasetFurther expanding their work, the authors considered all of the 3,003 time series of the M3 dataset for the next experiment. The following table shows the resulting sMAPE accuracy both per data frequency and total. The table also shows the reported results from a variety of forecasting methods originally submitted in the M3 competition. Ensemble-DL continues to be the most accurate forecasting approach overall. DL ensemble was outperformed by a small margin by the LGT model (discussed later) on yearly data, and DeepAR performed better on the data labeled as “Other” (no frequency information available). The Ensemble-DL improvement over Ensemble-S decreases as the data frequency is increased. Ensemble-DL performance is very similar to the N-BEATS method with N-BEATS performing better than Ensemble-DL for the “Other” series. Balancing the comparison among ensemblesNote that in the above discussion, Ensemble-S only used 2 models while Ensemble-DL had 4. To make a fair comparison, the authors also evaluated the DL models in pairs and found that all of the pairwise DL models performed better than Ensemble-S from 0.4-6.1%. Interestingly, ensembles that involve DeepAR, i.e., the most accurate individual DL model, consistently outperform the rest, while ensembles that involve WaveNet, i.e., the least accurate individual DL model, sometimes deteriorate forecasting performance. Also, the accuracy of Ensemble-DL is comparable to that of the best pairwise en","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:2:2","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#additional-notes-1"},{"categories":["Time Series"],"content":"\r“Statistical vs Deep Learning forecasting methods” by StatsforecastIn the paper above, the authors concluded that “We find that combinations of DL models perform better than most standard models, both statistical and ML, especially for the case of monthly series and long-term forecasts.” However, a recent experiment conducted by the authors of the Statsforecast library challenged this deduction by using a more powerful statistical ensemble8. They combined four statistical models: AutoARIMA, ETS, CES, and DynamicOptimizedTheta (same as their 6th place winning entry in the M4 competition) and shared sMAPE results on the same M3 data. As shown above, their statistical ensemble is consistently better than the Transformer, Wavenet, and Feed-Forward models. The following table summarizes their sMAPE results along with associated costs. Their statistical ensemble outperforms most individual deep-learning models with average SMAPE results similar to DeepAR but with computational savings of 99%. SummaryThe authors conclude that “deep-learning ensembles outperform statistical ensembles just by 0.36 points in SMAPE. However, the DL ensemble takes more than 14 days to run and costs around USD 11,000, while the statistical ensemble takes 6 minutes to run and costs $0.5c.”. Additional NotesIn another recent experiment, the authors of Statsforecast compared their statistical ensemble against Amazon Forecast 9. Amazon Forecast is a time-series forecasting service on AWS that includes algorithms ranging from commonly known statistical models like ARIMA to complex algorithms like CNN-QR, DeepAR+, etc. They showed that Amazon Forecast’s AutoML was 60% less accurate and 669 times more expensive than running their statistical alternative. ","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:2:3","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#statistical-vs-deep-learning-forecasting-methods-by-statsforecast"},{"categories":["Time Series"],"content":"\r“Statistical vs Deep Learning forecasting methods” by StatsforecastIn the paper above, the authors concluded that “We find that combinations of DL models perform better than most standard models, both statistical and ML, especially for the case of monthly series and long-term forecasts.” However, a recent experiment conducted by the authors of the Statsforecast library challenged this deduction by using a more powerful statistical ensemble8. They combined four statistical models: AutoARIMA, ETS, CES, and DynamicOptimizedTheta (same as their 6th place winning entry in the M4 competition) and shared sMAPE results on the same M3 data. As shown above, their statistical ensemble is consistently better than the Transformer, Wavenet, and Feed-Forward models. The following table summarizes their sMAPE results along with associated costs. Their statistical ensemble outperforms most individual deep-learning models with average SMAPE results similar to DeepAR but with computational savings of 99%. SummaryThe authors conclude that “deep-learning ensembles outperform statistical ensembles just by 0.36 points in SMAPE. However, the DL ensemble takes more than 14 days to run and costs around USD 11,000, while the statistical ensemble takes 6 minutes to run and costs $0.5c.”. Additional NotesIn another recent experiment, the authors of Statsforecast compared their statistical ensemble against Amazon Forecast 9. Amazon Forecast is a time-series forecasting service on AWS that includes algorithms ranging from commonly known statistical models like ARIMA to complex algorithms like CNN-QR, DeepAR+, etc. They showed that Amazon Forecast’s AutoML was 60% less accurate and 669 times more expensive than running their statistical alternative. ","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:2:3","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#summary-2"},{"categories":["Time Series"],"content":"\r“Statistical vs Deep Learning forecasting methods” by StatsforecastIn the paper above, the authors concluded that “We find that combinations of DL models perform better than most standard models, both statistical and ML, especially for the case of monthly series and long-term forecasts.” However, a recent experiment conducted by the authors of the Statsforecast library challenged this deduction by using a more powerful statistical ensemble8. They combined four statistical models: AutoARIMA, ETS, CES, and DynamicOptimizedTheta (same as their 6th place winning entry in the M4 competition) and shared sMAPE results on the same M3 data. As shown above, their statistical ensemble is consistently better than the Transformer, Wavenet, and Feed-Forward models. The following table summarizes their sMAPE results along with associated costs. Their statistical ensemble outperforms most individual deep-learning models with average SMAPE results similar to DeepAR but with computational savings of 99%. SummaryThe authors conclude that “deep-learning ensembles outperform statistical ensembles just by 0.36 points in SMAPE. However, the DL ensemble takes more than 14 days to run and costs around USD 11,000, while the statistical ensemble takes 6 minutes to run and costs $0.5c.”. Additional NotesIn another recent experiment, the authors of Statsforecast compared their statistical ensemble against Amazon Forecast 9. Amazon Forecast is a time-series forecasting service on AWS that includes algorithms ranging from commonly known statistical models like ARIMA to complex algorithms like CNN-QR, DeepAR+, etc. They showed that Amazon Forecast’s AutoML was 60% less accurate and 669 times more expensive than running their statistical alternative. ","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:2:3","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#additional-notes-2"},{"categories":["Time Series"],"content":"\rAre there any unique time series characteristics for which some methods perform better than others?","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:2:4","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#are-there-any-unique-time-series-characteristics-for-which-some-methods-perform-better-than-others"},{"categories":["Time Series"],"content":"\rAre there certain methods that can be expected to work reasonably well with data from certain data domains?We can define a time series in terms of its characteristics such as forecastability (aka spectral entropy), the strength of trend and seasonality, etc. Montero-Manso et al 10 utilized 42 such features in their runner-up model in the M4 competition. In Makridakis et al.5 authors used 5 out 6 features recommended by Kang et al 6 for the M3 dataset. The features were: Spectral Entropy, Strength of Trend, Strength of Seasonality, Linearity, and Stability. The sMAPE results for the statistical and deep ensembles were then correlated with these features using a Multiple Linear Regression (MLR). We can see that the Ensemble-DL is generally more effective in handling noisy and trended series, in contrast to Ensemble-S which provides more accurate forecasts for seasonal data, as well as for series that are stable or linear. Similarly, other works show that some methods can be expected to perform better when the time series exhibits certain features like intermittency, lumpiness, or smoothness2. Also, sparsity in data might make ML models less effective3. Additionally, research work done by Spiliotis et al.11 shows that certain features are dominant in time series data from the domains in the M4 dataset. This information can provide relevant criteria for model selection. For example, since the following table shows that the financial data tend to be noisy, untrended, skewed, and not seasonal, the Theta method could potentially be utilized to boost the forecasting performance. Accordingly, ETS and ARIMA could be powerful solutions for extrapolating demographic data that are characterized by strong seasonality, trend, and linearity. Additionally, a study of M5 contest submissions shows that the ML methods have entered the mainstream of forecasting applications, at least in the area of retail sales forecasting12. ","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:2:5","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#are-there-certain-methods-that-can-be-expected-to-work-reasonably-well-with-data-from-certain-data-domains"},{"categories":["Time Series"],"content":"\rDo some models tend to work better with high-dimensional data?","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:2:6","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#do-some-models-tend-to-work-better-with-high-dimensional-data"},{"categories":["Time Series"],"content":"\rWhy do statistical models work better than ML models for small datasets?","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:2:7","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#why-do-statistical-models-work-better-than-ml-models-for-small-datasets"},{"categories":["Time Series"],"content":"\rIs it better to train a model over multiple time series rather than training it per series?The author of “Machine learning in M4: What makes a good unstructured model?” paper 3 puts forward an excellent theory in order to explain why ML models tend to work better with high-dimensional data. Kang et al.13 showed in their work that certain combinations of time series features may not be possible. Due to their structured nature, statistical models define the class of manifold before the data are observed, which allows the manifold to be defined by a smaller amount of data. However, this comes with a drawback that if the data do not match that class of manifold, the model can never be accurately fitted to it. On the other hand, ML models learn the manifold by observing the data. This creates a more flexible model that can fit more types of relationships but needs more data to define the space. Another important observation is that Cross-Learning i.e. a single model learning across many related series simultaneously, as opposed to learning in a series-by-series fashion, can enhance forecasting performance. Modeling many series with similar properties is likely to generate a more densely populated, and thus better defined, manifold that is preferred over search spaces with data sparsity. Data normalization used by ML models (as opposed to decomposition-based preprocessing methods used by statistical methods) and rolling-origin cross-validation steps also help in densely populating the same manifold space. ","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:2:8","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#is-it-better-to-train-a-model-over-multiple-time-series-rather-than-training-it-per-series"},{"categories":["Time Series"],"content":"\rComparisons based on forecasting competitions","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:3:0","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#comparisons-based-on-forecasting-competitions"},{"categories":["Time Series"],"content":"\rLearnings from the M3 competitionThe M3 forecasting competition was held in 2002 and the winner used the Theta method. V. Assimakopoulos and K. Nikolopoulos proposed this method as a decompositional approach to forecasting 14. Rob Hyndman et al. later simplified the Theta method and showed that we can use ETS with a drift term to get equivalent results to the original Theta method, which is what is adapted into most of the implementations of the method that exist today 15. In “Extending to all 3,003 time series of M3 dataset” section, we also saw a few other methods that have since outperformed the Theta method for M3 data. Specifically, the LGT model that was built on the theme of using a statistical algorithm (ETS) for preprocessing followed by an LSTM-based neural network 16 performed the best on yearly forecast horizon (as measured by sMAPE). The study we discussed also found out that statistical models are better suited for short-term forecasts, while DL ones are better at capturing the long-term characteristics of the data. So it could be beneficial to combine statistical or ML models with DL ones depending on the time horizon of forecasting. LGT model was built on this theme using a statistical algorithm (ETS) for preprocessing followed by an LSTM-based neural network 16. Additional NotesThe authors of the LGT paper suggest using categorical variables, like error type, trend type (such as additive, multiplicative), etc. as additional information for improving forecasting accuracy. ","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:3:1","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#learnings-from-the-m3-competition"},{"categories":["Time Series"],"content":"\rLearnings from the M3 competitionThe M3 forecasting competition was held in 2002 and the winner used the Theta method. V. Assimakopoulos and K. Nikolopoulos proposed this method as a decompositional approach to forecasting 14. Rob Hyndman et al. later simplified the Theta method and showed that we can use ETS with a drift term to get equivalent results to the original Theta method, which is what is adapted into most of the implementations of the method that exist today 15. In “Extending to all 3,003 time series of M3 dataset” section, we also saw a few other methods that have since outperformed the Theta method for M3 data. Specifically, the LGT model that was built on the theme of using a statistical algorithm (ETS) for preprocessing followed by an LSTM-based neural network 16 performed the best on yearly forecast horizon (as measured by sMAPE). The study we discussed also found out that statistical models are better suited for short-term forecasts, while DL ones are better at capturing the long-term characteristics of the data. So it could be beneficial to combine statistical or ML models with DL ones depending on the time horizon of forecasting. LGT model was built on this theme using a statistical algorithm (ETS) for preprocessing followed by an LSTM-based neural network 16. Additional NotesThe authors of the LGT paper suggest using categorical variables, like error type, trend type (such as additive, multiplicative), etc. as additional information for improving forecasting accuracy. ","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:3:1","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#additional-notes-3"},{"categories":["Time Series"],"content":"\rLearnings from the M4 competitionThe author of the LGT model also authored the winning method of the M4 competition in a similar fashion. The contest-winning model was a truly hybrid forecasting approach that utilized both statistical and ML elements, using cross-learning. This approach used exponential smoothing mixed with a dilated LSTM network17. The ES formulas enable the method to capture the local components (like per-series seasonality), while LSTM allows it to learn non-linear trends (like NN parameters) and cross-learning. To account for seasonality, the author advocate for incorporating categorical seasonality components (one-hot vector of size 6 appended to time series derived features), and smoothing coefficients into model fitting to allow deseasonalizing of data in the absence of any calendar features like the day of the week or the month number. Additionally, the second-best M4 method was also based on a cross-learning approach, utilizing an ML algorithm (XGBoost) for selecting the most appropriate weights for combining various statistical methods (Theta, ARIMA, TBATS, etc.) 10. ","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:3:2","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#learnings-from-the-m4-competition"},{"categories":["Time Series"],"content":"\rLearnings from the M5 competitionBarker’s study 3 pointed out how learning to forecast using multiple time series simultaneously leads to narrower and more densely populated manifolds when the series have a commonality in their origin. The dataset of the M5 competition consisted of more than 30,000 series of daily sales at Walmart. So theoretically we would expect ML models trained in a cross-learning fashion to perform well on this dataset without having to specify additional features. Also, the statistical models that do assume interdependence between the series should perform well. It is worth noting that 48.4% of participating teams could beat the Naïve benchmark, 35.8% could beat the Seasonal Naïve, and only 7.5% of teams could beat the exponential smoothing-based benchmark. LightGBM, a decision tree-based ML approach, was used by almost all of the top 50 M5 competitors 12 including the winner, second, fourth, and fifth-positioned teams. The winning submission used an equally weighted combination of various LightGBM models trained on data pooled per store, store-category, and store-department. Runnerup submission used the LightGBM model that was externally adjusted through multipliers according to the forecasts product by an N-BEATS model for the top five aggregation levels of the dataset. The LightGBM models were trained using only some basic features of calendar effects and prices (past unit sales were not considered), and the N-BEATS model was based solely on historical unit sales. ","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:3:3","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#learnings-from-the-m5-competition"},{"categories":["Time Series"],"content":"\rConclusionThe No-Free-Lunch theorem, proposed for supervised machine learning, tells us that there is never likely to be a single method that fits all situations. Similarly, there is no one time series forecasting method that will always perform best. But the literature suggests a lot of useful learnings that can help us in making more informed decisions when it comes to modeling for time series forecasting. As there are “horses for courses”, there must also be forecasting methods that are more tailored to some types of data and some aggregation levels13. The following section lists all of the important takeaways from this article. ","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:4:0","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#conclusion"},{"categories":["Time Series"],"content":"\rTakeaways ML methods need more data for better forecasting especially when the series being predicted is non-stationary, displaying seasonality and trend. For many years, ML methods have been outperformed by simple, yet robust statistical approaches specifically for domains that involve shorter, non-stationary data, such as business forecasting. DL methods can effectively balance learning across multiple forecasting horizons, sacrificing part of their short-term accuracy to ensure adequate performance in the long term. Statistical models tend to do well on short-term forecasts, while DL ones are better at capturing the long-term characteristics of the data. So it could be beneficial to combine statistical or ML models with DL ones depending on the time horizon of forecasting. Ensemble methods outperform most of the individual statistical and ML models. Ensembles of some relatively simple statistical methods like ARIMA, ETS, and Theta, often provide similarly accurate or even more accurate short-term forecasts than the individual DL models and even their ensembles (on rare occasions), but inferior results when medium or long-term forecasts are considered. DL ensembles will almost always give better results than Stat ensembles but at a much higher cost and not necessarily a big margin. Although the ensembles of multiple DL models lead to more accurate results, depending on the particular characteristics of the series, the additional cost that has to be paid for using a DL model in order to improve forecasting accuracy to a small extent is extensive. Some studies suggest that DL methods can lead to greater accuracy improvements, compared to their statistical counterparts, when tasked with forecasting yearly or quarterly series. DL ensembles are generally more effective in handling noisy and trended series, in contrast to statistical ensembles that provide more accurate forecasts for seasonal data, as well as for series that are stable or linear. Although DL models are more robust to randomness, seasonality is more effectively modeled by statistical methods. ","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:4:1","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#takeaways"},{"categories":["Time Series"],"content":"\rReferences Barker, Jocelyn. (2019). Machine learning in M4: What makes a good unstructured model?. International Journal of Forecasting. 36. 10.1016/j.ijforecast.2019.06.001. ↩︎ Spiliotis, Evangelos \u0026 Makridakis, Spyros \u0026 Semenoglou, Artemios-Anargyros \u0026 Assimakopoulos, Vassilis. (2022). Comparison of statistical and machine learning methods for daily SKU demand forecasting. Operational Research. 22. 10.1007/s12351-020-00605-2. ↩︎ ↩︎ Barker, Jocelyn. (2019). Machine learning in M4: What makes a good unstructured model?. International Journal of Forecasting. 36. 10.1016/j.ijforecast.2019.06.001. ↩︎ ↩︎ ↩︎ ↩︎ Makridakis, Spyros \u0026 Spiliotis, Evangelos \u0026 Assimakopoulos, Vassilis. (2018). Statistical and Machine Learning forecasting methods: Concerns and ways forward. PLoS ONE. 13. 10.1371/journal.pone.0194889. ↩︎ Makridakis, Spyros \u0026 Spiliotis, Evangelos \u0026 Assimakopoulos, Vassilis \u0026 Semenoglou, Artemios-Anargyros \u0026 Mulder, Gary \u0026 Nikolopoulos, Konstantinos. (2022). Statistical, machine learning and deep learning forecasting methods: Comparisons and ways forward. Journal of the Operational Research Society. 1-20. 10.1080/01605682.2022.2118629. ↩︎ ↩︎ Koning, Alex \u0026 Franses, Philip \u0026 Hibon, Michele \u0026 Stekler, H.O.. (2005). The M3 competition: Statistical tests of the results. International Journal of Forecasting. 21. 397-409. 10.1016/j.ijforecast.2004.10.003. ↩︎ ↩︎ Oreshkin, Boris \u0026 Carpo, Dmitri \u0026 Chapados, Nicolas \u0026 Bengio, Yoshua. (2019). N-BEATS: Neural basis expansion analysis for interpretable time series forecasting. ↩︎ https://github.com/Nixtla/statsforecast/tree/main/experiments/m3 ↩︎ https://github.com/Nixtla/statsforecast/tree/main/experiments/amazon_forecast ↩︎ Montero-Manso, Pablo \u0026 Athanasopoulos, George \u0026 Hyndman, Rob \u0026 Talagala, Thiyanga. (2019). FFORMA: Feature-based forecast model averaging. International Journal of Forecasting. 36. 10.1016/j.ijforecast.2019.02.011. ↩︎ ↩︎ Spiliotis, Evangelos \u0026 Kouloumos, Andreas \u0026 Assimakopoulos, Vassilis \u0026 Makridakis, Spyros. (2018). Are forecasting competitions data representative of the reality?. International Journal of Forecasting. 10.1016/j.ijforecast.2018.12.007. ↩︎ Makridakis, Spyros \u0026 Spiliotis, Evangelos \u0026 Assimakopoulos, Vassilis. (2020). The M5 Accuracy competition: Results, findings and conclusions. ↩︎ ↩︎ Kang, Yanfei \u0026 Hyndman, Rob \u0026 Smith-Miles, Kate. (2017). Visualising forecasting algorithm performance using time series instance spaces. International Journal of Forecasting. 33. 345-358. 10.1016/j.ijforecast.2016.09.004. ↩︎ ↩︎ Assimakopoulos, Vassilis \u0026 Nikolopoulos, K.. (2000). The theta model: A decomposition approach to forecasting. International Journal of Forecasting. 16. 521-530. 10.1016/S0169-2070(00)00066-2. ↩︎ Hyndman, Rob \u0026 Billah, Baki. (2003). Unmasking the Theta method. International Journal of Forecasting. 19. 287-290. 10.1016/S0169-2070(01)00143-1. ↩︎ Smyl, Slawek \u0026 Kuber, Karthik. (2016). Data Preprocessing and Augmentation for Multiple Short Time Series Forecasting with Recurrent Neural Networks. ↩︎ ↩︎ Smyl, Slawek. (2019). A hybrid method of exponential smoothing and recurrent neural networks for time series forecasting. International Journal of Forecasting. 36. 10.1016/j.ijforecast.2019.03.017. ↩︎ ","date":"2022-12-20","objectID":"/posts/2022/12/stats-vs-ml-for-ts/:5:0","series":null,"tags":["forecasting","literature review"],"title":"Statistical vs Machine Learning vs Deep Learning Modeling for Time Series Forecasting","uri":"/posts/2022/12/stats-vs-ml-for-ts/#references"},{"categories":["Recommender systems"],"content":"Many machine learning domains, such as recommender systems, targeted advertisement, search ranking, and text analysis contain highly sparse data because of the large categorical variable domains. This sparsity makes it hard for ML algorithms to model second-order and above feature interactions. In this article, I summarize the need for modeling feature interactions and introduce some of the most popular ML architectures designed for estimating interactions from sparse data.","date":"2022-11-06","objectID":"/posts/2022/11/sparse-recsys/","series":null,"tags":["recsys","literature review"],"title":"Recommender Systems for Modeling Feature Interactions under Sparse Settings","uri":"/posts/2022/11/sparse-recsys/"},{"categories":["Recommender systems"],"content":"Many machine learning domains, such as recommender systems, targeted advertisement, search ranking, and text analysis contain highly sparse data because of the large categorical variable domains. This sparsity makes it hard for ML algorithms to model second-order and above feature interactions. In this article, I summarize the need for modeling feature interactions and introduce some of the most popular ML architectures designed for estimating interactions from sparse data. ","date":"2022-11-06","objectID":"/posts/2022/11/sparse-recsys/:0:0","series":null,"tags":["recsys","literature review"],"title":"Recommender Systems for Modeling Feature Interactions under Sparse Settings","uri":"/posts/2022/11/sparse-recsys/#"},{"categories":["Recommender systems"],"content":"\rIntroduction","date":"2022-11-06","objectID":"/posts/2022/11/sparse-recsys/:1:0","series":null,"tags":["recsys","literature review"],"title":"Recommender Systems for Modeling Feature Interactions under Sparse Settings","uri":"/posts/2022/11/sparse-recsys/#introduction"},{"categories":["Recommender systems"],"content":"\rProblem with sparse inputsA variety of Information Retrieval and Data Mining tasks, such as Recommender Systems, Targeted Advertising, Search Ranking, etc. model discrete and categorical variables extensively. As an example, these variables could be user IDs, product IDs, advertisement IDs, and user demographics such as gender and occupation. A common technique to use these categorical variables in machine learning algorithms is to convert them to a set of binary vectors via one-hot encoding. This means that for a system with a large userbase and catalog the resultant feature vector is highly sparse. As explained next, this highly sparse setting makes it difficult to incorporate feature interactions. ","date":"2022-11-06","objectID":"/posts/2022/11/sparse-recsys/:1:1","series":null,"tags":["recsys","literature review"],"title":"Recommender Systems for Modeling Feature Interactions under Sparse Settings","uri":"/posts/2022/11/sparse-recsys/#problem-with-sparse-inputs"},{"categories":["Recommender systems"],"content":"\rWhy modeling feature interaction is important?As an example, consider an artificial Click-through rate (CTR) dataset shown in the table below, where + and - represent the number of clicked and unclicked impressions respectively. We can see that an ad from Gucci has a high CTR on Vogue. It is difficult for linear models to learn this information because they learn the two weights Gucci and Vogue separately. To address this problem, a machine learning model will have to learn the effect of their feature interaction. Algorithms such as Poly2 (degree-2 polynomial) do this by learning a dedicated weight for each feature conjunction ($O(n^{2})$ time complexity). Note: An order-2 interaction can be between two features like app category and timestamp. For example, people often download Uber apps for food delivery at meal-time. Similarly, an order-3 interaction can be between app category, user gender, and age. For example, a report showed that male teenagers like shooting and RPG games. Research shows that considering low- and high-order feature interactions simultaneously brings additional improvement over the cases of considering either alone1. For a highly sparse dataset, these feature interactions are often hidden and difficult to identify a priori. ","date":"2022-11-06","objectID":"/posts/2022/11/sparse-recsys/:1:2","series":null,"tags":["recsys","literature review"],"title":"Recommender Systems for Modeling Feature Interactions under Sparse Settings","uri":"/posts/2022/11/sparse-recsys/#why-modeling-feature-interaction-is-important"},{"categories":["Recommender systems"],"content":"\rMemorization and Generalization paradigmsOne common challenge in building such applications is to achieve both memorization and generalization. Memorization is about learning the frequent co-occurrence of features and exploiting the correlations observed from the historical data. Google Play store recommender system might look into two features ‘installed_app’ and ‘impression_app’ to calculate the probability of a user installing the impression_app. Memorization-based algorithms use cross-product transformation and look at the features, such as “AND(previously_installed_app=netflix, current_impression_app=spotify)”, whose value is 1 and correlate it with the target label. While such recommendations are topical and directly relevant based on the user’s previous actions, they can not generalize when cross-feature interaction never happened in the training data. For example, there is no training data for the pair (NBC, Gucci) in the table above. Creating cross-product transformations may also require a lot of manual feature engineering effort. Generalization is based on the transitivity of correlation and exploring new feature combinations that have never or rarely occurred in the past. To achieve this generalization we use embeddings-based methods, such as factorization machines or deep neural networks by learning a low-dimensional dense embedding vector. While such recommenders tend to improve the diversity of the recommended items, they also suffer from the problem of over-generalizing. These methods have to learn effective low-dimensional representations from sparse and high-rank data. For cases such as users with specific preferences, or niche items with a narrow appeal, dense embeddings lead to nonzero predictions and thus make less relevant recommendations. Comparatively, it is easier for generalized linear models with cross-product feature transformations to memorize these “exception rules” with much fewer parameters. In this article, I will introduce, implement and compare seven popular frameworks to achieve both memorization and generalization in individual model architectures. ","date":"2022-11-06","objectID":"/posts/2022/11/sparse-recsys/:1:3","series":null,"tags":["recsys","literature review"],"title":"Recommender Systems for Modeling Feature Interactions under Sparse Settings","uri":"/posts/2022/11/sparse-recsys/#memorization-and-generalization-paradigms"},{"categories":["Recommender systems"],"content":"\rDataset PreprocessingFor this article, I will be using the MovieLens 100K dataset2 from Kaggle. It contains 100K ratings from 943 users on 1682 movies, along with demographic information for the user. Each user has rated at least 20 movies. The following transformations were applied to prepare the dataset for experimentation. Dataset was sorted by user_id and timestamp to create time ordering for each user. A target column was created which is time-wise the next movie that the user will watch. New columns such as average movie rating per user, average movie rating per genre per user, number of movies watched per user in each genre normalized by total movies watched by that user, etc. were created. Gender column was label encoded, and the occupation column was dummy encoded. Continuous features were scaled as appropriate. A sparse binary tensor was created indicating movies that the user has watched previously. Dataset was split into train-test with an 80-20 ratio. The following diagram shows the various features generated through preprocessing. Some or all of these features are used during experimentation depending upon the specific model architecture. Refer to the respective notebook for data preprocessing steps specific to each model. ","date":"2022-11-06","objectID":"/posts/2022/11/sparse-recsys/:2:0","series":null,"tags":["recsys","literature review"],"title":"Recommender Systems for Modeling Feature Interactions under Sparse Settings","uri":"/posts/2022/11/sparse-recsys/#dataset-preprocessing"},{"categories":["Recommender systems"],"content":"\rExperimentationEach of the model implementations is done in its standalone Jupyter notebook file and is linked in the corresponding section below. Every notebook contains code for data preprocessing, model definition, training, and evaluation as appropriate to the respective model. ","date":"2022-11-06","objectID":"/posts/2022/11/sparse-recsys/:3:0","series":null,"tags":["recsys","literature review"],"title":"Recommender Systems for Modeling Feature Interactions under Sparse Settings","uri":"/posts/2022/11/sparse-recsys/#experimentation"},{"categories":["Recommender systems"],"content":"\r1. Factorization Machine (FM)Factorization Machines (FMs) were originally purposed as a supervised machine learning method for collaborative recommendations in 2010. FMs allow parameter estimation under very sparse data where its predecessors, like SVMs failed, and unlike Matrix Factorization it isn’t limited to modeling the relation of two entities only. FMs learn second-order feature interactions on top of a linear model by modeling all interactions between each pair of features. They can also be optimized to work in linear time complexity. While in principle, FM can model high-order feature interaction, in practice usually only order-2 features are considered due to high complexity. where $w_{0}$ is the global bias, $w_{i}$ denotes the weight of the i-th feature, and $ŵ_{ij}$ denotes the weight of the cross feature $x_{i}x_{j}$, which is factorized as: $ŵ_{ij} = v_{i}^{T} v_{j}$, where $v_{i} \\in R^{k}$ denotes the embedding vector for feature i, and k denotes the size of embedding vector. Note that due to the coefficient $x_{i}x_{j}$, only interactions between non-zero features are considered. The example above shows a sparse input feature vector x with corresponding target y. We have binary indicator variables for the user, movie, and the last movie rated, along with a normalized rating for other movies and timestamps. Let’s say we want to estimate the interaction between the user Alice (A) and the movie Star Trek (ST). You will notice that in x we do not have any example in x with an interaction between A and ST. FM can still estimate this by using the factorized interaction parameters $\\langle v_{A}, v_{ST} \\rangle$ even in this case. ImplementationSuppose we have M training objects, n features and we want to factorize feature interaction with vectors of size k i.e. dimensionality of $v_{i}$. Let us denote our trainset as $X \\in R^{M×n}$, and matrix of $v_{i}$ (the ith row is $v_{i}$) as $V \\in R^{n×k}$. Also, let’s denote the feature vector for the jth object as $x_{j}$. So: The number in brackets indicates the index of the sample for x and the index of the feature for v. Also, the last term in the FM equation can be expressed as: $S_{1}$ is a dot product of feature vector $x_{j}$ and ith column of V. If we multiply X and V, we get: So if square XV element-wise and then find the sum of each row, we obtain a vector of $S_{1}^{2}$ terms for each training sample. Also, if we first square X and V element-wise, then multiply them, and finally sum by rows, we’ll get $S_{2}$ term for each training object. So, conceptually, we can express the final term like this: PyTorch Code class FM(nn.Module): def __init__(self, input_dim, n_class, k=5): super().__init__() self.V = nn.Parameter(torch.randn(input_dim, k),requires_grad=True) self.linear_layer = nn.Linear(input_dim, n_class, device=device) def forward(self, x): square_of_sum = torch.matmul(x, self.V.to(device)).pow(2).sum(1, keepdim=True) #S_1^2 sum_of_square = torch.matmul(x.pow(2), self.V.to(device).pow(2)).sum(1, keepdim=True) # S_2 out_inter = 0.5 * (square_of_sum - sum_of_square) out_lin = self.linear_layer(x) out = out_inter + out_lin return out Refer to this Gist for the complete code for this experiment: https://gist.github.com/reachsumit/2a639276fc781870c4dcd480a3417bf9 You can read more about FMs in the original paper and the official codebase. ","date":"2022-11-06","objectID":"/posts/2022/11/sparse-recsys/:3:1","series":null,"tags":["recsys","literature review"],"title":"Recommender Systems for Modeling Feature Interactions under Sparse Settings","uri":"/posts/2022/11/sparse-recsys/#1-factorization-machine-fm"},{"categories":["Recommender systems"],"content":"\r1. Factorization Machine (FM)Factorization Machines (FMs) were originally purposed as a supervised machine learning method for collaborative recommendations in 2010. FMs allow parameter estimation under very sparse data where its predecessors, like SVMs failed, and unlike Matrix Factorization it isn’t limited to modeling the relation of two entities only. FMs learn second-order feature interactions on top of a linear model by modeling all interactions between each pair of features. They can also be optimized to work in linear time complexity. While in principle, FM can model high-order feature interaction, in practice usually only order-2 features are considered due to high complexity. where $w_{0}$ is the global bias, $w_{i}$ denotes the weight of the i-th feature, and $ŵ_{ij}$ denotes the weight of the cross feature $x_{i}x_{j}$, which is factorized as: $ŵ_{ij} = v_{i}^{T} v_{j}$, where $v_{i} \\in R^{k}$ denotes the embedding vector for feature i, and k denotes the size of embedding vector. Note that due to the coefficient $x_{i}x_{j}$, only interactions between non-zero features are considered. The example above shows a sparse input feature vector x with corresponding target y. We have binary indicator variables for the user, movie, and the last movie rated, along with a normalized rating for other movies and timestamps. Let’s say we want to estimate the interaction between the user Alice (A) and the movie Star Trek (ST). You will notice that in x we do not have any example in x with an interaction between A and ST. FM can still estimate this by using the factorized interaction parameters $\\langle v_{A}, v_{ST} \\rangle$ even in this case. ImplementationSuppose we have M training objects, n features and we want to factorize feature interaction with vectors of size k i.e. dimensionality of $v_{i}$. Let us denote our trainset as $X \\in R^{M×n}$, and matrix of $v_{i}$ (the ith row is $v_{i}$) as $V \\in R^{n×k}$. Also, let’s denote the feature vector for the jth object as $x_{j}$. So: The number in brackets indicates the index of the sample for x and the index of the feature for v. Also, the last term in the FM equation can be expressed as: $S_{1}$ is a dot product of feature vector $x_{j}$ and ith column of V. If we multiply X and V, we get: So if square XV element-wise and then find the sum of each row, we obtain a vector of $S_{1}^{2}$ terms for each training sample. Also, if we first square X and V element-wise, then multiply them, and finally sum by rows, we’ll get $S_{2}$ term for each training object. So, conceptually, we can express the final term like this: PyTorch Code class FM(nn.Module): def __init__(self, input_dim, n_class, k=5): super().__init__() self.V = nn.Parameter(torch.randn(input_dim, k),requires_grad=True) self.linear_layer = nn.Linear(input_dim, n_class, device=device) def forward(self, x): square_of_sum = torch.matmul(x, self.V.to(device)).pow(2).sum(1, keepdim=True) #S_1^2 sum_of_square = torch.matmul(x.pow(2), self.V.to(device).pow(2)).sum(1, keepdim=True) # S_2 out_inter = 0.5 * (square_of_sum - sum_of_square) out_lin = self.linear_layer(x) out = out_inter + out_lin return out Refer to this Gist for the complete code for this experiment: https://gist.github.com/reachsumit/2a639276fc781870c4dcd480a3417bf9 You can read more about FMs in the original paper and the official codebase. ","date":"2022-11-06","objectID":"/posts/2022/11/sparse-recsys/:3:1","series":null,"tags":["recsys","literature review"],"title":"Recommender Systems for Modeling Feature Interactions under Sparse Settings","uri":"/posts/2022/11/sparse-recsys/#implementation"},{"categories":["Recommender systems"],"content":"\r2. Field-Aware Factorization Machine (FFM)Let’s revisit the artificial dataset example from earlier and add another column Gender to it. One observation of such a dataset might look like this: FMs will model the effect of feature conjunction as: $w_{ESPN} \\cdot w_{Nike} + w_{ESPN} \\cdot w_{Male} + w_{Nike} \\cdot w_{Male}$. Field-aware factorization machines (FFMs) extend FMs by introducing the concept of fields and features. With the same example, Publisher, Advertiser, and Gender will be called fields, while values like ESPN, Nike, and Male will be called their features. Note that in FMs every feature has only one latent vector to learn the latent effect with any other features. For example, for ESPN $w_{ESPN}$ is used to learn the latent effect with Nike ($w_{ESPN} \\cdot w_{Nike}$) and Male ($w_{ESPN} \\cdot w_{Male}$). However, because Nike and Male belong to different fields, the latent effects of (EPSN, Nike) and (EPSN, Male) may be different. In FFMs, each feature has several latent vectors. Depending on the field of other features, one of them is used to do the inner product. So, for the same example, the feature interaction effect is modelled by FFM as: $w_{ESPN,A} \\cdot w_{Nike,P} + w_{ESPN,G} \\cdot w_{Male,P} + w_{Nike,G} \\cdot w_{Male,A}$. To learn the latent effect of (ESPN, NIKE), for example, $w_{ESPN,A}$ is used because Nike belongs to the field Advertiser, and $w_{Nike,P}$ is used because ESPN belongs to the field Publisher. If f is the number of fields, then the number of variables of FFMs is nfk, and the time complexity is $O(\\bar{n}^{2}k)$. Note that because each latent vector in FFMs only needs to learn the effect with a specific field, usually: $k_{FFM} \\ll k_{FM}$. FFM authors empirically show that for large, sparse datasets with many categorical features, FFMs perform better than FMs. Whereas for small and dense datasets or numerical datasets, FMs perform better than FFMs. ImplementationPyTorch Code class FFM(nn.Module): def __init__(self, continuous_dim, field_dims, n_class, embed_dim=16, pad_idx=0): super().__init__() self.bias = nn.Parameter(torch.zeros((n_class,))) self.embeddings = nn.Embedding(sum(field_dims), n_class, padding_idx=pad_idx, device=device) self.num_fields = len(field_dims) self.embeddings_interaction = nn.ModuleList([ nn.Embedding(sum(field_dims), embed_dim, padding_idx=pad_idx, device=device) for _ in range(self.num_fields) ]) self.linear_layer = nn.Linear(continuous_dim, n_class, device=device) def forward(self, continuous_X, categorical_X): embeds_out = torch.sum(self.embeddings(categorical_X), dim=1) + self.bias.to(device) field_wise_emb_list = [self.embeddings_interaction[i](categorical_X) for i in range(self.num_fields)] ix = list() for i in range(self.num_fields - 1): for j in range(i + 1, self.num_fields): ix.append(field_wise_emb_list[j][:, i] * field_wise_emb_list[i][:, j]) ix = torch.stack(ix, dim=1) ffm_interaction_term = torch.sum(torch.sum(ix, dim=1), dim=1, keepdim=True) output = self.linear_layer(continuous_X) + embeds_out + ffm_interaction_term return output Refer to this Gist for the complete code for this experiment: https://gist.github.com/reachsumit/c6a8037f4596a8181376313fdba33ffd You can read more about FFMs in the original paper and the official codebase. ","date":"2022-11-06","objectID":"/posts/2022/11/sparse-recsys/:3:2","series":null,"tags":["recsys","literature review"],"title":"Recommender Systems for Modeling Feature Interactions under Sparse Settings","uri":"/posts/2022/11/sparse-recsys/#2-field-aware-factorization-machine-ffm"},{"categories":["Recommender systems"],"content":"\r2. Field-Aware Factorization Machine (FFM)Let’s revisit the artificial dataset example from earlier and add another column Gender to it. One observation of such a dataset might look like this: FMs will model the effect of feature conjunction as: $w_{ESPN} \\cdot w_{Nike} + w_{ESPN} \\cdot w_{Male} + w_{Nike} \\cdot w_{Male}$. Field-aware factorization machines (FFMs) extend FMs by introducing the concept of fields and features. With the same example, Publisher, Advertiser, and Gender will be called fields, while values like ESPN, Nike, and Male will be called their features. Note that in FMs every feature has only one latent vector to learn the latent effect with any other features. For example, for ESPN $w_{ESPN}$ is used to learn the latent effect with Nike ($w_{ESPN} \\cdot w_{Nike}$) and Male ($w_{ESPN} \\cdot w_{Male}$). However, because Nike and Male belong to different fields, the latent effects of (EPSN, Nike) and (EPSN, Male) may be different. In FFMs, each feature has several latent vectors. Depending on the field of other features, one of them is used to do the inner product. So, for the same example, the feature interaction effect is modelled by FFM as: $w_{ESPN,A} \\cdot w_{Nike,P} + w_{ESPN,G} \\cdot w_{Male,P} + w_{Nike,G} \\cdot w_{Male,A}$. To learn the latent effect of (ESPN, NIKE), for example, $w_{ESPN,A}$ is used because Nike belongs to the field Advertiser, and $w_{Nike,P}$ is used because ESPN belongs to the field Publisher. If f is the number of fields, then the number of variables of FFMs is nfk, and the time complexity is $O(\\bar{n}^{2}k)$. Note that because each latent vector in FFMs only needs to learn the effect with a specific field, usually: $k_{FFM} \\ll k_{FM}$. FFM authors empirically show that for large, sparse datasets with many categorical features, FFMs perform better than FMs. Whereas for small and dense datasets or numerical datasets, FMs perform better than FFMs. ImplementationPyTorch Code class FFM(nn.Module): def __init__(self, continuous_dim, field_dims, n_class, embed_dim=16, pad_idx=0): super().__init__() self.bias = nn.Parameter(torch.zeros((n_class,))) self.embeddings = nn.Embedding(sum(field_dims), n_class, padding_idx=pad_idx, device=device) self.num_fields = len(field_dims) self.embeddings_interaction = nn.ModuleList([ nn.Embedding(sum(field_dims), embed_dim, padding_idx=pad_idx, device=device) for _ in range(self.num_fields) ]) self.linear_layer = nn.Linear(continuous_dim, n_class, device=device) def forward(self, continuous_X, categorical_X): embeds_out = torch.sum(self.embeddings(categorical_X), dim=1) + self.bias.to(device) field_wise_emb_list = [self.embeddings_interaction[i](categorical_X) for i in range(self.num_fields)] ix = list() for i in range(self.num_fields - 1): for j in range(i + 1, self.num_fields): ix.append(field_wise_emb_list[j][:, i] * field_wise_emb_list[i][:, j]) ix = torch.stack(ix, dim=1) ffm_interaction_term = torch.sum(torch.sum(ix, dim=1), dim=1, keepdim=True) output = self.linear_layer(continuous_X) + embeds_out + ffm_interaction_term return output Refer to this Gist for the complete code for this experiment: https://gist.github.com/reachsumit/c6a8037f4596a8181376313fdba33ffd You can read more about FFMs in the original paper and the official codebase. ","date":"2022-11-06","objectID":"/posts/2022/11/sparse-recsys/:3:2","series":null,"tags":["recsys","literature review"],"title":"Recommender Systems for Modeling Feature Interactions under Sparse Settings","uri":"/posts/2022/11/sparse-recsys/#implementation-1"},{"categories":["Recommender systems"],"content":"\r3. Attentional Factorization Machine (AFM)Note how all interactions in FM are weighted with interaction weight matrix W. This W is a positive definite matrix and it can be shown that given a sufficiently large value of k, there will always be a matrix V such that $W=V\\cdot V^{T}$. In sparse settings, we usually restrict k to a smaller value which also leads to better generalizability. However, despite its effectiveness, FMs model all feature interactions with the same weight, but not all feature interactions are equally useful and predictive. For example, the interactions with useless features may even introduce noise leading to degraded model performance. Attentional Factorization Machines (AFMs) fix this by learning the importance of each feature interaction from data via a neural attention network. AFM starts with sparse input and embedding layer, and inspired by FM’s inner product, it expands m vectors to m(m-1)/2 interacted vectors, where each interacted vector is the element-wise product of two distinct vectors to encode their interaction. The output of the pair-wise interaction layer is fed into an attention-based pooling layer, the idea here is to allow different parts to contribute differently when compressing them to a single representation. An attention mechanism is applied to feature interactions by performing a weighted sum on the interacted vectors. This weight or attention score can be thought of as the importance of weight $\\hat{w}_{ij}$ in predicting the target. However, we have a problem when the features have never co-occurred in the training data. To address this problem, the authors further parameterize attention scores with a multi-layer perceptron (MLP). The attention scores are also normalized through the softmax function. One shortcoming of AFM architecture is that it models only second-order feature interactions. ImplementationPyTorch Code class AFM(nn.Module): def __init__(self, continuous_dim, field_dims, n_class, attn_size=16, embed_size=16, pad_idx=0): super().__init__() self.embeddings = nn.Embedding(sum(field_dims), embed_size, padding_idx=pad_idx, device=device) self.attention = nn.Linear(embed_size, attn_size, device=device) self.projection = nn.Linear(attn_size, 1, device=device) self.fc = nn.Linear(embed_size, n_class, device=device) self.linear_layer = nn.Linear(continuous_dim, n_class, device=device) def forward(self, continuous_X, categorical_X): embeds_out = self.embeddings(categorical_X) num_fields = embeds_out.shape[1] row, col = list(), list() for i in range(num_fields - 1): for j in range(i + 1, num_fields): row.append(i), col.append(j) p, q = embeds_out[:, row], embeds_out[:, col] inner_product = p * q attn_scores = nn.functional.relu(self.attention(inner_product)) attn_scores = nn.functional.softmax(self.projection(attn_scores), dim=1) attn_output = torch.sum(attn_scores * inner_product, dim=1) output = self.linear_layer(continuous_X) + self.fc(attn_output) return output Refer to this Gist for the complete code for this experiment: https://gist.github.com/reachsumit/85fe046691c66221bec00bc7e59e145b You can read more about AFMs in the original paper and the official codebase. ","date":"2022-11-06","objectID":"/posts/2022/11/sparse-recsys/:3:3","series":null,"tags":["recsys","literature review"],"title":"Recommender Systems for Modeling Feature Interactions under Sparse Settings","uri":"/posts/2022/11/sparse-recsys/#3-attentional-factorization-machine-afm"},{"categories":["Recommender systems"],"content":"\r3. Attentional Factorization Machine (AFM)Note how all interactions in FM are weighted with interaction weight matrix W. This W is a positive definite matrix and it can be shown that given a sufficiently large value of k, there will always be a matrix V such that $W=V\\cdot V^{T}$. In sparse settings, we usually restrict k to a smaller value which also leads to better generalizability. However, despite its effectiveness, FMs model all feature interactions with the same weight, but not all feature interactions are equally useful and predictive. For example, the interactions with useless features may even introduce noise leading to degraded model performance. Attentional Factorization Machines (AFMs) fix this by learning the importance of each feature interaction from data via a neural attention network. AFM starts with sparse input and embedding layer, and inspired by FM’s inner product, it expands m vectors to m(m-1)/2 interacted vectors, where each interacted vector is the element-wise product of two distinct vectors to encode their interaction. The output of the pair-wise interaction layer is fed into an attention-based pooling layer, the idea here is to allow different parts to contribute differently when compressing them to a single representation. An attention mechanism is applied to feature interactions by performing a weighted sum on the interacted vectors. This weight or attention score can be thought of as the importance of weight $\\hat{w}_{ij}$ in predicting the target. However, we have a problem when the features have never co-occurred in the training data. To address this problem, the authors further parameterize attention scores with a multi-layer perceptron (MLP). The attention scores are also normalized through the softmax function. One shortcoming of AFM architecture is that it models only second-order feature interactions. ImplementationPyTorch Code class AFM(nn.Module): def __init__(self, continuous_dim, field_dims, n_class, attn_size=16, embed_size=16, pad_idx=0): super().__init__() self.embeddings = nn.Embedding(sum(field_dims), embed_size, padding_idx=pad_idx, device=device) self.attention = nn.Linear(embed_size, attn_size, device=device) self.projection = nn.Linear(attn_size, 1, device=device) self.fc = nn.Linear(embed_size, n_class, device=device) self.linear_layer = nn.Linear(continuous_dim, n_class, device=device) def forward(self, continuous_X, categorical_X): embeds_out = self.embeddings(categorical_X) num_fields = embeds_out.shape[1] row, col = list(), list() for i in range(num_fields - 1): for j in range(i + 1, num_fields): row.append(i), col.append(j) p, q = embeds_out[:, row], embeds_out[:, col] inner_product = p * q attn_scores = nn.functional.relu(self.attention(inner_product)) attn_scores = nn.functional.softmax(self.projection(attn_scores), dim=1) attn_output = torch.sum(attn_scores * inner_product, dim=1) output = self.linear_layer(continuous_X) + self.fc(attn_output) return output Refer to this Gist for the complete code for this experiment: https://gist.github.com/reachsumit/85fe046691c66221bec00bc7e59e145b You can read more about AFMs in the original paper and the official codebase. ","date":"2022-11-06","objectID":"/posts/2022/11/sparse-recsys/:3:3","series":null,"tags":["recsys","literature review"],"title":"Recommender Systems for Modeling Feature Interactions under Sparse Settings","uri":"/posts/2022/11/sparse-recsys/#implementation-2"},{"categories":["Recommender systems"],"content":"\r4. Wide \u0026 Deep LearningThe Wide \u0026 Deep learning framework was proposed by Google to achieve both memorization and generalization in one model, by jointly training a linear model component and a neural network component. The wide component is a generalized linear model to which raw input features and transformed features (such as cross-product transformations) are supplied. The deep component is a feed-forward neural network, which consumes sparse categorical features in embedding vector form. The wide and deep part are combined using a weighted sum of their output log odds as the prediction which is then fed to a common loss function for joint training. The authors claim that wide linear models can effectively memorize sparse feature interactions using cross-product feature transformations, while deep neural networks can generalize to previously unseen feature interactions through low-dimensional embeddings. Note that the wide and deep parts work with two different inputs and the input to the wide part still relies on expertise feature engineering. The model also suffers from the same problem as the linear models like Polynomial Regression that features interactions can not be learned for unobserved cross features. ImplementationPyTorch Code class WideDeep(nn.Module): def __init__(self, wide_dim, n_class, deep_dim, embed_dim, embed_size, pad_idx=0): super(WideDeep, self).__init__() self.embed = nn.Embedding(embed_dim, embed_size, padding_idx=pad_idx, device=device) self.linear_relu_stack = nn.Sequential( nn.Linear(embed_size, 1024, device=device), nn.ReLU(), nn.Linear(1024, 512, device=device), nn.ReLU(), nn.Linear(512, 256, device=device), nn.ReLU() ) self.output = nn.Linear(256+wide_dim, n_class, device=device) def forward(self, X_w, X_sparse_idx): embed = self.embed(X_sparse_idx) embed = torch.mean(embed, dim=1) deep_logits = self.linear_relu_stack(embed) total_logits = self.output(torch.cat((deep_logits, X_w), dim=1)) return total_logits Refer to this Gist for the complete code for this experiment: https://gist.github.com/reachsumit/a6ab97ed6bc053aaf3d73320b4b31b97 You can read more about Wide\u0026Deep in the original paper and the official codebase. ","date":"2022-11-06","objectID":"/posts/2022/11/sparse-recsys/:3:4","series":null,"tags":["recsys","literature review"],"title":"Recommender Systems for Modeling Feature Interactions under Sparse Settings","uri":"/posts/2022/11/sparse-recsys/#4-wide--deep-learning"},{"categories":["Recommender systems"],"content":"\r4. Wide \u0026 Deep LearningThe Wide \u0026 Deep learning framework was proposed by Google to achieve both memorization and generalization in one model, by jointly training a linear model component and a neural network component. The wide component is a generalized linear model to which raw input features and transformed features (such as cross-product transformations) are supplied. The deep component is a feed-forward neural network, which consumes sparse categorical features in embedding vector form. The wide and deep part are combined using a weighted sum of their output log odds as the prediction which is then fed to a common loss function for joint training. The authors claim that wide linear models can effectively memorize sparse feature interactions using cross-product feature transformations, while deep neural networks can generalize to previously unseen feature interactions through low-dimensional embeddings. Note that the wide and deep parts work with two different inputs and the input to the wide part still relies on expertise feature engineering. The model also suffers from the same problem as the linear models like Polynomial Regression that features interactions can not be learned for unobserved cross features. ImplementationPyTorch Code class WideDeep(nn.Module): def __init__(self, wide_dim, n_class, deep_dim, embed_dim, embed_size, pad_idx=0): super(WideDeep, self).__init__() self.embed = nn.Embedding(embed_dim, embed_size, padding_idx=pad_idx, device=device) self.linear_relu_stack = nn.Sequential( nn.Linear(embed_size, 1024, device=device), nn.ReLU(), nn.Linear(1024, 512, device=device), nn.ReLU(), nn.Linear(512, 256, device=device), nn.ReLU() ) self.output = nn.Linear(256+wide_dim, n_class, device=device) def forward(self, X_w, X_sparse_idx): embed = self.embed(X_sparse_idx) embed = torch.mean(embed, dim=1) deep_logits = self.linear_relu_stack(embed) total_logits = self.output(torch.cat((deep_logits, X_w), dim=1)) return total_logits Refer to this Gist for the complete code for this experiment: https://gist.github.com/reachsumit/a6ab97ed6bc053aaf3d73320b4b31b97 You can read more about Wide\u0026Deep in the original paper and the official codebase. ","date":"2022-11-06","objectID":"/posts/2022/11/sparse-recsys/:3:4","series":null,"tags":["recsys","literature review"],"title":"Recommender Systems for Modeling Feature Interactions under Sparse Settings","uri":"/posts/2022/11/sparse-recsys/#implementation-3"},{"categories":["Recommender systems"],"content":"\r5. DeepFMDeepFM model architecture combines the power of FMs and deep learning to overcome the issues with Wide\u0026Deep networks. DeepFM uses a single shared input to its wide and deep parts, with no need of any special feature engineering besides raw features. It models low-order feature interactions like FM and models high-order feature interactions like DNN. ImplementationPyTorch Code class DeepFM(nn.Module): def __init__(self, embed_dim, embed_size, wide_dim, deep_dim, n_class, pad_idx=0): super().__init__() self.embedding = nn.Embedding(embed_dim, embed_size, padding_idx=pad_idx, device=device) self.linear_layer = nn.Linear(wide_dim, n_class, device=device) self.linear_relu_stack = nn.Sequential( nn.Linear(embed_size + deep_dim, 1024, device=device), nn.ReLU(), nn.Linear(1024, 512, device=device), nn.ReLU(), nn.Linear(512, 256, device=device), nn.ReLU() ) self.output = nn.Linear(256+1, n_class, device=device) def forward(self, X_w, X_d, X_sparse_idx): embed_x = self.embedding(X_sparse_idx) embed_x = torch.mean(embed_x, dim=1) # FM square_of_sum = torch.sum(embed_x, dim=1) ** 2 sum_of_square = torch.sum(embed_x ** 2, dim=1) out_inter = 0.5 * (square_of_sum - sum_of_square) # Linear out_lin = self.linear_layer(X_w) # Deep out_deep = self.linear_relu_stack(torch.cat((X_d, embed_x), dim=1)) output = self.output(torch.cat((out_inter.unsqueeze(1), out_deep), dim=1)) + out_lin return output Refer to this Gist for the complete code for this experiment: https://gist.github.com/reachsumit/79237a4de62b4033e2576c55df3dc056 You can read more about DeepFM in the original paper. ","date":"2022-11-06","objectID":"/posts/2022/11/sparse-recsys/:3:5","series":null,"tags":["recsys","literature review"],"title":"Recommender Systems for Modeling Feature Interactions under Sparse Settings","uri":"/posts/2022/11/sparse-recsys/#5-deepfm"},{"categories":["Recommender systems"],"content":"\r5. DeepFMDeepFM model architecture combines the power of FMs and deep learning to overcome the issues with Wide\u0026Deep networks. DeepFM uses a single shared input to its wide and deep parts, with no need of any special feature engineering besides raw features. It models low-order feature interactions like FM and models high-order feature interactions like DNN. ImplementationPyTorch Code class DeepFM(nn.Module): def __init__(self, embed_dim, embed_size, wide_dim, deep_dim, n_class, pad_idx=0): super().__init__() self.embedding = nn.Embedding(embed_dim, embed_size, padding_idx=pad_idx, device=device) self.linear_layer = nn.Linear(wide_dim, n_class, device=device) self.linear_relu_stack = nn.Sequential( nn.Linear(embed_size + deep_dim, 1024, device=device), nn.ReLU(), nn.Linear(1024, 512, device=device), nn.ReLU(), nn.Linear(512, 256, device=device), nn.ReLU() ) self.output = nn.Linear(256+1, n_class, device=device) def forward(self, X_w, X_d, X_sparse_idx): embed_x = self.embedding(X_sparse_idx) embed_x = torch.mean(embed_x, dim=1) # FM square_of_sum = torch.sum(embed_x, dim=1) ** 2 sum_of_square = torch.sum(embed_x ** 2, dim=1) out_inter = 0.5 * (square_of_sum - sum_of_square) # Linear out_lin = self.linear_layer(X_w) # Deep out_deep = self.linear_relu_stack(torch.cat((X_d, embed_x), dim=1)) output = self.output(torch.cat((out_inter.unsqueeze(1), out_deep), dim=1)) + out_lin return output Refer to this Gist for the complete code for this experiment: https://gist.github.com/reachsumit/79237a4de62b4033e2576c55df3dc056 You can read more about DeepFM in the original paper. ","date":"2022-11-06","objectID":"/posts/2022/11/sparse-recsys/:3:5","series":null,"tags":["recsys","literature review"],"title":"Recommender Systems for Modeling Feature Interactions under Sparse Settings","uri":"/posts/2022/11/sparse-recsys/#implementation-4"},{"categories":["Recommender systems"],"content":"\r6. Neural Factorization Machine (NFM)Neural Factorization Machine (NFM) model architecture was also proposed by the authors of the AFM paper with the same goal of overcoming the insufficient linear modeling of feature interactions in FM. After the sparse input and embedding layer, this time the authors propose a Bi-Interaction layer that models the second-order feature interactions. This layer is a pooling layer that converts a set of the embedding vectors set input to one vector by performing an element-wise product of vectors: $\\sum_{i=1}^{n} \\sum_{j=i+1}^{n} x_{i}v_{i} \\odot x_{j}v_{j}$, and passes it on to another set of fully connected layers. ImplementationPyTorch Code class NFM(nn.Module): def __init__(self, embed_dim, embed_size, wide_dim, deep_dim, n_class, pad_idx=0): super().__init__() self.embedding = nn.Embedding(embed_dim, embed_size, padding_idx=pad_idx, device=device) self.linear_layer = nn.Linear(wide_dim, n_class, device=device) self.linear_relu_stack = nn.Sequential( nn.Linear(deep_dim + 1, 1024, device=device), nn.ReLU(), nn.Linear(1024, 512, device=device), nn.ReLU(), nn.Linear(512, n_class, device=device), nn.ReLU() ) def forward(self, X_w, X_d, X_sparse_idx): embed_x = self.embedding(X_sparse_idx) # movies_train_idx embed_x = torch.mean(embed_x, dim=1) # FM square_of_sum = torch.sum(embed_x, dim=1) ** 2 sum_of_square = torch.sum(embed_x ** 2, dim=1) out_inter = 0.5 * (square_of_sum - sum_of_square) # Linear out_lin = self.linear_layer(X_w) # Deep out_deep = self.linear_relu_stack(torch.cat((X_d, out_inter.unsqueeze(1)), dim=1)) output = out_deep + out_lin return output Refer to this Gist for the complete code for this experiment: https://gist.github.com/reachsumit/f29d7001b7687785f33636c9bca302c3 You can read more about NFM in the original paper and the official codebase. ","date":"2022-11-06","objectID":"/posts/2022/11/sparse-recsys/:3:6","series":null,"tags":["recsys","literature review"],"title":"Recommender Systems for Modeling Feature Interactions under Sparse Settings","uri":"/posts/2022/11/sparse-recsys/#6-neural-factorization-machine-nfm"},{"categories":["Recommender systems"],"content":"\r6. Neural Factorization Machine (NFM)Neural Factorization Machine (NFM) model architecture was also proposed by the authors of the AFM paper with the same goal of overcoming the insufficient linear modeling of feature interactions in FM. After the sparse input and embedding layer, this time the authors propose a Bi-Interaction layer that models the second-order feature interactions. This layer is a pooling layer that converts a set of the embedding vectors set input to one vector by performing an element-wise product of vectors: $\\sum_{i=1}^{n} \\sum_{j=i+1}^{n} x_{i}v_{i} \\odot x_{j}v_{j}$, and passes it on to another set of fully connected layers. ImplementationPyTorch Code class NFM(nn.Module): def __init__(self, embed_dim, embed_size, wide_dim, deep_dim, n_class, pad_idx=0): super().__init__() self.embedding = nn.Embedding(embed_dim, embed_size, padding_idx=pad_idx, device=device) self.linear_layer = nn.Linear(wide_dim, n_class, device=device) self.linear_relu_stack = nn.Sequential( nn.Linear(deep_dim + 1, 1024, device=device), nn.ReLU(), nn.Linear(1024, 512, device=device), nn.ReLU(), nn.Linear(512, n_class, device=device), nn.ReLU() ) def forward(self, X_w, X_d, X_sparse_idx): embed_x = self.embedding(X_sparse_idx) # movies_train_idx embed_x = torch.mean(embed_x, dim=1) # FM square_of_sum = torch.sum(embed_x, dim=1) ** 2 sum_of_square = torch.sum(embed_x ** 2, dim=1) out_inter = 0.5 * (square_of_sum - sum_of_square) # Linear out_lin = self.linear_layer(X_w) # Deep out_deep = self.linear_relu_stack(torch.cat((X_d, out_inter.unsqueeze(1)), dim=1)) output = out_deep + out_lin return output Refer to this Gist for the complete code for this experiment: https://gist.github.com/reachsumit/f29d7001b7687785f33636c9bca302c3 You can read more about NFM in the original paper and the official codebase. ","date":"2022-11-06","objectID":"/posts/2022/11/sparse-recsys/:3:6","series":null,"tags":["recsys","literature review"],"title":"Recommender Systems for Modeling Feature Interactions under Sparse Settings","uri":"/posts/2022/11/sparse-recsys/#implementation-5"},{"categories":["Recommender systems"],"content":"\r7. Deep Learning Recommendation Model (DLRM)Deep Learning Recommendation Model (DLRM) was proposed by Facebook in 2019. The DLRM architecture can be thought of as a simplified version of DeepFM architecture. DLRM tries to stay away from high-order interactions by not passing the embedded categorical features through an MLP layer. First, it makes the continuous features go through a “bottom” MLP layer such that they have the same length as the embedding vectors. Then, it computes the dot product between all combinations of embedding vectors and the bottom MLP output from the previous step. The dot product output is then concatenated with the bottom MLP output and is passed to a “top” MLP layer to compute the final output. This architecture design is tailored to mimic the way Factorization Machines compute the second-order interactions between the embeddings, and the paper also says: We argue that higher-order interactions beyond second-order found in other networks may not necessarily be worth the additional computational/memory cost. ImplementationThe paper also notes that DLRM contains far more parameters than common models like CNNs, RNNs, GANs, and transformers, making the training time for this model go up to several weeks. They also propose a framework to parallelize DLRM operations. Due to high compute requirements, I couldn’t train the DLRM model myself. DLRM results in this experimentation are with a simplified implementation using a concatenation of bottom MLP output and embedding output, instead of their dot product. PyTorch Code class DLRM(nn.Module): def __init__(self, embed_dim, embed_size, deep_dim, n_fields, n_class, pad_idx=0, interaction_op=\"cat\"): super().__init__() self.embedding = nn.Embedding(embed_dim, embed_size, padding_idx=pad_idx, device=device) self.bottom_mlp_stack = nn.Sequential( nn.Linear(deep_dim, 1024, device=device), nn.ReLU(), nn.Linear(1024, 512, device=device), nn.ReLU(), nn.Linear(512, embed_size, device=device), nn.ReLU() ) self.interaction_op = interaction_op # [\"dot\", \"cat\"] if self.interaction_op == \"dot\": p, q = zip(*list(combinations(range(n_fields), 2))) self.field_p = nn.Parameter(torch.LongTensor(p), requires_grad=False) self.field_q = nn.Parameter(torch.LongTensor(q), requires_grad=False) self.interaction_units = int(n_fields * (n_fields - 1) / 2) self.upper_triange_mask = nn.Parameter(torch.triu(torch.ones(n_fields, n_fields), 1).type(torch.ByteTensor), requires_grad=False) # torchrec style implementation (as an alterante to above) # self.triu_indices: torch.Tensor = torch.triu_indices( # self.n_fields + 1, self.n_fields + 1, offset=1 # ) self.top_input_dim = (n_fields * (n_fields - 1)) // 2 + embed_size elif self.interaction_op == \"cat\": self.top_input_dim = (n_fields+1) * embed_size self.top_mlp_stack = nn.Sequential( nn.Linear(self.top_input_dim, 1024, device=device), nn.ReLU(), nn.Linear(1024, 512, device=device), nn.ReLU(), nn.Linear(512, n_class, device=device), nn.ReLU() ) def forward(self, X_d, X_sparse_idx): embed_x = self.embedding(X_sparse_idx) # movies_train_idx # bottom mlp dense_out = self.bottom_mlp_stack(X_d).unsqueeze(1) feat_emb = torch.cat([embed_x, dense_out], dim=1) # interaction if self.interaction_op == \"dot\": inner_product_matrix = torch.bmm(feat_emb, feat_emb.transpose(1, 2)) flat_upper_triange = torch.masked_select(inner_product_matrix, self.upper_triange_mask) interact_out = flat_upper_triange.view(-1, self.interaction_units) # torchrec style implementation (as an alterante to above) # interactions = torch.bmm( # feat_emb, torch.transpose(feat_emb, 1, 2) # ) # interactions_flat = interactions[:, self.triu_indices[0], self.triu_indices[1]] # interact_out = torch.cat((dense_out, interactions_flat), dim=1) else: interact_out = feat_emb.flatten(start_dim=1) # torch.Size([76228, 11792]) 737*16 # top mlp output = self.top_mlp_stack(interact_out) return output Refer to this Gist for the complete code for this experiment: https://gist.github.com/reachsumit/a0","date":"2022-11-06","objectID":"/posts/2022/11/sparse-recsys/:3:7","series":null,"tags":["recsys","literature review"],"title":"Recommender Systems for Modeling Feature Interactions under Sparse Settings","uri":"/posts/2022/11/sparse-recsys/#7-deep-learning-recommendation-model-dlrm"},{"categories":["Recommender systems"],"content":"\r7. Deep Learning Recommendation Model (DLRM)Deep Learning Recommendation Model (DLRM) was proposed by Facebook in 2019. The DLRM architecture can be thought of as a simplified version of DeepFM architecture. DLRM tries to stay away from high-order interactions by not passing the embedded categorical features through an MLP layer. First, it makes the continuous features go through a “bottom” MLP layer such that they have the same length as the embedding vectors. Then, it computes the dot product between all combinations of embedding vectors and the bottom MLP output from the previous step. The dot product output is then concatenated with the bottom MLP output and is passed to a “top” MLP layer to compute the final output. This architecture design is tailored to mimic the way Factorization Machines compute the second-order interactions between the embeddings, and the paper also says: We argue that higher-order interactions beyond second-order found in other networks may not necessarily be worth the additional computational/memory cost. ImplementationThe paper also notes that DLRM contains far more parameters than common models like CNNs, RNNs, GANs, and transformers, making the training time for this model go up to several weeks. They also propose a framework to parallelize DLRM operations. Due to high compute requirements, I couldn’t train the DLRM model myself. DLRM results in this experimentation are with a simplified implementation using a concatenation of bottom MLP output and embedding output, instead of their dot product. PyTorch Code class DLRM(nn.Module): def __init__(self, embed_dim, embed_size, deep_dim, n_fields, n_class, pad_idx=0, interaction_op=\"cat\"): super().__init__() self.embedding = nn.Embedding(embed_dim, embed_size, padding_idx=pad_idx, device=device) self.bottom_mlp_stack = nn.Sequential( nn.Linear(deep_dim, 1024, device=device), nn.ReLU(), nn.Linear(1024, 512, device=device), nn.ReLU(), nn.Linear(512, embed_size, device=device), nn.ReLU() ) self.interaction_op = interaction_op # [\"dot\", \"cat\"] if self.interaction_op == \"dot\": p, q = zip(*list(combinations(range(n_fields), 2))) self.field_p = nn.Parameter(torch.LongTensor(p), requires_grad=False) self.field_q = nn.Parameter(torch.LongTensor(q), requires_grad=False) self.interaction_units = int(n_fields * (n_fields - 1) / 2) self.upper_triange_mask = nn.Parameter(torch.triu(torch.ones(n_fields, n_fields), 1).type(torch.ByteTensor), requires_grad=False) # torchrec style implementation (as an alterante to above) # self.triu_indices: torch.Tensor = torch.triu_indices( # self.n_fields + 1, self.n_fields + 1, offset=1 # ) self.top_input_dim = (n_fields * (n_fields - 1)) // 2 + embed_size elif self.interaction_op == \"cat\": self.top_input_dim = (n_fields+1) * embed_size self.top_mlp_stack = nn.Sequential( nn.Linear(self.top_input_dim, 1024, device=device), nn.ReLU(), nn.Linear(1024, 512, device=device), nn.ReLU(), nn.Linear(512, n_class, device=device), nn.ReLU() ) def forward(self, X_d, X_sparse_idx): embed_x = self.embedding(X_sparse_idx) # movies_train_idx # bottom mlp dense_out = self.bottom_mlp_stack(X_d).unsqueeze(1) feat_emb = torch.cat([embed_x, dense_out], dim=1) # interaction if self.interaction_op == \"dot\": inner_product_matrix = torch.bmm(feat_emb, feat_emb.transpose(1, 2)) flat_upper_triange = torch.masked_select(inner_product_matrix, self.upper_triange_mask) interact_out = flat_upper_triange.view(-1, self.interaction_units) # torchrec style implementation (as an alterante to above) # interactions = torch.bmm( # feat_emb, torch.transpose(feat_emb, 1, 2) # ) # interactions_flat = interactions[:, self.triu_indices[0], self.triu_indices[1]] # interact_out = torch.cat((dense_out, interactions_flat), dim=1) else: interact_out = feat_emb.flatten(start_dim=1) # torch.Size([76228, 11792]) 737*16 # top mlp output = self.top_mlp_stack(interact_out) return output Refer to this Gist for the complete code for this experiment: https://gist.github.com/reachsumit/a0","date":"2022-11-06","objectID":"/posts/2022/11/sparse-recsys/:3:7","series":null,"tags":["recsys","literature review"],"title":"Recommender Systems for Modeling Feature Interactions under Sparse Settings","uri":"/posts/2022/11/sparse-recsys/#implementation-6"},{"categories":["Recommender systems"],"content":"\rComparing all modelsThe mean rank of the test set examples was computed using each model, and the following chart shows their comparison based on the best rank achieved on the test set. In short, for this experiment: AFM \u003e FFM » DeepFM \u003e DLRM_simplified \u003e Wide \u0026 Deep \u003e NFM \u003e FM. ","date":"2022-11-06","objectID":"/posts/2022/11/sparse-recsys/:3:8","series":null,"tags":["recsys","literature review"],"title":"Recommender Systems for Modeling Feature Interactions under Sparse Settings","uri":"/posts/2022/11/sparse-recsys/#comparing-all-models"},{"categories":["Recommender systems"],"content":"\rSummaryIn this article, we defined the need for modeling feature interactions and then looked at some of the most popular machine learning algorithms designed to estimate feature interactions under sparse settings. We implemented all of the algorithms in Python and compared their results on a toy dataset. ","date":"2022-11-06","objectID":"/posts/2022/11/sparse-recsys/:4:0","series":null,"tags":["recsys","literature review"],"title":"Recommender Systems for Modeling Feature Interactions under Sparse Settings","uri":"/posts/2022/11/sparse-recsys/#summary"},{"categories":["Recommender systems"],"content":"\rReferences Cheng et al. (2016). Wide \u0026 Deep Learning for Recommender Systems. 7-10. 10.1145/2988450.2988454. ↩︎ https://www.kaggle.com/datasets/prajitdatta/movielens-100k-dataset ↩︎ ","date":"2022-11-06","objectID":"/posts/2022/11/sparse-recsys/:5:0","series":null,"tags":["recsys","literature review"],"title":"Recommender Systems for Modeling Feature Interactions under Sparse Settings","uri":"/posts/2022/11/sparse-recsys/#references"},{"categories":["Recommender systems"],"content":"\rDifferent Flavors of RecommendersAt a high-level, Recommender systems work based on two different strategies (or a hybrid of the two) for recommending content. Collaborative Filtering: Algorithms that use usage data, such as explicit or implicit feedback from the user. Content-based Filtering: Algorithms that use content metadata and user profile. For example, a movie can be profiled based on its genre, IMDb ratings, box-office sales, etc., and a user can be profile based on their demographic information or their answers to an onboarding survey. So, while collaborative filtering needs much feedback from the users to work properly, content-based filtering needs good descriptions of the items. ","date":"2022-09-25","objectID":"/posts/2022/09/explicit-implicit-cf/:1:0","series":null,"tags":["recsys","literature review"],"title":"Collaborative Filtering based Recommender Systems for Implicit Feedback Data","uri":"/posts/2022/09/explicit-implicit-cf/#different-flavors-of-recommenders"},{"categories":["Recommender systems"],"content":"\rChallenges With Using Explicit DataBroadly speaking, the usage data can be either an explicit or implicit signal from the user. Explicit feedback is likely the most accurate input for the recommender system because it is pure information provided by the user about their preference for certain content. This feedback is usually collected using controls such as upvote/downvote buttons or star ratings. Despite their flaws, explicit feedback is used by a vast majority of recommender systems probably thanks to the convenience of using such an input. However, explicit feedback is not always available and often lacks nuances. Not everyone who purchases a product on Amazon or watches a movie on Netflix likes to leave feedback, people might rate a product lower because of their bad experience with an Amazon delivery agent, and someone might rate a movie higher not because of the content of the movie but simply because they liked the movie’s cover art. Explicit feedback collection is not even possible for products like movies or shows broadcasted on television. Similarly, a platform may not have a feedback system with 0 or negative ratings, so if you don’t like a product, the lowest rating you can give to it might still be a 1-star. This dilutes how strongly you may dislike the product, and for the recommender system, it means that the dataset lacks substantial evidence on which products consumers dislike. Despite their flaws, explicit feedback is used by a large number of recommender systems probably thanks to the convenience of using such an input. The missing information is simply considered as empty cells in a sparse matrix input and is omitted from the analysis. Using explicit signals requires additional boosting algorithms to get user feedback on new content because collaborative filtering will not be able to recommend the new content until someone rates it (also known as the cold start problem). ","date":"2022-09-25","objectID":"/posts/2022/09/explicit-implicit-cf/:2:0","series":null,"tags":["recsys","literature review"],"title":"Collaborative Filtering based Recommender Systems for Implicit Feedback Data","uri":"/posts/2022/09/explicit-implicit-cf/#challenges-with-using-explicit-data"},{"categories":["Recommender systems"],"content":"\rBuilding a Recommender System Using Explicit FeedbackBefore we talk about using implicit signals, let’s implement a collaborative filtering based recommender system using explicit signals. We will implement a matrix factorization based algorithm using NumPy’s SVD 1 on MovieLens 100K dataset 2. This dataset contains 100,000 ratings given by 943 users for 1682 movies, with each user having rated at least 20 movies. ","date":"2022-09-25","objectID":"/posts/2022/09/explicit-implicit-cf/:3:0","series":null,"tags":["recsys","literature review"],"title":"Collaborative Filtering based Recommender Systems for Implicit Feedback Data","uri":"/posts/2022/09/explicit-implicit-cf/#building-a-recommender-system-using-explicit-feedback"},{"categories":["Recommender systems"],"content":"\rQuick Notes on How Matrix Factorization WorksWe start with a U x M rating matrix (943x1682 for our dataset) where each value $r_{ui}$ represents the rating given by user u for a movie i. We then divide this matrix into two much smaller matrices such that we approximately get the same U x M matrix back if we multiply the two new matrices together. Basically, we compress a high-dimensional sparse input matrix to a low-dimensional dense matrix space because this generalization leads to a better understanding of the data. Refer to Yehuda Koren’s paper to read more about how the matrix factorization technique is used for recommender systems 3. To learn how SVD works, refer to Jeremy Kun’s article on Math ∩ Programming 4. By splitting this matrix into lower dimensions (factors), we represent each user as well as each movie by a 50-dimensional dense vector. To recommend movies similar to a given movie, we can compute the dot product (cosine similarity) between that movie and other movies and take the closest candidates as the output. Alternatively, we can compute the dot product of a user vector u and an item vector i to get the predicted score and sort all unrated items’ predicted score for a given user to get the list of recommended items for a user. ","date":"2022-09-25","objectID":"/posts/2022/09/explicit-implicit-cf/:3:1","series":null,"tags":["recsys","literature review"],"title":"Collaborative Filtering based Recommender Systems for Implicit Feedback Data","uri":"/posts/2022/09/explicit-implicit-cf/#quick-notes-on-how-matrix-factorization-works"},{"categories":["Recommender systems"],"content":"\rPython Implementation import numpy as np import pandas as pd from numpy import bincount, log, log1p from scipy.sparse import coo_matrix, linalg class ExplicitCF: def __init__(self): self.df = pd.read_csv(\"ml-100k/u.data\", sep='\\t', header=None, names=['user', 'item', 'rating'], usecols=range(3)) self.df['user'] = self.df['user'].astype(\"category\") self.df['item'] = self.df['item'].astype(\"category\") self.df.dropna(inplace=True) self.rating_matrix = coo_matrix((self.df['rating'].astype(float), (self.df['item'].cat.codes, self.df['user'].cat.codes))) def _bm25_weight(self, X, K1=100, B=0.8): \"\"\"Weighs each row of a sparse matrix X by BM25 weighting\"\"\" # calculate idf per term (user) X = coo_matrix(X) N = float(X.shape[0]) idf = log(N) - log1p(bincount(X.col)) # calculate length_norm per document (artist) row_sums = np.ravel(X.sum(axis=1)) average_length = row_sums.mean() length_norm = (1.0 - B) + B * row_sums / average_length # weight matrix rows by bm25 X.data = X.data * (K1 + 1.0) / (K1 * length_norm[X.row] + X.data) * idf[X.col] return X def factorize(self): item_factor, _, user_factor = linalg.svds(self._bm25_weight(self.rating_matrix), 50) return item_factor, user_factor def init_predict(self, x_factors): # fully normalize factors, so can compare with only the dot product norms = np.linalg.norm(x_factors, axis=-1) self.factors = x_factors / norms[:, np.newaxis] def get_related(self, x_id, N=5): scores = self.factors.dot(self.factors[x_id]) best = np.argpartition(scores, -N)[-N:] print(\"Recommendations:\") for _id, score in sorted(zip(best, scores[best]), key=lambda x: -x[1]): print(f\"item id: {_id}, score: {score}\") cf_object = ExplicitCF() print(cf_object.df.head()) # user item rating #0 196 242 3 #1 186 302 3 #2 22 377 1 #3 244 51 2 #4 166 346 1 print(cf_object.df.user.nunique()) # 943 print(cf_object.df.item.nunique()) # 1682 print(cf_object.df.rating.describe()) #count 100000.000000 #mean 3.529860 #std 1.125674 #min 1.000000 #25% 3.000000 #50% 4.000000 #75% 4.000000 #max 5.000000 #Name: rating, dtype: float64 print(cf_object.rating_matrix.shape) # (1682, 943) item_factor, user_factor = cf_object.factorize() print(item_factor.shape) # (1682, 50) print(user_factor.shape) # (50, 943) cf_object.init_predict(item_factor) print(cf_object.factors.shape) # (1682, 50) cf_object.get_related(314) #Recommendations: #item id: 314, score: 1.0 #item id: 315, score: 0.8940031189407059 #item id: 346, score: 0.8509562164687848 #item id: 271, score: 0.8441764974934266 #item id: 312, score: 0.7475076699852435 Code is also available on this GitHub Gist. ","date":"2022-09-25","objectID":"/posts/2022/09/explicit-implicit-cf/:3:2","series":null,"tags":["recsys","literature review"],"title":"Collaborative Filtering based Recommender Systems for Implicit Feedback Data","uri":"/posts/2022/09/explicit-implicit-cf/#python-implementation"},{"categories":["Recommender systems"],"content":"\rChallenges With Using Implicit DataImplicit user feedback includes purchase history, browsing history, search patterns, impressions, clicks, or even mouse movements. While with explicit feedback, the user specifies the degree of positive or negative preference for an item, we do not have the same symmetry with implicit feedback. For example, a user might not have read a book either because they didn’t like the book or they didn’t know about the book or the book wasn’t available in their region, or maybe its cost was prohibitive for the user. Focusing on only the gathered implicit signals means that we could end up focusing heavily on positive signals. So we also have to address the missing data because a lot of negative feedback might exist there. This unfortunately also means that we may no longer have a sparse matrix and we may not be able to use compute-efficient sparsity-based algorithmic optimizations. Implicit feedback is also inherently noisy. A user might have purchased an item in the past only to give it away as a gift, someone might have “watched” a 2+ hour long movie in a theatre, but they might have been asleep throughout it, a user might have missed watching a tv show because there was a live sports event at the same time. ","date":"2022-09-25","objectID":"/posts/2022/09/explicit-implicit-cf/:4:0","series":null,"tags":["recsys","literature review"],"title":"Collaborative Filtering based Recommender Systems for Implicit Feedback Data","uri":"/posts/2022/09/explicit-implicit-cf/#challenges-with-using-implicit-data"},{"categories":["Recommender systems"],"content":"\rPreference vs ConfidenceOne way to think about the difference between the two feedback types is that the explicit feedback indicates user preference, whereas the implicit feedback numerical value indicates confidence. For example, using the count of the number of times the user has watched a show does not indicates a higher preference, but it tells us about the confidence we have in a certain observation. A one-time event might be caused by various reasons that have nothing to do with user preferences. However, a recurring event is more likely to reflect the user’s opinion. ","date":"2022-09-25","objectID":"/posts/2022/09/explicit-implicit-cf/:4:1","series":null,"tags":["recsys","literature review"],"title":"Collaborative Filtering based Recommender Systems for Implicit Feedback Data","uri":"/posts/2022/09/explicit-implicit-cf/#preference-vs-confidence"},{"categories":["Recommender systems"],"content":"\rBuilding a Recommender System Using Implicit FeedbackThe most common collaborative filtering algorithms are either neighborhood-based or latent factor models. Neighborhood models, such as clustering or top-N, find the closest users or closest items to generate recommendations. But with implicit data, they do not make a distinction between user preferences and the confidence we might have in those preferences. Factorization models, uncover latent features that explain observed ratings. We already saw an example of a factorization method using explicit data in the previous section. Most of the research in this domain use explicit feedback datasets and model the observed ratings directly as shown below: Parameters in the above equation are often learned through stochastic gradient descent. Work done by Hu et. al divides raw observation $r_{ui}$ into two separate magnitudes with distinct interpretations: preference ($p_{ui}$) and confidence levels ($c_{ui}$). Hu et. al.5 in their seminal paper (Winner of the 2017 IEEE ICDM 10-Year Highest-Impact Paper Award) described an implicit feedback based factorization model idea that gained a lot of popularity. First, they formalized the notion of preference and confidence. Preference $p_{ui}$ is indicated using a binary variable that is 1 for all non-zero and positive $r_{ui}$ values, and is 0 otherwise (when u never consumed i). $p_{ui} = \\begin{cases} 1 \u0026 r_{ui}\u003e0\\ 0 \u0026 r_{ui}=0 \\end{cases}$ $r_{ui}$ indicates the observations for user actions, for example $r_{ui}$ = 0.7 could indicate that the user u watched 70% of the movie i. If $r_{ui}$ grows, so does our confidence that the user indeed likes the item. Hence the confidence $c_{ui}$ in observing $p_{ui}$ is defined as $1 + \\alpha r_{ui}$. This varying level of confidence gracefully handles the noisy implicit feedback issues described previously. Also, the missing values ($p_{ui} = 0$) will have low corresponding confidence. Similar to the least-squares model for explicit feedback, our goal is to find a vector $x_{u}$ (user factor) for each user u and vector $y_{i}$ (item factor) for each item i that will factor respective user and item preferences. In other words, preferences are assumed to be the inner products: $p_{ui} = x_{u}^{T}y_{i}$. Hence the cost function becomes: Note that the cost function now contains $m \\times n$ terms where m and n are the dimensions of the rating matrix. As this number can easily reach a few billion, the author proposed a clever modification to the alternating least squares method (as opposed to using stochastic gradient descent). The algorithm alternates between re-computing user factors and item factors and each step is guaranteed to lower the value of the cost function. Basically $x_{u}$ can be minimized to: $x_{u} = (Y^{T} C^{u}Y + \\lambda I)^{-1} Y^{T} C^{u} p(u)$, which can be further sped up by computing $Y^{T} C^{u}Y$ as $Y^{T}Y + Y^{T}(C^{u}-I)Y$. Similar we calculate $y_{i}$ as $(X^{T} C^{i}X + \\lambda I)^{-1} X^{T} C^{i} p(i)$. A slight modification to this calculation also allows for explainable recommendations. Refer to their paper for further details5. ","date":"2022-09-25","objectID":"/posts/2022/09/explicit-implicit-cf/:5:0","series":null,"tags":["recsys","literature review"],"title":"Collaborative Filtering based Recommender Systems for Implicit Feedback Data","uri":"/posts/2022/09/explicit-implicit-cf/#building-a-recommender-system-using-implicit-feedback"},{"categories":["Recommender systems"],"content":"\rImplementing Implicit Feedback Recommender in PythonTo implement a recommender based on the above idea, we will use the Last.fm 360K dataset 6, and the Python implementation based on Ben Frederickson’s implicit Python package7. import numpy as np import pandas as pd from numpy import bincount, log, log1p from scipy.sparse import coo_matrix, linalg class ImplicitCF: def __init__(self): self.df = pd.read_csv(\"lastfm-dataset-360K/usersha1-artmbid-artname-plays.tsv\", sep='\\t', header=None, names=['user', 'artist', 'plays'], usecols=[0,2,3]) self.df['user'] = self.df['user'].astype(\"category\") self.df['artist'] = self.df['artist'].astype(\"category\") self.df.dropna(inplace=True) self.plays = coo_matrix((self.df['plays'].astype(float), (self.df['artist'].cat.codes, self.df['user'].cat.codes))) def _bm25_weight(self, X, K1=100, B=0.8): \"\"\"Weighs each row of a sparse matrix X by BM25 weighting\"\"\" # calculate idf per term (user) X = coo_matrix(X) N = float(X.shape[0]) idf = log(N) - log1p(bincount(X.col)) # calculate length_norm per document (artist) row_sums = np.ravel(X.sum(axis=1)) average_length = row_sums.mean() length_norm = (1.0 - B) + B * row_sums / average_length # weight matrix rows by bm25 X.data = X.data * (K1 + 1.0) / (K1 * length_norm[X.row] + X.data) * idf[X.col] return X def _alternating_least_squares(self, Cui, factors, regularization, iterations=20): artists, users = Cui.shape X = np.random.rand(artists, factors) * 0.01 Y = np.random.rand(users, factors) * 0.01 Ciu = Cui.T.tocsr() for iteration in range(iterations): self._least_squares(Cui, X, Y, regularization) self._least_squares(Ciu, Y, X, regularization) return X, Y def _least_squares(self, Cui, X, Y, regularization): artists, factors = X.shape YtY = Y.T.dot(Y) for u in range(artists): # accumulate YtCuY + regularization * I in A A = YtY + regularization * np.eye(factors) # accumulate YtCuPu in b b = np.zeros(factors) for i in Cui[u,:].indices: confidence = Cui[u,i] factor = Y[i] A += (confidence - 1) * np.outer(factor, factor) b += confidence * factor # Xu = (YtCuY + regularization * I)^-1 (YtCuPu) X[u] = np.linalg.solve(A, b) def factorize(self): artist_factor, user_factor = self._alternating_least_squares(self._bm25_weight(self.plays).tocsr(), 50, 1, 10) return artist_factor, user_factor def init_predict(self, x_factors): # fully normalize factors, so can compare with only the dot product norms = np.linalg.norm(x_factors, axis=-1) self.factors = x_factors / norms[:, np.newaxis] def get_related(self, x_id, N=5): scores = self.factors.dot(self.factors[x_id]) best = np.argpartition(scores, -N)[-N:] print(\"Recommendations:\") for _id, score in sorted(zip(best, scores[best]), key=lambda x: -x[1]): print(f\"artist id: {_id}, artist_name: {self.df.artist[_id]} score: {score:.5f}\") cf_object = ImplicitCF() print(cf_object.df.head()) # user artist plays #0 00000c289a1829a808ac09c00daf10bc3c4e223b betty blowtorch 2137 #1 00000c289a1829a808ac09c00daf10bc3c4e223b die Ärzte 1099 #2 00000c289a1829a808ac09c00daf10bc3c4e223b melissa etheridge 897 #3 00000c289a1829a808ac09c00daf10bc3c4e223b elvenking 717 #4 00000c289a1829a808ac09c00daf10bc3c4e223b juliette \u0026 the licks 706 print(cf_object.df.user.nunique()) # 358868 print(cf_object.df.artist.nunique()) # 292364 print(cf_object.df.plays.describe()) #count 1.753565e+07 #mean 2.151932e+02 #std 6.144815e+02 #min 0.000000e+00 #25% 3.500000e+01 #50% 9.400000e+01 #75% 2.240000e+02 #max 4.191570e+05 #Name: plays, dtype: float64 print(cf_object.plays.shape) # (292364, 358868) artist_factor, user_factor = cf_object.factorize() print(artist_factor.shape) # (292364, 50) print(user_factor.shape) # (358868, 50) cf_object.init_predict(artist_factor) print(cf_object.factors.shape) # (292364, 50) cf_object.get_related(2170) #Recommendations: #artist id: 2170, artist_name: maroon 5 score: 1.00000 #artist id: 170436, artist_name: vilma palma e vampiros score: 1.00000 #artist id: 257, artist_name: the beatles score: 1.00000 #artist id: 24297, artist_","date":"2022-09-25","objectID":"/posts/2022/09/explicit-implicit-cf/:5:1","series":null,"tags":["recsys","literature review"],"title":"Collaborative Filtering based Recommender Systems for Implicit Feedback Data","uri":"/posts/2022/09/explicit-implicit-cf/#implementing-implicit-feedback-recommender-in-python"},{"categories":["Recommender systems"],"content":"\rSummaryIn this article, I discussed defining characteristics of explicit and implicit feedback along with their respective shortcomings. Next, I went over one of the most popular research on a factor model which is specially tailored for implicit feedback recommenders. We also implemented factorization-based recommender systems in Python for both explicit and implicit datasets. ","date":"2022-09-25","objectID":"/posts/2022/09/explicit-implicit-cf/:6:0","series":null,"tags":["recsys","literature review"],"title":"Collaborative Filtering based Recommender Systems for Implicit Feedback Data","uri":"/posts/2022/09/explicit-implicit-cf/#summary"},{"categories":["Recommender systems"],"content":"\rReferences https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html ↩︎ https://grouplens.org/datasets/movielens/100k/ ↩︎ Y. Koren, R. Bell and C. Volinsky, “Matrix Factorization Techniques for Recommender Systems,” in Computer, vol. 42, no. 8, pp. 30-37, Aug. 2009, doi: 10.1109/MC.2009.263. https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf ↩︎ https://jeremykun.com/2016/04/18/singular-value-decomposition-part-1-perspectives-on-linear-algebra/ ↩︎ Hu, Yifan \u0026 Koren, Yehuda \u0026 Volinsky, Chris. (2008). Collaborative Filtering for Implicit Feedback Datasets. Proceedings - IEEE International Conference on Data Mining, ICDM. 263-272. 10.1109/ICDM.2008.22. ↩︎ ↩︎ https://www.upf.edu/web/mtg/lastfm360k ↩︎ https://www.benfrederickson.com/matrix-factorization/ ↩︎ ","date":"2022-09-25","objectID":"/posts/2022/09/explicit-implicit-cf/:7:0","series":null,"tags":["recsys","literature review"],"title":"Collaborative Filtering based Recommender Systems for Implicit Feedback Data","uri":"/posts/2022/09/explicit-implicit-cf/#references"},{"categories":["Software Engineering"],"content":"In the last article, I did a literature review defining the origin of SQL, NoSQL, and NewSQL. I also went over the important theorems and properties that could help in the categorization and comparison of the different types of databases. If you haven’t read that article yet, I highly recommend you to go over it: SQL vs NoSQL vs NewSQL: An In-depth Literature Review. In this article, we will build upon those concepts and learn how to categorize NoSQL databases based on the type of data we intend to store. ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:0:0","series":null,"tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/#"},{"categories":["Software Engineering"],"content":"\rTackling the Data VarietyVariety is one of defining aspects, the 7 Vs, of Big Data. It refers to the diverse, complex, and unstructured nature of the data generated by new channels and emerging technologies, such as social media, IoT, mobile devices, and online advertising. A majority of these processes generate semi-structured or unstructured data. This includes tabular data (databases), hierarchical data, documents, XML, emails, blogs, instant messaging, click streams, log files, data metering, images, audio, video, and information about share rates (stock ticker), financial transactions, etc 1. NoSQL provides a simple and flexible technology for the storage and retrieval of such data. This new DBMS drops the structure, entity relationships, and most of the other principles of the ACID (Atomicity, Consistency, Isolation, Durability) model. Instead, it proposes a more flexible model named Basic Availability Soft State Eventual Consistency (BASE). ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:1:0","series":null,"tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/#tackling-the-data-variety"},{"categories":["Software Engineering"],"content":"\rNoSQL Database ModelsThe most commonly employed distinction between NoSQL databases is the way they store and allow access to data. While Relational databases require all data to neatly fit into tables, NoSQL offers different database solutions for specific contexts without having to use a specific schema. The “one size fits all” approach of relational databases no longer applies. According to the classification on the NoSQL database official website 2, there are 15 categories of NoSQL databases. This classification is based on the logical organization of the data (also called data models), meaning the way the data are stored in the database. Its query model specifies how to retrieve and update the data. There are hundreds of readily available NoSQL databases, and each has different use case scenarios. They are usually divided into four categories, according to their data model and storage: Key-Value Stores, Document Stores, Column Stores, and Graph databases. ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:2:0","series":null,"tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/#nosql-database-models"},{"categories":["Software Engineering"],"content":"\rKey-Value StoresAs the name suggests a key-value store consists of a set of key-value pairs with unique keys. The data is stored and accessed using keys and almost all queries are key lookup based. Key-value stores do not assume the structure of the stored data, meaning that both the key and the value can be of any structure and NoSQL has no information on the data; it just delivers the data based on the key. If the use-case requires any assumptions about the structure of the stored data, it has to be implicitly encoded in the application logic (i.e. schema-on-read), and not explicitly defined through a data definition language (i.e. schema-on-write) 3. These stores do not support operations beyond the get-put operations-based CRUD (Create, Read, Update, Delete), and their model can be likened to a distributed hash table. They also usually do not support composite keys and secondary indexes. A logical data structure that can contain any number of key-value pairs is called a namespace. Key-value stores commonly apply a hashing function to the key to obtain a specific node of the physical network where the value will be finally stored. To allow easy addition and/or removal of nodes, several databases take advantage of the concept of Consistent Hashing. Amazon’s DynamoDB made further improvements in this partitioning scheme by introducing virtual nodes. Similar to Consistent Hashing, some databases like Membase/CouchBase use Virtual Buckets or vBuckets to overcome the problem of redistributing keys when a node is added or removed. Figure: A simple key-value store example for serving static web content. Key-value stores can be categorized into three types in terms of storage options: temporary, permanent, and hybrid stores. In temporary stores, all the data are stored in memory, hence the access to data is fast. However, data will be lost if the system is down. Permanent key-value stores ensure the high availability of data by storing the data on the hard disk but with the price of the lower speed of I/O operations on the hard disk. Hybrid approaches combine the best of both temporary and permanent types by storing the data into memory and then writing the input to the hard disk when a set of specified conditions are met 4. Some of the in-memory key-value systems are Redis, Ehcache, Aerospike, and Tarantool. Persistent key-value systems include RocksDB, and LevelDB 5. Strengths of Key-Value Stores The simplicity of such a system makes it attractive in certain circumstances. For example, resource-efficient key-value stores are often applied in embedded systems or as high-performance in-process databases They are horizontally scalable. Partitioning and data querying is easy because of their simple abstraction. They provide low latency and high throughput. The time complexity of its data access is O(1) They can be optimized for reads or writes. Weaknesses of Key-Value Stores They are not powerful enough if an application requires complex operations such as range querying. KV Stores needs to return the entire value when accessing a key because the stored data is schemaless. In-place updates are generally not supported. It is hard for these stores to support reporting, analytics, aggregation, or ordered values. Example Applications of Key-value StoresKey-value stores are generally good solutions if you have a simple application with only one kind of object, and you only need to look up objects based on one attribute 4. Some examples of use-cases for KV stores are listed below 6. Data storage with simple querying needs Profiles, user preferences Data from a cart of purchases Sensor data, logs Data cache for rarely updated data Large-scale sessions management for users ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:2:1","series":null,"tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/#key-value-stores"},{"categories":["Software Engineering"],"content":"\rKey-Value StoresAs the name suggests a key-value store consists of a set of key-value pairs with unique keys. The data is stored and accessed using keys and almost all queries are key lookup based. Key-value stores do not assume the structure of the stored data, meaning that both the key and the value can be of any structure and NoSQL has no information on the data; it just delivers the data based on the key. If the use-case requires any assumptions about the structure of the stored data, it has to be implicitly encoded in the application logic (i.e. schema-on-read), and not explicitly defined through a data definition language (i.e. schema-on-write) 3. These stores do not support operations beyond the get-put operations-based CRUD (Create, Read, Update, Delete), and their model can be likened to a distributed hash table. They also usually do not support composite keys and secondary indexes. A logical data structure that can contain any number of key-value pairs is called a namespace. Key-value stores commonly apply a hashing function to the key to obtain a specific node of the physical network where the value will be finally stored. To allow easy addition and/or removal of nodes, several databases take advantage of the concept of Consistent Hashing. Amazon’s DynamoDB made further improvements in this partitioning scheme by introducing virtual nodes. Similar to Consistent Hashing, some databases like Membase/CouchBase use Virtual Buckets or vBuckets to overcome the problem of redistributing keys when a node is added or removed. Figure: A simple key-value store example for serving static web content. Key-value stores can be categorized into three types in terms of storage options: temporary, permanent, and hybrid stores. In temporary stores, all the data are stored in memory, hence the access to data is fast. However, data will be lost if the system is down. Permanent key-value stores ensure the high availability of data by storing the data on the hard disk but with the price of the lower speed of I/O operations on the hard disk. Hybrid approaches combine the best of both temporary and permanent types by storing the data into memory and then writing the input to the hard disk when a set of specified conditions are met 4. Some of the in-memory key-value systems are Redis, Ehcache, Aerospike, and Tarantool. Persistent key-value systems include RocksDB, and LevelDB 5. Strengths of Key-Value Stores The simplicity of such a system makes it attractive in certain circumstances. For example, resource-efficient key-value stores are often applied in embedded systems or as high-performance in-process databases They are horizontally scalable. Partitioning and data querying is easy because of their simple abstraction. They provide low latency and high throughput. The time complexity of its data access is O(1) They can be optimized for reads or writes. Weaknesses of Key-Value Stores They are not powerful enough if an application requires complex operations such as range querying. KV Stores needs to return the entire value when accessing a key because the stored data is schemaless. In-place updates are generally not supported. It is hard for these stores to support reporting, analytics, aggregation, or ordered values. Example Applications of Key-value StoresKey-value stores are generally good solutions if you have a simple application with only one kind of object, and you only need to look up objects based on one attribute 4. Some examples of use-cases for KV stores are listed below 6. Data storage with simple querying needs Profiles, user preferences Data from a cart of purchases Sensor data, logs Data cache for rarely updated data Large-scale sessions management for users ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:2:1","series":null,"tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/#strengths-of-key-value-stores"},{"categories":["Software Engineering"],"content":"\rKey-Value StoresAs the name suggests a key-value store consists of a set of key-value pairs with unique keys. The data is stored and accessed using keys and almost all queries are key lookup based. Key-value stores do not assume the structure of the stored data, meaning that both the key and the value can be of any structure and NoSQL has no information on the data; it just delivers the data based on the key. If the use-case requires any assumptions about the structure of the stored data, it has to be implicitly encoded in the application logic (i.e. schema-on-read), and not explicitly defined through a data definition language (i.e. schema-on-write) 3. These stores do not support operations beyond the get-put operations-based CRUD (Create, Read, Update, Delete), and their model can be likened to a distributed hash table. They also usually do not support composite keys and secondary indexes. A logical data structure that can contain any number of key-value pairs is called a namespace. Key-value stores commonly apply a hashing function to the key to obtain a specific node of the physical network where the value will be finally stored. To allow easy addition and/or removal of nodes, several databases take advantage of the concept of Consistent Hashing. Amazon’s DynamoDB made further improvements in this partitioning scheme by introducing virtual nodes. Similar to Consistent Hashing, some databases like Membase/CouchBase use Virtual Buckets or vBuckets to overcome the problem of redistributing keys when a node is added or removed. Figure: A simple key-value store example for serving static web content. Key-value stores can be categorized into three types in terms of storage options: temporary, permanent, and hybrid stores. In temporary stores, all the data are stored in memory, hence the access to data is fast. However, data will be lost if the system is down. Permanent key-value stores ensure the high availability of data by storing the data on the hard disk but with the price of the lower speed of I/O operations on the hard disk. Hybrid approaches combine the best of both temporary and permanent types by storing the data into memory and then writing the input to the hard disk when a set of specified conditions are met 4. Some of the in-memory key-value systems are Redis, Ehcache, Aerospike, and Tarantool. Persistent key-value systems include RocksDB, and LevelDB 5. Strengths of Key-Value Stores The simplicity of such a system makes it attractive in certain circumstances. For example, resource-efficient key-value stores are often applied in embedded systems or as high-performance in-process databases They are horizontally scalable. Partitioning and data querying is easy because of their simple abstraction. They provide low latency and high throughput. The time complexity of its data access is O(1) They can be optimized for reads or writes. Weaknesses of Key-Value Stores They are not powerful enough if an application requires complex operations such as range querying. KV Stores needs to return the entire value when accessing a key because the stored data is schemaless. In-place updates are generally not supported. It is hard for these stores to support reporting, analytics, aggregation, or ordered values. Example Applications of Key-value StoresKey-value stores are generally good solutions if you have a simple application with only one kind of object, and you only need to look up objects based on one attribute 4. Some examples of use-cases for KV stores are listed below 6. Data storage with simple querying needs Profiles, user preferences Data from a cart of purchases Sensor data, logs Data cache for rarely updated data Large-scale sessions management for users ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:2:1","series":null,"tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/#weaknesses-of-key-value-stores"},{"categories":["Software Engineering"],"content":"\rKey-Value StoresAs the name suggests a key-value store consists of a set of key-value pairs with unique keys. The data is stored and accessed using keys and almost all queries are key lookup based. Key-value stores do not assume the structure of the stored data, meaning that both the key and the value can be of any structure and NoSQL has no information on the data; it just delivers the data based on the key. If the use-case requires any assumptions about the structure of the stored data, it has to be implicitly encoded in the application logic (i.e. schema-on-read), and not explicitly defined through a data definition language (i.e. schema-on-write) 3. These stores do not support operations beyond the get-put operations-based CRUD (Create, Read, Update, Delete), and their model can be likened to a distributed hash table. They also usually do not support composite keys and secondary indexes. A logical data structure that can contain any number of key-value pairs is called a namespace. Key-value stores commonly apply a hashing function to the key to obtain a specific node of the physical network where the value will be finally stored. To allow easy addition and/or removal of nodes, several databases take advantage of the concept of Consistent Hashing. Amazon’s DynamoDB made further improvements in this partitioning scheme by introducing virtual nodes. Similar to Consistent Hashing, some databases like Membase/CouchBase use Virtual Buckets or vBuckets to overcome the problem of redistributing keys when a node is added or removed. Figure: A simple key-value store example for serving static web content. Key-value stores can be categorized into three types in terms of storage options: temporary, permanent, and hybrid stores. In temporary stores, all the data are stored in memory, hence the access to data is fast. However, data will be lost if the system is down. Permanent key-value stores ensure the high availability of data by storing the data on the hard disk but with the price of the lower speed of I/O operations on the hard disk. Hybrid approaches combine the best of both temporary and permanent types by storing the data into memory and then writing the input to the hard disk when a set of specified conditions are met 4. Some of the in-memory key-value systems are Redis, Ehcache, Aerospike, and Tarantool. Persistent key-value systems include RocksDB, and LevelDB 5. Strengths of Key-Value Stores The simplicity of such a system makes it attractive in certain circumstances. For example, resource-efficient key-value stores are often applied in embedded systems or as high-performance in-process databases They are horizontally scalable. Partitioning and data querying is easy because of their simple abstraction. They provide low latency and high throughput. The time complexity of its data access is O(1) They can be optimized for reads or writes. Weaknesses of Key-Value Stores They are not powerful enough if an application requires complex operations such as range querying. KV Stores needs to return the entire value when accessing a key because the stored data is schemaless. In-place updates are generally not supported. It is hard for these stores to support reporting, analytics, aggregation, or ordered values. Example Applications of Key-value StoresKey-value stores are generally good solutions if you have a simple application with only one kind of object, and you only need to look up objects based on one attribute 4. Some examples of use-cases for KV stores are listed below 6. Data storage with simple querying needs Profiles, user preferences Data from a cart of purchases Sensor data, logs Data cache for rarely updated data Large-scale sessions management for users ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:2:1","series":null,"tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/#example-applications-of-key-value-stores"},{"categories":["Software Engineering"],"content":"\rDocument-Oriented StoresDocument stores extend the unstructured storage paradigm of key-value stores and make the values to be semi-structured, specifically attribute name/value pairs, called documents. A document, in this context, is a JSON or JSON-like object (such as XML, YAML, or BSON). This restriction brings flexibility in accessing the data because it is now possible to fetch either an entire document by its key or to only retrieve parts of the document. The document-oriented database still regards the document as a whole, instead of partitioning the document into many key/value pairs. This enables the documents of different structures to be put into the same set. The document-oriented database supports document index, including not only primary identifiers but also document properties 7. Also, unlike key-value stores, both keys and values are fully searchable in document databases. They also usually support complex keys along with secondary indexes. Each document can store hundreds of attributes, and the number and type of attributes stored can vary from document to document (schema-free!). The attribute names are dynamically defined for each document at runtime, and values can be nested documents, or lists, as well as scalar values. Most of the databases available under this category provide data access typically over HTTP protocol using RESTful API or over Apache Thrift protocol for cross-language interoperability. A group of documents is called a collection. The documents within a collection are usually related to the same subject, such as employees, products, and so on. Figure: A simple Document store example for customer data. Document stores make it possible to add and delete value fields, modify certain fields, and query the database by fields. Queries can be done on any field using patterns, so, ranges, logical operators, wildcards, and more, can also be used in queries. The drawback is that for each type of query a new index needs to be created 8. According to db-engines 9, some highly ranked document-oriented systems include MongoDB, Couchbase, CouchDB, Realm, MarkLogic, OrientDB, RavenDB, PouchDB, and RethinkDB. Strengths of Document Stores They tend to support more complex data models than key-value stores. They usually support multiple indexes and range querying. They support data access over RESTful API. Documents can be nested due to the schemaless nature of the store. They typically support low latency reads. Weaknesses of Document Stores Joins are still not available within the database. A query requiring documents from two collections will require two separate queries. A lot of document stores have security concerns in regards to data leakage on web apps. However, this problem is prevalent among other NoSQL models as well. Example Applications of Document StoresA Document-oriented database is useful when the number of fields cannot be fully determined at application design time. Some examples of use-cases for document stores are as follows. Content Management Systems Real-time Web Analytics Products Catalog Blogs Analytical Platforms ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:2:2","series":null,"tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/#document-oriented-stores"},{"categories":["Software Engineering"],"content":"\rDocument-Oriented StoresDocument stores extend the unstructured storage paradigm of key-value stores and make the values to be semi-structured, specifically attribute name/value pairs, called documents. A document, in this context, is a JSON or JSON-like object (such as XML, YAML, or BSON). This restriction brings flexibility in accessing the data because it is now possible to fetch either an entire document by its key or to only retrieve parts of the document. The document-oriented database still regards the document as a whole, instead of partitioning the document into many key/value pairs. This enables the documents of different structures to be put into the same set. The document-oriented database supports document index, including not only primary identifiers but also document properties 7. Also, unlike key-value stores, both keys and values are fully searchable in document databases. They also usually support complex keys along with secondary indexes. Each document can store hundreds of attributes, and the number and type of attributes stored can vary from document to document (schema-free!). The attribute names are dynamically defined for each document at runtime, and values can be nested documents, or lists, as well as scalar values. Most of the databases available under this category provide data access typically over HTTP protocol using RESTful API or over Apache Thrift protocol for cross-language interoperability. A group of documents is called a collection. The documents within a collection are usually related to the same subject, such as employees, products, and so on. Figure: A simple Document store example for customer data. Document stores make it possible to add and delete value fields, modify certain fields, and query the database by fields. Queries can be done on any field using patterns, so, ranges, logical operators, wildcards, and more, can also be used in queries. The drawback is that for each type of query a new index needs to be created 8. According to db-engines 9, some highly ranked document-oriented systems include MongoDB, Couchbase, CouchDB, Realm, MarkLogic, OrientDB, RavenDB, PouchDB, and RethinkDB. Strengths of Document Stores They tend to support more complex data models than key-value stores. They usually support multiple indexes and range querying. They support data access over RESTful API. Documents can be nested due to the schemaless nature of the store. They typically support low latency reads. Weaknesses of Document Stores Joins are still not available within the database. A query requiring documents from two collections will require two separate queries. A lot of document stores have security concerns in regards to data leakage on web apps. However, this problem is prevalent among other NoSQL models as well. Example Applications of Document StoresA Document-oriented database is useful when the number of fields cannot be fully determined at application design time. Some examples of use-cases for document stores are as follows. Content Management Systems Real-time Web Analytics Products Catalog Blogs Analytical Platforms ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:2:2","series":null,"tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/#strengths-of-document-stores"},{"categories":["Software Engineering"],"content":"\rDocument-Oriented StoresDocument stores extend the unstructured storage paradigm of key-value stores and make the values to be semi-structured, specifically attribute name/value pairs, called documents. A document, in this context, is a JSON or JSON-like object (such as XML, YAML, or BSON). This restriction brings flexibility in accessing the data because it is now possible to fetch either an entire document by its key or to only retrieve parts of the document. The document-oriented database still regards the document as a whole, instead of partitioning the document into many key/value pairs. This enables the documents of different structures to be put into the same set. The document-oriented database supports document index, including not only primary identifiers but also document properties 7. Also, unlike key-value stores, both keys and values are fully searchable in document databases. They also usually support complex keys along with secondary indexes. Each document can store hundreds of attributes, and the number and type of attributes stored can vary from document to document (schema-free!). The attribute names are dynamically defined for each document at runtime, and values can be nested documents, or lists, as well as scalar values. Most of the databases available under this category provide data access typically over HTTP protocol using RESTful API or over Apache Thrift protocol for cross-language interoperability. A group of documents is called a collection. The documents within a collection are usually related to the same subject, such as employees, products, and so on. Figure: A simple Document store example for customer data. Document stores make it possible to add and delete value fields, modify certain fields, and query the database by fields. Queries can be done on any field using patterns, so, ranges, logical operators, wildcards, and more, can also be used in queries. The drawback is that for each type of query a new index needs to be created 8. According to db-engines 9, some highly ranked document-oriented systems include MongoDB, Couchbase, CouchDB, Realm, MarkLogic, OrientDB, RavenDB, PouchDB, and RethinkDB. Strengths of Document Stores They tend to support more complex data models than key-value stores. They usually support multiple indexes and range querying. They support data access over RESTful API. Documents can be nested due to the schemaless nature of the store. They typically support low latency reads. Weaknesses of Document Stores Joins are still not available within the database. A query requiring documents from two collections will require two separate queries. A lot of document stores have security concerns in regards to data leakage on web apps. However, this problem is prevalent among other NoSQL models as well. Example Applications of Document StoresA Document-oriented database is useful when the number of fields cannot be fully determined at application design time. Some examples of use-cases for document stores are as follows. Content Management Systems Real-time Web Analytics Products Catalog Blogs Analytical Platforms ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:2:2","series":null,"tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/#weaknesses-of-document-stores"},{"categories":["Software Engineering"],"content":"\rDocument-Oriented StoresDocument stores extend the unstructured storage paradigm of key-value stores and make the values to be semi-structured, specifically attribute name/value pairs, called documents. A document, in this context, is a JSON or JSON-like object (such as XML, YAML, or BSON). This restriction brings flexibility in accessing the data because it is now possible to fetch either an entire document by its key or to only retrieve parts of the document. The document-oriented database still regards the document as a whole, instead of partitioning the document into many key/value pairs. This enables the documents of different structures to be put into the same set. The document-oriented database supports document index, including not only primary identifiers but also document properties 7. Also, unlike key-value stores, both keys and values are fully searchable in document databases. They also usually support complex keys along with secondary indexes. Each document can store hundreds of attributes, and the number and type of attributes stored can vary from document to document (schema-free!). The attribute names are dynamically defined for each document at runtime, and values can be nested documents, or lists, as well as scalar values. Most of the databases available under this category provide data access typically over HTTP protocol using RESTful API or over Apache Thrift protocol for cross-language interoperability. A group of documents is called a collection. The documents within a collection are usually related to the same subject, such as employees, products, and so on. Figure: A simple Document store example for customer data. Document stores make it possible to add and delete value fields, modify certain fields, and query the database by fields. Queries can be done on any field using patterns, so, ranges, logical operators, wildcards, and more, can also be used in queries. The drawback is that for each type of query a new index needs to be created 8. According to db-engines 9, some highly ranked document-oriented systems include MongoDB, Couchbase, CouchDB, Realm, MarkLogic, OrientDB, RavenDB, PouchDB, and RethinkDB. Strengths of Document Stores They tend to support more complex data models than key-value stores. They usually support multiple indexes and range querying. They support data access over RESTful API. Documents can be nested due to the schemaless nature of the store. They typically support low latency reads. Weaknesses of Document Stores Joins are still not available within the database. A query requiring documents from two collections will require two separate queries. A lot of document stores have security concerns in regards to data leakage on web apps. However, this problem is prevalent among other NoSQL models as well. Example Applications of Document StoresA Document-oriented database is useful when the number of fields cannot be fully determined at application design time. Some examples of use-cases for document stores are as follows. Content Management Systems Real-time Web Analytics Products Catalog Blogs Analytical Platforms ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:2:2","series":null,"tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/#example-applications-of-document--stores"},{"categories":["Software Engineering"],"content":"\rColumn-Oriented StoresWhile the relational databases store and process the data with a row as the unit, column-oriented stores process and store the data with a column as the unit. A column-oriented store (also known as extensible record stores, wide-column stores, and column-family stores) stores each row by splitting it vertically and horizontally across the physical nodes of distributed systems. Each row can have one or more column families and is identified by a row key. Each column family can also act as a key to manipulate one or more columns. Technically a wide-column store is closer to a distributed multi-level sorted map (or a two-dimensional key-value store): the first-level keys identify rows that themselves consist of key-value pairs. The first-level keys are called row keys, the second-level keys are called column keys 3. Figure: General structure for Wide-Column databases. A Keyspace is used as the repository for the tables. The row key is similar to the key used in key-value systems for data manipulation. The set of all columns is partitioned into so-called column families to colocate columns on disks that are usually accessed together. Each column family acts as a key to manipulate one or more containing columns. Wide column stores are also called extensible record stores because they store data in records with the ability to hold very large numbers (billions) of columns (schema-free!). These column-wide data management systems are flexible enough to store the data of any format or type. The following figure represents a sample table in a wide-column datastore 4. Figure: An example of a data table in a wide column store. On disk, wide-column stores do not co-locate all data from each row, but instead values of the same column family and from the same row. Hence, an entity (a row) cannot be retrieved by one single lookup as in a document store but has to be joined together from the columns of all column families. However, this storage layout usually enables highly efficient data compression and makes retrieving only a portion of an entity very efficient. According to db-engines 10, some of the column-oriented systems that are ranked high include Cassandra, HBase, Google Cloud Bigtable, Accumulo, and ScyllaDB. Strengths of Column-Oriented Stores Wide Column or Column Families databases store data by columns and do not impose a rigid scheme on user data. This means that some rows may or may not have columns of a certain type. Since data stored by column have the same data type, compression algorithms can be used to decrease the required space. Because there is no column key without a corresponding value, null values can be stored without any space overhead. It is also possible to do functional partitioning per column so that columns that are frequently accessed are placed in the same physical location. The column-oriented database is so scalable that the increase in data size will not result in decreased processing speed. So it is well-suited for processing a huge amount of data. They have better querying capabilities than the key-value stores due to the usage of row-level and column-level keys. Wide-column stores allow different access levels for different columns. Weaknesses of Column-Oriented Stores Complexity in the development of such databases is considerably high. The application domain of Wide Column databases is limited to particular problems: data to be stored need to be structured and potentially reach the order of petabytes, but the search can only be done through the primary key, i.e., the ID of the row. Queries on certain columns are not possible as this would imply having an index on the entire dataset or traversing it 8. Referential integrity and joins are not supported. They are much less efficient when processing many columns simultaneously. Designing an effective indexing schema is difficult and time-consuming. Even then, the said schema would still not be as effective as simple relational data","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:2:3","series":null,"tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/#column-oriented-stores"},{"categories":["Software Engineering"],"content":"\rColumn-Oriented StoresWhile the relational databases store and process the data with a row as the unit, column-oriented stores process and store the data with a column as the unit. A column-oriented store (also known as extensible record stores, wide-column stores, and column-family stores) stores each row by splitting it vertically and horizontally across the physical nodes of distributed systems. Each row can have one or more column families and is identified by a row key. Each column family can also act as a key to manipulate one or more columns. Technically a wide-column store is closer to a distributed multi-level sorted map (or a two-dimensional key-value store): the first-level keys identify rows that themselves consist of key-value pairs. The first-level keys are called row keys, the second-level keys are called column keys 3. Figure: General structure for Wide-Column databases. A Keyspace is used as the repository for the tables. The row key is similar to the key used in key-value systems for data manipulation. The set of all columns is partitioned into so-called column families to colocate columns on disks that are usually accessed together. Each column family acts as a key to manipulate one or more containing columns. Wide column stores are also called extensible record stores because they store data in records with the ability to hold very large numbers (billions) of columns (schema-free!). These column-wide data management systems are flexible enough to store the data of any format or type. The following figure represents a sample table in a wide-column datastore 4. Figure: An example of a data table in a wide column store. On disk, wide-column stores do not co-locate all data from each row, but instead values of the same column family and from the same row. Hence, an entity (a row) cannot be retrieved by one single lookup as in a document store but has to be joined together from the columns of all column families. However, this storage layout usually enables highly efficient data compression and makes retrieving only a portion of an entity very efficient. According to db-engines 10, some of the column-oriented systems that are ranked high include Cassandra, HBase, Google Cloud Bigtable, Accumulo, and ScyllaDB. Strengths of Column-Oriented Stores Wide Column or Column Families databases store data by columns and do not impose a rigid scheme on user data. This means that some rows may or may not have columns of a certain type. Since data stored by column have the same data type, compression algorithms can be used to decrease the required space. Because there is no column key without a corresponding value, null values can be stored without any space overhead. It is also possible to do functional partitioning per column so that columns that are frequently accessed are placed in the same physical location. The column-oriented database is so scalable that the increase in data size will not result in decreased processing speed. So it is well-suited for processing a huge amount of data. They have better querying capabilities than the key-value stores due to the usage of row-level and column-level keys. Wide-column stores allow different access levels for different columns. Weaknesses of Column-Oriented Stores Complexity in the development of such databases is considerably high. The application domain of Wide Column databases is limited to particular problems: data to be stored need to be structured and potentially reach the order of petabytes, but the search can only be done through the primary key, i.e., the ID of the row. Queries on certain columns are not possible as this would imply having an index on the entire dataset or traversing it 8. Referential integrity and joins are not supported. They are much less efficient when processing many columns simultaneously. Designing an effective indexing schema is difficult and time-consuming. Even then, the said schema would still not be as effective as simple relational data","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:2:3","series":null,"tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/#strengths-of-column-oriented-stores"},{"categories":["Software Engineering"],"content":"\rColumn-Oriented StoresWhile the relational databases store and process the data with a row as the unit, column-oriented stores process and store the data with a column as the unit. A column-oriented store (also known as extensible record stores, wide-column stores, and column-family stores) stores each row by splitting it vertically and horizontally across the physical nodes of distributed systems. Each row can have one or more column families and is identified by a row key. Each column family can also act as a key to manipulate one or more columns. Technically a wide-column store is closer to a distributed multi-level sorted map (or a two-dimensional key-value store): the first-level keys identify rows that themselves consist of key-value pairs. The first-level keys are called row keys, the second-level keys are called column keys 3. Figure: General structure for Wide-Column databases. A Keyspace is used as the repository for the tables. The row key is similar to the key used in key-value systems for data manipulation. The set of all columns is partitioned into so-called column families to colocate columns on disks that are usually accessed together. Each column family acts as a key to manipulate one or more containing columns. Wide column stores are also called extensible record stores because they store data in records with the ability to hold very large numbers (billions) of columns (schema-free!). These column-wide data management systems are flexible enough to store the data of any format or type. The following figure represents a sample table in a wide-column datastore 4. Figure: An example of a data table in a wide column store. On disk, wide-column stores do not co-locate all data from each row, but instead values of the same column family and from the same row. Hence, an entity (a row) cannot be retrieved by one single lookup as in a document store but has to be joined together from the columns of all column families. However, this storage layout usually enables highly efficient data compression and makes retrieving only a portion of an entity very efficient. According to db-engines 10, some of the column-oriented systems that are ranked high include Cassandra, HBase, Google Cloud Bigtable, Accumulo, and ScyllaDB. Strengths of Column-Oriented Stores Wide Column or Column Families databases store data by columns and do not impose a rigid scheme on user data. This means that some rows may or may not have columns of a certain type. Since data stored by column have the same data type, compression algorithms can be used to decrease the required space. Because there is no column key without a corresponding value, null values can be stored without any space overhead. It is also possible to do functional partitioning per column so that columns that are frequently accessed are placed in the same physical location. The column-oriented database is so scalable that the increase in data size will not result in decreased processing speed. So it is well-suited for processing a huge amount of data. They have better querying capabilities than the key-value stores due to the usage of row-level and column-level keys. Wide-column stores allow different access levels for different columns. Weaknesses of Column-Oriented Stores Complexity in the development of such databases is considerably high. The application domain of Wide Column databases is limited to particular problems: data to be stored need to be structured and potentially reach the order of petabytes, but the search can only be done through the primary key, i.e., the ID of the row. Queries on certain columns are not possible as this would imply having an index on the entire dataset or traversing it 8. Referential integrity and joins are not supported. They are much less efficient when processing many columns simultaneously. Designing an effective indexing schema is difficult and time-consuming. Even then, the said schema would still not be as effective as simple relational data","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:2:3","series":null,"tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/#weaknesses-of-column-oriented-stores"},{"categories":["Software Engineering"],"content":"\rColumn-Oriented StoresWhile the relational databases store and process the data with a row as the unit, column-oriented stores process and store the data with a column as the unit. A column-oriented store (also known as extensible record stores, wide-column stores, and column-family stores) stores each row by splitting it vertically and horizontally across the physical nodes of distributed systems. Each row can have one or more column families and is identified by a row key. Each column family can also act as a key to manipulate one or more columns. Technically a wide-column store is closer to a distributed multi-level sorted map (or a two-dimensional key-value store): the first-level keys identify rows that themselves consist of key-value pairs. The first-level keys are called row keys, the second-level keys are called column keys 3. Figure: General structure for Wide-Column databases. A Keyspace is used as the repository for the tables. The row key is similar to the key used in key-value systems for data manipulation. The set of all columns is partitioned into so-called column families to colocate columns on disks that are usually accessed together. Each column family acts as a key to manipulate one or more containing columns. Wide column stores are also called extensible record stores because they store data in records with the ability to hold very large numbers (billions) of columns (schema-free!). These column-wide data management systems are flexible enough to store the data of any format or type. The following figure represents a sample table in a wide-column datastore 4. Figure: An example of a data table in a wide column store. On disk, wide-column stores do not co-locate all data from each row, but instead values of the same column family and from the same row. Hence, an entity (a row) cannot be retrieved by one single lookup as in a document store but has to be joined together from the columns of all column families. However, this storage layout usually enables highly efficient data compression and makes retrieving only a portion of an entity very efficient. According to db-engines 10, some of the column-oriented systems that are ranked high include Cassandra, HBase, Google Cloud Bigtable, Accumulo, and ScyllaDB. Strengths of Column-Oriented Stores Wide Column or Column Families databases store data by columns and do not impose a rigid scheme on user data. This means that some rows may or may not have columns of a certain type. Since data stored by column have the same data type, compression algorithms can be used to decrease the required space. Because there is no column key without a corresponding value, null values can be stored without any space overhead. It is also possible to do functional partitioning per column so that columns that are frequently accessed are placed in the same physical location. The column-oriented database is so scalable that the increase in data size will not result in decreased processing speed. So it is well-suited for processing a huge amount of data. They have better querying capabilities than the key-value stores due to the usage of row-level and column-level keys. Wide-column stores allow different access levels for different columns. Weaknesses of Column-Oriented Stores Complexity in the development of such databases is considerably high. The application domain of Wide Column databases is limited to particular problems: data to be stored need to be structured and potentially reach the order of petabytes, but the search can only be done through the primary key, i.e., the ID of the row. Queries on certain columns are not possible as this would imply having an index on the entire dataset or traversing it 8. Referential integrity and joins are not supported. They are much less efficient when processing many columns simultaneously. Designing an effective indexing schema is difficult and time-consuming. Even then, the said schema would still not be as effective as simple relational data","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:2:3","series":null,"tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/#example-applications-of-column-oriented-stores"},{"categories":["Software Engineering"],"content":"\rGraph-Oriented StoresGraph-oriented systems are based on graph theory that represents the objects as nodes of the graph and their relationships as the edges. In graph-oriented systems, the main focus is on the relationships, which naturally represent data and their relationships. This reduces the overhead of joins that involves the merging of columns from one or more tables. Instead of joins, each node possesses a record list of the relationships that it holds with the other nodes, eliminating the need for foreign keys and join operations 11. The graph-oriented systems are widely used in applications that embody complex relationships such as recommendation systems and social networking applications. Graph databases help us to implement graph processing requirements in the same query language level as we use for fetching graph data without the extra layer of abstraction for graph nodes and edges. This means less overhead for graph-related processing and more flexibility and performance. Figure: Example of vertex representation in a Graph-oriented database. The above figure shows an example where users in a system may have friendship relationships (i.e., undirected edges) and send messages to each other (i.e., directed edges). Undirected edges are commutative and can be stored once, whereas directed edges are often stored in two different structures to provide faster access for queries in different directions. Additionally, vertices and edges have specific properties associated with them, and thereby, these properties must be stored in separate structures. According to db-engines 12, some of the graph-oriented systems that are ranked high include Neo4j, JanusGraph, TigerGraph, Dgraph, Giraph, Nebula Graph, and FlockDB. Strengths of Graph-Oriented Stores They provide support for graph-based algorithms and queries. For example, deep traversals in graph databased systems are faster than relational databases. Graph-based stores allow for very fast execution of complex pattern-matching queries. They also allow for a compact representation of the data, basing its implementation on bitmaps. Most of them are transactional. Weaknesses of Graph-Oriented Stores Distributed graph processing is challenging. Additionally, a partitioning strategy is difficult to express in source code because the structure of the graph (unlike lists or hashes) is not known a priori. This is one of the greatest pitfalls in the application of MapReduce-based frameworks in graph processing 8. Similar to partitioning, computing locality is also challenging. Traversal algorithms generate a significant communication overhead when the vertices are located in different computing nodes. Most graph-based stores do not have a declarative language. Example Applications of Graph-Oriented Stores Generating recommendations Business Intelligence (BI) Semantic Web, Social networks analysis Geospatial Data, geolocation Genealogy, Web of things, Routing Services Authentication and authorization systems Fraud detection in financial services ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:2:4","series":null,"tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/#graph-oriented-stores"},{"categories":["Software Engineering"],"content":"\rGraph-Oriented StoresGraph-oriented systems are based on graph theory that represents the objects as nodes of the graph and their relationships as the edges. In graph-oriented systems, the main focus is on the relationships, which naturally represent data and their relationships. This reduces the overhead of joins that involves the merging of columns from one or more tables. Instead of joins, each node possesses a record list of the relationships that it holds with the other nodes, eliminating the need for foreign keys and join operations 11. The graph-oriented systems are widely used in applications that embody complex relationships such as recommendation systems and social networking applications. Graph databases help us to implement graph processing requirements in the same query language level as we use for fetching graph data without the extra layer of abstraction for graph nodes and edges. This means less overhead for graph-related processing and more flexibility and performance. Figure: Example of vertex representation in a Graph-oriented database. The above figure shows an example where users in a system may have friendship relationships (i.e., undirected edges) and send messages to each other (i.e., directed edges). Undirected edges are commutative and can be stored once, whereas directed edges are often stored in two different structures to provide faster access for queries in different directions. Additionally, vertices and edges have specific properties associated with them, and thereby, these properties must be stored in separate structures. According to db-engines 12, some of the graph-oriented systems that are ranked high include Neo4j, JanusGraph, TigerGraph, Dgraph, Giraph, Nebula Graph, and FlockDB. Strengths of Graph-Oriented Stores They provide support for graph-based algorithms and queries. For example, deep traversals in graph databased systems are faster than relational databases. Graph-based stores allow for very fast execution of complex pattern-matching queries. They also allow for a compact representation of the data, basing its implementation on bitmaps. Most of them are transactional. Weaknesses of Graph-Oriented Stores Distributed graph processing is challenging. Additionally, a partitioning strategy is difficult to express in source code because the structure of the graph (unlike lists or hashes) is not known a priori. This is one of the greatest pitfalls in the application of MapReduce-based frameworks in graph processing 8. Similar to partitioning, computing locality is also challenging. Traversal algorithms generate a significant communication overhead when the vertices are located in different computing nodes. Most graph-based stores do not have a declarative language. Example Applications of Graph-Oriented Stores Generating recommendations Business Intelligence (BI) Semantic Web, Social networks analysis Geospatial Data, geolocation Genealogy, Web of things, Routing Services Authentication and authorization systems Fraud detection in financial services ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:2:4","series":null,"tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/#strengths-of-graph-oriented-stores"},{"categories":["Software Engineering"],"content":"\rGraph-Oriented StoresGraph-oriented systems are based on graph theory that represents the objects as nodes of the graph and their relationships as the edges. In graph-oriented systems, the main focus is on the relationships, which naturally represent data and their relationships. This reduces the overhead of joins that involves the merging of columns from one or more tables. Instead of joins, each node possesses a record list of the relationships that it holds with the other nodes, eliminating the need for foreign keys and join operations 11. The graph-oriented systems are widely used in applications that embody complex relationships such as recommendation systems and social networking applications. Graph databases help us to implement graph processing requirements in the same query language level as we use for fetching graph data without the extra layer of abstraction for graph nodes and edges. This means less overhead for graph-related processing and more flexibility and performance. Figure: Example of vertex representation in a Graph-oriented database. The above figure shows an example where users in a system may have friendship relationships (i.e., undirected edges) and send messages to each other (i.e., directed edges). Undirected edges are commutative and can be stored once, whereas directed edges are often stored in two different structures to provide faster access for queries in different directions. Additionally, vertices and edges have specific properties associated with them, and thereby, these properties must be stored in separate structures. According to db-engines 12, some of the graph-oriented systems that are ranked high include Neo4j, JanusGraph, TigerGraph, Dgraph, Giraph, Nebula Graph, and FlockDB. Strengths of Graph-Oriented Stores They provide support for graph-based algorithms and queries. For example, deep traversals in graph databased systems are faster than relational databases. Graph-based stores allow for very fast execution of complex pattern-matching queries. They also allow for a compact representation of the data, basing its implementation on bitmaps. Most of them are transactional. Weaknesses of Graph-Oriented Stores Distributed graph processing is challenging. Additionally, a partitioning strategy is difficult to express in source code because the structure of the graph (unlike lists or hashes) is not known a priori. This is one of the greatest pitfalls in the application of MapReduce-based frameworks in graph processing 8. Similar to partitioning, computing locality is also challenging. Traversal algorithms generate a significant communication overhead when the vertices are located in different computing nodes. Most graph-based stores do not have a declarative language. Example Applications of Graph-Oriented Stores Generating recommendations Business Intelligence (BI) Semantic Web, Social networks analysis Geospatial Data, geolocation Genealogy, Web of things, Routing Services Authentication and authorization systems Fraud detection in financial services ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:2:4","series":null,"tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/#weaknesses-of-graph-oriented-stores"},{"categories":["Software Engineering"],"content":"\rGraph-Oriented StoresGraph-oriented systems are based on graph theory that represents the objects as nodes of the graph and their relationships as the edges. In graph-oriented systems, the main focus is on the relationships, which naturally represent data and their relationships. This reduces the overhead of joins that involves the merging of columns from one or more tables. Instead of joins, each node possesses a record list of the relationships that it holds with the other nodes, eliminating the need for foreign keys and join operations 11. The graph-oriented systems are widely used in applications that embody complex relationships such as recommendation systems and social networking applications. Graph databases help us to implement graph processing requirements in the same query language level as we use for fetching graph data without the extra layer of abstraction for graph nodes and edges. This means less overhead for graph-related processing and more flexibility and performance. Figure: Example of vertex representation in a Graph-oriented database. The above figure shows an example where users in a system may have friendship relationships (i.e., undirected edges) and send messages to each other (i.e., directed edges). Undirected edges are commutative and can be stored once, whereas directed edges are often stored in two different structures to provide faster access for queries in different directions. Additionally, vertices and edges have specific properties associated with them, and thereby, these properties must be stored in separate structures. According to db-engines 12, some of the graph-oriented systems that are ranked high include Neo4j, JanusGraph, TigerGraph, Dgraph, Giraph, Nebula Graph, and FlockDB. Strengths of Graph-Oriented Stores They provide support for graph-based algorithms and queries. For example, deep traversals in graph databased systems are faster than relational databases. Graph-based stores allow for very fast execution of complex pattern-matching queries. They also allow for a compact representation of the data, basing its implementation on bitmaps. Most of them are transactional. Weaknesses of Graph-Oriented Stores Distributed graph processing is challenging. Additionally, a partitioning strategy is difficult to express in source code because the structure of the graph (unlike lists or hashes) is not known a priori. This is one of the greatest pitfalls in the application of MapReduce-based frameworks in graph processing 8. Similar to partitioning, computing locality is also challenging. Traversal algorithms generate a significant communication overhead when the vertices are located in different computing nodes. Most graph-based stores do not have a declarative language. Example Applications of Graph-Oriented Stores Generating recommendations Business Intelligence (BI) Semantic Web, Social networks analysis Geospatial Data, geolocation Genealogy, Web of things, Routing Services Authentication and authorization systems Fraud detection in financial services ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:2:4","series":null,"tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/#example-applications-of-graph-oriented-stores"},{"categories":["Software Engineering"],"content":"\rOther NoSQL StoresSo far we have seen the four major categories of NoSQL database models. NoSQL website defines 11 other categories: Multi-model databases, Object databases, Grid/Cloud databases, XML databases, Multidimensional databases, Multi-value databases, Event Sourcing databases, Time Series databases, Scientific/Specialized databases, Others, and Unresolved and Uncategorized. The following table summarizes the suitable data processing feature for each of these databases 13. Figure: Summary of suitable data features for NoSQL databases. ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:2:5","series":null,"tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/#other-nosql-stores"},{"categories":["Software Engineering"],"content":"\rComparing NoSQL Database ModelsWithin the family of NoSQL databases, different systems handle size, data model, and query complexity differently. Big data requires NoSQL database solutions to address velocity and variety of data more than data volume. This notion comes from the belief that 90% of web-based companies will likely never rise to the volume levels implied by NoSQL 4. The following figure plots the different NoSQL data stores on the data volume vs. modeling complexity dimensions. There is a tradeoff between size and complexity, as can be noticed from the figure. Figure: NoSQL solutions; complexity vs size. Further, these NoSQL stores can also be compared based on non-functional requirements as shown below 14. Data Store Type Performance Scalability Flexibility Complexity Key-value store high high high none Column store high high moderate low Document store high variable (high) high low Graph store variable variable high high ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:3:0","series":null,"tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/#comparing-nosql-database-models"},{"categories":["Software Engineering"],"content":"\rSummaryThe most commonly employed distinction between NoSQL databases is the way they store and allow access to data. Understanding the storage model characteristics of the NoSQL databases can help individuals or organizations in selecting the most appropriate class of databases for specific data feature requirements. This article explored the four most popular NoSQL data storage models. We looked at strengths, weaknesses, and specific application domains for each of those database models, along with a basic comparison among them. ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:4:0","series":null,"tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/#summary"},{"categories":["Software Engineering"],"content":"\rWhat’s NextIn the next article, we will look at the most popular database examples from each of the NoSQL models explained above. We will compare those individual databases against each other on functional and non-functional requirements. That comparison will further help the users in selecting the most appropriate database for their specific data application requirements. ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:5:0","series":null,"tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/#whats-next"},{"categories":["Software Engineering"],"content":"\rReferences Alexandru, Adriana \u0026 Alexandru, Cristina \u0026 Coardos, Dora \u0026 Tudora, Eleonora. (2016). Big data: Concepts, Technologies and Applications in the Public Sector. International Journal of Computer, Electrical, Automation, Control and Information Engineering. 10. 1629-1635. ↩︎ NoSQL Databases. Available online: http://nosql-database.org/ ↩︎ Gessert, Felix \u0026 Wingerath, Wolfram \u0026 Friedrich, Steffen \u0026 Ritter, Norbert. (2017). NoSQL database systems: a survey and decision guidance. Computer Science - Research and Development. 32. 10.1007/s00450-016-0334-3 ↩︎ ↩︎ Khazaei, Hamzeh \u0026 Fokaefs, Marios \u0026 Zareian, Saeed \u0026 Beigi, Nasim \u0026 Ramprasad, Brian \u0026 Shtern, Mark \u0026 Gaikwad, Purwa \u0026 Litoiu, Marin. (2015). How do I choose the right NoSQL solution? A comprehensive theoretical and experimental survey. Journal of Big Data and Information Analytics (BDIA). 2. 10.3934/bdia.2016004. ↩︎ ↩︎ ↩︎ ↩︎ DB-Engines Ranking of Key-value Stores. Available online: https://db-engines.com/en/ranking/key-value+store ↩︎ Hajoui, Omar \u0026 Dehbi, Rachid \u0026 Talea, Mohamed \u0026 Ibn Batouta, Zouhair. (2015). An advanced comparative study of the most promising NoSQL and NewSQL databases with a multi-criteria analysis method. 81. 579-588. ↩︎ Han, Jing, E. Haihong, Guan Le and Jian Du. “Survey on NoSQL database.” 2011 6th International Conference on Pervasive Computing and Applications (2011): 363-366. ↩︎ Corbellini, Alejandro \u0026 Mateos, Cristian \u0026 Zunino, Alejandro \u0026 Godoy, Daniela \u0026 Schiaffino, Silvia. (2017). Persisting big data: The NoSQL landscape. Information Systems. 63. 1-23. 10.1016/j.is.2016.07.009. ↩︎ ↩︎ ↩︎ DB-Engines Ranking of Document Stores. Available online: https://db-engines.com/en/ranking/document+store ↩︎ DB-Engines Ranking of Wide-Column Stores. Available online: https://db-engines.com/en/ranking/wide+column+store ↩︎ Chaudhry, Natalia \u0026 Yousaf, Muhammad. (2020). Architectural assessment of NoSQL and NewSQL systems. Distributed and Parallel Databases. 38. 10.1007/s10619-020-07310-1. ↩︎ DB-Engines Ranking of Graph DBMS. Available online: https://db-engines.com/en/ranking/graph+dbms ↩︎ Chen, \u0026 Lee,. (2019). An Introduction of NoSQL Databases Based on Their Categories and Application Industries. Algorithms. 12. 106. 10.3390/a12050106. ↩︎ Kotecha, Bansari H., and Hetal Joshiyara. “A Survey of Non-Relational Databases with Big Data.” International Journal on Recent and Innovation Trends in Computing and Communication 5.11 (2017): 143-148. ↩︎ ","date":"2022-06-20","objectID":"/posts/2022/06/nosql-models-comparison/:6:0","series":null,"tags":["system design","database"],"title":"Comparing NoSQL Models: A Guide for Selecting the Best-fitting Database Model","uri":"/posts/2022/06/nosql-models-comparison/#references"},{"categories":["Software Engineering"],"content":"\rSQL Databases OriginThe concept of Relational Databases was originally developed in the 1970s by IBM 1. They are also known as SQL (Structured Query Language) databases, named after the query language used for managing data in Relational Database Management Systems (RDBMS). Over multiple years of research and development, an unmatched level of reliability, stability, and strong mechanisms to store and query data have been baked into Relational Databases. They have been the storage of choice for a majority of transactional data management applications such as banking, airline reservation, online e-commerce, and supply chain management applications. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:1:0","series":null,"tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/#sql-databases-origin"},{"categories":["Software Engineering"],"content":"\rRelations in Relational DatabasesRelational Databases model the data to be stored as a collection of relations. A relation is a two-dimensional, normalized table to organize data. Each relation is composed of two parts: relation schema and relation instances. Relation schema includes the name of the relation, names of the attributes in the relation, along with their domain. Relation instances refer to the data records stored in the relation at a specific time. The following table gives an example of a relation 2. SID: Char(5) Name: Char(10) Telephone: Char(11) Birthday: Date (dd/mm/yy) S0001 Alice 05-65976597 10/4/1994 S0002 Dora 06-45714571 15/5/1995 S0003 Ella 07-57865869 20/6/1996 S0004 Kevin 06-57995611 22/7/1997 Table Name: Students Here “Students” is the relation; SID, name, telephone, and birthday are the names of the attributes; domain for Birthday is Date, and the relation has four data records stored. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:1:1","series":null,"tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/#relations-in-relational-databases"},{"categories":["Software Engineering"],"content":"\rCharacteristics of Relational Databases Mature Ecosystem: Relational databases are well known for reliability, stability, and support achieved through decades of development. Support for Vertical Scaling: They can be scaled vertically to handle growing data volume by beefing up the existing server. Fixed Schema: They enforce strict structure (schema) constraints on the data to be stored, and provide powerful mechanisms to store and query this structured data. Constraints: As part of an RDB design, integrity constraints are used to check the correctness of the data input into the database. Integrity constraints not only prevent authorized users from storing illegal data into the database but also avoid data inconsistency between relations. The four common kinds of integrity constraints are: Primary Key constraints, Domain constraints, Entity integrity constraints, and Referential integrity constraints. RDBMS uses a relational model which has a relationship between tables using foreign keys, primary keys, and indexes. Because of this, fetching and storing of data becomes faster than the older Navigational models that RDBMS replaced 3. Support for Transactions: RDBs especially shine when it comes to managing transactions. The concept of a transaction was first proposed by Jim Gray in 1970 4, and it represents a work unit in a database system that contains a set of operations. For a transaction to behave in a safe manner it should exhibit four main properties: atomicity, consistency, isolation, and durability (ACID). ACID PropertiesThis set of properties, known as ACID, increases the complexity of database systems 5, but also guarantees strong consistency and serializability. Atomicity: a guarantee that all of the operations of a transaction will be completed as a whole or none at all Consistency: a guarantee that the saved data will always be valid and the database will be consistent both before and after the transaction Integrity: a guarantee that the different transactions will not interfere with each other Durability: a guarantee that the changes of a successful transaction will persist permanently in the database even if a system failure occurs Relational databases are ACID-compliant and hence simplify the work of the developer by guaranteeing that every operation will leave the database in a consistent state. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:1:2","series":null,"tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/#characteristics-of-relational-databases"},{"categories":["Software Engineering"],"content":"\rCharacteristics of Relational Databases Mature Ecosystem: Relational databases are well known for reliability, stability, and support achieved through decades of development. Support for Vertical Scaling: They can be scaled vertically to handle growing data volume by beefing up the existing server. Fixed Schema: They enforce strict structure (schema) constraints on the data to be stored, and provide powerful mechanisms to store and query this structured data. Constraints: As part of an RDB design, integrity constraints are used to check the correctness of the data input into the database. Integrity constraints not only prevent authorized users from storing illegal data into the database but also avoid data inconsistency between relations. The four common kinds of integrity constraints are: Primary Key constraints, Domain constraints, Entity integrity constraints, and Referential integrity constraints. RDBMS uses a relational model which has a relationship between tables using foreign keys, primary keys, and indexes. Because of this, fetching and storing of data becomes faster than the older Navigational models that RDBMS replaced 3. Support for Transactions: RDBs especially shine when it comes to managing transactions. The concept of a transaction was first proposed by Jim Gray in 1970 4, and it represents a work unit in a database system that contains a set of operations. For a transaction to behave in a safe manner it should exhibit four main properties: atomicity, consistency, isolation, and durability (ACID). ACID PropertiesThis set of properties, known as ACID, increases the complexity of database systems 5, but also guarantees strong consistency and serializability. Atomicity: a guarantee that all of the operations of a transaction will be completed as a whole or none at all Consistency: a guarantee that the saved data will always be valid and the database will be consistent both before and after the transaction Integrity: a guarantee that the different transactions will not interfere with each other Durability: a guarantee that the changes of a successful transaction will persist permanently in the database even if a system failure occurs Relational databases are ACID-compliant and hence simplify the work of the developer by guaranteeing that every operation will leave the database in a consistent state. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:1:2","series":null,"tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/#acid-properties"},{"categories":["Software Engineering"],"content":"\rThe Rise of NoSQL Databases","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:2:0","series":null,"tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/#the-rise-of-nosql-databases"},{"categories":["Software Engineering"],"content":"\rBig Data EraIn recent years, the amount of useful data in some application areas has become so vast that it cannot be stored or processed by traditional database solutions. With the launch of Web 2.0, a large amount of valuable business data started being generated. This data can be structured or unstructured and can come from multiple sources. Social networks, products viewed in virtual stores, information read by sensors, GPS signals from mobile devices, lP addresses, cookies, bar codes, etc. are examples of this phenomenon commonly referred to as Big Data 6. Big Data!\rBig data is often characterized by 7 Vs: Volume (great size), Velocity (rapid procreation), Variety (various types), Veracity (data messiness or constancy), Value (huge value but low density), Variability (constantly changing), and Visualization (data presentation).\r","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:2:1","series":null,"tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/#big-data-era"},{"categories":["Software Engineering"],"content":"\rShortcomings of Relational DatabasesBig Data significantly changed the way organizations viewed data. The data volume, accumulation speed, and various data types required operating in ways that Relational databases were not quite designed for. A popular survey suggested that 37.5% of leading companies were unable to analyze big data 7. Some of the major shortcomings that Relational databases exhibited are as follows. Scalability: Most SQL-oriented databases do not support distributed data processing and storage, making it a challenge to work with data of high volume, therefore, resulting in a need for a server with exceptional computing capabilities to handle the large data volume, which is an expensive undesirable solution. Flexibility: Big Data systems are primarily unstructured in nature, which meant that the need for a predefined structure of the data, i.e. explicitly defining a schema for the data being stored became difficult. Not all data could be fit into tables, so custom solutions targeting the new data models were required. Complexity: Database performance often decreases significantly since joins and transactions are costly in distributed environments. Others: Servers based on SQL standards are now prone to memory footprint, security risks, and performance issues 8. All in all, this does not mean RDBMSs have become obsolete, but rather they have been designed with other requirements in mind and work well when extreme scalability is not required. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:2:2","series":null,"tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/#shortcomings-of-relational-databases"},{"categories":["Software Engineering"],"content":"\rIntroducing NoSQL databasesTraditional databases could not cope with the generation of massive amounts of information by different devices, including GPS information, RFIDs, IP addresses, unique Identifiers, data and metadata about the devices, sensor data, and historical data. A class of novel data storage systems able to cope with Big Data has subsumed under the term NoSQL databases. NoSQL!\rThe term “NoSQL” was first coined in 1998 by Carlo Strozzi for his RDBMS, Strozzi NoSQL. However, Strozzi used the term simply to distinguish his solution from other RDBMS. He used the term NoSQL just for the reason that his database did not expose a SQL interface. It was then brought back in 2009 for naming an event that highlighted new non-relational databases, such as BigTable and Dynamo. While originally the term stood for “No SQL”, it has been restated as “Not Only SQL” to highlight that these systems rarely fully drop the relational model. Thus, despite being a recurrent theme in literature, NoSQL is a very broad term, encompassing very distinct database systems [^18].\r","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:2:3","series":null,"tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/#introducing-nosql-databases"},{"categories":["Software Engineering"],"content":"\rNoSQL PropertiesThe major properties of NoSQL databases are as follows 9 10. Highly and Easily Scalable: NoSQL databases are designed to expand horizontally, meaning that you scale out by adding more machines into your pool of resources, while still being able to run CRUD operations over many servers. Sharding: Data can also be distributed across nodes in a non-overlapping way; this technique is known as Sharding, and the locations of the stored data are managed by metadata. Replication: To provision scaling, a lot of NoSQL databases are designed under a “let it crash” philosophy, where nodes are allowed to crash and their replicas are always ready to receive requests. NoSQL databases mostly support master-slave replication or peer-to-peer replication, making it easier for NoSQL databases to ensure high availability. No-Sharing Architecture: They are based on ‘No Sharing Architecture’ in which neither memory nor storage is shared. This enables each node to operate independently. The scaling thus becomes very convenient as new nodes can be easily added or removed from the system to meet the data processing requirement. Flexible Schema: The data to be stored in NoSQL databases do not need to conform to a strict schema. This also enables the ability to dynamically add new attributes to data records. Less expensive to maintain: The scalability of NoSQL databases allows the database admins to make use of pay-as-you-go pricing models of cloud-based database service providers. NoSQL databases can run on low specs devices. Integrated Caching: In general, as NoSQL databases are designed to be distributed, the location of the data in the cluster is leveraged to improve network usage, usually by caching remote data, and making queries to those nodes located closer in the network topology. This mechanism is often referred to as location awareness or data affinity. No Support for Joins: In general, NoSQL databases do not use a relational database model, and do not support SQL join operations or have very limited support for it. So the related data needs to be stored together to improve the speed of data access. Others: Unlike most RDBs that require a fee to purchase, most NoSQL databases are open source and free to download. NoSQL databases often have easy-to-use APIs as well. Past researches, such as Stonebraker et al. 11, have argued that the main reasons to move to NoSQL databases are performance and flexibility. Performance is mainly focused on sharing and management of distributed data (i.e. dealing with “Big Data”), while flexibility relates to the semi-structured or unstructured data that may arise on the web. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:2:4","series":null,"tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/#nosql-properties"},{"categories":["Software Engineering"],"content":"\rMost Suited Applications for NoSQLNoSQL is primarily suited 12 for applications with: No need for ACID properties Flexible Schema No constraints and validation to be executed in database Temporary data Different data types High data volume ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:2:5","series":null,"tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/#most-suited-applications-for-nosql"},{"categories":["Software Engineering"],"content":"\rLimitations of NoSQL Databases NoSQL databases are open-source which is its greatest strength but at the same time, it can be considered its greatest weakness because there are not many defined standards for NoSQL databases; so, no two NoSQL databases are equal. Tools like GUI mode, management console, etc. to access the database are not widely available in the market. Joins and transactions are costly operations for NoSQL databases. Most NoSQL databases do not support ACID properties, hence data consistency remains a fundamental challenge for NoSQL. Due to a lack of standardization, the design and query languages for NoSQL databases vary widely. This results in a steeper learning curve for NoSQL databases. Despite being designed with horizontal scaling in mind, not all NoSQL databases are good at automating the process of sharding. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:2:6","series":null,"tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/#limitations-of-nosql-databases"},{"categories":["Software Engineering"],"content":"\rThe NewSQL ParadigmNewSQL is a class of modern relational database management systems that aim to combine the best of SQL and NoSQL databases. NoSQL databases seek to provide the same scalable performance as NoSQL systems for online transaction processing(OLTP) read-write workloads while still maintaining the ACID guarantees of a traditional database system. Basically, it is a combination of NoSQL (Scalable) properties with Relational (ACID) Properties 13. Example systems in this category are NuoDB, VoltDB, Google Spanner, and Clustrix. NewSQL!\rNewSQL is not only used to overcome some limitations of the SQL, but it could also be an alternative for NoSQL in certain applications where the need for analytics and decision making has to be found on request with high consistency [^12]. It helps in executing read-write transactions that are short-lived and operated using index loops and executing the same number of queries with different inputs. It works on lock-free concurrency control and share-nothing architecture [^10].\r","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:3:0","series":null,"tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/#the-newsql-paradigm"},{"categories":["Software Engineering"],"content":"\rCharacteristics of NewSQL Databases Distributed: NewSQL databases are relational databases supporting share-nothing architecture, sharding, automatic replication, and distributed transaction processing, i.e., providing ACID guarantees even across shards. Schemas: NewSQL databases support fixed schemas as well as schema-free databases. ACID-compliant: NewSQL systems preserve the ACID properties of relational databases. SQL Support: NewSQL databases support SQL but query complexity is very high. They also support SQL extensions. RAM Support: NewSQL gives high performance by keeping all data in RAM and enabling in-memory computations. Data Affinity: Scalability is provided by employing partitioning and replication in such a way that queries generally do not have to communicate between multiple machines. They get the required information from a single host. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:3:1","series":null,"tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/#characteristics-of-newsql-databases"},{"categories":["Software Engineering"],"content":"\rApplication of NewSQL: Google SpannerGoogle’s Spanner is a globally distributed database system created at Google that supports distributed transactions, designed as a replacement to Megastore, a BigTable-based storage. It is a database that performs sharding of data that is a horizontal partition of data and it is spread across many Paxos state machines. Paxos state machines are used to solve consensus which is the process of agreeing upon one result when there are multiple participants. The datacenters are spread all over the world. The databases should be available globally. So for this purpose replication is used. The replication isn’t completely random. It considers the geographic locality and what kind of data is required more frequently. Spanner works dynamically. It reshards and migrates data automatically for the purpose of load balancing. For achieving low latency and high availability most applications would probably replicate data over three or five datacenters in one geographic region. Spanner is useful when applications want strong consistency and are distributed over a large area. Spanner performs versioning of the data and stores the time stamp which is the same as the commit time. Unrequired data can be deleted with proper policies for handling old data. Spanner is very useful for OLTP concerning Big Data. It also uses SQL query language. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:3:2","series":null,"tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/#application-of-newsql-google-spanner"},{"categories":["Software Engineering"],"content":"\rSQL vs NoSQL vs NewSQL database comparison Distinguishing Feature SQL NoSQL NewSQL Relational Yes No Yes ACID Yes No Yes SQL Yes No Yes OLTP Not fully Supported Supported Fully Supported Horizontal Scaling No Yes Yes Query Complexity Low High Very High Distributed No Yes Yes Source [^3] ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:4:0","series":null,"tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/#sql-vs-nosql-vs-newsql-database-comparison"},{"categories":["Software Engineering"],"content":"\rTransactional Properties and TheoremsNow we turn our attention to some fundamental properties and theorems that will lay the foundations for understanding the key differentiating core philosophies among the different database implementations. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:5:0","series":null,"tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/#transactional-properties-and-theorems"},{"categories":["Software Engineering"],"content":"\rACID PropertiesWe discussed ACID properties earlier on in this article. ACID is a combination of four properties namely Atomicity, Consistency, Isolation, and Durability. These transactional properties of the database guarantee the reliability of transactions 14. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:5:1","series":null,"tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/#acid-properties-1"},{"categories":["Software Engineering"],"content":"\rThe CAP TheoremThe CAP Theorem, also known as Brewer’s theorem was presented by Eric Brewer in 2000 and later proven by Gilbert and Lynch 15, is one of the truly influential impossibility results in the field of distributed computing, because it places an ultimate upper bound on what can be accomplished by a distributed system 6. It states that it is impossible for a distributed computer system to simultaneously provide all three of the following guarantees: Consistency (C): Reads and writes are always executed atomically and are strictly consistent (linearizable). Put differently, all clients have the same view on the data at all times. Availability (A): All clients can always find at least one copy of the requested data, even if some of the machines in a cluster are down. Put differently, it is a guarantee that every request receives a response about whether it succeeded or failed. Partition Tolerance (P): The system can continue operating normally in the presence of network partitions. These occur if two or more “islands” of network nodes arise that (temporarily or permanently) cannot connect to each other due to network failures. Eric Brewer conjectured that at any given moment in time only two out of the three mentioned characteristics can be guaranteed, concluding that only distributed systems accomplishing the following combinations can be created: AP (Availability-Partition Tolerance), CP (Consistency-Partition Tolerance), or AC (Availability-Consistency). Most of the databases fall in the “AP” (eventual consistent systems) or the “CP” group because resigning P (Partition Tolerance) in a distributed system means assuming that the underlying network will never drop packages or disconnect, which is not feasible. Some systems usually are available and consistent, but fail completely when there is a partition (CA), for example, single-node relational database systems. Figure: CAP theorem with databases that “choose” CA, CP, and AP [^18] ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:5:2","series":null,"tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/#the-cap-theorem"},{"categories":["Software Engineering"],"content":"\rBASE PropertiesMany of the NoSQL databases have loosened up the requirements on Consistency to achieve better Availability and Partitioning. This resulted in systems known as the BASE (Basically Available, Soft-state, Eventually consistent). The idea behind the systems implementing this concept is to allow partial failures instead of full system failure, which leads to a perception of greater system availability. Basically Available: Basically available ensures the availability of data. It means that there will always be a guaranteed response to each request. Some NoSQL DBs typically keep several copies of specific data on different servers, which allows the DB system to respond to all queries even if a few of the servers fail. Soft-state: Soft state means the system may change as it switches from one state to another even without any input, i.e. the data can be volatile, stored, and recovered easily and can be regenerated, unlike the hard state of the system in which the predictable inputs produce the predictable outputs. This soft state is due to the concept of eventual consistency in which the state of the system may become consistent after some time even without input. Eventually consistent: Eventually consistent means that the consistency is achieved after some indeterminate time because after a write operation, the replicas need to be synchronized. Therefore, strict consistency is difficult to ensure. The design of BASE systems, and in particular BASE NoSQL databases, allows certain operations to be performed leaving the replicas (i.e., copies of the data) in an inconsistent state. ACID principles are used when data reliability and consistency are important because the focus is on consistency and availability. Whereas BASE principles are used when data viability and speed are important because the focus is on focuses on availability and partition tolerance 16. Figure: Big Data characteristics and NoSQL System features ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:5:3","series":null,"tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/#base-properties"},{"categories":["Software Engineering"],"content":"\rPACELC TheoremIt is important to note that the CAP Theorem actually does not state anything on normal operation; it merely tells us whether a system favors availability or consistency in the face of a network partition. The CAP theorem assumes a failure model that allows arbitrary messages to be dropped, reordered, or delayed indefinitely. This lack of the CAP Theorem is addressed in an article by Daniel Abadi 17 in which he points out that the CAP Theorem fails to capture the trade-off between latency and consistency during normal operation. He formulates PACELC which unifies both trade-offs and thus portrays the design space of distributed systems more accurately. From PACELC, we learn that in the case of a Partition, there is an Availability-Consistency trade-off; Else, i.e. in normal operation, there is a Latency-Consistency trade-off. This classification basically offers two possible choices for the partition scenario (A/C) and also two for normal operation (L/C) and thus appears more fine-grained than the CAP classification. However, many systems cannot be assigned exclusively to one single PACELC class and one of the four PACELC classes, namely PC/EL, can hardly be assigned to any system 18. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:5:4","series":null,"tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/#pacelc-theorem"},{"categories":["Software Engineering"],"content":"\rBASIC PropertiesSQL databases use ACID, which is the strongest consistency level, while NoSQL databases use BASE, which is a weak consistency model. Some NewSQL databases 19 support a consistency level between ACID and BASE, called BASIC (Basic Availability, Scalability, Instant Consistency) 20. Basic Availability: The system always responds to read/write queries. Scalability: Scalability allows for adding more resources if the workload is increased. Instant Consistency: Instant consistency, a better consistency level than eventual consistency, ensures that if a write and read operation is carried out consecutively, then the result returned by a read operation must be the same as written by a write operation. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:5:5","series":null,"tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/#basic-properties"},{"categories":["Software Engineering"],"content":"\rSummarySQL Databases, also known as RDBMS (Relational Database Management Systems) are the most common and traditional approach to database solutions. SQL solves problems ranging from fast write-oriented transactions to scan-intensive deep analytics. SQL allows users to apply their knowledge across systems and provides support for third-party add-ons and tools because it is standardized. The data is stored in a structured way in form of tables or Relations. With the advent of Big Data, however, the structured approach falls short to serve the needs of Big Data systems which are primarily unstructured in nature. Increasing the capacity of SQL although allows a huge amount of data to be managed, it does not really count as a solution to Big Data needs, which expects a fast response and quick scalability. To solve this problem a new kind of Database system called NoSQL was introduced to provide the scalability and unstructured platform for Big Data applications. The data structures used by NoSQL databases (e.g. key-value, graph, or document) differ slightly from those used by default in relational databases, making some operations faster in NoSQL and others faster in relational databases. NoSQL stands for Not Only SQL. It is also horizontally Scalable as opposed to vertical scaling in RDBMS. NoSQL was introduced to us for resolving scalability issues but consistency issues after scalability moved us from NoSQL to NewSQL. NoSQL provided great promises to be a perfect database system for Big Data applications; it however falls short because of some major drawbacks like NoSQL does not guarantee ACID properties (Atomicity, Consistency, Isolation, and Durability) of SQL systems. It is also not compatible with earlier versions of the database. This is where NewSQL comes into the picture. NewSQL is the latest development in the world of database systems, and it is basically a Relational Database with the scalability properties of NoSQL. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:6:0","series":null,"tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/#summary"},{"categories":["Software Engineering"],"content":"\rWhat’s NextI hope you found this database technologies review to be useful. In the next few articles, I will compare different NoSQL data models on several criteria, then I will compare the most popular databases from each NoSQL model, followed by explaining structured approaches for picking the best database for your use-case. ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:7:0","series":null,"tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/#whats-next"},{"categories":["Software Engineering"],"content":"\rReferences Don Chamberlin : “SQL”, IBM Almaden Research Center, San Jose, CA ↩︎ Chen, J.-K.; Lee, W.-Z. An Introduction of NoSQL Databases Based on Their Categories and Application Industries. Algorithms 2019, 12, 106. ↩︎ Binani, Sneha \u0026 Gutti, Ajinkya \u0026 Upadhyay, Shivam. (2016). SQL vs. NoSQL vs. NewSQL- A Comparative Study. Communications on Applied Electronics. 6. 43-46. 10.5120/cae2016652418. ↩︎ Jim Gray, Andreas Reuter, Transaction Processing: Concepts and Techniques, 1st ed. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1992 ↩︎ Corbellini, Alejandro \u0026 Mateos, Cristian \u0026 Zunino, Alejandro \u0026 Godoy, Daniela \u0026 Schiaffino, Silvia. (2017). Persisting big data: The NoSQL landscape. Information Systems. 63. 1-23. 10.1016/j.is.2016.07.009. ↩︎ Gessert, Felix \u0026 Wingerath, Wolfram \u0026 Friedrich, Steffen \u0026 Ritter, Norbert. (2017). NoSQL database systems: a survey and decision guidance. Computer Science - Research and Development. 32. 10.1007/s00450-016-0334-3. ↩︎ ↩︎ S. J. Veloso, 2015, Data Analytics Topic: Big Data [Online]. ↩︎ Venkatraman, S., Fahd, K., Kaspi, S., \u0026 Venkatraman, R. (2016). SQL Versus NoSQL Movement with Big Data Analytics. International Journal of Information Technology and Computer Science, 8, 59-66. ↩︎ Kotecha, Bansari H., and Hetal Joshiyara. “A Survey of Non-Relational Databases with Big Data.” International Journal on Recent and Innovation Trends in Computing and Communication 5.11 (2017): 143-148. ↩︎ Hajoui, Omar \u0026 Dehbi, Rachid \u0026 Talea, Mohamed \u0026 Ibn Batouta, Zouhair. (2015). An advanced comparative study of the most promising NoSQL and NewSQL databases with a multi-criteria analysis method. 81. 579-588. ↩︎ Stonebraker M (2010) Sql databases v. nosql databases. Commun ACM. 53(4):10–11 ↩︎ Khasawneh, Tariq \u0026 Alsahlee, Mahmoud \u0026 Safieh, Ali. (2020). SQL, NewSQL, and NOSQL Databases: A Comparative Survey. 013-021. 10.1109/ICICS49469.2020.239513. ↩︎ Chandra, Umesh. (2017). A Comparative Study On: Nosql, Newsql And Polygot Persistence. International Journal Of Soft Computing and Engineering (IJSE). 7. ↩︎ Chaudhry, Natalia \u0026 Yousaf, Muhammad. (2020). Architectural assessment of NoSQL and NewSQL systems. Distributed and Parallel Databases. 38. 10.1007/s10619-020-07310-1. ↩︎ Gilbert S, Lynch N (2002) Brewer’s conjecture and the feasibility of consistent, available, partition-tolerant web services. SIGACT News 33(2):51–59 ↩︎ Khazaei, Hamzeh \u0026 Fokaefs, Marios \u0026 Zareian, Saeed \u0026 Beigi, Nasim \u0026 Ramprasad, Brian \u0026 Shtern, Mark \u0026 Gaikwad, Purwa \u0026 Litoiu, Marin. (2015). How do I choose the right NoSQL solution? A comprehensive theoretical and experimental survey. Journal of Big Data and Information Analytics (BDIA). 2. 10.3934/bdia.2016004. ↩︎ Abadi D (2012) Consistency tradeoffs in modern distributed data- base system design: cap is only part of the story. Computer 45(2):37–42 ↩︎ Lourenço, João \u0026 Cabral, Bruno \u0026 Carreiro, Paulo \u0026 Vieira, Marco \u0026 Bernardino, Jorge. (2015). Choosing the right NoSQL database for the job: a quality attribute evaluation. Journal of Big Data. 2. 18. 10.1186/s40537-015-0025-0. ↩︎ Yuan, Li-Yan \u0026 Wu, Lengdong \u0026 You, Jia-Huai \u0026 Shanghai, Yan \u0026 Software, Shifang. (2015). A Demonstration of Rubato DB: A Highly Scalable NewSQL Database System for OLTP and Big Data Applications. ↩︎ Wu, Lengdong \u0026 Yuan, Li-Yan \u0026 You, Jia-Huai. (2014). BASIC: An alternative to BASE for large-scale data management system. Proceedings - 2014 IEEE International Conference on Big Data, IEEE Big Data 2014. 10.1109/BigData.2014.7004206. ↩︎ ","date":"2022-06-09","objectID":"/posts/2022/06/sql-nosql-newsql/:8:0","series":null,"tags":["system design","database"],"title":"SQL vs NoSQL vs NewSQL: An In-depth Literature Review","uri":"/posts/2022/06/sql-nosql-newsql/#references"},{"categories":["NLP"],"content":"Recognizing feelings in the conversation partner and replying empathetically is a trivial skill for humans. But how can we infuse empathy into responses generated by a conversational dialogue agent or any of the text generation algorithm in Natural Language Processing? In this article, I will describe what empathy means through the lens of various academic disciplines and then do an in-depth review of the prior and current state-of-the-art NLU systems that can simulate empathy.","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/","series":null,"tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/"},{"categories":["NLP"],"content":"Recognizing feelings in the conversation partner and replying empathetically is a trivial skill for humans. But how can we infuse empathy into responses generated by a conversational dialogue agent or any of the text generation algorithm in Natural Language Processing? In this article, I will describe what empathy means through the lens of various academic disciplines and then do an in-depth review of the prior and current state-of-the-art NLU systems that can simulate empathy. ","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:0:0","series":null,"tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/#"},{"categories":["NLP"],"content":"\rEmpathy and its Linguistic OriginThe word empathy has a rather interesting linguistic history. In 1909, this word was first introduced in English by psychologist Edward Titchener as a translation of the German word Einfühlung, which means “feeling into” or “in-feeling”. The German word itself was adapted from the Ancient Greek word “ἐμπάθεια” or “empátheia” meaning “in passion” (from Greek ’en pathos’). Einfühlung first appeared in Robert Vischer’s 1873 Ph.D. dissertation, where Vischer used it to describe the human ability to enter into a piece of art or literature and feel the emotions of its creator1. Even though in modern Greek, the word empátheia has an opposite meaning, a strong negative feeling or prejudice against someone; the English word empathy does not carry those negative connotations. The modern-day usage of the word empathy pertains to the range of psychological capacities that play a central role in establishing humans as social and moral animals. It enables us to “put ourselves into someone else’s shoes”. Before the introduction of “empathy” in the English language, the word sympathy was used to describe a related phenomenon of understanding others’ feelings2. However, empathy is used as a broader concept to address a phenomenon of not just understanding someone’s emotions and but also viscerally feeling them. ","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:1:0","series":null,"tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/#empathy-and-its-linguistic-origin"},{"categories":["NLP"],"content":"\rThe Importance of EmpathyTheodor Lipps was a German philosopher who posited the theory that empathy should be understood as the primary epistemic means for gaining knowledge of other minds. While this theory has been a topic of contentious debate in the field of philosophy, the study and scientific exploration of empathy as a social science phenomenon have been less critical. There are two major focus areas involving empathy in social science. The first one treats empathy as a cognitive phenomenon and attempts to measure the accuracy of one’s abilities to recognize others’ personality, attitude, and moral traits. It concerns with the factors that affect empathy. For example, do age, gender, upbringing, family history, relationships impact empathy in a person3? The second focus area treats empathy as a rather emotional phenomenon and finds means to measure empathy and other perceptual factors that trigger empathetic responses. The interdisciplinary field of neuroscience, on the other hand, researches into the processes that neurologically enable a person to feel what another is feeling4. Empathy enhances social functioning5. The ability to understand and share feelings of other people around us enables us to also understand their present and future mental state and actions. It can even encourage prosocial behaviors by motivating humans to act altruistically towards kin, mates and, allies6 7. In their book “Empathy Reconsidered: New Directions in Psychotherapy”, Arthur Bohart and Leslie Greenberg, explored the role that empathy plays in psychotherapy8. Their work propounded that all forms of psychotherapy are effective as a result of empathetic processes, and made the case for ensuring that psychotherapists are empathetically engaging with their clients. Other researches have shown the positive impact of empathy in mental healthcare, nursing, and even primary care. Researchers Stewart Mercer and William Reynolds highlighted the importance of empathy in the quality of primary care, in their paper titled “Empathy and quality of care”9. There are a substantial number of similar studies that show a wide range of applications of empathy in healthcare. ","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:2:0","series":null,"tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/#the-importance-of-empathy"},{"categories":["NLP"],"content":"\rWhat does Empathy Mean for NLP?Natural Language Processing has proved to be an exceedingly viable tool to bridge the conversational gap between humans and machines. Various industries are now using conversational AI assistants (or chatbots) to improve their customer service. Not only these artificial conversational agents can understand users’ intent and respond to them, but they are also increasingly becoming capable of understanding users’ emotions. NLP researchers are looking into ways to infuse the human trait of empathy into conversational agents to create more empathetic end-user experience. As an example, consider the following two scenarios where an Amazon customer reaches out to a customer service bot to complain about their order not being delivered on time. The choice of words used by the customer in the two scenarios convey differently charged emotions. Compared to scenario 1, the customer sounds more distressed in scenario 2. An optimal response from the bot in the second scenario should not feel like an off-the-shelf template response. An ideal response may start with first acknowledging the understanding of the customer’s frustration and displaying empathy to subdue their negatively charged emotions. A compassionate choice of words in the response can not only alleviate some of the customer annoyance, but it may also help in better customer retention in the long run. ","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:3:0","series":null,"tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/#what-does-empathy-mean-for-nlp"},{"categories":["NLP"],"content":"\rNLP Research on Empathy","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:4:0","series":null,"tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/#nlp-research-on-empathy"},{"categories":["NLP"],"content":"\rEarly WorksThe topics of identifying and generating empathy in natural langue processing haven’t seen as explosive research growth and application as topics like sentiment analysis. Perhaps the earliest work in identifying empathy in text data was done by Xiao et al., 201210, when they developed an N-gram language model-based maximum likelihood strategy to classify empathetic vs non-empathetic utterances from a dataset of clinical trial studies on substance use by college students. In 2015, Gibson et al.11 proposed computation of features based upon psycholinguistic norms, and in 2017, Khanpour et al.12 used a simple combination of Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks to identify empathetic messages in online health communities. It’s worth highlighting that most of these researches used datasets that weren’t publicly available to the NLP community. In 2018, Buechel et al.13 published their research which they claimed to be the first gold-standard for empathy prediction. Their paper looked at a more nuanced form of empathy that was based on psychology and included empathic concern, and personal distress. Also, the empathy ratings in their dataset were provided by writers instead of other annotators. They used Ridge regression, simple feed-forward neural nets, and CNN for their empathy prediction tasks on newswire articles. In somewhat related research, Perez-Rosas et al.14 looked into behavioral counseling and proposed a quantitative approach to understand the dynamics of counseling interactions and counselor empathy during motivational interviewing. They first identified linguistic and acoustic empathy markers and used those along with raw features to train classifiers that were able to predict counselor empathy. Some of the other prominent researches are listed below: Zara The SupergirlZara was an Empathetic Personality Recognition System created by Fung, Dey et al.15. Their research showed an interactive dialogue system that did sentiment analysis, emotion recognition, facial and speech recognition to extract user emotions in a human-robot conversational setup. They deployed their virtual robot as a webapp where users can interact with an animated cartoon character, Zara. You can watch their demo video on YouTube and also interact with their online webapp. Zara assesses a user’s personality by asking a series of questions, along with the follow-up inquiries, on different topics like the user’s childhood memory, last vacation, challenges at work, etc. It uses OpenSmile and Kaldi to perform emotion recognition from user audio, keyword matches from a pool of positive and negative emotion lexicons to perform sentiment analysis, and then uses these results to calculate the scores in four personality dimensions - extroversion, intuitive, judging, perceiving. Nora the Empathetic PsychologistSimilar to Zara, researchers Winata, Kampman et al.16 crated a virtual psychologist, an empathetic dialogue system named Nora, that could mimic a conversation with a psychologist. Nora employed a natural language understanding (NLU) module to classify user intent and slots, a dialogue management module to evaluate NLU output and manage dialog turns, and a language generation module to respond to the user. A simple CNN was used to detect stress, personality, sentiment, and six different emotions from audio. Real-Time Speech Emotion and Sentiment Recognition for Interactive Dialogue SystemsBertero, Siddique et al.17 also used a CNN-based approach to recognize emotion and sentiment from raw audio data in real-time to enable an empathetic conversational dialogue system. They used the Kaldi speech recognition toolkit to train deep neural network hidden Markov models (DNN-HMMs) that used the raw audio together with encode-decode parallel audio and outperformed the SVM baseline. Their work avoided any feature engineering to enable real-time speech processing. Generating Emotionally Flexible ResponsesOne of the","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:4:1","series":null,"tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/#early-works"},{"categories":["NLP"],"content":"\rEarly WorksThe topics of identifying and generating empathy in natural langue processing haven’t seen as explosive research growth and application as topics like sentiment analysis. Perhaps the earliest work in identifying empathy in text data was done by Xiao et al., 201210, when they developed an N-gram language model-based maximum likelihood strategy to classify empathetic vs non-empathetic utterances from a dataset of clinical trial studies on substance use by college students. In 2015, Gibson et al.11 proposed computation of features based upon psycholinguistic norms, and in 2017, Khanpour et al.12 used a simple combination of Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks to identify empathetic messages in online health communities. It’s worth highlighting that most of these researches used datasets that weren’t publicly available to the NLP community. In 2018, Buechel et al.13 published their research which they claimed to be the first gold-standard for empathy prediction. Their paper looked at a more nuanced form of empathy that was based on psychology and included empathic concern, and personal distress. Also, the empathy ratings in their dataset were provided by writers instead of other annotators. They used Ridge regression, simple feed-forward neural nets, and CNN for their empathy prediction tasks on newswire articles. In somewhat related research, Perez-Rosas et al.14 looked into behavioral counseling and proposed a quantitative approach to understand the dynamics of counseling interactions and counselor empathy during motivational interviewing. They first identified linguistic and acoustic empathy markers and used those along with raw features to train classifiers that were able to predict counselor empathy. Some of the other prominent researches are listed below: Zara The SupergirlZara was an Empathetic Personality Recognition System created by Fung, Dey et al.15. Their research showed an interactive dialogue system that did sentiment analysis, emotion recognition, facial and speech recognition to extract user emotions in a human-robot conversational setup. They deployed their virtual robot as a webapp where users can interact with an animated cartoon character, Zara. You can watch their demo video on YouTube and also interact with their online webapp. Zara assesses a user’s personality by asking a series of questions, along with the follow-up inquiries, on different topics like the user’s childhood memory, last vacation, challenges at work, etc. It uses OpenSmile and Kaldi to perform emotion recognition from user audio, keyword matches from a pool of positive and negative emotion lexicons to perform sentiment analysis, and then uses these results to calculate the scores in four personality dimensions - extroversion, intuitive, judging, perceiving. Nora the Empathetic PsychologistSimilar to Zara, researchers Winata, Kampman et al.16 crated a virtual psychologist, an empathetic dialogue system named Nora, that could mimic a conversation with a psychologist. Nora employed a natural language understanding (NLU) module to classify user intent and slots, a dialogue management module to evaluate NLU output and manage dialog turns, and a language generation module to respond to the user. A simple CNN was used to detect stress, personality, sentiment, and six different emotions from audio. Real-Time Speech Emotion and Sentiment Recognition for Interactive Dialogue SystemsBertero, Siddique et al.17 also used a CNN-based approach to recognize emotion and sentiment from raw audio data in real-time to enable an empathetic conversational dialogue system. They used the Kaldi speech recognition toolkit to train deep neural network hidden Markov models (DNN-HMMs) that used the raw audio together with encode-decode parallel audio and outperformed the SVM baseline. Their work avoided any feature engineering to enable real-time speech processing. Generating Emotionally Flexible ResponsesOne of the","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:4:1","series":null,"tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/#zara-the-supergirl"},{"categories":["NLP"],"content":"\rEarly WorksThe topics of identifying and generating empathy in natural langue processing haven’t seen as explosive research growth and application as topics like sentiment analysis. Perhaps the earliest work in identifying empathy in text data was done by Xiao et al., 201210, when they developed an N-gram language model-based maximum likelihood strategy to classify empathetic vs non-empathetic utterances from a dataset of clinical trial studies on substance use by college students. In 2015, Gibson et al.11 proposed computation of features based upon psycholinguistic norms, and in 2017, Khanpour et al.12 used a simple combination of Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks to identify empathetic messages in online health communities. It’s worth highlighting that most of these researches used datasets that weren’t publicly available to the NLP community. In 2018, Buechel et al.13 published their research which they claimed to be the first gold-standard for empathy prediction. Their paper looked at a more nuanced form of empathy that was based on psychology and included empathic concern, and personal distress. Also, the empathy ratings in their dataset were provided by writers instead of other annotators. They used Ridge regression, simple feed-forward neural nets, and CNN for their empathy prediction tasks on newswire articles. In somewhat related research, Perez-Rosas et al.14 looked into behavioral counseling and proposed a quantitative approach to understand the dynamics of counseling interactions and counselor empathy during motivational interviewing. They first identified linguistic and acoustic empathy markers and used those along with raw features to train classifiers that were able to predict counselor empathy. Some of the other prominent researches are listed below: Zara The SupergirlZara was an Empathetic Personality Recognition System created by Fung, Dey et al.15. Their research showed an interactive dialogue system that did sentiment analysis, emotion recognition, facial and speech recognition to extract user emotions in a human-robot conversational setup. They deployed their virtual robot as a webapp where users can interact with an animated cartoon character, Zara. You can watch their demo video on YouTube and also interact with their online webapp. Zara assesses a user’s personality by asking a series of questions, along with the follow-up inquiries, on different topics like the user’s childhood memory, last vacation, challenges at work, etc. It uses OpenSmile and Kaldi to perform emotion recognition from user audio, keyword matches from a pool of positive and negative emotion lexicons to perform sentiment analysis, and then uses these results to calculate the scores in four personality dimensions - extroversion, intuitive, judging, perceiving. Nora the Empathetic PsychologistSimilar to Zara, researchers Winata, Kampman et al.16 crated a virtual psychologist, an empathetic dialogue system named Nora, that could mimic a conversation with a psychologist. Nora employed a natural language understanding (NLU) module to classify user intent and slots, a dialogue management module to evaluate NLU output and manage dialog turns, and a language generation module to respond to the user. A simple CNN was used to detect stress, personality, sentiment, and six different emotions from audio. Real-Time Speech Emotion and Sentiment Recognition for Interactive Dialogue SystemsBertero, Siddique et al.17 also used a CNN-based approach to recognize emotion and sentiment from raw audio data in real-time to enable an empathetic conversational dialogue system. They used the Kaldi speech recognition toolkit to train deep neural network hidden Markov models (DNN-HMMs) that used the raw audio together with encode-decode parallel audio and outperformed the SVM baseline. Their work avoided any feature engineering to enable real-time speech processing. Generating Emotionally Flexible ResponsesOne of the","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:4:1","series":null,"tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/#nora-the-empathetic-psychologist"},{"categories":["NLP"],"content":"\rEarly WorksThe topics of identifying and generating empathy in natural langue processing haven’t seen as explosive research growth and application as topics like sentiment analysis. Perhaps the earliest work in identifying empathy in text data was done by Xiao et al., 201210, when they developed an N-gram language model-based maximum likelihood strategy to classify empathetic vs non-empathetic utterances from a dataset of clinical trial studies on substance use by college students. In 2015, Gibson et al.11 proposed computation of features based upon psycholinguistic norms, and in 2017, Khanpour et al.12 used a simple combination of Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks to identify empathetic messages in online health communities. It’s worth highlighting that most of these researches used datasets that weren’t publicly available to the NLP community. In 2018, Buechel et al.13 published their research which they claimed to be the first gold-standard for empathy prediction. Their paper looked at a more nuanced form of empathy that was based on psychology and included empathic concern, and personal distress. Also, the empathy ratings in their dataset were provided by writers instead of other annotators. They used Ridge regression, simple feed-forward neural nets, and CNN for their empathy prediction tasks on newswire articles. In somewhat related research, Perez-Rosas et al.14 looked into behavioral counseling and proposed a quantitative approach to understand the dynamics of counseling interactions and counselor empathy during motivational interviewing. They first identified linguistic and acoustic empathy markers and used those along with raw features to train classifiers that were able to predict counselor empathy. Some of the other prominent researches are listed below: Zara The SupergirlZara was an Empathetic Personality Recognition System created by Fung, Dey et al.15. Their research showed an interactive dialogue system that did sentiment analysis, emotion recognition, facial and speech recognition to extract user emotions in a human-robot conversational setup. They deployed their virtual robot as a webapp where users can interact with an animated cartoon character, Zara. You can watch their demo video on YouTube and also interact with their online webapp. Zara assesses a user’s personality by asking a series of questions, along with the follow-up inquiries, on different topics like the user’s childhood memory, last vacation, challenges at work, etc. It uses OpenSmile and Kaldi to perform emotion recognition from user audio, keyword matches from a pool of positive and negative emotion lexicons to perform sentiment analysis, and then uses these results to calculate the scores in four personality dimensions - extroversion, intuitive, judging, perceiving. Nora the Empathetic PsychologistSimilar to Zara, researchers Winata, Kampman et al.16 crated a virtual psychologist, an empathetic dialogue system named Nora, that could mimic a conversation with a psychologist. Nora employed a natural language understanding (NLU) module to classify user intent and slots, a dialogue management module to evaluate NLU output and manage dialog turns, and a language generation module to respond to the user. A simple CNN was used to detect stress, personality, sentiment, and six different emotions from audio. Real-Time Speech Emotion and Sentiment Recognition for Interactive Dialogue SystemsBertero, Siddique et al.17 also used a CNN-based approach to recognize emotion and sentiment from raw audio data in real-time to enable an empathetic conversational dialogue system. They used the Kaldi speech recognition toolkit to train deep neural network hidden Markov models (DNN-HMMs) that used the raw audio together with encode-decode parallel audio and outperformed the SVM baseline. Their work avoided any feature engineering to enable real-time speech processing. Generating Emotionally Flexible ResponsesOne of the","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:4:1","series":null,"tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/#real-time-speech-emotion-and-sentiment-recognition-for-interactive-dialogue-systems"},{"categories":["NLP"],"content":"\rEarly WorksThe topics of identifying and generating empathy in natural langue processing haven’t seen as explosive research growth and application as topics like sentiment analysis. Perhaps the earliest work in identifying empathy in text data was done by Xiao et al., 201210, when they developed an N-gram language model-based maximum likelihood strategy to classify empathetic vs non-empathetic utterances from a dataset of clinical trial studies on substance use by college students. In 2015, Gibson et al.11 proposed computation of features based upon psycholinguistic norms, and in 2017, Khanpour et al.12 used a simple combination of Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks to identify empathetic messages in online health communities. It’s worth highlighting that most of these researches used datasets that weren’t publicly available to the NLP community. In 2018, Buechel et al.13 published their research which they claimed to be the first gold-standard for empathy prediction. Their paper looked at a more nuanced form of empathy that was based on psychology and included empathic concern, and personal distress. Also, the empathy ratings in their dataset were provided by writers instead of other annotators. They used Ridge regression, simple feed-forward neural nets, and CNN for their empathy prediction tasks on newswire articles. In somewhat related research, Perez-Rosas et al.14 looked into behavioral counseling and proposed a quantitative approach to understand the dynamics of counseling interactions and counselor empathy during motivational interviewing. They first identified linguistic and acoustic empathy markers and used those along with raw features to train classifiers that were able to predict counselor empathy. Some of the other prominent researches are listed below: Zara The SupergirlZara was an Empathetic Personality Recognition System created by Fung, Dey et al.15. Their research showed an interactive dialogue system that did sentiment analysis, emotion recognition, facial and speech recognition to extract user emotions in a human-robot conversational setup. They deployed their virtual robot as a webapp where users can interact with an animated cartoon character, Zara. You can watch their demo video on YouTube and also interact with their online webapp. Zara assesses a user’s personality by asking a series of questions, along with the follow-up inquiries, on different topics like the user’s childhood memory, last vacation, challenges at work, etc. It uses OpenSmile and Kaldi to perform emotion recognition from user audio, keyword matches from a pool of positive and negative emotion lexicons to perform sentiment analysis, and then uses these results to calculate the scores in four personality dimensions - extroversion, intuitive, judging, perceiving. Nora the Empathetic PsychologistSimilar to Zara, researchers Winata, Kampman et al.16 crated a virtual psychologist, an empathetic dialogue system named Nora, that could mimic a conversation with a psychologist. Nora employed a natural language understanding (NLU) module to classify user intent and slots, a dialogue management module to evaluate NLU output and manage dialog turns, and a language generation module to respond to the user. A simple CNN was used to detect stress, personality, sentiment, and six different emotions from audio. Real-Time Speech Emotion and Sentiment Recognition for Interactive Dialogue SystemsBertero, Siddique et al.17 also used a CNN-based approach to recognize emotion and sentiment from raw audio data in real-time to enable an empathetic conversational dialogue system. They used the Kaldi speech recognition toolkit to train deep neural network hidden Markov models (DNN-HMMs) that used the raw audio together with encode-decode parallel audio and outperformed the SVM baseline. Their work avoided any feature engineering to enable real-time speech processing. Generating Emotionally Flexible ResponsesOne of the","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:4:1","series":null,"tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/#generating-emotionally-flexible-responses"},{"categories":["NLP"],"content":"\rEarly WorksThe topics of identifying and generating empathy in natural langue processing haven’t seen as explosive research growth and application as topics like sentiment analysis. Perhaps the earliest work in identifying empathy in text data was done by Xiao et al., 201210, when they developed an N-gram language model-based maximum likelihood strategy to classify empathetic vs non-empathetic utterances from a dataset of clinical trial studies on substance use by college students. In 2015, Gibson et al.11 proposed computation of features based upon psycholinguistic norms, and in 2017, Khanpour et al.12 used a simple combination of Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks to identify empathetic messages in online health communities. It’s worth highlighting that most of these researches used datasets that weren’t publicly available to the NLP community. In 2018, Buechel et al.13 published their research which they claimed to be the first gold-standard for empathy prediction. Their paper looked at a more nuanced form of empathy that was based on psychology and included empathic concern, and personal distress. Also, the empathy ratings in their dataset were provided by writers instead of other annotators. They used Ridge regression, simple feed-forward neural nets, and CNN for their empathy prediction tasks on newswire articles. In somewhat related research, Perez-Rosas et al.14 looked into behavioral counseling and proposed a quantitative approach to understand the dynamics of counseling interactions and counselor empathy during motivational interviewing. They first identified linguistic and acoustic empathy markers and used those along with raw features to train classifiers that were able to predict counselor empathy. Some of the other prominent researches are listed below: Zara The SupergirlZara was an Empathetic Personality Recognition System created by Fung, Dey et al.15. Their research showed an interactive dialogue system that did sentiment analysis, emotion recognition, facial and speech recognition to extract user emotions in a human-robot conversational setup. They deployed their virtual robot as a webapp where users can interact with an animated cartoon character, Zara. You can watch their demo video on YouTube and also interact with their online webapp. Zara assesses a user’s personality by asking a series of questions, along with the follow-up inquiries, on different topics like the user’s childhood memory, last vacation, challenges at work, etc. It uses OpenSmile and Kaldi to perform emotion recognition from user audio, keyword matches from a pool of positive and negative emotion lexicons to perform sentiment analysis, and then uses these results to calculate the scores in four personality dimensions - extroversion, intuitive, judging, perceiving. Nora the Empathetic PsychologistSimilar to Zara, researchers Winata, Kampman et al.16 crated a virtual psychologist, an empathetic dialogue system named Nora, that could mimic a conversation with a psychologist. Nora employed a natural language understanding (NLU) module to classify user intent and slots, a dialogue management module to evaluate NLU output and manage dialog turns, and a language generation module to respond to the user. A simple CNN was used to detect stress, personality, sentiment, and six different emotions from audio. Real-Time Speech Emotion and Sentiment Recognition for Interactive Dialogue SystemsBertero, Siddique et al.17 also used a CNN-based approach to recognize emotion and sentiment from raw audio data in real-time to enable an empathetic conversational dialogue system. They used the Kaldi speech recognition toolkit to train deep neural network hidden Markov models (DNN-HMMs) that used the raw audio together with encode-decode parallel audio and outperformed the SVM baseline. Their work avoided any feature engineering to enable real-time speech processing. Generating Emotionally Flexible ResponsesOne of the","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:4:1","series":null,"tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/#using-natural-labels-to-generate-an-emotional-response"},{"categories":["NLP"],"content":"\rEarly WorksThe topics of identifying and generating empathy in natural langue processing haven’t seen as explosive research growth and application as topics like sentiment analysis. Perhaps the earliest work in identifying empathy in text data was done by Xiao et al., 201210, when they developed an N-gram language model-based maximum likelihood strategy to classify empathetic vs non-empathetic utterances from a dataset of clinical trial studies on substance use by college students. In 2015, Gibson et al.11 proposed computation of features based upon psycholinguistic norms, and in 2017, Khanpour et al.12 used a simple combination of Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks to identify empathetic messages in online health communities. It’s worth highlighting that most of these researches used datasets that weren’t publicly available to the NLP community. In 2018, Buechel et al.13 published their research which they claimed to be the first gold-standard for empathy prediction. Their paper looked at a more nuanced form of empathy that was based on psychology and included empathic concern, and personal distress. Also, the empathy ratings in their dataset were provided by writers instead of other annotators. They used Ridge regression, simple feed-forward neural nets, and CNN for their empathy prediction tasks on newswire articles. In somewhat related research, Perez-Rosas et al.14 looked into behavioral counseling and proposed a quantitative approach to understand the dynamics of counseling interactions and counselor empathy during motivational interviewing. They first identified linguistic and acoustic empathy markers and used those along with raw features to train classifiers that were able to predict counselor empathy. Some of the other prominent researches are listed below: Zara The SupergirlZara was an Empathetic Personality Recognition System created by Fung, Dey et al.15. Their research showed an interactive dialogue system that did sentiment analysis, emotion recognition, facial and speech recognition to extract user emotions in a human-robot conversational setup. They deployed their virtual robot as a webapp where users can interact with an animated cartoon character, Zara. You can watch their demo video on YouTube and also interact with their online webapp. Zara assesses a user’s personality by asking a series of questions, along with the follow-up inquiries, on different topics like the user’s childhood memory, last vacation, challenges at work, etc. It uses OpenSmile and Kaldi to perform emotion recognition from user audio, keyword matches from a pool of positive and negative emotion lexicons to perform sentiment analysis, and then uses these results to calculate the scores in four personality dimensions - extroversion, intuitive, judging, perceiving. Nora the Empathetic PsychologistSimilar to Zara, researchers Winata, Kampman et al.16 crated a virtual psychologist, an empathetic dialogue system named Nora, that could mimic a conversation with a psychologist. Nora employed a natural language understanding (NLU) module to classify user intent and slots, a dialogue management module to evaluate NLU output and manage dialog turns, and a language generation module to respond to the user. A simple CNN was used to detect stress, personality, sentiment, and six different emotions from audio. Real-Time Speech Emotion and Sentiment Recognition for Interactive Dialogue SystemsBertero, Siddique et al.17 also used a CNN-based approach to recognize emotion and sentiment from raw audio data in real-time to enable an empathetic conversational dialogue system. They used the Kaldi speech recognition toolkit to train deep neural network hidden Markov models (DNN-HMMs) that used the raw audio together with encode-decode parallel audio and outperformed the SVM baseline. Their work avoided any feature engineering to enable real-time speech processing. Generating Emotionally Flexible ResponsesOne of the","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:4:1","series":null,"tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/#using-gan-to-generate-emotionally-diverse-responses"},{"categories":["NLP"],"content":"\rEarly WorksThe topics of identifying and generating empathy in natural langue processing haven’t seen as explosive research growth and application as topics like sentiment analysis. Perhaps the earliest work in identifying empathy in text data was done by Xiao et al., 201210, when they developed an N-gram language model-based maximum likelihood strategy to classify empathetic vs non-empathetic utterances from a dataset of clinical trial studies on substance use by college students. In 2015, Gibson et al.11 proposed computation of features based upon psycholinguistic norms, and in 2017, Khanpour et al.12 used a simple combination of Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks to identify empathetic messages in online health communities. It’s worth highlighting that most of these researches used datasets that weren’t publicly available to the NLP community. In 2018, Buechel et al.13 published their research which they claimed to be the first gold-standard for empathy prediction. Their paper looked at a more nuanced form of empathy that was based on psychology and included empathic concern, and personal distress. Also, the empathy ratings in their dataset were provided by writers instead of other annotators. They used Ridge regression, simple feed-forward neural nets, and CNN for their empathy prediction tasks on newswire articles. In somewhat related research, Perez-Rosas et al.14 looked into behavioral counseling and proposed a quantitative approach to understand the dynamics of counseling interactions and counselor empathy during motivational interviewing. They first identified linguistic and acoustic empathy markers and used those along with raw features to train classifiers that were able to predict counselor empathy. Some of the other prominent researches are listed below: Zara The SupergirlZara was an Empathetic Personality Recognition System created by Fung, Dey et al.15. Their research showed an interactive dialogue system that did sentiment analysis, emotion recognition, facial and speech recognition to extract user emotions in a human-robot conversational setup. They deployed their virtual robot as a webapp where users can interact with an animated cartoon character, Zara. You can watch their demo video on YouTube and also interact with their online webapp. Zara assesses a user’s personality by asking a series of questions, along with the follow-up inquiries, on different topics like the user’s childhood memory, last vacation, challenges at work, etc. It uses OpenSmile and Kaldi to perform emotion recognition from user audio, keyword matches from a pool of positive and negative emotion lexicons to perform sentiment analysis, and then uses these results to calculate the scores in four personality dimensions - extroversion, intuitive, judging, perceiving. Nora the Empathetic PsychologistSimilar to Zara, researchers Winata, Kampman et al.16 crated a virtual psychologist, an empathetic dialogue system named Nora, that could mimic a conversation with a psychologist. Nora employed a natural language understanding (NLU) module to classify user intent and slots, a dialogue management module to evaluate NLU output and manage dialog turns, and a language generation module to respond to the user. A simple CNN was used to detect stress, personality, sentiment, and six different emotions from audio. Real-Time Speech Emotion and Sentiment Recognition for Interactive Dialogue SystemsBertero, Siddique et al.17 also used a CNN-based approach to recognize emotion and sentiment from raw audio data in real-time to enable an empathetic conversational dialogue system. They used the Kaldi speech recognition toolkit to train deep neural network hidden Markov models (DNN-HMMs) that used the raw audio together with encode-decode parallel audio and outperformed the SVM baseline. Their work avoided any feature engineering to enable real-time speech processing. Generating Emotionally Flexible ResponsesOne of the","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:4:1","series":null,"tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/#a-dual-decoder-framework-to-generate-a-response-with-given-sentiment"},{"categories":["NLP"],"content":"\rEarly WorksThe topics of identifying and generating empathy in natural langue processing haven’t seen as explosive research growth and application as topics like sentiment analysis. Perhaps the earliest work in identifying empathy in text data was done by Xiao et al., 201210, when they developed an N-gram language model-based maximum likelihood strategy to classify empathetic vs non-empathetic utterances from a dataset of clinical trial studies on substance use by college students. In 2015, Gibson et al.11 proposed computation of features based upon psycholinguistic norms, and in 2017, Khanpour et al.12 used a simple combination of Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks to identify empathetic messages in online health communities. It’s worth highlighting that most of these researches used datasets that weren’t publicly available to the NLP community. In 2018, Buechel et al.13 published their research which they claimed to be the first gold-standard for empathy prediction. Their paper looked at a more nuanced form of empathy that was based on psychology and included empathic concern, and personal distress. Also, the empathy ratings in their dataset were provided by writers instead of other annotators. They used Ridge regression, simple feed-forward neural nets, and CNN for their empathy prediction tasks on newswire articles. In somewhat related research, Perez-Rosas et al.14 looked into behavioral counseling and proposed a quantitative approach to understand the dynamics of counseling interactions and counselor empathy during motivational interviewing. They first identified linguistic and acoustic empathy markers and used those along with raw features to train classifiers that were able to predict counselor empathy. Some of the other prominent researches are listed below: Zara The SupergirlZara was an Empathetic Personality Recognition System created by Fung, Dey et al.15. Their research showed an interactive dialogue system that did sentiment analysis, emotion recognition, facial and speech recognition to extract user emotions in a human-robot conversational setup. They deployed their virtual robot as a webapp where users can interact with an animated cartoon character, Zara. You can watch their demo video on YouTube and also interact with their online webapp. Zara assesses a user’s personality by asking a series of questions, along with the follow-up inquiries, on different topics like the user’s childhood memory, last vacation, challenges at work, etc. It uses OpenSmile and Kaldi to perform emotion recognition from user audio, keyword matches from a pool of positive and negative emotion lexicons to perform sentiment analysis, and then uses these results to calculate the scores in four personality dimensions - extroversion, intuitive, judging, perceiving. Nora the Empathetic PsychologistSimilar to Zara, researchers Winata, Kampman et al.16 crated a virtual psychologist, an empathetic dialogue system named Nora, that could mimic a conversation with a psychologist. Nora employed a natural language understanding (NLU) module to classify user intent and slots, a dialogue management module to evaluate NLU output and manage dialog turns, and a language generation module to respond to the user. A simple CNN was used to detect stress, personality, sentiment, and six different emotions from audio. Real-Time Speech Emotion and Sentiment Recognition for Interactive Dialogue SystemsBertero, Siddique et al.17 also used a CNN-based approach to recognize emotion and sentiment from raw audio data in real-time to enable an empathetic conversational dialogue system. They used the Kaldi speech recognition toolkit to train deep neural network hidden Markov models (DNN-HMMs) that used the raw audio together with encode-decode parallel audio and outperformed the SVM baseline. Their work avoided any feature engineering to enable real-time speech processing. Generating Emotionally Flexible ResponsesOne of the","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:4:1","series":null,"tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/#using-reinforcement-learning-to-reward-future-emotional-states"},{"categories":["NLP"],"content":"\rEarly WorksThe topics of identifying and generating empathy in natural langue processing haven’t seen as explosive research growth and application as topics like sentiment analysis. Perhaps the earliest work in identifying empathy in text data was done by Xiao et al., 201210, when they developed an N-gram language model-based maximum likelihood strategy to classify empathetic vs non-empathetic utterances from a dataset of clinical trial studies on substance use by college students. In 2015, Gibson et al.11 proposed computation of features based upon psycholinguistic norms, and in 2017, Khanpour et al.12 used a simple combination of Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks to identify empathetic messages in online health communities. It’s worth highlighting that most of these researches used datasets that weren’t publicly available to the NLP community. In 2018, Buechel et al.13 published their research which they claimed to be the first gold-standard for empathy prediction. Their paper looked at a more nuanced form of empathy that was based on psychology and included empathic concern, and personal distress. Also, the empathy ratings in their dataset were provided by writers instead of other annotators. They used Ridge regression, simple feed-forward neural nets, and CNN for their empathy prediction tasks on newswire articles. In somewhat related research, Perez-Rosas et al.14 looked into behavioral counseling and proposed a quantitative approach to understand the dynamics of counseling interactions and counselor empathy during motivational interviewing. They first identified linguistic and acoustic empathy markers and used those along with raw features to train classifiers that were able to predict counselor empathy. Some of the other prominent researches are listed below: Zara The SupergirlZara was an Empathetic Personality Recognition System created by Fung, Dey et al.15. Their research showed an interactive dialogue system that did sentiment analysis, emotion recognition, facial and speech recognition to extract user emotions in a human-robot conversational setup. They deployed their virtual robot as a webapp where users can interact with an animated cartoon character, Zara. You can watch their demo video on YouTube and also interact with their online webapp. Zara assesses a user’s personality by asking a series of questions, along with the follow-up inquiries, on different topics like the user’s childhood memory, last vacation, challenges at work, etc. It uses OpenSmile and Kaldi to perform emotion recognition from user audio, keyword matches from a pool of positive and negative emotion lexicons to perform sentiment analysis, and then uses these results to calculate the scores in four personality dimensions - extroversion, intuitive, judging, perceiving. Nora the Empathetic PsychologistSimilar to Zara, researchers Winata, Kampman et al.16 crated a virtual psychologist, an empathetic dialogue system named Nora, that could mimic a conversation with a psychologist. Nora employed a natural language understanding (NLU) module to classify user intent and slots, a dialogue management module to evaluate NLU output and manage dialog turns, and a language generation module to respond to the user. A simple CNN was used to detect stress, personality, sentiment, and six different emotions from audio. Real-Time Speech Emotion and Sentiment Recognition for Interactive Dialogue SystemsBertero, Siddique et al.17 also used a CNN-based approach to recognize emotion and sentiment from raw audio data in real-time to enable an empathetic conversational dialogue system. They used the Kaldi speech recognition toolkit to train deep neural network hidden Markov models (DNN-HMMs) that used the raw audio together with encode-decode parallel audio and outperformed the SVM baseline. Their work avoided any feature engineering to enable real-time speech processing. Generating Emotionally Flexible ResponsesOne of the","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:4:1","series":null,"tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/#formalizing-empathy-generation-a-new-dataset-from-facebook-ai-research"},{"categories":["NLP"],"content":"\rEarly WorksThe topics of identifying and generating empathy in natural langue processing haven’t seen as explosive research growth and application as topics like sentiment analysis. Perhaps the earliest work in identifying empathy in text data was done by Xiao et al., 201210, when they developed an N-gram language model-based maximum likelihood strategy to classify empathetic vs non-empathetic utterances from a dataset of clinical trial studies on substance use by college students. In 2015, Gibson et al.11 proposed computation of features based upon psycholinguistic norms, and in 2017, Khanpour et al.12 used a simple combination of Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks to identify empathetic messages in online health communities. It’s worth highlighting that most of these researches used datasets that weren’t publicly available to the NLP community. In 2018, Buechel et al.13 published their research which they claimed to be the first gold-standard for empathy prediction. Their paper looked at a more nuanced form of empathy that was based on psychology and included empathic concern, and personal distress. Also, the empathy ratings in their dataset were provided by writers instead of other annotators. They used Ridge regression, simple feed-forward neural nets, and CNN for their empathy prediction tasks on newswire articles. In somewhat related research, Perez-Rosas et al.14 looked into behavioral counseling and proposed a quantitative approach to understand the dynamics of counseling interactions and counselor empathy during motivational interviewing. They first identified linguistic and acoustic empathy markers and used those along with raw features to train classifiers that were able to predict counselor empathy. Some of the other prominent researches are listed below: Zara The SupergirlZara was an Empathetic Personality Recognition System created by Fung, Dey et al.15. Their research showed an interactive dialogue system that did sentiment analysis, emotion recognition, facial and speech recognition to extract user emotions in a human-robot conversational setup. They deployed their virtual robot as a webapp where users can interact with an animated cartoon character, Zara. You can watch their demo video on YouTube and also interact with their online webapp. Zara assesses a user’s personality by asking a series of questions, along with the follow-up inquiries, on different topics like the user’s childhood memory, last vacation, challenges at work, etc. It uses OpenSmile and Kaldi to perform emotion recognition from user audio, keyword matches from a pool of positive and negative emotion lexicons to perform sentiment analysis, and then uses these results to calculate the scores in four personality dimensions - extroversion, intuitive, judging, perceiving. Nora the Empathetic PsychologistSimilar to Zara, researchers Winata, Kampman et al.16 crated a virtual psychologist, an empathetic dialogue system named Nora, that could mimic a conversation with a psychologist. Nora employed a natural language understanding (NLU) module to classify user intent and slots, a dialogue management module to evaluate NLU output and manage dialog turns, and a language generation module to respond to the user. A simple CNN was used to detect stress, personality, sentiment, and six different emotions from audio. Real-Time Speech Emotion and Sentiment Recognition for Interactive Dialogue SystemsBertero, Siddique et al.17 also used a CNN-based approach to recognize emotion and sentiment from raw audio data in real-time to enable an empathetic conversational dialogue system. They used the Kaldi speech recognition toolkit to train deep neural network hidden Markov models (DNN-HMMs) that used the raw audio together with encode-decode parallel audio and outperformed the SVM baseline. Their work avoided any feature engineering to enable real-time speech processing. Generating Emotionally Flexible ResponsesOne of the","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:4:1","series":null,"tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/#maximizing-positive-arousal"},{"categories":["NLP"],"content":"\rCurrent State-of-the-Art Research on Empathetic Response GenerationThe sheer pace of progress in NLP makes it difficult to keep track of current state-of-the-art researches. And, often it is difficult to pin-down one research work as the current state-of-the-art, simply because there might be some other research that shows improvement in certain but not all of the shared metrics or different research work might have used completely different dataset for training and evaluation. However, the following two recent papers stand out for the quality of their results for empathetic response generation in dialogue systems on empathetic-dialogues dataset23. MoEL: Mixture of Empathetic ListenersThe authors of this research (Lin et al., 2019)26 argue that prior researches made assumptions such as- understanding the emotional state of the user is enough for the model to implicitly learn how to respond appropriately without any additional inductive bias. However, this assumption may lead to generic response outputs from the single decoder that is learning to produce all emotions. Some of the other works assume that the emotion to condition the generation on is given as input, which may not always be true. There could be multiple emotions present in different turns. Hence a dialogue state embedding is also incorporated along with word embedding and standard positional embedding to produce context embedding. This idea was originally proposed by Wolf et al., 201927. Similar to the prior art, MoEL first encodes the dialogue context and uses it to recognize the emotional state (out of n possible states). But this architecture contains n decoders, called listeners, which are optimized to react to each context emotion. There is another listener, called meta-listener, that is trained along with other listeners and learns to softly combine the output states of all decoders according to the emotional classification distribution. This idea of having independent specialized experts (Listeners) was originally inspired by Shazeer et al. (2017)28. The 3-main components in this architecture are described below. Emotion Tracker: It is simply a standard Transformer encoder that encodes context and also computes a distribution over the possible user emotions. A query token QRY at the beginning of each input sequence, as in BERT, to compute the weighted sum of the output tensor. Emotion-aware Listeners: These are standard Transformer decoders that independently attend to the distribution produced by the emotional tracker and compute their own representation. There is also a shared listener that learns shared information for all emotions. The output from the shared listener is expected to be a general representation that can help the model to capture the dialogue context. But each empathetic listener learns how to respond to a particular emotion. Hence, different weights are assigned to each empathetic listener according to the user emotion distribution, while assigning a fixed weight of 1 to the shared listener. Meta Listener: Finally, the meta listener takes the weighted sum of representations from the listeners and generates the final response. The intuition is that each listener specializes in a certain emotion and the Meta Listener gathers the opinions generated by multiple listeners to produce the final response. In the experiments conducted on empathetic-dialogues dataset23, MoEL shows improvements over the baseline in Empathy and Relevance, while the baseline had a higher score on Fluency. MIME: MIMicking Emotions for Empathetic Response GenerationMimicry is one of the key components related to empathy. Research in Psychology shows that mimicry contributes substantially to an empathic response. Sonnby-Borgstrom, 200229 proposed that mimicry enables one to automatically share and understand another’s emotions. Their proposal also receives support from studies showing a (notably, weak) correlation between the strength of the mimicry response and trait m","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:4:2","series":null,"tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/#current-state-of-the-art-research-on-empathetic-response-generation"},{"categories":["NLP"],"content":"\rCurrent State-of-the-Art Research on Empathetic Response GenerationThe sheer pace of progress in NLP makes it difficult to keep track of current state-of-the-art researches. And, often it is difficult to pin-down one research work as the current state-of-the-art, simply because there might be some other research that shows improvement in certain but not all of the shared metrics or different research work might have used completely different dataset for training and evaluation. However, the following two recent papers stand out for the quality of their results for empathetic response generation in dialogue systems on empathetic-dialogues dataset23. MoEL: Mixture of Empathetic ListenersThe authors of this research (Lin et al., 2019)26 argue that prior researches made assumptions such as- understanding the emotional state of the user is enough for the model to implicitly learn how to respond appropriately without any additional inductive bias. However, this assumption may lead to generic response outputs from the single decoder that is learning to produce all emotions. Some of the other works assume that the emotion to condition the generation on is given as input, which may not always be true. There could be multiple emotions present in different turns. Hence a dialogue state embedding is also incorporated along with word embedding and standard positional embedding to produce context embedding. This idea was originally proposed by Wolf et al., 201927. Similar to the prior art, MoEL first encodes the dialogue context and uses it to recognize the emotional state (out of n possible states). But this architecture contains n decoders, called listeners, which are optimized to react to each context emotion. There is another listener, called meta-listener, that is trained along with other listeners and learns to softly combine the output states of all decoders according to the emotional classification distribution. This idea of having independent specialized experts (Listeners) was originally inspired by Shazeer et al. (2017)28. The 3-main components in this architecture are described below. Emotion Tracker: It is simply a standard Transformer encoder that encodes context and also computes a distribution over the possible user emotions. A query token QRY at the beginning of each input sequence, as in BERT, to compute the weighted sum of the output tensor. Emotion-aware Listeners: These are standard Transformer decoders that independently attend to the distribution produced by the emotional tracker and compute their own representation. There is also a shared listener that learns shared information for all emotions. The output from the shared listener is expected to be a general representation that can help the model to capture the dialogue context. But each empathetic listener learns how to respond to a particular emotion. Hence, different weights are assigned to each empathetic listener according to the user emotion distribution, while assigning a fixed weight of 1 to the shared listener. Meta Listener: Finally, the meta listener takes the weighted sum of representations from the listeners and generates the final response. The intuition is that each listener specializes in a certain emotion and the Meta Listener gathers the opinions generated by multiple listeners to produce the final response. In the experiments conducted on empathetic-dialogues dataset23, MoEL shows improvements over the baseline in Empathy and Relevance, while the baseline had a higher score on Fluency. MIME: MIMicking Emotions for Empathetic Response GenerationMimicry is one of the key components related to empathy. Research in Psychology shows that mimicry contributes substantially to an empathic response. Sonnby-Borgstrom, 200229 proposed that mimicry enables one to automatically share and understand another’s emotions. Their proposal also receives support from studies showing a (notably, weak) correlation between the strength of the mimicry response and trait m","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:4:2","series":null,"tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/#moel-mixture-of-empathetic-listeners"},{"categories":["NLP"],"content":"\rCurrent State-of-the-Art Research on Empathetic Response GenerationThe sheer pace of progress in NLP makes it difficult to keep track of current state-of-the-art researches. And, often it is difficult to pin-down one research work as the current state-of-the-art, simply because there might be some other research that shows improvement in certain but not all of the shared metrics or different research work might have used completely different dataset for training and evaluation. However, the following two recent papers stand out for the quality of their results for empathetic response generation in dialogue systems on empathetic-dialogues dataset23. MoEL: Mixture of Empathetic ListenersThe authors of this research (Lin et al., 2019)26 argue that prior researches made assumptions such as- understanding the emotional state of the user is enough for the model to implicitly learn how to respond appropriately without any additional inductive bias. However, this assumption may lead to generic response outputs from the single decoder that is learning to produce all emotions. Some of the other works assume that the emotion to condition the generation on is given as input, which may not always be true. There could be multiple emotions present in different turns. Hence a dialogue state embedding is also incorporated along with word embedding and standard positional embedding to produce context embedding. This idea was originally proposed by Wolf et al., 201927. Similar to the prior art, MoEL first encodes the dialogue context and uses it to recognize the emotional state (out of n possible states). But this architecture contains n decoders, called listeners, which are optimized to react to each context emotion. There is another listener, called meta-listener, that is trained along with other listeners and learns to softly combine the output states of all decoders according to the emotional classification distribution. This idea of having independent specialized experts (Listeners) was originally inspired by Shazeer et al. (2017)28. The 3-main components in this architecture are described below. Emotion Tracker: It is simply a standard Transformer encoder that encodes context and also computes a distribution over the possible user emotions. A query token QRY at the beginning of each input sequence, as in BERT, to compute the weighted sum of the output tensor. Emotion-aware Listeners: These are standard Transformer decoders that independently attend to the distribution produced by the emotional tracker and compute their own representation. There is also a shared listener that learns shared information for all emotions. The output from the shared listener is expected to be a general representation that can help the model to capture the dialogue context. But each empathetic listener learns how to respond to a particular emotion. Hence, different weights are assigned to each empathetic listener according to the user emotion distribution, while assigning a fixed weight of 1 to the shared listener. Meta Listener: Finally, the meta listener takes the weighted sum of representations from the listeners and generates the final response. The intuition is that each listener specializes in a certain emotion and the Meta Listener gathers the opinions generated by multiple listeners to produce the final response. In the experiments conducted on empathetic-dialogues dataset23, MoEL shows improvements over the baseline in Empathy and Relevance, while the baseline had a higher score on Fluency. MIME: MIMicking Emotions for Empathetic Response GenerationMimicry is one of the key components related to empathy. Research in Psychology shows that mimicry contributes substantially to an empathic response. Sonnby-Borgstrom, 200229 proposed that mimicry enables one to automatically share and understand another’s emotions. Their proposal also receives support from studies showing a (notably, weak) correlation between the strength of the mimicry response and trait m","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:4:2","series":null,"tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/#mime-mimicking-emotions-for-empathetic-response-generation"},{"categories":["NLP"],"content":"\rConclusionIn this article, we looked at what Empathy means from a philosophical, psychological, and neuroscience perspective. Then we did a deep dive into research on making Empathetic NLU systems. We went through several datasets and model architectures, including a peek into the current state-of-the-art systems that can generate empathetic responses in conversational dialogue systems. I hope you learned some new things from this post. And, I will love to hear your feedback in the comments below. ","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:5:0","series":null,"tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/#conclusion"},{"categories":["NLP"],"content":"\rReferences Empathy, Stanford Encyclopedia of Philosophy ↩︎ Einfühlung and Empathy: What do they mean?, Karal McLaren ↩︎ Personality and Empathy, Rosalind Daymond ↩︎ The Social Neuroscience of Empathy, Tania Singer and Claus Lamm ↩︎ ↩︎ Machiavellian intelligence: Social expertise and the evolution of intellect in monkeys, apes, and humans, Richard Byrne ↩︎ Empathy: A social psychological approach, Mark H. Davis ↩︎ Altruism and human nature: Resolving the evolutionary paradox, Ian Vine ↩︎ Empathy Reconsidered: New Directions in Psychotherapy, Arthur Bohart, Leslie Greenberg ↩︎ Empathy and quality of care, Stewart Mercer; William Reynolds ↩︎ Analyzing the Language of Therapist Empathy in Motivational Interview based Psychotherapy, Xiao et al ↩︎ Predicting Therapist Empathy in Motivational Interviews using Language Features Inspired by Psycholinguistic Norms, Gibson et al. ↩︎ Identifying Empathetic Messages in Online Health Communities, Khanour et al. ↩︎ Modeling Empathy and Distress in Reaction to News Stories, Buechel et al. ↩︎ Understanding and Predicting Empathic Behavior in Counseling Therapy, Perez-Rosas et al. ↩︎ Zara The Supergirl: An Empathetic Personality Recognition System, Fung, Dey et al. ↩︎ Nora the Empathetic Psychologist, Winata, Kampman et al. ↩︎ Real-Time Speech Emotion and Sentiment Recognition for Interactive Dialogue Systems, Bertero, Siddique et al. ↩︎ Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory, Zhou, Huang et al. ↩︎ Affect-lm: A neural language model for customizable affective text generation, Ghosh et al. ↩︎ MojiTalk: Generating Emotional Responses at Scale, Zhou and Wang, 2018 ↩︎ SentiGAN: Generating Sentimental Texts via Mixture Adversarial Networks, Wang and Wan, 2018 ↩︎ A Simple Dual-decoder Model for Generating Response with Sentiment, Xiuyu and Yunfang, 2019; [Source Code] ↩︎ ↩︎ I Know The Feeling: Learning To Converse With Empathy, Rashkin, Smith et al. 2019 ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ Eliciting Positive Emotion through Affect-Sensitive Dialogue Response Generation: A Neural Network Approach, Lubis et al. 2018 ↩︎ Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models, Serban et al. 2016 ↩︎ MoEL: Mixture of Empathetic Listeners, Lin et al., 2019; [Source Code] ↩︎ TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents, Wolf et al., 2019 ↩︎ Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer, Shazeer et al. (2017) ↩︎ Automatic mimicry reactions as related to differences in emotional empathy, Sonnby–Borgström, 2008 ↩︎ Neural mechanisms of empathy in humans: A relay from neural systems for imitation to limbic areas, Carr et al., 2003 ↩︎ MIME: MIMicking Emotions for Empathetic Response Generation, Majumder et al. 2020 [Source Code] ↩︎ ","date":"2020-12-07","objectID":"/posts/2020/12/generating-empathetic-responses/:6:0","series":null,"tags":["dialogue systems","literature review"],"title":"Towards Empathetic Dialogue Systems","uri":"/posts/2020/12/generating-empathetic-responses/#references"},{"categories":["Software Engineering"],"content":"Identifying patterns among questions is quite an effective strategy when you are grinding LeetCode in preparation for your upcoming software engineering interviews. In this article, you will develop intuitions about Sliding Window pattern. You will also get a template approach to write code to solve these problems. I will also walk you through some LeetCode questions to show how to apply the template and at the end, there will be some LeetCode exercises for you to practice what you learn.","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/","series":null,"tags":["leetcode","algorithms"],"title":"Effective LeetCode: Understanding the Sliding Window Pattern","uri":"/posts/2020/10/leetcode-sliding-window/"},{"categories":["Software Engineering"],"content":"Identifying patterns among questions is quite an effective strategy when you are grinding LeetCode in preparation for your upcoming software engineering interviews. In this article, you will develop intuitions about Sliding Window pattern. You will also get a template approach to write code to solve these problems. I will also walk you through some LeetCode questions to show how to apply the template and at the end, there will be some LeetCode exercises for you to practice what you learn. ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:0:0","series":null,"tags":["leetcode","algorithms"],"title":"Effective LeetCode: Understanding the Sliding Window Pattern","uri":"/posts/2020/10/leetcode-sliding-window/#"},{"categories":["Software Engineering"],"content":"\rWhy Should One Focus on Programming Question Patterns?Whether you like them or not, solving programming challenges is a prevalent part of software engineering interviews. Most newbies and seasoned developers turn to leetcode.com to find a wide set of questions and discussion forums to help their interview preparations. However, many are baffled upon finding a more than a thousand questions on the website and not knowing where to start from. Many people in LeetCode and other discussion forums have shared their learning paths with other learners in the form of compiled list of questions that vary from difficulty level, question types, interviewing company and so on. A common occurring themes among those recommendations is to spend time on identifying patterns among questions. It helps in reinforcing your brain to think about the solution in more general terms so that you don’t have to cram your memory with specific details for individual questions. This way you will be better prepared to take on such programming challenges that may quiz you on a diverse range of Data Structure and Algorithm questions. Through a series of articles on these patterns, I will share the tips and tricks from what I have learned while solving LeetCode problems. And hopefully it will help you to prepare more effectively and faster for your software programming rounds of interviews. ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:1:0","series":null,"tags":["leetcode","algorithms"],"title":"Effective LeetCode: Understanding the Sliding Window Pattern","uri":"/posts/2020/10/leetcode-sliding-window/#why-should-one-focus-on-programming-question-patterns"},{"categories":["Software Engineering"],"content":"\rSliding Window PatternGenerally speaking, in sliding window problems, you would be given a sequence of values, and your task will be to return the best solution given a limiting constraint. The best solution would be defined in terms of a maximum or minimum value that occurs in a range or “window” over the given sequence. I know, it sounds a bit too generic. So, let’s try to materialize the idea with a fun example. ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:2:0","series":null,"tags":["leetcode","algorithms"],"title":"Effective LeetCode: Understanding the Sliding Window Pattern","uri":"/posts/2020/10/leetcode-sliding-window/#sliding-window-pattern"},{"categories":["Software Engineering"],"content":"\rAn Intuitive AnalogyLet’s assume you are playing an Animal-Crossing-meets-SIMS game. And you are in-charge of many cute little characters currently living on different islands in the game doing some fun tasks. This game allows these characters to collaborate such that they can visit other islands and do tasks together. However, in order to connect any two islands, you will have to make some standard-length connecting bridges. And, some of the islands may be farther away from each other, so they may require more than one connecting bridge. For simplicity, we are going to assume that the islands are located linearly as shown in the figure. As you play new quests in the game, and finish daily tasks, you can unlock more of these connecting bridges. But at any given point of time, you will have a limited number of bridges in your collection. Now your task is to use the available bridge count and connect the islands such that you end up with maximum possible number of characters connected with each other. Say if you had 2 bridges available, would you choose move 1 or move 2 from below? With move 1, you connect the first two islands and the two characters on those islands together. However, with move 2, you connect the last three islands and four of your characters together. So the second move give you the best answer. As an exercise, can you think of any other moves in the above setting? What results would you get in those? While thinking of a solution to the above questions, you were given a constraint (limited number of available bridges) and you found an optimized solution (maximum number of characters connected). Because there were a limited number of islands and characters, you were able to simply eyeball and easily come up with a solution that was the best with the given constraint. But as you can imagine, the things can get a lot trickier if you have tens of thousands of islands with thousands of characters living on them and maybe hundreds of bridges available at your disposal. So, let’s summon the coder in you and formulate this problem as a programming challenge. ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:2:1","series":null,"tags":["leetcode","algorithms"],"title":"Effective LeetCode: Understanding the Sliding Window Pattern","uri":"/posts/2020/10/leetcode-sliding-window/#an-intuitive-analogy"},{"categories":["Software Engineering"],"content":"\rThe Sliding Window ApproachFirst, we are going to represent the arrangement of islands, bridges and characters in terms of a sequence of values, i.e. an array. So, let’s assume that an index in array with value -1 mean an empty location between islands where you could build a bridge, value 0 signifies the presence of a bridge at that location, and any other number is simply a count of the number of characters on the island at that index. So your goal is to find the maximum count for connected characters given a fixed number of bridges, B (for example, B=2). Before jumping into code, we need to first develop solid intuitions about Sliding Window approach and how it can help us in this problem. As you saw in the last section, we got our solution from a contiguous subsequence, i.e. a subarray. This subarray highlighted in red, is what we call a window. The left end, L, of the window is at index 3 and right end, R, is at index 7 (assuming index start at 0). This window spans over 5 array elements, so we can also say that the window length is 5. In Sliding Window pattern problems, we will calculate multiple solutions over varying length of the window, but we will only save the most optimal solution. A naïve approach could be to evaluate the input array for all possible length of windows, i.e. all possible placements for windows of length 1, 2, 3, 4, … , 8 (size of array), and calculate the result for each window, but save only the optimal value (maximum, in our example). However, as any seasoned LeetCode-er will tell you, your program will easily hit a Time Limit Exceeded wall even with a moderate sized array, because you have way too many potential solutions in your search space. So we need a better solution. Let’s solve the above problem that had a constraint of 2 bridges with a Sliding Window approach. In a sliding Window based solution, we will generally start from the left of the array and with a window of size 1, i.e. both the left and right ends, L and R respectively, of the window will be at index 0. At each step, we also calculate the current answer, i.e. the current number of connected characters inside the window, and save the most optimal solution, i.e. the maximum count so far. Now, let’s expand the window by moving the right end, R, as much as our constraint allows us (think of this as an outer loop in the code). The constraint in this example being the count of available bridges (B = 2). We used one bridge, but we do have one more left. So we keep expanding to right. We are out of bridges to use, but we can still move to the right, as there’s an island there and our constraint will still hold (max 2 bridges). We now have 2 connected characters, which is also our best answer so far. But we also have a problem, we can’t move to the right because that’s an empty slot and we are out of bridges to use. So now we will start to shrink our window from the left marker one step at a time, and keep doing it until we are allowed to move R to the right and still satisfy our constraint (think of it as another loop inside the outer one). At this point, we still can’t move R to the right, because we are out of bridges. So let’s move L to the right one more time and reclaim one bridge. Now we’re back on track. Moving R to the right requires a new bridge, but we do have that in inventory. Moving R to the right is still valid, so let’s do it. Moving R requires a bridge. So we go back to moving L to the right, and reclaim one bridge with the very first move. Moving R will make us use the bridge that we have in inventory. We can still move R more to the right. We can’t move R anymore. That brings us to the end of our algorithm. The best answer that we have is 4, which is also the most optimal answer. 😊 Notice the followings: We started with 2 markers left (L) and right (R) at index 0 R moved to the right towards the end of the array (we can use an outer loop for this) Inside the above looping process, if we hit a state where moving R wi","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:2:2","series":null,"tags":["leetcode","algorithms"],"title":"Effective LeetCode: Understanding the Sliding Window Pattern","uri":"/posts/2020/10/leetcode-sliding-window/#the-sliding-window-approach"},{"categories":["Software Engineering"],"content":"\rSolving LeetCode Problems with Sliding Window Pattern","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:3:0","series":null,"tags":["leetcode","algorithms"],"title":"Effective LeetCode: Understanding the Sliding Window Pattern","uri":"/posts/2020/10/leetcode-sliding-window/#solving-leetcode-problems-with-sliding-window-pattern"},{"categories":["Software Engineering"],"content":"\rLeetCode 1004: Max Consecutive Ones IIILet’s try to apply what we you have learned above and solve this LeetCode problem: https://leetcode.com/problems/max-consecutive-ones-iii/ You are given an array input that has 0s and 1s. You are given a constraint that you can change only K number of values from 0 to 1. And, your task is to find the longest subarray that contains only 1s. So, we are looking for the maximum window size (i.e. R - L + 1), such that the window contains all 1s, and given the constraint that we can change up to K number of values in the window from 0 to 1. Let’s first setup some basic variables: current_change_count = 0 # This is our \"inventory\", i.e. a count for the number of changes from 0 to 1 L = 0 # This is the left marker of our Sliding Window answer = -1 # This is the variable that will store the best answer Based on the earlier template, we need an outer loop, that will move R to the right. for R in range(len(A)): # Here A is the input array ... Inside the for loop, we need to update our “inventory”, i.e. current_change_count, if required i.e. if the value of the current position is 0, we need to make it 1 to include it in the window if A[R] == 0: current_change_count += 1 And we need an inner loop to move L to the right, if required i.e. if the constraint “current_change_count \u003c= K” doesn’t hold. while current_change_count \u003e K and L \u003c len(A): if A[L] == 0: current_change_count -= 1 L += 1 Finally we save the current answer as the best answer, if required, i.e. if the current window length (R-L+1) is greater than answer. answer = max(answer, R-L+1) So, our complete solution will be: class Solution: def longestOnes(self, A: List[int], K: int) -\u003e int: if not A: return 0 current_change_count = 0 L = 0 answer = -1 for R in range(len(A)): if A[R] == 0: current_change_count += 1 while current_change_count \u003e K and L \u003c len(A): if A[L] == 0: current_change_count -= 1 L += 1 answer = max(answer, R-L+1) return answer ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:3:1","series":null,"tags":["leetcode","algorithms"],"title":"Effective LeetCode: Understanding the Sliding Window Pattern","uri":"/posts/2020/10/leetcode-sliding-window/#leetcode-1004-max-consecutive-ones-iii"},{"categories":["Software Engineering"],"content":"\rLeetCode 1358: Number of Substrings Containing All Three CharactersLet’s take another example: https://leetcode.com/problems/number-of-substrings-containing-all-three-characters/ A valid window in this case is the one that satisfies the constraint that all of the three characters ‘a’, ‘b’ and ‘c’ are at least once present in the window. Let’s setup some basic variables: current_answer = 0 looking_for = [0, 0, 0] # We will save the count of 'a', 'b' and 'c' occurances in the window on the 3 indexes in this array/list L = 0 best_answer = 0 The outer loop will simply be: for R in S: ... Our inventory, in this case, keeps track of the count of ‘a’, ‘b’ and ‘c’ in “looking_for” list. Since we know each character in the input will be one of the three characters, we can directly update the index like this: looking_for[ord(R)-ord('a')] += 1 and the inner loop will move L to the right if the window has ‘a’, ‘b’ and ‘c’ occurring at least once. while L \u003c len(s) and looking_for[0] and looking_for[1] and looking_for[2]: looking_for[ord(s[L])-ord('a')] -= 1 # subtract count for the character at L L += 1 current_answer += 1 The optimal answer will simply add up the overall answers found in the Sliding Window. best_answer += current_answer So, our complete solution will be: class Solution: def numberOfSubstrings(self, s: str) -\u003e int: if len(s) \u003c 3: return 0 current_answer = 0 looking_for = [0, 0, 0] L = 0 best_answer = 0 for R in s: looking_for[ord(R)-ord('a')] += 1 while L \u003c len(s) and looking_for[0] and looking_for[1] and looking_for[2]: looking_for[ord(s[L])-ord('a')] -= 1 L += 1 current_answer += 1 best_answer += current_answer return best_answer ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:3:2","series":null,"tags":["leetcode","algorithms"],"title":"Effective LeetCode: Understanding the Sliding Window Pattern","uri":"/posts/2020/10/leetcode-sliding-window/#leetcode-1358-number-of-substrings-containing-all-three-characters"},{"categories":["Software Engineering"],"content":"\rLeetCode 904: Fruit Into BasketsLet’s solve yet another LeetCode question with this approach: https://leetcode.com/problems/fruit-into-baskets/ In this problem, we maintain an inventory of two baskets with a each count containing a fruit type (kind of a “key”), and a fruit count (kind of a “value”), so we can simply make it a hashmap, i.e. a Python dictionary. The constraint here is that we can only have maximum 2 baskets, with each having a unique fruit type. Our goal is to find the maximum count of fruits in both baskets. First, we setup some basic variables: L = 0 inventory_hashmap = {} best_answer = 0 Then we setup an outer loop for moving R. for R in range(len(tree)): ... We update the inventory, i.e. our hashmap with keys representing the fruit type and the values representing the number of fruits picked during current window. inventory_hashmap[tree[R]] = inventory_hashmap.get(tree[R], 0) + 1 Next, we move L to the right, and remove the corresponding count and fruit type from the inventory, if the constraint is broken. while L \u003c len(tree) and len(inventory_hashmap) \u003e 2: inventory_hashmap[tree[L]]-= 1 if inventory_hashmap[tree[L]] == 0: del inventory_hashmap[tree[L]] L += 1 Finally, we update the best answer by finding the maximum value between the best answer so far, and the current number of fruits in our inventory. best_answer = max(best_answer, sum(inventory_hashmap.values())) Adding all the components together, our solution will be this: class Solution: def totalFruit(self, tree: List[int]) -\u003e int: if not tree: return 0 L = 0 inventory_hashmap = {} best_answer = 0 for R in range(len(tree)): inventory_hashmap[tree[R]] = inventory_hashmap.get(tree[R], 0) + 1 while L \u003c len(tree) and len(inventory_hashmap) \u003e 2: inventory_hashmap[tree[L]]-= 1 if inventory_hashmap[tree[L]] == 0: del inventory_hashmap[tree[L]] L += 1 best_answer = max(best_answer, sum(inventory_hashmap.values())) return best_answer ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:3:3","series":null,"tags":["leetcode","algorithms"],"title":"Effective LeetCode: Understanding the Sliding Window Pattern","uri":"/posts/2020/10/leetcode-sliding-window/#leetcode-904-fruit-into-baskets"},{"categories":["Software Engineering"],"content":"\rSumming UpI hope these examples were good enough to drive home the intuitions required to build a Sliding Window based solution. I would also like to point out that the template that I talked explained in this article is the most common strategy with Sliding Window problems, however it is not the only kind of Sliding Window approach. As you saw, in our template, the right marker R moves faster than the left marker L, therefore this approach is sometimes referred to as Fast/Slow Sliding Window approach. There are other approaches like Fast/Catch-Up and Front/Back that I will talk about in a future post. ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:4:0","series":null,"tags":["leetcode","algorithms"],"title":"Effective LeetCode: Understanding the Sliding Window Pattern","uri":"/posts/2020/10/leetcode-sliding-window/#summing-up"},{"categories":["Software Engineering"],"content":"\rExercise for the ReadersIf you want to apply what you have learned in this article, and solve a few LeetCode challenges, then you can try your hand on the following Sliding Window problems on LeetCode website: https://leetcode.com/problems/longest-continuous-subarray-with-absolute-diff-less-than-or-equal-to-limit/ https://leetcode.com/problems/number-of-substrings-containing-all-three-characters/ https://leetcode.com/problems/replace-the-substring-for-balanced-string/ https://leetcode.com/problems/max-consecutive-ones-iii/ https://leetcode.com/problems/subarrays-with-k-different-integers/ https://leetcode.com/problems/fruit-into-baskets/ https://leetcode.com/problems/get-equal-substrings-within-budget/ https://leetcode.com/problems/longest-repeating-character-replacement/ https://leetcode.com/problems/shortest-subarray-with-sum-at-least-k/ https://leetcode.com/problems/minimum-size-subarray-sum/ https://leetcode.com/problems/sliding-window-maximum/ I also want to thank the LeetCode user wh0ami for compiling this list of questions and sharing this idea, in his C++ post at LeetCode discussion forums here. ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:5:0","series":null,"tags":["leetcode","algorithms"],"title":"Effective LeetCode: Understanding the Sliding Window Pattern","uri":"/posts/2020/10/leetcode-sliding-window/#exercise-for-the-readers"},{"categories":["Software Engineering"],"content":"In June 2020, Twitter announced a major overhaul in its storage and retrieval systems. These changes allowed Twitter to reduce the search index latency from 15 seconds to 1 second. So, what did they do to get such impressive gains? Allow me to explain!","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/","series":null,"tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/"},{"categories":["Software Engineering"],"content":"In June 2020, Twitter announced a major overhaul in its storage and retrieval systems. These changes allowed Twitter to reduce the search index latency from 15 seconds to 1 second. So, what did they do to get such impressive gains? Allow me to explain! ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:0:0","series":null,"tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/#"},{"categories":["Software Engineering"],"content":"\rTwitter’s Search ArchitectureDesigning a search system for Twitter is an incredibly complex task. Twitter has more the 145 million daily users 1 and over half a billion tweets are sent each day 2. Their search system gets hundreds of thousands of search queries per second. For any search system indexing latency is an important metric. The indexing latency can be defined as the amount of time it takes for new information (“tweets” in this case) to be available in the search index. And as you can imagine, with a large amount of data generated every day, we have an interesting engineering problem at hand. But why is search indexing latency important for Twitter?\rNote that not every service out there needs to have low update their search index quickly. For example, a Costco warehouse could update their search index once every couple of hours or so. For Twitter, however, real-time access to the content can be really important, for cases like following some breaking news, ongoing conversations, delivering timelines etc.\rBefore we take a look at what’s new, let’s first understand how the search workflow looked like at Twitter before these changes took place. In their 2012 paper titled: “Earlybird: Real-Time Search at Twitter” 3 4, Twitter released details about its search system project codenamed “Earlybird”. With Earlybird Twitter adopted their custom implementation of Apache Lucene which was a replacement for Twitter’s earlier MySQL-indexes based search algorithm. Some of the enhancements included image/video search support, searchable IndexWriter buffer, efficient relevance based search in time sorted index etc. This enabled Twitter to launch relevance-based product features like ranked home timeline. Although the search was still limited to last x days, but they later added the support for performing archive search on SSD with vanilla Lucene, as shown by the bottom row in the diagram below. ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:1:0","series":null,"tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/#twitters-search-architecture"},{"categories":["Software Engineering"],"content":"\rLucene BasicsKnowing basics of Lucene helps to better understand Twitter’s implementation for their search system. Lucene is an open-source search engine library that is completely written in Java and is used by tech companies like LinkedIn, Twitter, Slack, Evernote etc. As you can imagine, we can’t afford to search record-by-record on large data specially for a time-sensitive application, we use a Lucene like solution that provides us Inverted Indexes. In information retrieval terminologies, a string is called a “term” (for example, English words), a sequence of terms is called a “field” (for example, sentences), a sequence of fields is called a document (for example, tweets), and a sequence of documents is called an index 5. Following is an example of inverted index creation, taken from “Inverted files for text search engines” research paper  6 document id document text 1 The old night keeper keeps the keep in the town 2 In the big old house in the big old gown 3 The house in the town had the big old keep 4 Where the old night keeper never did sleep 5 The night keeper keeps the keep in the night 6 And keeps in the dark and sleeps in the night Table: Input documents term‏‏‎ | freq inverted list for t and | 1 \u003c6\u003e big | 2 \u003c2\u003e, \u003c3\u003e dark | 1 \u003c6\u003e did | 1 \u003c4\u003e gown | 1 \u003c2\u003e had | 1 \u003c3\u003e house | 2 \u003c2\u003e, \u003c3\u003e in | 5 \u003c1\u003e, \u003c2\u003e, \u003c3\u003e, \u003c5\u003e, \u003c6\u003e keep | 3 \u003c1\u003e, \u003c3\u003e, \u003c5\u003e keeper | 3 \u003c1\u003e, \u003c4\u003e, \u003c5\u003e keeps | 3 \u003c1\u003e, \u003c5\u003e, \u003c6\u003e light | 1 \u003c6\u003e never | 1 \u003c4\u003e night |3 \u003c1\u003e, \u003c4\u003e, \u003c5\u003e old | 4 \u003c1\u003e, \u003c2\u003e, \u003c3\u003e, \u003c4\u003e sleep | 1 \u003c4\u003e sleeps | 1 \u003c6\u003e the | 6 \u003c1\u003e, \u003c2\u003e, \u003c3\u003e, \u003c4\u003e, \u003c5\u003e, \u003c6\u003e town | 2 \u003c1\u003e, \u003c3\u003e where | 1 \u003c4\u003e Table: inverted file You can use the above inverted index to answer questions like “Which documents have the word house in them?” Also, note that a term or a document could also be a single word, a conjunction, a phrase or a number. Each row of this index above, maps a term to a Posting List. In real implementations, the posting list may also include additional information, such as the position (index) of the term in the document or some other application-specific payload. ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:1:1","series":null,"tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/#lucene-basics"},{"categories":["Software Engineering"],"content":"\rChanges in Ingestion PipelineLet’s get back to Twitter’s ingestion pipeline, which used to look like this: Click to zoom\rFlaw 1: Synchronous Workflow\rOne major problem with this design is that the Tweet data that we want to index isn’t available as soon as the Tweet is created. With ranked Home timeline feature, the timeline needs additional “relevance” detail about each Tweet. Among other factors, this relevance depends on fields that may not be immediately available. For example, a shortened URL (https://t.co/foo) needs to be expanded to provide more context for ranking, geo-coding resolution might take longer.\rFix: Twitter decided to stop waiting for the delayed fields to become available by adding an asynchronous part to their ingestion pipeline. click to zoom\rNow most of the data will be sent to indexing system as soon as the Tweet is posted. Another update will be sent once the additional information is available. Even though the indexing service doesn’t have full information at the beginning, at least for search system we have enough information to fetch results. ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:2:0","series":null,"tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/#changes-in-ingestion-pipeline"},{"categories":["Software Engineering"],"content":"\rChanges in Data Structures and Algorithms","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:3:0","series":null,"tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/#changes-in-data-structures-and-algorithms"},{"categories":["Software Engineering"],"content":"\rEliminating Sorting StepLucene’s vanilla implementation stored document ids using delta encoding, such that the key at an index depends on the previous index. Also, the document ids were generated starting from 0 up to the size of the segment storing those documents (a segment is just a “committed” i.e. immutable index). This essentially meant that the search could only be performed from left to right, i.e. from the oldest tweet to latest tweet. However, like many other products at Twitter, search is also designed to prioritize recent tweets over the older ones. So, twitter decided to customize this implementation in Earlybird. Earlybird diverged from standard Lucene approach and allocated document ids to incoming tweet from high value to low value, i.e. from the {size of the index - 1} to 0. This lets searching algorithm to traverse from latest tweet to oldest tweet and with the possibility of returning early if a client-specified number of hits have been recorded. Flaw 2: Decoupled Search System and Tweet Creation System\rBy design, Tweet creation system at Twitter is decoupled from the search system. Hence the indexing service can’t guarantee that the order in which it receives the tweets is also the order of their creation. Hence to fix this, Twitter had to include a buffer in their ingestion pipeline to temporarily store and then sort all of the incoming tweets to the indexing service. This unfortunately adds additional delays in search indexing.\rFIx: Twitter changed it’s ID assignment scheme such that each tweet is given a document ID based on its time of creation. They fit the document id to 31 bits (positive Java Integer), such that 27 bits store the timestamp with microseconds granularity. The rest 4 bits are used as a counter for the all of the tweets received at the same microsecond. Although the probability of such \\( 2^4\\) events is rare, but all tweets after 16, that are created at the same microsecond precision, would be assigned to the next segment, and would appear slightly out of order in search results, if selected. This means that Twitter can eliminate the need for explicit sorting step and reduce latency even further. ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:3:1","series":null,"tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/#eliminating-sorting-step"},{"categories":["Software Engineering"],"content":"\rOptimizing Operations on Posting ListsFor their posting list implementation, Twitter had been using Unrolled Linked Lists. This implementation had only one writer thread and multiple searcher threads (one per CPU core, with tens of thousands of cores per server) without any locks. Writing would be done only at the head of the list, and the searcher would search starting from the new head pointer or the older head (still, valid) pointer to the end of the list. (notice in the figure above how document id assignment is done from high value to low value. ) Unrolled linked lists are cache friendly data structures that also reduce some of the overhead of storing pointer references. However, we can’t use them to cost-effectively insert at arbitrary location in the list, they also depend largely upon the assumption that the input will be strictly ordered. Fix: Twitter decided to opt for Skip Lists as the replacement for unrolled linked lists. Skip Lists allow for O(\\( \\log{n}\\)) search and insertions, and also support concurrency. I wrote an introductory article on Skip Lists here. I highly recommend you to read through the article to better understand the concept behind Skip Lists. Twitter’s Skip List implementationSkip Lists are probabilistic data structures, such that after inserting a new node at list \\( L_{i} \\), we insert the node at the layer \\( L_{i+1}\\) with some probability. Twitter’s found 20% ( \\( \\frac{1}{5}\\) ) is a good tradeoff between space and time complexity (memory used vs. speed). In their implementation, Twitter used several optimizations. The implemented the Skip List in a single flat array, such that the “next” pointer becomes just an index into the array. Here’s a breakdown of the other optimizations in their Skip List: Insertion would simply append the new value at the end of the array, and the corresponding pointer will be updated appropriately after traversing the Skip List. New values at the higher levels will be added with a 20% probability. At every search operation, the descent down the different levels is recorded and saved as a “search finger”. This helps in lookups when we are finding a document with id greater than the one for which we already have the search finger. It reduces the lookup time complexity from O(\\( \\log{n} \\)) to O(\\( \\log{d} \\)), where n is the number of items in the posting list and d is the number of items in the list between the first value and the second value, in case of conjunction search queries. Using primitive array also has an advantage of reducing pointer management overheads, such as garbage collection. Allocating vertical levels of Skip List in adjacent location, eliminates the need for storing “above” and “below” pointers. One obvious disadvantage, however, is that the Skip Lists are not cache friendly. However, being able to run in logarithmic time in case of sparse documents is a big advantage. ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:3:2","series":null,"tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/#optimizing-operations-on-posting-lists"},{"categories":["Software Engineering"],"content":"\rOptimizing Operations on Posting ListsFor their posting list implementation, Twitter had been using Unrolled Linked Lists. This implementation had only one writer thread and multiple searcher threads (one per CPU core, with tens of thousands of cores per server) without any locks. Writing would be done only at the head of the list, and the searcher would search starting from the new head pointer or the older head (still, valid) pointer to the end of the list. (notice in the figure above how document id assignment is done from high value to low value. ) Unrolled linked lists are cache friendly data structures that also reduce some of the overhead of storing pointer references. However, we can’t use them to cost-effectively insert at arbitrary location in the list, they also depend largely upon the assumption that the input will be strictly ordered. Fix: Twitter decided to opt for Skip Lists as the replacement for unrolled linked lists. Skip Lists allow for O(\\( \\log{n}\\)) search and insertions, and also support concurrency. I wrote an introductory article on Skip Lists here. I highly recommend you to read through the article to better understand the concept behind Skip Lists. Twitter’s Skip List implementationSkip Lists are probabilistic data structures, such that after inserting a new node at list \\( L_{i} \\), we insert the node at the layer \\( L_{i+1}\\) with some probability. Twitter’s found 20% ( \\( \\frac{1}{5}\\) ) is a good tradeoff between space and time complexity (memory used vs. speed). In their implementation, Twitter used several optimizations. The implemented the Skip List in a single flat array, such that the “next” pointer becomes just an index into the array. Here’s a breakdown of the other optimizations in their Skip List: Insertion would simply append the new value at the end of the array, and the corresponding pointer will be updated appropriately after traversing the Skip List. New values at the higher levels will be added with a 20% probability. At every search operation, the descent down the different levels is recorded and saved as a “search finger”. This helps in lookups when we are finding a document with id greater than the one for which we already have the search finger. It reduces the lookup time complexity from O(\\( \\log{n} \\)) to O(\\( \\log{d} \\)), where n is the number of items in the posting list and d is the number of items in the list between the first value and the second value, in case of conjunction search queries. Using primitive array also has an advantage of reducing pointer management overheads, such as garbage collection. Allocating vertical levels of Skip List in adjacent location, eliminates the need for storing “above” and “below” pointers. One obvious disadvantage, however, is that the Skip Lists are not cache friendly. However, being able to run in logarithmic time in case of sparse documents is a big advantage. ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:3:2","series":null,"tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/#twitters-skip-list-implementation"},{"categories":["Software Engineering"],"content":"\rAdditional Details Twitter’s new Skip List implementation also uses a “published pointer” that points to the current maximum pointer into array such that searchers never traverse beyond this pointer. This allows for ensuring atomicity in search and retrievals, and a document is searched only if all of its terms are indexed. Considering this was a change of large magnitude, Twitter rolled it out gradually by first evaluating the results in Dark Launch mode. This was done to ensure that the clients do not have any reliance or assumptions on the earlier 15 seconds delay in search indexing. ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:4:0","series":null,"tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/#additional-details"},{"categories":["Software Engineering"],"content":"\rConclusionIn order to reduce search indexing latency, Twitter made big changes to its storage system and retrieval system. They shared their approach and learnings in a whitepaper 7 that I summarized here. The search indexing improvement from 15 seconds to 1 second means that the new content should be available for Twitter users to access almost instantaneously. Twitter Q3-2019 - Selected Financial and Metrics ↩︎ Twitter Engagement Report 2018 ↩︎ http://users.umiacs.umd.edu/~jimmylin/publications/Busch_etal_ICDE2012.pdf ↩︎ https://www.youtube.com/watch?v=KUmFJc3fFuM ↩︎ https://lucene.apache.org/core/8_6_0/core/org/apache/lucene/codecs/lucene86/package-summary.html#package.description ↩︎ http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.105.8844 ↩︎ Twitter Infrastructure: Reducing search indexing latency ↩︎ ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:5:0","series":null,"tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/#conclusion"},{"categories":["NLP"],"content":"Word vector representations with subword information are great for NLP modeling. But can we make lexical corrections using a trained embeddings space? Can its accuracy be high enough to beat Peter Norvig's spell-corrector? Let's find out!","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/","series":null,"tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/"},{"categories":["NLP"],"content":"Word vector representations with subword information are great for NLP modeling. But can we make lexical corrections using a trained embeddings space? Can its accuracy be high enough to beat Peter Norvig’s spell-corrector? Let’s find out! ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:0:0","series":null,"tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/#"},{"categories":["NLP"],"content":"\rIntroductionLet’s first define some terminology. ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:1:0","series":null,"tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/#introduction"},{"categories":["NLP"],"content":"\rWhat are word embeddings?Word Embedding techniques have been an important factor behind recent advancements and successes in the field of Natural Language Processing. Word Embeddings provide a way for Machine Learning modelers to represent textual information as the input to ML algorithms. Simply put, they are a hashmap where the key is a language word, and the corresponding value is a vector of real numbers fed to the models in place of that word. There are different kinds of word embeddings available out there that vary in the way they learn and transform a word to a vector. The word vector representations can be as simple as a hot-encoded vector, or they can be more complex (and more successful) representations that are trained on large corpus, take context into account, break the words into subword representations etc ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:1:1","series":null,"tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/#what-are-word-embeddings"},{"categories":["NLP"],"content":"\rWhat are subword embeddings?Suppose you have a deep learning NLP model, say a chatbot, running in production. Your customers are directly interacting with the ML model. The model is being fed the vector values, such that each word from the customer is queried against a hashmap and the value corresponding to the word is the input vector. All of the keys in your hashmap represents the vocabulary, i.e. all the words you know. Now, how would you handle a case where the customer uses a word that is not already present in your vocabulary? There are many ways to solve this out-of-vocabulary (OOV) problem. One popular approach is to split the words into “subword” units, and use those subwords to learn your hashmap during model training stage. At the time of inference, you would again divide each incoming word into smaller subword units, find the word vector corresponding to each subword unit using the hashmap, then aggregate each subword vector to get the vector representation for the complete word. For example, let’s say we have the word tiktok, the corresponding subwords could be tik, ikt, kto, tok, tikt, ikto, ktok etc. This is the character n-gram division for the input word, where n is the subword sequence length, and is fixed by the modeler. Figure: example subword representations ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:1:2","series":null,"tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/#what-are-subword-embeddings"},{"categories":["NLP"],"content":"\rWhat is FastText?FastText is an open-source project from Facebook Research. It is a library for fast text-representations and classifications. It is written in C++ and supports multiprocessing. It can be used to train unsupervised word vectors and supervised classification tasks. For learning word-vectors it supports both skipgram and continuous bag-of-words approaches. FastText supports subword representations such that each word can be represented as a bag of character n-grams in addition to the word itself. Incorporating finer (subword level) information is pretty good for handling rare words. You can read more about the FastText approach in their paper here . For an example, let’s say you have a word “superman” in FastText trained word embeddings (“hashmap”). Let’s assume the hyperparameters minimum and maximum length of ngram was set to 4. Corresponding to this word, the hashmap would have the following keys: Original word: superman n-gram size subword 4 \u003csup 4 supe 4 uper 4 perm 4 erma 4 rman 4 man\u003e where “\u003c” and “\u003e” characters mark the start and end of a word respectively ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:1:3","series":null,"tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/#what-is-fasttext"},{"categories":["NLP"],"content":"\rThe Task: Spell-CheckingIn NLP, the learnt word embedding vectors not only have lexical representations for words, but the vector values also have semantically related positioning in the embedding space. We are going to try and build a spell-checker application based on FastText word vectors such that given a misspelled word, our task will be to find the word vector representation closest to the vector representation of that word in trained embedding space. We will work based on this simple heuristic: heuristic\rIF word exists in the Vocabulary ​ Do not change the word ELSE ​ Replace the word with the one closest to its sub-word representation ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:2:0","series":null,"tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/#the-task-spell-checking"},{"categories":["NLP"],"content":"\rDataset \u0026 BenchmarkFor fun, let’s build and evaluate our spell-checker on the same training and testing data as this classic article: “How to write a Spelling Corrector” by Peter Norvig , Director of Research at Google. In this article, Norvig build a simple spelling corrector based on basic probability theory. Let’s see how does this FastText based approach hold up against it. ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:2:1","series":null,"tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/#dataset--benchmark"},{"categories":["NLP"],"content":"\rInstalling FastTextInstallation for FastText is straightforward. FastText can be used as a command line tool or via Python client. Click here to access the latest installation instructions for both approaches. ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:2:2","series":null,"tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/#installing-fasttext"},{"categories":["NLP"],"content":"\rMethod 1: Using Pre-trained Word VectorsFastText provides pretrained word vectors based on common-crawl and wikipedia datasets. The details and download instructions for the embeddings can be found here. For a quick experiment, let’s load the largest pretrained model available from FastText and use that to perform spelling-correction. Download and unzip the trained vectors and binary model file. -\u003e wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip -\u003e unzip crawl-300d-2M-subword.zip There are two files inside the zip: crawl-300d-2M-subword.vec : This file contains the number of words (2M) and the size of each word (vector dimensions; 300) in the first line. All of the following lines start with the word (or the subword) followed by the 300 real number values representing the learnt word vector. crawl-300d-2M-subword.bin: This binary file is the exported model trained on the Common-Crawl dataset. As mentioned in his article, Norvig used spell-testset1.txt and spell-testset2.txt as development and test set respectively, to evaluate the performance of his spelling-corrector. I’m going to load the pretrained FastText model and make predictions based on the heurisitic defined above. I’m also going to borrow some of the evaluation code from Norvig. import io import fasttext def load_vectors(fname): fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore') n, d = map(int, fin.readline().split()) data = {} for line in fin: tokens = line.rstrip().split(' ') data[tokens[0]] = map(float, tokens[1:]) return data def spelltest(tests, model, vocab): \"Run correction(wrong) on all (right, wrong) pairs; report results.\" import time start = time.clock() good, unknown = 0, 0 n = len(tests) for right, wrong in tests: w = wrong if w in vocab: print('word: {} exists in the vocabulary. No correction required'.format(w)) else: w_old = w w = model.get_nearest_neighbors(w, k=1)[0][1] print(\"found replacement: {} for word: {}\".format(w, w_old)) good += (w == right) dt = time.clock() - start print('{:.0%} of {} correct at {:.0f} words per second ' .format(good / n, n, n / dt)) def Testset(lines): \"Parse 'right: wrong1 wrong2' lines into [('right', 'wrong1'), ('right', 'wrong2')] pairs.\" return [(right, wrong) for (right, wrongs) in (line.split(':') for line in lines) for wrong in wrongs.split()] if __name__ == \"__main__\": model = fasttext.load_model(\"crawl-300d-2M-subword.bin\") vocab = load_vectors(\"crawl-300d-2M-subword.vec\") spelltest(Testset(open('spell-testset1.txt')), model, vocab) spelltest(Testset(open('spell-testset2.txt')), model, vocab) For the sake of brevity, I’m reducing the output log to just the metrics trace. output\r… 0% of 270 correct at 3 words per second. … 0% of 400 correct at 4 words per second.\rThat didn’t go well! 😅 None of the corrections from the model were right. Let’s dig into the results. Here are a few snapshots from the output log: output\r… word: sysem exists in the vocabulary. No correction required word: controled exists in the vocabulary. No correction required word: reffered exists in the vocabulary. No correction required word: irrelavent exists in the vocabulary. No correction required word: financialy exists in the vocabulary. No correction required word: whould exists in the vocabulary. No correction required … found replacement: reequipped for word: reequired found replacement: putput for word: oputput found replacement: catecholate for word: colate found replacement: yeahNow for word: yesars found replacement: detale for word: segemnt found replacement: \u003cli\u003e\u003cstrong\u003eStyle for word: earlyest …\rLooks like there are a lot of garbage subwords in the pretrained vocabulary that directly matches our misspelled input. Also, the performed lexical corrections show that the model replaced misspelled input words with the closest semantic neighbor. However none of those neighbors are meaningful English words. We can verify this by checking out the neighbors for random misspel","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:2:3","series":null,"tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/#method-1-using-pre-trained-word-vectors"},{"categories":["NLP"],"content":"\rMethod 2: Trained Our Own Word VectorsFor a fair comparison with Norvig’s spell-checker, let’s use the same training data that he used (big.txt). From Norvig's article:\r… by counting the number of times each word appears in a text file of about a million words, big.txt. It is a concatenation of public domain book excerpts from Project Gutenberg and lists of most frequent words from Wiktionary and the British National Corpus.\r-\u003e wc big.txt 128457 1095695 6488666 big.txt The training data has around 128K lines, 1M words, 6.5M characters. As discussed above, we should try with keeping the wordNgrams hyperparameter to 1, and use the trained FastText model to perform spell-checking. import io import fasttext def load_vectors(fname): fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore') n, d = map(int, fin.readline().split()) data = {} for line in fin: tokens = line.rstrip().split(' ') data[tokens[0]] = map(float, tokens[1:]) return data def spelltest(tests, model): \"Run correction(wrong) on all (right, wrong) pairs; report results.\" import time start = time.clock() good, unknown = 0, 0 n = len(tests) for right, wrong in tests: w_old = wrong w = wrong if w in model.words: pass else: w = model.get_nearest_neighbors(w, k=1)[0][1] good += (w == right) if not (w == right): if w_old != w: print(\"Edited {} to {}, but the correct word is: {}\".format(w_old, w, right)) dt = time.clock() - start print('{:.0%} of {} correct at {:.0f} words per second ' .format(good / n, n, n / dt)) def Testset(lines): \"Parse 'right: wrong1 wrong2' lines into [('right', 'wrong1'), ('right', 'wrong2')] pairs.\" return [(right, wrong) for (right, wrongs) in (line.split(':') for line in lines) for wrong in wrongs.split()] if __name__ == \"__main__\": model = fasttext.train_unsupervised('big.txt', wordNgrams=1, minn=1, maxn=2, dim=300, ws=8, neg=8, epoch=4, minCount=1, bucket=900000) spelltest(Testset(open('spell-testset1.txt')), model) spelltest(Testset(open('spell-testset2.txt')), model) There are a couple of changes in this code. We are training the model with specific hyper-parameters, and the output traces will inform us what went wrong in our decisions. Note that FastText doesn’t provide an option to set seed in the above implementation, so the results may vary by 1%-2% on every execution. -\u003e python fasttext_trained.py Read 1M words Number of words: 81398 Number of labels: 0 Progress: 100.0% words/sec/thread: 20348 lr: 0.000000 avg. loss 1.943424 ETA: 0h 0m 0s ... 73% of 270 correct at 46 words per second. ... 69% of 400 correct at 48 words per second. This is a big improvement! 😄 We have almost the same accuracy as Norvig’s spell-corrector. Here are Norvig’s results for comparison. 75% of 270 correct at 41 words per second 68% of 400 correct at 35 words per second Looking at some of the edits in the output traces, I can see that the model output isn’t essentially incorrect, but the model is biased to certain edit-based operations. output\r… Edited reffered to referred, but the correct word is: refered Edited applogised to apologized, but the correct word is: apologised Edited speeking to seeking, but the correct word is: speaking Edited guidlines to guideline, but the correct word is: guidelines Edited throut to throughout, but the correct word is: through Edited nite to unite, but the correct word is: night Edited oranised to organised, but the correct word is: organized Edited thay to thy, but the correct word is: they Edited stoped to stooped, but the correct word is: stopped Edited upplied to supplied, but the correct word is: applied Edited goegraphicaly to geographical, but the correct word is: geographically …\rAs you can see our trained model is nicely producing dictionary-based words as output. It’s likely that with contextual training approaches and evaluations, along with more training data, we can come up with an even better approaches that would understand context in a full sentence and produce the correct word as the spell-chec","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:2:4","series":null,"tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/#method-2-trained-our-own-word-vectors"},{"categories":["NLP"],"content":"\rConclusionOur model was able to match Peter Norvig’s spell-corrector. 😊 Looking at the failing cases, we realize that the model could potentially do even better with more training data and a contextual training and evaluation strategy. ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:3:0","series":null,"tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/#conclusion"},{"categories":["Software Engineering"],"content":"\rWhy am I writing about Skip Lists?Skip List is one of those data structures that I have seen briefly mentioned in academic books; but never seen it being used in academic or industrial projects (or in LeetCode questions 🙂 ). However I was pleasantly surprised to see Skip List being one of the major contributors behind Twitter’s recent success in reducing their search indexing latency by more than 90% (absolute). This encouraged me to brush up my understanding of Skip List and also learn from Twitter’s solution. ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:1:0","series":null,"tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/#why-am-i-writing-about-skip-lists"},{"categories":["Software Engineering"],"content":"\rWhy do we need Skip Lists?I believe it’s easier to understand the concept of Skip Lists by seeing their application. So let’s first build a case for Skip List. ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:2:0","series":null,"tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/#why-do-we-need-skip-lists"},{"categories":["Software Engineering"],"content":"\rThe Problem: SearchLet’s say you have an array of sorted values as shown below, and your task is to find whether a given value exists in the array or not. Given an array: find whether value 57 exists in this array or not ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:2:1","series":null,"tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/#the-problem-search"},{"categories":["Software Engineering"],"content":"\rOption 1: Linearly Searching an ArrayAs the name suggests, in linear search you can simply traverse the list from one end to the other and compare each item you see with the target value you’re looking for. You can stop the linear traversal if you find a matching value. You can stop the search if at any point of time you see an item whose value is bigger than the one you’re looking for. Is this a good solution? [Click here]\rThis solution is alright. But as you would notice, it doesn’t take advantage of the fact that the given sequence of values are in sorted order. The worst case time complexity for this algorithm would be order of the number of values in the sequence, i.e. O(n). We will be comparing our target value with each member of sequence if the element is at the very end of the sequence or does not exist in the sequence.\rCan we do better?\rYes, with Binary Search!\r","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:3:0","series":null,"tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/#option-1-linearly-searching-an-array"},{"categories":["Software Engineering"],"content":"\rOption 2: Binary Search on an ArrayThe binary search concept is simple. Think of how you search for a given word in a dictionary. Say if you are trying to find the word “quirky” in the dictionary, you do not need to linearly read each word in the dictionary one-by-one to know if the word exists in the dictionary. Because you have established that the dictionary has the all of the words in alphabetical sorted order. You can simply flip through the book, notice which word you are at, then either go left or right depending on whether the word starts with a character that comes before or after ‘q’. Going back to our example, let’s first check the value at the center of the sequence, if the value matches our target. we can successfully end the search. But if it is not the value we are looking for, we will compare the value of the middle position with our target value. If our target value is smaller, we need to look into smaller numbers, so we go towards left and ignore the values on the right. By following this binary search approach have reduced our search space and the required number of comparisons by half. Also, we repeat the same process on the “valid” half, i.e. we directly go the middle value of the left sub-array and see if we have a match, if not, we again decide whether to go left or right, and hence reduce our candidate search space by half again. What is the time complexity now?\rThe time complexity of this algorithm is again defined by the number of comparisons. If you recognize the pattern in this algorithm, you would know that the algorithm belongs to the category for which the size of a variable is continuously divided by a constant (2 in this case). And hence our algorithmic time complexity is O(\\(log_2n\\)). [Read this for an intuitive explanation]\rThis is excellent. Logarithmic time complexity means that our algorithm will scale really well with large number of values being stored in the sequential array. Is this a perfect solution?\rOne big issue, here, is that while sequential storage (for example, arrays) makes in easy and cheap to design binary search algorithms, it makes it costlier for us to implement an insertion and deletion strategy. For example, if you insert an item at a specific index in the array, you will have to shift the values on its right by one place to the right. Similarly if you delete a value at an arbitrary index inside the array, you will have to left shift all the values to the right of that index by one place. (see examples below)\rAs you can imagine, having extra space on the right of the array would be convenient else you would have to call for dynamic allocation of memory at every insertion and copy over the whole array to newly allocated array memory. List data structure in Python, allocates list in a similar way that leaves capacity at the end of the sequence for future expansion. If we run out of space in the list, Python allocates a new and much larger (double the size, for example) sized list, copies over each value from the old list to new list, and then deletes the old list. (This is why the insertion in a Python list is Amortized O(1) complexity) Could there be a way to have a data structure with better insertion and deletion costs?\rYes, we can use Linked Lists!\r","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:4:0","series":null,"tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/#option-2-binary-search-on-an-array"},{"categories":["Software Engineering"],"content":"\rOption 3: Linear search with Linked ListsLinked Lists are dynamic data structures that store values in non-contiguous memory blocks (called nodes). Each block refers to the other using memory pointers. These pointers are sort of the links in a chain that define the ordering among the non-contiguous nodes. A Singly Linked list represents a sequential structure where each node maintains a pointer to the element that follows it in the sequence. In linked lists we also maintain additional reference to the list nodes. Head is one such reference which is variable that stores the address for the first element in the list (stores null pointer if the list is empty). Insertion and deletion at any arbitrary location is straightforward, as you would simply reconnect the links of the nodes previous and next positions from the target position. We can also have a Doubly Linked List where each node stores the pointer to the previous as well as the next node in the sequence. A tail reference may also be used to store pointer to the last node in the linked list. As you would notice, random access in linked lists are not possible. And to find a value in the list, we would simply have to traverse the list from one end, say from the node referred by Head, towards the other end till we find what we are looking for. So, even though you can insert and delete in O(1), you still need the reference to the target location. Finding the target location is again O(n). With Linked Lists, while we solved the issue of costly insertions and deletions, we haven’t quite found a good solution to quickly access a target value in the list (or find if the value exists). Wouldn’t it be great if we could have the random access (or something close to it) in the list to simply our find, insert and delete operations? Well this is where Skip List come handy! ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:5:0","series":null,"tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/#option-3-linear-search-with-linked-lists"},{"categories":["Software Engineering"],"content":"\rOption 4: Pseudo-binary search with Linked Lists [Skip List]Technical advances have made low latency highly desirable for software applications. How well the underlying algorithm scales to demand can make or break a product. On the other hand, the memory/storage costs is getting cheaper by day. This propels many system designers to make decisions that balance compromises that spend extra money on storage costs, while opting for algorithms with low time complexities. Skip Lists is a good example of one such compromise. Skip Lists were proposed by WIlliam Pugh in 1989 (read the original paper  here) as a probabilistic alternative to balanced trees. Skip lists are relatively easy to implement, specially when compared to balanced trees like AVL Trees or Red Black Trees. If you have used the Balanced Trees before, you may remember how they are great data structures to maintain relative order of elements in a sequence, while making search operations easy. However, insertions and deletions in Balanced Trees are quite complex as we have the additional task to strictly ensure the tree is balanced at all times. Skip Lists introduce randomness in by making balancing of the data structure to be probabilistic. While it definitely makes evaluating complexity of the algorithm to be much harder, the insertion, deletion operations become much faster and simpler to implement. The complexities are calculated based on expectation, but they have a high probability, such that we can say with high confidence (but not absolute guarantee) that the run time complexity would be O(log(n)) apart from some specific worst-case scenarios as we would see soon. ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:6:0","series":null,"tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/#option-4-pseudo-binary-search-with-linked-lists-skip-list"},{"categories":["Software Engineering"],"content":"\rSkip Lists - IntuitionFollowing is the map of New York Subway’s “7” train. Each point on the horizontal line represents on train stop on a specific street. The line at the top is the local train that has much more number of stops compared to the express line on the below it. Government decides where to put the stops based on factors like nearby facilities, neighborhood, business centers etc. Let’s simplify the above route map and display stations by the street numbers. In the simplified map above, the express line is shown at the top and local line is below it. Now if you had this map and you were supposed to travel from stop 14 to stop 59. How would you plan your trip route? For simplicity let’s assume we can’t overshoot, meaning that we can’t go some station to the right and then travel back towards left. You would probably want to take the express line from stop 14 and get down on stop 42. Then take the local train from stop 42 and reach the destination 59 from there, as shown below. As you would notice, by taking the express route you would be able to skip some stations along the way and maybe even reach the destination much faster compared to taking the local train from a original stop and traveling each station one by one to reach a destination stop. This is exactly the core idea behind Skip List. With Skip Lists, we introduce redundant travel paths at the cost of additional space, but the additional travel paths have lesser and lesser number of “stops” such that the linear traversal is much faster if we travel in those additional lanes. ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:6:1","series":null,"tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/#skip-lists---intuition"},{"categories":["Software Engineering"],"content":"\rSkip Lists - BasicsLet’s now build the concept of Skips Lists from bottom to top. Assume that we have a Doubly Linked List which is unsorted. What do you think would be the search complexity for this list? Search would be done in O(n), because you would have to traverse the list linearly to find your target node. Let’s assume that the list is now sorted as shown below. What do you think is the search complexity now? Well, it’s still O(n). Even if it sorted, the linked list structure doesn’t provide us random access, hence we can’t simply calculate the next middle index and access that middle node in O(1). Now, let’s put our “express line” on top of the existing structure. We are going to create a second Doubly Linked List and place the second list above of the first list such that each node has four reference to neighboring nodes: left, right, above, below. which can be visualized as: Now given this Skip Lists structure, you can find the number 73 with lesser number of comparison operations. You can start traversing from the top list first, and at every node see if the next node overshoots the value that you are looking for. If it doesn’t simply go the next node in the list on the top, else go to the list on the bottom and continue your linear search in the bottom list. We can summarize the searching approach in a simple algorithm. Algorithm\rWalk right in the Linked List on top (L1) until going right is going too far Walk down to the bottom list (L0) Walk right in L0 until element is found or we overshoot. So what is the kind of search cost we are looking at? We would want to travel the length of L1 as much as possible because that gives us skips over unnecessary comparisons. And eventually we may go down to L0 and traverse a portion of it. The idea is to uniformly disperse the nodes in the top lane. Search Cost = \\(|L1| + \\frac{|L0|}{|L1|}\\) How many “stops” (nodes) do you need in the top lane? It can be mathematically shown1 that if the cardinality of the bottom list is n (i.e. it has n nodes), we will get an optimal solution if we have \\(\\sqrt{n}\\) nodes in the top layer. So, Search Cost = \\(\\sqrt{n} + \\frac{n}{\\sqrt{n}}\\) = 2\\(\\sqrt{n}\\) = O(\\(\\sqrt{n})\\)) However, our Skip List don’t have to limited to 2 lists. We can have more lists (“lanes”), meaning our search costs would scale as: ‎‏‏‎2 sorted lists → \\(2 * \\sqrt[2]{n}\\) 3 sorted lists → \\(3 * \\sqrt[3]{n}\\) ‎‏‏‎… k sorted lists → \\(k * \\sqrt[k]{n}\\) (if we have log(n) number of sorted lists:)‎‎‎‎‎‎‎‎‎‎‎‏‏‎ ‎‎‎‎‎‎‎‎‎‎‎log(n) sorted lists → \\(log(n) * \\sqrt[log(n)]{n}\\) = 2 \\(\\log{n}\\) ‎‎‎‎‎‏‎‎‏‏ So, if the nodes are optimally dispersed, theoretically the structure gives us our favorite logarithmic complexity 🙂 (Although in implementation when we use randomization to insert the nodes, things get a little tricky). Now think of the structure we have created. At the bottom we have n nodes, the layer on top has interspersed \\(\\frac{n}{2}\\) nodes and the layer above it has \\(\\frac{n}{4}\\) nodes. Do you how this structure looks quite similar to a tree? We just need to figure out how “balancing” works in this tree (hint: it’s probabilistic). ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:6:2","series":null,"tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/#skip-lists---basics"},{"categories":["Software Engineering"],"content":"\rSkip List - DefinitionSkip List S consists of a series of lists {\\(S_0,S_1 .. S_h\\)} such that each list \\(S_i\\) stores a subset of the items sorted by increasing keys. Also note:\rh represents the height of the Skip List S each list also has two sentinel nodes -\\(\\infty\\) and +\\(\\infty\\) . -\\(\\infty\\) is smaller than any possible key in the list and +\\(\\infty\\) is greater than any possible key in the list. Hence the node with -\\(\\infty\\) key is always on the leftmost position and node with +\\(\\infty\\) key is always the rightmost node in the list For visualization, it is customary to have the \\(S_0\\) list on the bottom and lists \\(S_1, S_2, .. S_h\\) above it Each node in a list has to be “sitting” on another node with same key below it. Meaning that if \\(L_{i+1}\\) has a node with key k, then \\(L_i, L_{i-1} ..\\) all valid lists below it will have the same key in them We may chose to opt for an implementation of Skip List that only uses Single Linked Lists. However it may only improve the asymptotic complexity by a constant factor Skip List also needs to maintain the “head” pointer, i.e. the reference to the first member (sentinel node -\\(\\infty\\)) in the topmost list On average the Skip List will have O(n) space complexity Let’s first simplify the visualization for our nodes and connection in Skip List. A standard, simplified Skip List would look like this: Now, let’s see how some of the basic operations are performed on a Skip List. ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:6:3","series":null,"tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/#skip-list---definition"},{"categories":["Software Engineering"],"content":"\rSearching in Skip ListSearching in a Skip List in straightforward. We have already seen an example above with two lists. We can extend the same approach to a Skip List with h lists. We will simply start with the leftmost node in the list on top. Go down if the next value in the same list is greater than the one we are looking for, go right otherwise. The main idea is again to skip comparisons with as many keys as possible, while compromising a little on the extra storage required in the additional lists containing subset of all keys. On average, “expected” Search complexity for Skip List is O(log(n)) ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:6:4","series":null,"tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/#searching-in-skip-list"},{"categories":["Software Engineering"],"content":"\rInserting in Skip ListTo insert a new node, we would first find the location where this new key should be inserted in the list at the very bottom. This can be simply done be re-using the search logic, we start traversing to the right from the list on top and we go one step down if the next item is bigger than the key that we want to insert, else go right. Once we reach at a position in bottom list where we can’t go more to the right, we insert the new value on the right side of that position. Skip List after inserting 67: Now comes the probabilistic part, after inserting the new node in the list \\(L_i\\) we have to also insert the node in list above it, i.e. list \\(L_{i+1}\\) with probability \\(\\frac{1}{2}\\). We do this with a simple coin toss, such that if we get a head we insert the node in the list \\(L_{i+1}\\) and toss the coin again, we stop when we get a tail. We may repeat the coin toss till we keep getting heads, even if we have to insert a new layer (list) at the top. Note that we also need to “re-hook” the links for the newly inserted node. The new node needs to have it’s ‘below’ reference pointing to the node below, and the node below would have ‘above’ reference pointer to the new node. To find the ’left’ neighbor for the new node, we simply traverse towards left from the node below it and return ‘above’ pointer from a node for which ‘above’ pointer is not null. The ‘right’ reference of the ’left’ neighbor is used to update the ‘right’ reference of the new node. Also, the ’left’ reference for the new node’s right neighbor is also updated to the new node. This re-hooking operation is actually pretty easy to implement with Linked Lists. Let’s toss the coin again. Last toss gave us another Head, so let’s toss the coin again. We got a Tail this time, so no more insertions are required. As you see the probability of a node also getting inserted in the layer above it get reduced by half after every layer. Still, there is a worst case possibility that you would keep getting Heads indefinitely, although the probability of that happening is extremely small. To avoid such cases when you may get a large number of heads sequentially, you could also use a termination condition where you stop inserting if you reach a predefined threshold for the number of layers (height) or a predefined threshold for the number of nodes in a specific layer. Although the expected time complexity would still be O(log(n)) for all operations. ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:6:5","series":null,"tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/#inserting-in-skip-list"},{"categories":["Software Engineering"],"content":"\rDeletion in Skip ListDeletion operation in Skip List is pretty straightforward. We first perform search operation to find the location of the node. If we find the node, we simply delete it at all levels. Before deleting a node, we simply ensure that no other node is pointing to it. Release all references to this node. And move to the node below it. Remove all references to this node and release it. Move to the node below in the bottom list. Release this node as well. This is how our Skip List looks like after deletion. ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:6:6","series":null,"tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/#deletion-in-skip-list"},{"categories":["Software Engineering"],"content":"\rConclusionIn this article we saw how Skip List is a probabilistic data structure that loosely resembles a balanced binary tree. It is also easy to implement and very fast as the search, insert, delete operations have an expected time complexity of O(log(n)), along with O(n) expected space complexity. in next article, I will introduce a practical application of Skip List and explain how Twitter used Skip List to get a huge improvement in their search indexing latency. ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:7:0","series":null,"tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/#conclusion"},{"categories":["Software Engineering"],"content":"\rReference and Further Reading MIT OCW 6.046 Design and Analysis of Algorithms Lecture 7: Randomization: Skip Lists ↩︎ ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:8:0","series":null,"tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/#reference-and-further-reading"},{"categories":null,"content":" Sumit Kumar - Senior Machine Learning Engineer at Meta USA; UChicago Alum. Previous: Senior MLE at TikTok, Research Scientist 2 at Alexa AI, Amazon, SDE2@Amazon, Lead Engineer@Samsung R\u0026D, SSE@Aricent ‎‎‎‎‎‏‏‎ ‎ ","date":"2020-07-15","objectID":"/about/:0:0","series":null,"tags":null,"title":"About me","uri":"/about/#"},{"categories":null,"content":"\rBriefMy name is Sumit Kumar. I love learning new concepts and building things. I have more than 10 years of work experience in Software Development, Machine Learning/Deep Learning research. Currently I work as a Senior Machine Learning Engineer at Meta USA. And I work with research and development of Recommendation Systems built on Deep Learning concepts. In the past I have worked as a Senior MLE in Recommender Systems at TikTok USA, Research Scientist II in Natural Language Processing (NLP) for Alexa AI, Amazon in Seattle, USA, and Lead Engineer for Samsung Research Labs, India. ","date":"2020-07-15","objectID":"/about/:0:1","series":null,"tags":null,"title":"About me","uri":"/about/#brief"},{"categories":null,"content":"\rLong Form from Archives 😄[This introduction is from my uChicago faculty profile from Dec 2018] Sumit joined the University of Chicago’s Graham School as a full-time, international student in the autumn 2017 cohort. He is currently a software development engineer 2 at Amazon’s headquarters in Seattle. He has seven years of experience in software engineering and research and has worked extensively with major programming languages including C, C++, JavaScript, Java, MATLAB, R, and Python. After earning his bachelor’s degree in computer science, he started his career at Aricent Group as a software developer where he worked on different telecom projects, writing software for simulating radio network controllers that were used by Nokia-Siemens Networks. In 2012, Sumit joined Samsung R\u0026D in Noida, India and developed the L3 software protocol stack used today in Samsung’s smartphone modems. He has worked on various telecom projects in Canada, Australia, South Korea, and India, where he developed features and algorithms compliant with 3GPP standard for WCDMA/LTE wireless communication. During this period, he also collaborated with various telecom vendors across the world and was instrumental in success of many critical projects. In 2015, Sumit was promoted to lead engineer and started working in the advanced R\u0026D division at Samsung. He mentored engineers in addition to his independent research work. Sumit developed various novel solutions and algorithms during his research at Samsung and is the inventor and co-author for seven algorithms filed for patents by Samsung. Apart from his research work, he also created tools to automate many manual processes at Samsung. He did an extensive amount of machine-learning based research in audio DSP domain to solve problems such as Blind Source Separation (Cocktail Party Problem) and Audio Directionality and Speaker Diarization. During this time, he also volunteered as a teacher in Samsung’s Corporate Social Responsibility (CSR) mission to help under-funded schools. During his time as a student at the Graham School, Sumit also worked as a research assistant at the Research Computing Center (RCC), UChicago. There he worked on web development projects, based on Django frameworks in Python, to help researchers from different universities connect with each other and share large volumes of data. He was the International Student Representative for the Master of Science in Analytics (MScA) program at Graham School to enhance international students’ experience and engagement and was also a member of the UChicago cricket team. Sumit has also taught Advanced Analytics and Machine Learning certificate course at The University of Chicago and Python for Internet Programming certificate course at The University of Washington. [Source] [Web Archive backup] Have something to talk about? Let’s chat: hello@reachsumit.com. ","date":"2020-07-15","objectID":"/about/:0:2","series":null,"tags":null,"title":"About me","uri":"/about/#long-form-from-archives-"}]