[{"categories":["Software Engineering"],"content":"Identifying patterns among questions is quite an effective strategy when you are grinding LeetCode in preparation for your upcoming software engineering interviews. In this article, you will develop intuitions about Sliding Window pattern. You will also get a template approach to write code to solve these problems. I will also walk you through some LeetCode questions to show how to apply the template and at the end, there will be some LeetCode exercises for you to practice what you learn.","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/","tags":["leetcode","algorithms"],"title":"Decoding the LeetCode: Sliding Window Pattern Explained","uri":"/posts/2020/10/leetcode-sliding-window/"},{"categories":["Software Engineering"],"content":"Identifying patterns among questions is quite an effective strategy when you are grinding LeetCode in preparation for your upcoming software engineering interviews. In this article, you will develop intuitions about Sliding Window pattern. You will also get a template approach to write code to solve these problems. I will also walk you through some LeetCode questions to show how to apply the template and at the end, there will be some LeetCode exercises for you to practice what you learn. ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:0:0","tags":["leetcode","algorithms"],"title":"Decoding the LeetCode: Sliding Window Pattern Explained","uri":"/posts/2020/10/leetcode-sliding-window/"},{"categories":["Software Engineering"],"content":"Why Should One Focus on Programming Question Patterns? Whether you like them or not, solving programming challenges is a prevalent part of software engineering interviews. Most newbies and seasoned developers turn to leetcode.com to find a wide set of questions and discussion forums to help their interview preparations. However, many are baffled upon finding a more than a thousand questions on the website and not knowing where to start from. Many people in LeetCode and other discussion forums have shared their learning paths with other learners in the form of compiled list of questions that vary from difficulty level, question types, interviewing company and so on. A common occurring themes among those recommendations is to spend time on identifying patterns among questions. It helps in reinforcing your brain to think about the solution in more general terms so that you don’t have to cram your memory with specific details for individual questions. This way you will be better prepared to take on such programming challenges that may quiz you on a diverse range of Data Structure and Algorithm questions. Through a series of articles on these patterns, I will share the tips and tricks from what I have learned while solving LeetCode problems. And hopefully it will help you to prepare more effectively and faster for your software programming rounds of interviews. ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:1:0","tags":["leetcode","algorithms"],"title":"Decoding the LeetCode: Sliding Window Pattern Explained","uri":"/posts/2020/10/leetcode-sliding-window/"},{"categories":["Software Engineering"],"content":"Sliding Window Pattern Generally speaking, in sliding window problems, you would be given a sequence of values, and your task will be to return the best solution given a limiting constraint. The best solution would be defined in terms of a maximum or minimum value that occurs in a range or “window” over the given sequence. I know, it sounds a bit too generic. So, let’s try to materialize the idea with a fun example. ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:2:0","tags":["leetcode","algorithms"],"title":"Decoding the LeetCode: Sliding Window Pattern Explained","uri":"/posts/2020/10/leetcode-sliding-window/"},{"categories":["Software Engineering"],"content":"An Intuitive Analogy Let’s assume you are playing an Animal-Crossing-meets-SIMS game. And you are in-charge of many cute little characters currently living on different islands in the game doing some fun tasks. This game allows these characters to collaborate such that they can visit other islands and do tasks together. However, in order to connect any two islands, you will have to make some standard-length connecting bridges. And, some of the islands may be farther away from each other, so they may require more than one connecting bridge. For simplicity, we are going to assume that the islands are located linearly as shown in the figure. As you play new quests in the game, and finish daily tasks, you can unlock more of these connecting bridges. But at any given point of time, you will have a limited number of bridges in your collection. Now your task is to use the available bridge count and connect the islands such that you end up with maximum possible number of characters connected with each other. Say if you had 2 bridges available, would you choose move 1 or move 2 from below? With move 1, you connect the first two islands and the two characters on those islands together. However, with move 2, you connect the last three islands and four of your characters together. So the second move give you the best answer. As an exercise, can you think of any other moves in the above setting? What results would you get in those? While thinking of a solution to the above questions, you were given a constraint (limited number of available bridges) and you found an optimized solution (maximum number of characters connected). Because there were a limited number of islands and characters, you were able to simply eyeball and easily come up with a solution that was the best with the given constraint. But as you can imagine, the things can get a lot trickier if you have tens of thousands of islands with thousands of characters living on them and maybe hundreds of bridges available at your disposal. So, let’s summon the coder in you and formulate this problem as a programming challenge. ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:2:1","tags":["leetcode","algorithms"],"title":"Decoding the LeetCode: Sliding Window Pattern Explained","uri":"/posts/2020/10/leetcode-sliding-window/"},{"categories":["Software Engineering"],"content":"The Sliding Window Approach First, we are going to represent the arrangement of islands, bridges and characters in terms of a sequence of values, i.e. an array. So, let’s assume that an index in array with value -1 mean an empty location between islands where you could build a bridge, value 0 signifies the presence of a bridge at that location, and any other number is simply a count of the number of characters on the island at that index. So your goal is to find the maximum count for connected characters given a fixed number of bridges, B (for example, B=2). Before jumping into code, we need to first develop solid intuitions about Sliding Window approach and how it can help us in this problem. As you saw in the last section, we got our solution from a contiguous subsequence, i.e. a subarray. This subarray highlighted in red, is what we call a window. The left end, L, of the window is at index 3 and right end, R, is at index 7 (assuming index start at 0). This window spans over 5 array elements, so we can also say that the window length is 5. In Sliding Window pattern problems, we will calculate multiple solutions over varying length of the window, but we will only save the most optimal solution. A naïve approach could be to evaluate the input array for all possible length of windows, i.e. all possible placements for windows of length 1, 2, 3, 4, … , 8 (size of array), and calculate the result for each window, but save only the optimal value (maximum, in our example). However, as any seasoned LeetCode-er will tell you, your program will easily hit a Time Limit Exceeded wall even with a moderate sized array, because you have way too many potential solutions in your search space. So we need a better solution. Let’s solve the above problem that had a constraint of 2 bridges with a Sliding Window approach. In a sliding Window based solution, we will generally start from the left of the array and with a window of size 1, i.e. both the left and right ends, L and R respectively, of the window will be at index 0. At each step, we also calculate the current answer, i.e. the current number of connected characters inside the window, and save the most optimal solution, i.e. the maximum count so far. Now, let’s expand the window by moving the right end, R, as much as our constraint allows us (think of this as an outer loop in the code). The constraint in this example being the count of available bridges (B = 2). We used one bridge, but we do have one more left. So we keep expanding to right. We are out of bridges to use, but we can still move to the right, as there’s an island there and our constraint will still hold (max 2 bridges). We now have 2 connected characters, which is also our best answer so far. But we also have a problem, we can’t move to the right because that’s an empty slot and we are out of bridges to use. So now we will start to shrink our window from the left marker one step at a time, and keep doing it until we are allowed to move R to the right and still satisfy our constraint (think of it as another loop inside the outer one). At this point, we still can’t move R to the right, because we are out of bridges. So let’s move L to the right one more time and reclaim one bridge. Now we’re back on track. Moving R to the right requires a new bridge, but we do have that in inventory. Moving R to the right is still valid, so let’s do it. Moving R requires a bridge. So we go back to moving L to the right, and reclaim one bridge with the very first move. Moving R will make us use the bridge that we have in inventory. We can still move R more to the right. We can’t move R anymore. That brings us to the end of our algorithm. The best answer that we have is 4, which is also the most optimal answer. 😊 Notice the followings: We started with 2 markers left (L) and right (R) at index 0 R moved to the right towards the end of the array (we can use an outer loop for this) Inside the above looping process, if we hit a state where moving R wi","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:2:2","tags":["leetcode","algorithms"],"title":"Decoding the LeetCode: Sliding Window Pattern Explained","uri":"/posts/2020/10/leetcode-sliding-window/"},{"categories":["Software Engineering"],"content":"Solving LeetCode Problems with Sliding Window Pattern ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:3:0","tags":["leetcode","algorithms"],"title":"Decoding the LeetCode: Sliding Window Pattern Explained","uri":"/posts/2020/10/leetcode-sliding-window/"},{"categories":["Software Engineering"],"content":"LeetCode 1004: Max Consecutive Ones III Let’s try to apply what we you have learned above and solve this LeetCode problem: https://leetcode.com/problems/max-consecutive-ones-iii/ You are given an array input that has 0s and 1s. You are given a constraint that you can change only K number of values from 0 to 1. And, your task is to find the longest subarray that contains only 1s. So, we are looking for the maximum window size (i.e. R - L + 1), such that the window contains all 1s, and given the constraint that we can change up to K number of values in the window from 0 to 1. Let’s first setup some basic variables: current_change_count = 0 # This is our \"inventory\", i.e. a count for the number of changes from 0 to 1 L = 0 # This is the left marker of our Sliding Window answer = -1 # This is the variable that will store the best answer Based on the earlier template, we need an outer loop, that will move R to the right. for R in range(len(A)): # Here A is the input array ... Inside the for loop, we need to update our “inventory”, i.e. current_change_count, if required i.e. if the value of the current position is 0, we need to make it 1 to include it in the window if A[R] == 0: current_change_count += 1 And we need an inner loop to move L to the right, if required i.e. if the constraint “current_change_count \u003c= K” doesn’t hold. while current_change_count \u003e K and L \u003c len(A): if A[L] == 0: current_change_count -= 1 L += 1 Finally we save the current answer as the best answer, if required, i.e. if the current window length (R-L+1) is greater than answer. answer = max(answer, R-L+1) So, our complete solution will be: class Solution: def longestOnes(self, A: List[int], K: int) -\u003e int: if not A: return 0 current_change_count = 0 L = 0 answer = -1 for R in range(len(A)): if A[R] == 0: current_change_count += 1 while current_change_count \u003e K and L \u003c len(A): if A[L] == 0: current_change_count -= 1 L += 1 answer = max(answer, R-L+1) return answer ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:3:1","tags":["leetcode","algorithms"],"title":"Decoding the LeetCode: Sliding Window Pattern Explained","uri":"/posts/2020/10/leetcode-sliding-window/"},{"categories":["Software Engineering"],"content":"LeetCode 1358: Number of Substrings Containing All Three Characters Let’s take another example: https://leetcode.com/problems/number-of-substrings-containing-all-three-characters/ A valid window in this case is the one that satisfies the constraint that all of the three characters ‘a’, ‘b’ and ‘c’ are at least once present in the window. Let’s setup some basic variables: current_answer = 0 looking_for = [0, 0, 0] # We will save the count of 'a', 'b' and 'c' occurances in the window on the 3 indexes in this array/list L = 0 best_answer = 0 The outer loop will simply be: for R in S: ... Our inventory, in this case, keeps track of the count of ‘a’, ‘b’ and ‘c’ in “looking_for” list. Since we know each character in the input will be one of the three characters, we can directly update the index like this: looking_for[ord(R)-ord('a')] += 1 and the inner loop will move L to the right if the window has ‘a’, ‘b’ and ‘c’ occurring at least once. while L \u003c len(s) and looking_for[0] and looking_for[1] and looking_for[2]: looking_for[ord(s[L])-ord('a')] -= 1 # subtract count for the character at L L += 1 current_answer += 1 The optimal answer will simply add up the overall answers found in the Sliding Window. best_answer += current_answer So, our complete solution will be: class Solution: def numberOfSubstrings(self, s: str) -\u003e int: if len(s) \u003c 3: return 0 current_answer = 0 looking_for = [0, 0, 0] L = 0 best_answer = 0 for R in s: looking_for[ord(R)-ord('a')] += 1 while L \u003c len(s) and looking_for[0] and looking_for[1] and looking_for[2]: looking_for[ord(s[L])-ord('a')] -= 1 L += 1 current_answer += 1 best_answer += current_answer return best_answer ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:3:2","tags":["leetcode","algorithms"],"title":"Decoding the LeetCode: Sliding Window Pattern Explained","uri":"/posts/2020/10/leetcode-sliding-window/"},{"categories":["Software Engineering"],"content":"LeetCode 904: Fruit Into Baskets Let’s solve yet another LeetCode question with this approach: https://leetcode.com/problems/fruit-into-baskets/ In this problem, we maintain an inventory of two baskets with a each count containing a fruit type (kind of a “key”), and a fruit count (kind of a “value”), so we can simply make it a hashmap, i.e. a Python dictionary. The constraint here is that we can only have maximum 2 baskets, with each having a unique fruit type. Our goal is to find the maximum count of fruits in both baskets. First, we setup some basic variables: L = 0 inventory_hashmap = {} best_answer = 0 Then we setup an outer loop for moving R. for R in range(len(tree)): ... We update the inventory, i.e. our hashmap with keys representing the fruit type and the values representing the number of fruits picked during current window. inventory_hashmap[tree[R]] = inventory_hashmap.get(tree[R], 0) + 1 Next, we move L to the right, and remove the corresponding count and fruit type from the inventory, if the constraint is broken. while L \u003c len(tree) and len(inventory_hashmap) \u003e 2: inventory_hashmap[tree[L]]-= 1 if inventory_hashmap[tree[L]] == 0: del inventory_hashmap[tree[L]] L += 1 Finally, we update the best answer by finding the maximum value between the best answer so far, and the current number of fruits in our inventory. best_answer = max(best_answer, sum(inventory_hashmap.values())) Adding all the components together, our solution will be this: class Solution: def totalFruit(self, tree: List[int]) -\u003e int: if not tree: return 0 L = 0 inventory_hashmap = {} best_answer = 0 for R in range(len(tree)): inventory_hashmap[tree[R]] = inventory_hashmap.get(tree[R], 0) + 1 while L \u003c len(tree) and len(inventory_hashmap) \u003e 2: inventory_hashmap[tree[L]]-= 1 if inventory_hashmap[tree[L]] == 0: del inventory_hashmap[tree[L]] L += 1 best_answer = max(best_answer, sum(inventory_hashmap.values())) return best_answer ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:3:3","tags":["leetcode","algorithms"],"title":"Decoding the LeetCode: Sliding Window Pattern Explained","uri":"/posts/2020/10/leetcode-sliding-window/"},{"categories":["Software Engineering"],"content":"Summing Up I hope these examples were good enough to drive home the intuitions required to build a Sliding Window based solution. I would also like to point out that the template that I talked explained in this article is the most common strategy with Sliding Window problems, however it is not the only kind of Sliding Window approach. As you saw, in our template, the right marker R moves faster than the left marker L, therefore this approach is sometimes referred to as Fast/Slow Sliding Window approach. There are other approaches like Fast/Catch-Up and Front/Back that I will talk about in a future post. ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:4:0","tags":["leetcode","algorithms"],"title":"Decoding the LeetCode: Sliding Window Pattern Explained","uri":"/posts/2020/10/leetcode-sliding-window/"},{"categories":["Software Engineering"],"content":"Exercise for the Readers If you want to apply what you have learned in this article, and solve a few LeetCode challenges, then you can try your hand on the following Sliding Window problems on LeetCode website: https://leetcode.com/problems/longest-continuous-subarray-with-absolute-diff-less-than-or-equal-to-limit/ https://leetcode.com/problems/number-of-substrings-containing-all-three-characters/ https://leetcode.com/problems/replace-the-substring-for-balanced-string/ https://leetcode.com/problems/max-consecutive-ones-iii/ https://leetcode.com/problems/subarrays-with-k-different-integers/ https://leetcode.com/problems/fruit-into-baskets/ https://leetcode.com/problems/get-equal-substrings-within-budget/ https://leetcode.com/problems/longest-repeating-character-replacement/ https://leetcode.com/problems/shortest-subarray-with-sum-at-least-k/ https://leetcode.com/problems/minimum-size-subarray-sum/ https://leetcode.com/problems/sliding-window-maximum/ I also want to thank the LeetCode user wh0ami for compiling this list of questions and sharing this idea, in his Java post in LeetCode discussion forums here. ","date":"2020-10-26","objectID":"/posts/2020/10/leetcode-sliding-window/:5:0","tags":["leetcode","algorithms"],"title":"Decoding the LeetCode: Sliding Window Pattern Explained","uri":"/posts/2020/10/leetcode-sliding-window/"},{"categories":["Software Engineering"],"content":"In June 2020, Twitter announced a major overhaul in its storage and retrieval systems. These changes allowed Twitter to reduce the search index latency from 15 seconds to 1 second. So, what did they do to get such impressive gains? Allow me to explain!","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/","tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/"},{"categories":["Software Engineering"],"content":"In June 2020, Twitter announced a major overhaul in its storage and retrieval systems. These changes allowed Twitter to reduce the search index latency from 15 seconds to 1 second. So, what did they do to get such impressive gains? Allow me to explain! ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:0:0","tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/"},{"categories":["Software Engineering"],"content":"Twitter’s Search Architecture Designing a search system for Twitter is an incredibly complex task. Twitter has more the 145 million daily users 1 and over half a billion tweets are sent each day 2. Their search system gets hundreds of thousands of search queries per second. For any search system indexing latency is an important metric. The indexing latency can be defined as the amount of time it takes for new information (“tweets” in this case) to be available in the search index. And as you can imagine, with a large amount of data generated every day, we have an interesting engineering problem at hand. But why is search indexing latency important for Twitter?\r\rNote that not every service out there needs to have low update their search index quickly. For example, a Costco warehouse could update their search index once every couple of hours or so. For Twitter, however, real-time access to the content can be really important, for cases like following some breaking news, ongoing conversations, delivering timelines etc.\r\r Before we take a look at what’s new, let’s first understand how the search workflow looked like at Twitter before these changes took place. In their 2012 paper titled: “Earlybird: Real-Time Search at Twitter” 3 4, Twitter released details about its search system project codenamed “Earlybird”. With Earlybird Twitter adopted their custom implementation of Apache Lucene which was a replacement for Twitter’s earlier MySQL-indexes based search algorithm. Some of the enhancements included image/video search support, searchable IndexWriter buffer, efficient relevance based search in time sorted index etc. This enabled Twitter to launch relevance-based product features like ranked home timeline. Although the search was still limited to last x days, but they later added the support for performing archive search on SSD with vanilla Lucene, as shown by the bottom row in the diagram below. ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:1:0","tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/"},{"categories":["Software Engineering"],"content":"Lucene Basics Knowing basics of Lucene helps to better understand Twitter’s implementation for their search system. Lucene is an open-source search engine library that is completely written in Java and is used by tech companies like LinkedIn, Twitter, Slack, Evernote etc. As you can imagine, we can’t afford to search record-by-record on large data specially for a time-sensitive application, we use a Lucene like solution that provides us Inverted Indexes. In information retrieval terminologies, a string is called a “term” (for example, English words), a sequence of terms is called a “field” (for example, sentences), a sequence of fields is called a document (for example, tweets), and a sequence of documents is called an index 5. Following is an example of inverted index creation, taken from “Inverted files for text search engines” research paper  6 document id document text 1 The old night keeper keeps the keep in the town 2 In the big old house in the big old gown 3 The house in the town had the big old keep 4 Where the old night keeper never did sleep 5 The night keeper keeps the keep in the night 6 And keeps in the dark and sleeps in the night Table: Input documents term‏‏‎ | freq inverted list for t and | 1 \u003c6\u003e big | 2 \u003c2\u003e, \u003c3\u003e dark | 1 \u003c6\u003e did | 1 \u003c4\u003e gown | 1 \u003c2\u003e had | 1 \u003c3\u003e house | 2 \u003c2\u003e, \u003c3\u003e in | 5 \u003c1\u003e, \u003c2\u003e, \u003c3\u003e, \u003c5\u003e, \u003c6\u003e keep | 3 \u003c1\u003e, \u003c3\u003e, \u003c5\u003e keeper | 3 \u003c1\u003e, \u003c4\u003e, \u003c5\u003e keeps | 3 \u003c1\u003e, \u003c5\u003e, \u003c6\u003e light | 1 \u003c6\u003e never | 1 \u003c4\u003e night |3 \u003c1\u003e, \u003c4\u003e, \u003c5\u003e old | 4 \u003c1\u003e, \u003c2\u003e, \u003c3\u003e, \u003c4\u003e sleep | 1 \u003c4\u003e sleeps | 1 \u003c6\u003e the | 6 \u003c1\u003e, \u003c2\u003e, \u003c3\u003e, \u003c4\u003e, \u003c5\u003e, \u003c6\u003e town | 2 \u003c1\u003e, \u003c3\u003e where | 1 \u003c4\u003e Table: inverted file You can use the above inverted index to answer questions like “Which documents have the word house in them?” Also, note that a term or a document could also be a single word, a conjunction, a phrase or a number. Each row of this index above, maps a term to a Posting List. In real implementations, the posting list may also include additional information, such as the position (index) of the term in the document or some other application-specific payload. ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:1:1","tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/"},{"categories":["Software Engineering"],"content":"Changes in Ingestion Pipeline Let’s get back to Twitter’s ingestion pipeline, which used to look like this: Click to zoomTwitter\u0026rsquo;s ingestion pipeline \"\rClick to zoom\r Flaw 1: Synchronous Workflow\r\rOne major problem with this design is that the Tweet data that we want to index isn’t available as soon as the Tweet is created. With ranked Home timeline feature, the timeline needs additional “relevance” detail about each Tweet. Among other factors, this relevance depends on fields that may not be immediately available. For example, a shortened URL (https://t.co/foo) needs to be expanded to provide more context for ranking, geo-coding resolution might take longer.\r\r Fix: Twitter decided to stop waiting for the delayed fields to become available by adding an asynchronous part to their ingestion pipeline. click to zoomTwitter\u0026rsquo;s new ingestion pipeline \"\rclick to zoom\r Now most of the data will be sent to indexing system as soon as the Tweet is posted. Another update will be sent once the additional information is available. Even though the indexing service doesn’t have full information at the beginning, at least for search system we have enough information to fetch results. ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:2:0","tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/"},{"categories":["Software Engineering"],"content":"Changes in Data Structures and Algorithms ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:3:0","tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/"},{"categories":["Software Engineering"],"content":"Eliminating Sorting Step Lucene’s vanilla implementation stored document ids using delta encoding, such that the key at an index depends on the previous index. Also, the document ids were generated starting from 0 up to the size of the segment storing those documents (a segment is just a “committed” i.e. immutable index). This essentially meant that the search could only be performed from left to right, i.e. from the oldest tweet to latest tweet. However, like many other products at Twitter, search is also designed to prioritize recent tweets over the older ones. So, twitter decided to customize this implementation in Earlybird. Earlybird diverged from standard Lucene approach and allocated document ids to incoming tweet from high value to low value, i.e. from the {size of the index - 1} to 0. This lets searching algorithm to traverse from latest tweet to oldest tweet and with the possibility of returning early if a client-specified number of hits have been recorded. Flaw 2: Decoupled Search System and Tweet Creation System\r\rBy design, Tweet creation system at Twitter is decoupled from the search system. Hence the indexing service can’t guarantee that the order in which it receives the tweets is also the order of their creation. Hence to fix this, Twitter had to include a buffer in their ingestion pipeline to temporarily store and then sort all of the incoming tweets to the indexing service. This unfortunately adds additional delays in search indexing.\r\r FIx: Twitter changed it’s ID assignment scheme such that each tweet is given a document ID based on its time of creation. They fit the document id to 31 bits (positive Java Integer), such that 27 bits store the timestamp with microseconds granularity. The rest 4 bits are used as a counter for the all of the tweets received at the same microsecond. Although the probability of such \\( 2^4\\) events is rare, but all tweets after 16, that are created at the same microsecond precision, would be assigned to the next segment, and would appear slightly out of order in search results, if selected. This means that Twitter can eliminate the need for explicit sorting step and reduce latency even further. ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:3:1","tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/"},{"categories":["Software Engineering"],"content":"Optimizing Operations on Posting Lists For their posting list implementation, Twitter had been using Unrolled Linked Lists. This implementation had only one writer thread and multiple searcher threads (one per CPU core, with tens of thousands of cores per server) without any locks. Writing would be done only at the head of the list, and the searcher would search starting from the new head pointer or the older head (still, valid) pointer to the end of the list. (notice in the figure above how document id assignment is done from high value to low value. ) Unrolled linked lists are cache friendly data structures that also reduce some of the overhead of storing pointer references. However, we can’t use them to cost-effectively insert at arbitrary location in the list, they also depend largely upon the assumption that the input will be strictly ordered. Fix: Twitter decided to opt for Skip Lists as the replacement for unrolled linked lists. Skip Lists allow for O(\\( \\log{n}\\)) search and insertions, and also support concurrency. I wrote an introductory article on Skip Lists here. I highly recommend you to read through the article to better understand the concept behind Skip Lists. Twitter’s Skip List implementation Skip Lists are probabilistic data structures, such that after inserting a new node at list \\( L_{i} \\), we insert the node at the layer \\( L_{i+1}\\) with some probability. Twitter’s found 20% ( \\( \\frac{1}{5}\\) ) is a good tradeoff between space and time complexity (memory used vs. speed). In their implementation, Twitter used several optimizations. The implemented the Skip List in a single flat array, such that the “next” pointer becomes just an index into the array. Here’s a breakdown of the other optimizations in their Skip List: Insertion would simply append the new value at the end of the array, and the corresponding pointer will be updated appropriately after traversing the Skip List. New values at the higher levels will be added with a 20% probability. At every search operation, the descent down the different levels is recorded and saved as a “search finger”. This helps in lookups when we are finding a document with id greater than the one for which we already have the search finger. It reduces the lookup time complexity from O(\\( \\log{n} \\)) to O(\\( \\log{d} \\)), where n is the number of items in the posting list and d is the number of items in the list between the first value and the second value, in case of conjunction search queries. Using primitive array also has an advantage of reducing pointer management overheads, such as garbage collection. Allocating vertical levels of Skip List in adjacent location, eliminates the need for storing “above” and “below” pointers. One obvious disadvantage, however, is that the Skip Lists are not cache friendly. However, being able to run in logarithmic time in case of sparse documents is a big advantage. ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:3:2","tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/"},{"categories":["Software Engineering"],"content":"Additional Details Twitter’s new Skip List implementation also uses a “published pointer” that points to the current maximum pointer into array such that searchers never traverse beyond this pointer. This allows for ensuring atomicity in search and retrievals, and a document is searched only if all of its terms are indexed. Considering this was a change of large magnitude, Twitter rolled it out gradually by first evaluating the results in Dark Launch mode. This was done to ensure that the clients do not have any reliance or assumptions on the earlier 15 seconds delay in search indexing. ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:4:0","tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/"},{"categories":["Software Engineering"],"content":"Conclusion In order to reduce search indexing latency, Twitter made big changes to its storage system and retrieval system. They shared their approach and learnings in a whitepaper 7 that I summarized here. The search indexing improvement from 15 seconds to 1 second means that the new content should be available for Twitter users to access almost instantaneously. Twitter Q3-2019 - Selected Financial and Metrics ↩︎ Twitter Engagement Report 2018 ↩︎ http://users.umiacs.umd.edu/~jimmylin/publications/Busch_etal_ICDE2012.pdf ↩︎ https://www.youtube.com/watch?v=KUmFJc3fFuM ↩︎ https://lucene.apache.org/core/8_6_0/core/org/apache/lucene/codecs/lucene86/package-summary.html#package.description ↩︎ http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.105.8844 ↩︎ Twitter Infrastructure: Reducing search indexing latency ↩︎ ","date":"2020-07-19","objectID":"/posts/2020/07/twitter-search-redesign/:5:0","tags":["system design","data structure"],"title":"How Twitter Reduced Search Indexing Latency to One Second","uri":"/posts/2020/07/twitter-search-redesign/"},{"categories":["NLP"],"content":"Word vector representations with subword information are great for NLP modeling. But can we make lexical corrections using a trained embeddings space? Can its accuracy be high enough to beat Peter Norvig's spell-corrector? Let's find out!","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/"},{"categories":["NLP"],"content":"Word vector representations with subword information are great for NLP modeling. But can we make lexical corrections using a trained embeddings space? Can its accuracy be high enough to beat Peter Norvig’s spell-corrector? Let’s find out! ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:0:0","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/"},{"categories":["NLP"],"content":"Introduction Let’s first define some terminology. ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:1:0","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/"},{"categories":["NLP"],"content":"What are word embeddings? Word Embedding techniques have been an important factor behind recent advancements and successes in the field of Natural Language Processing. Word Embeddings provide a way for Machine Learning modelers to represent textual information as the input to ML algorithms. Simply put, they are a hashmap where the key is a language word, and the corresponding value is a vector of real numbers fed to the models in place of that word. There are different kinds of word embeddings available out there that vary in the way they learn and transform a word to a vector. The word vector representations can be as simple as a hot-encoded vector, or they can be more complex (and more successful) representations that are trained on large corpus, take context into account, break the words into subword representations etc ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:1:1","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/"},{"categories":["NLP"],"content":"What are subword embeddings? Suppose you have a deep learning NLP model, say a chatbot, running in production. Your customers are directly interacting with the ML model. The model is being fed the vector values, such that each word from the customer is queried against a hashmap and the value corresponding to the word is the input vector. All of the keys in your hashmap represents the vocabulary, i.e. all the words you know. Now, how would you handle a case where the customer uses a word that is not already present in your vocabulary? There are many ways to solve this out-of-vocabulary (OOV) problem. One popular approach is to split the words into “subword” units, and use those subwords to learn your hashmap during model training stage. At the time of inference, you would again divide each incoming word into smaller subword units, find the word vector corresponding to each subword unit using the hashmap, then aggregate each subword vector to get the vector representation for the complete word. For example, let’s say we have the word tiktok, the corresponding subwords could be tik, ikt, kto, tok, tikt, ikto, ktok etc. This is the character n-gram division for the input word, where n is the subword sequence length, and is fixed by the modeler. Figure: example subword representations ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:1:2","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/"},{"categories":["NLP"],"content":"What is FastText? FastText is an open-source project from Facebook Research. It is a library for fast text-representations and classifications. It is written in C++ and supports multiprocessing. It can be used to train unsupervised word vectors and supervised classification tasks. For learning word-vectors it supports both skipgram and continuous bag-of-words approaches. FastText supports subword representations such that each word can be represented as a bag of character n-grams in addition to the word itself. Incorporating finer (subword level) information is pretty good for handling rare words. You can read more about the FastText approach in their paper here . For an example, let’s say you have a word “superman” in FastText trained word embeddings (“hashmap”). Let’s assume the hyperparameters minimum and maximum length of ngram was set to 4. Corresponding to this word, the hashmap would have the following keys: Original word: superman n-gram size subword 4 \u003csup 4 supe 4 uper 4 perm 4 erma 4 rman 4 man\u003e where “\u003c” and “\u003e” characters mark the start and end of a word respectively ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:1:3","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/"},{"categories":["NLP"],"content":"The Task: Spell-Checking In NLP, the learnt word embedding vectors not only have lexical representations for words, but the vector values also have semantically related positioning in the embedding space. We are going to try and build a spell-checker application based on FastText word vectors such that given a misspelled word, our task will be to find the word vector representation closest to the vector representation of that word in trained embedding space. We will work based on this simple heuristic: heuristic\r\rIF word exists in the Vocabulary ​ Do not change the word ELSE ​ Replace the word with the one closest to its sub-word representation \r\r ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:2:0","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/"},{"categories":["NLP"],"content":"Dataset \u0026 Benchmark For fun, let’s build and evaluate our spell-checker on the same training and testing data as this classic article: “How to write a Spelling Corrector” by Peter Norvig , Director of Research at Google. In this article, Norvig build a simple spelling corrector based on basic probability theory. Let’s see how does this FastText based approach hold up against it. ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:2:1","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/"},{"categories":["NLP"],"content":"Installing FastText Installation for FastText is straightforward. FastText can be used as a command line tool or via Python client. Click here to access the latest installation instructions for both approaches. ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:2:2","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/"},{"categories":["NLP"],"content":"Method 1: Using Pre-trained Word Vectors FastText provides pretrained word vectors based on common-crawl and wikipedia datasets. The details and download instructions for the embeddings can be found here. For a quick experiment, let’s load the largest pretrained model available from FastText and use that to perform spelling-correction. Download and unzip the trained vectors and binary model file. -\u003e wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip -\u003e unzip crawl-300d-2M-subword.zip There are two files inside the zip: crawl-300d-2M-subword.vec : This file contains the number of words (2M) and the size of each word (vector dimensions; 300) in the first line. All of the following lines start with the word (or the subword) followed by the 300 real number values representing the learnt word vector. crawl-300d-2M-subword.bin: This binary file is the exported model trained on the Common-Crawl dataset. As mentioned in his article, Norvig used spell-testset1.txt and spell-testset2.txt as development and test set respectively, to evaluate the performance of his spelling-corrector. I’m going to load the pretrained FastText model and make predictions based on the heurisitic defined above. I’m also going to borrow some of the evaluation code from Norvig. import io import fasttext def load_vectors(fname): fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore') n, d = map(int, fin.readline().split()) data = {} for line in fin: tokens = line.rstrip().split(' ') data[tokens[0]] = map(float, tokens[1:]) return data def spelltest(tests, model, vocab): \"Run correction(wrong) on all (right, wrong) pairs; report results.\" import time start = time.clock() good, unknown = 0, 0 n = len(tests) for right, wrong in tests: w = wrong if w in vocab: print('word: {} exists in the vocabulary. No correction required'.format(w)) else: w_old = w w = model.get_nearest_neighbors(w, k=1)[0][1] print(\"found replacement: {} for word: {}\".format(w, w_old)) good += (w == right) dt = time.clock() - start print('{:.0%} of {} correct at {:.0f} words per second ' .format(good / n, n, n / dt)) def Testset(lines): \"Parse 'right: wrong1 wrong2' lines into [('right', 'wrong1'), ('right', 'wrong2')] pairs.\" return [(right, wrong) for (right, wrongs) in (line.split(':') for line in lines) for wrong in wrongs.split()] if __name__ == \"__main__\": model = fasttext.load_model(\"crawl-300d-2M-subword.bin\") vocab = load_vectors(\"crawl-300d-2M-subword.vec\") spelltest(Testset(open('spell-testset1.txt')), model, vocab) spelltest(Testset(open('spell-testset2.txt')), model, vocab) For the sake of brevity, I’m reducing the output log to just the metrics trace. output\r\r… 0% of 270 correct at 3 words per second. … 0% of 400 correct at 4 words per second.\r\r That didn’t go well! 😅 None of the corrections from the model were right. Let’s dig into the results. Here are a few snapshots from the output log: output\r\r… word: sysem exists in the vocabulary. No correction required word: controled exists in the vocabulary. No correction required word: reffered exists in the vocabulary. No correction required word: irrelavent exists in the vocabulary. No correction required word: financialy exists in the vocabulary. No correction required word: whould exists in the vocabulary. No correction required … found replacement: reequipped for word: reequired found replacement: putput for word: oputput found replacement: catecholate for word: colate found replacement: yeahNow for word: yesars found replacement: detale for word: segemnt found replacement: \u003cli\u003e\u003cstrong\u003eStyle for word: earlyest …\r\r Looks like there are a lot of garbage subwords in the pretrained vocabulary that directly matches our misspelled input. Also, the performed lexical corrections show that the model replaced misspelled input words with the closest semantic neighbor. However none of those neighbors are meaningful English words. We can verify this by checking out the neighbors for random m","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:2:3","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/"},{"categories":["NLP"],"content":"Method 2: Trained Our Own Word Vectors For a fair comparison with Norvig’s spell-checker, let’s use the same training data that he used (big.txt). From Norvig's article:\r\r… by counting the number of times each word appears in a text file of about a million words, big.txt. It is a concatenation of public domain book excerpts from Project Gutenberg and lists of most frequent words from Wiktionary and the British National Corpus.\r\r -\u003e wc big.txt 128457 1095695 6488666 big.txt The training data has around 128K lines, 1M words, 6.5M characters. As discussed above, we should try with keeping the wordNgrams hyperparameter to 1, and use the trained FastText model to perform spell-checking. import io import fasttext def load_vectors(fname): fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore') n, d = map(int, fin.readline().split()) data = {} for line in fin: tokens = line.rstrip().split(' ') data[tokens[0]] = map(float, tokens[1:]) return data def spelltest(tests, model): \"Run correction(wrong) on all (right, wrong) pairs; report results.\" import time start = time.clock() good, unknown = 0, 0 n = len(tests) for right, wrong in tests: w_old = wrong w = wrong if w in model.words: pass else: w = model.get_nearest_neighbors(w, k=1)[0][1] good += (w == right) if not (w == right): if w_old != w: print(\"Edited {} to {}, but the correct word is: {}\".format(w_old, w, right)) dt = time.clock() - start print('{:.0%} of {} correct at {:.0f} words per second ' .format(good / n, n, n / dt)) def Testset(lines): \"Parse 'right: wrong1 wrong2' lines into [('right', 'wrong1'), ('right', 'wrong2')] pairs.\" return [(right, wrong) for (right, wrongs) in (line.split(':') for line in lines) for wrong in wrongs.split()] if __name__ == \"__main__\": model = fasttext.train_unsupervised('big.txt', wordNgrams=1, minn=1, maxn=2, dim=300, ws=8, neg=8, epoch=4, minCount=1, bucket=900000) spelltest(Testset(open('spell-testset1.txt')), model) spelltest(Testset(open('spell-testset2.txt')), model) There are a couple of changes in this code. We are training the model with specific hyper-parameters, and the output traces will inform us what went wrong in our decisions. Note that FastText doesn’t provide an option to set seed in the above implementation, so the results may vary by 1%-2% on every execution. -\u003e python fasttext_trained.py Read 1M words Number of words: 81398 Number of labels: 0 Progress: 100.0% words/sec/thread: 20348 lr: 0.000000 avg. loss 1.943424 ETA: 0h 0m 0s ... 73% of 270 correct at 46 words per second. ... 69% of 400 correct at 48 words per second. This is a big improvement! 😄 We have almost the same accuracy as Norvig’s spell-corrector. Here are Norvig’s results for comparison. 75% of 270 correct at 41 words per second 68% of 400 correct at 35 words per second Looking at some of the edits in the output traces, I can see that the model output isn’t essentially incorrect, but the model is biased to certain edit-based operations. output\r\r… Edited reffered to referred, but the correct word is: refered Edited applogised to apologized, but the correct word is: apologised Edited speeking to seeking, but the correct word is: speaking Edited guidlines to guideline, but the correct word is: guidelines Edited throut to throughout, but the correct word is: through Edited nite to unite, but the correct word is: night Edited oranised to organised, but the correct word is: organized Edited thay to thy, but the correct word is: they Edited stoped to stooped, but the correct word is: stopped Edited upplied to supplied, but the correct word is: applied Edited goegraphicaly to geographical, but the correct word is: geographically …\r\r As you can see our trained model is nicely producing dictionary-based words as output. It’s likely that with contextual training approaches and evaluations, along with more training data, we can come up with an even better approaches that would understand context in a full sentence and produce the correct word as the spel","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:2:4","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/"},{"categories":["NLP"],"content":"Conclusion Our model was able to match Peter Norvig’s spell-corrector. 😊 Looking at the failing cases, we realize that the model could potentially do even better with more training data and a contextual training and evaluation strategy. ","date":"2020-07-18","objectID":"/posts/2020/07/spell-checker-fasttext/:3:0","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/posts/2020/07/spell-checker-fasttext/"},{"categories":["Software Engineering"],"content":"Analyzing randomness is hard. So why would you want to choose a probabilistic data structure in your system implementation? Allow me to explain!","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Analyzing randomness is hard. So why would you want to choose a probabilistic data structure in your system implementation? Allow me to explain! ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:0:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Why am I writing about Skip Lists? Skip List is one of those data structures that I have seen briefly mentioned in academic books; but never seen it being used in academic or industrial projects (or in LeetCode questions 🙂 ). However I was pleasantly surprised to see Skip List being one of the major contributors behind Twitter’s recent success in reducing their search indexing latency by more than 90% (absolute). This encouraged me to brush up my understanding of Skip List and also learn from Twitter’s solution. ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:1:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Why do we need Skip Lists? I believe it’s easier to understand the concept of Skip Lists by seeing their application. So let’s first build a case for Skip List. ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:2:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"The Problem: Search Let’s say you have an array of sorted values as shown below, and your task is to find whether a given value exists in the array or not. Given an array: find whether value 57 exists in this array or not ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:2:1","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Option 1: Linearly Searching an Array As the name suggests, in linear search you can simply traverse the list from one end to the other and compare each item you see with the target value you’re looking for. You can stop the linear traversal if you find a matching value. You can stop the search if at any point of time you see an item whose value is bigger than the one you’re looking for. Is this a good solution? [Click here]\r\rThis solution is alright. But as you would notice, it doesn’t take advantage of the fact that the given sequence of values are in sorted order. The worst case time complexity for this algorithm would be order of the number of values in the sequence, i.e. O(n). We will be comparing our target value with each member of sequence if the element is at the very end of the sequence or does not exist in the sequence.\r\r Can we do better?\r\rYes, with Binary Search!\r\r ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:3:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Option 2: Binary Search on an Array The binary search concept is simple. Think of how you search for a given word in a dictionary. Say if you are trying to find the word “quirky” in the dictionary, you do not need to linearly read each word in the dictionary one-by-one to know if the word exists in the dictionary. Because you have established that the dictionary has the all of the words in alphabetical sorted order. You can simply flip through the book, notice which word you are at, then either go left or right depending on whether the word starts with a character that comes before or after ‘q’. Going back to our example, let’s first check the value at the center of the sequence, if the value matches our target. we can successfully end the search. But if it is not the value we are looking for, we will compare the value of the middle position with our target value. If our target value is smaller, we need to look into smaller numbers, so we go towards left and ignore the values on the right. By following this binary search approach have reduced our search space and the required number of comparisons by half. Also, we repeat the same process on the “valid” half, i.e. we directly go the middle value of the left sub-array and see if we have a match, if not, we again decide whether to go left or right, and hence reduce our candidate search space by half again. What is the time complexity now?\r\rThe time complexity of this algorithm is again defined by the number of comparisons. If you recognize the pattern in this algorithm, you would know that the algorithm belongs to the category for which the size of a variable is continuously divided by a constant (2 in this case). And hence our algorithmic time complexity is O(\\(log_2n\\)). [Read this for an intuitive explanation]\r\r This is excellent. Logarithmic time complexity means that our algorithm will scale really well with large number of values being stored in the sequential array. Is this a perfect solution?\r\rOne big issue, here, is that while sequential storage (for example, arrays) makes in easy and cheap to design binary search algorithms, it makes it costlier for us to implement an insertion and deletion strategy. For example, if you insert an item at a specific index in the array, you will have to shift the values on its right by one place to the right. Similarly if you delete a value at an arbitrary index inside the array, you will have to left shift all the values to the right of that index by one place. (see examples below)\r\r As you can imagine, having extra space on the right of the array would be convenient else you would have to call for dynamic allocation of memory at every insertion and copy over the whole array to newly allocated array memory. List data structure in Python, allocates list in a similar way that leaves capacity at the end of the sequence for future expansion. If we run out of space in the list, Python allocates a new and much larger (double the size, for example) sized list, copies over each value from the old list to new list, and then deletes the old list. (This is why the insertion in a Python list is Amortized O(1) complexity) Could there be a way to have a data structure with better insertion and deletion costs?\r\rYes, we can use Linked Lists!\r\r ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:4:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Option 3: Linear search with Linked Lists Linked Lists are dynamic data structures that store values in non-contiguous memory blocks (called nodes). Each block refers to the other using memory pointers. These pointers are sort of the links in a chain that define the ordering among the non-contiguous nodes. A Singly Linked list represents a sequential structure where each node maintains a pointer to the element that follows it in the sequence. In linked lists we also maintain additional reference to the list nodes. Head is one such reference which is variable that stores the address for the first element in the list (stores null pointer if the list is empty). Insertion and deletion at any arbitrary location is straightforward, as you would simply reconnect the links of the nodes previous and next positions from the target position. We can also have a Doubly Linked List where each node stores the pointer to the previous as well as the next node in the sequence. A tail reference may also be used to store pointer to the last node in the linked list. As you would notice, random access in linked lists are not possible. And to find a value in the list, we would simply have to traverse the list from one end, say from the node referred by Head, towards the other end till we find what we are looking for. So, even though you can insert and delete in O(1), you still need the reference to the target location. Finding the target location is again O(n). With Linked Lists, while we solved the issue of costly insertions and deletions, we haven’t quite found a good solution to quickly access a target value in the list (or find if the value exists). Wouldn’t it be great if we could have the random access (or something close to it) in the list to simply our find, insert and delete operations? Well this is where Skip List come handy! ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:5:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Option 4: Pseudo-binary search with Linked Lists [Skip List] Technical advances have made low latency highly desirable for software applications. How well the underlying algorithm scales to demand can make or break a product. On the other hand, the memory/storage costs is getting cheaper by day. This propels many system designers to make decisions that balance compromises that spend extra money on storage costs, while opting for algorithms with low time complexities. Skip Lists is a good example of one such compromise. Skip Lists were proposed by WIlliam Pugh in 1989 (read the original paper  here) as a probabilistic alternative to balanced trees. Skip lists are relatively easy to implement, specially when compared to balanced trees like AVL Trees or Red Black Trees. If you have used the Balanced Trees before, you may remember how they are great data structures to maintain relative order of elements in a sequence, while making search operations easy. However, insertions and deletions in Balanced Trees are quite complex as we have the additional task to strictly ensure the tree is balanced at all times. Skip Lists introduce randomness in by making balancing of the data structure to be probabilistic. While it definitely makes evaluating complexity of the algorithm to be much harder, the insertion, deletion operations become much faster and simpler to implement. The complexities are calculated based on expectation, but they have a high probability, such that we can say with high confidence (but not absolute guarantee) that the run time complexity would be O(log(n)) apart from some specific worst-case scenarios as we would see soon. ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:6:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Skip Lists - Intuition Following is the map of New York Subway’s “7” train. Each point on the horizontal line represents on train stop on a specific street. The line at the top is the local train that has much more number of stops compared to the express line on the below it. Government decides where to put the stops based on factors like nearby facilities, neighborhood, business centers etc. Let’s simplify the above route map and display stations by the street numbers. In the simplified map above, the express line is shown at the top and local line is below it. Now if you had this map and you were supposed to travel from stop 14 to stop 59. How would you plan your trip route? For simplicity let’s assume we can’t overshoot, meaning that we can’t go some station to the right and then travel back towards left. You would probably want to take the express line from stop 14 and get down on stop 42. Then take the local train from stop 42 and reach the destination 59 from there, as shown below. As you would notice, by taking the express route you would be able to skip some stations along the way and maybe even reach the destination much faster compared to taking the local train from a original stop and traveling each station one by one to reach a destination stop. This is exactly the core idea behind Skip List. With Skip Lists, we introduce redundant travel paths at the cost of additional space, but the additional travel paths have lesser and lesser number of “stops” such that the linear traversal is much faster if we travel in those additional lanes. ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:6:1","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Skip Lists - Basics Let’s now build the concept of Skips Lists from bottom to top. Assume that we have a Doubly Linked List which is unsorted. What do you think would be the search complexity for this list? Search would be done in O(n), because you would have to traverse the list linearly to find your target node. Let’s assume that the list is now sorted as shown below. What do you think is the search complexity now? Well, it’s still O(n). Even if it sorted, the linked list structure doesn’t provide us random access, hence we can’t simply calculate the next middle index and access that middle node in O(1). Now, let’s put our “express line” on top of the existing structure. We are going to create a second Doubly Linked List and place the second list above of the first list such that each node has four reference to neighboring nodes: left, right, above, below. which can be visualized as: Now given this Skip Lists structure, you can find the number 73 with lesser number of comparison operations. You can start traversing from the top list first, and at every node see if the next node overshoots the value that you are looking for. If it doesn’t simply go the next node in the list on the top, else go to the list on the bottom and continue your linear search in the bottom list. We can summarize the searching approach in a simple algorithm. Algorithm\r\r Walk right in the Linked List on top (L1) until going right is going too far Walk down to the bottom list (L0) Walk right in L0 until element is found or we overshoot. \r\r So what is the kind of search cost we are looking at? We would want to travel the length of L1 as much as possible because that gives us skips over unnecessary comparisons. And eventually we may go down to L0 and traverse a portion of it. The idea is to uniformly disperse the nodes in the top lane. Search Cost = \\(|L1| + \\frac{|L0|}{|L1|}\\) How many “stops” (nodes) do you need in the top lane? It can be mathematically shown1 that if the cardinality of the bottom list is n (i.e. it has n nodes), we will get an optimal solution if we have \\(\\sqrt{n}\\) nodes in the top layer. So, Search Cost = \\(\\sqrt{n} + \\frac{n}{\\sqrt{n}}\\) = 2\\(\\sqrt{n}\\) = O(\\(\\sqrt{n})\\)) However, our Skip List don’t have to limited to 2 lists. We can have more lists (“lanes”), meaning our search costs would scale as: ‎‏‏‎2 sorted lists → \\(2 * \\sqrt[2]{n}\\) 3 sorted lists → \\(3 * \\sqrt[3]{n}\\) ‎‏‏‎… k sorted lists → \\(k * \\sqrt[k]{n}\\) (if we have log(n) number of sorted lists:)‎‎‎‎‎‎‎‎‎‎‎‏‏‎ ‎‎‎‎‎‎‎‎‎‎‎log(n) sorted lists → \\(log(n) * \\sqrt[log(n)]{n}\\) = 2 \\(\\log{n}\\) ‎‎‎‎‎‏‎‎‏‏ So, if the nodes are optimally dispersed, theoretically the structure gives us our favorite logarithmic complexity 🙂 (Although in implementation when we use randomization to insert the nodes, things get a little tricky). Now think of the structure we have created. At the bottom we have n nodes, the layer on top has interspersed \\(\\frac{n}{2}\\) nodes and the layer above it has \\(\\frac{n}{4}\\) nodes. Do you how this structure looks quite similar to a tree? We just need to figure out how “balancing” works in this tree (hint: it’s probabilistic). ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:6:2","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Skip List - Definition Skip List S consists of a series of lists {\\(S_0,S_1 .. S_h\\)} such that each list \\(S_i\\) stores a subset of the items sorted by increasing keys. Also note:\r\r h represents the height of the Skip List S each list also has two sentinel nodes -\\(\\infty\\) and +\\(\\infty\\) . -\\(\\infty\\) is smaller than any possible key in the list and +\\(\\infty\\) is greater than any possible key in the list. Hence the node with -\\(\\infty\\) key is always on the leftmost position and node with +\\(\\infty\\) key is always the rightmost node in the list For visualization, it is customary to have the \\(S_0\\) list on the bottom and lists \\(S_1, S_2, .. S_h\\) above it Each node in a list has to be “sitting” on another node with same key below it. Meaning that if \\(L_{i+1}\\) has a node with key k, then \\(L_i, L_{i-1} ..\\) all valid lists below it will have the same key in them We may chose to opt for an implementation of Skip List that only uses Single Linked Lists. However it may only improve the asymptotic complexity by a constant factor Skip List also needs to maintain the “head” pointer, i.e. the reference to the first member (sentinel node -\\(\\infty\\)) in the topmost list On average the Skip List will have O(n) space complexity \r\r Let’s first simplify the visualization for our nodes and connection in Skip List. A standard, simplified Skip List would look like this: Now, let’s see how some of the basic operations are performed on a Skip List. ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:6:3","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Searching in Skip List Searching in a Skip List in straightforward. We have already seen an example above with two lists. We can extend the same approach to a Skip List with h lists. We will simply start with the leftmost node in the list on top. Go down if the next value in the same list is greater than the one we are looking for, go right otherwise. The main idea is again to skip comparisons with as many keys as possible, while compromising a little on the extra storage required in the additional lists containing subset of all keys. On average, “expected” Search complexity for Skip List is O(log(n)) ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:6:4","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Inserting in Skip List To insert a new node, we would first find the location where this new key should be inserted in the list at the very bottom. This can be simply done be re-using the search logic, we start traversing to the right from the list on top and we go one step down if the next item is bigger than the key that we want to insert, else go right. Once we reach at a position in bottom list where we can’t go more to the right, we insert the new value on the right side of that position. Skip List after inserting 67: Now comes the probabilistic part, after inserting the new node in the list \\(L_i\\) we have to also insert the node in list above it, i.e. list \\(L_{i+1}\\) with probability \\(\\frac{1}{2}\\). We do this with a simple coin toss, such that if we get a head we insert the node in the list \\(L_{i+1}\\) and toss the coin again, we stop when we get a tail. We may repeat the coin toss till we keep getting heads, even if we have to insert a new layer (list) at the top. Note that we also need to “re-hook” the links for the newly inserted node. The new node needs to have it’s ‘below’ reference pointing to the node below, and the node below would have ‘above’ reference pointer to the new node. To find the ‘left’ neighbor for the new node, we simply traverse towards left from the node below it and return ‘above’ pointer from a node for which ‘above’ pointer is not null. The ‘right’ reference of the ‘left’ neighbor is used to update the ‘right’ reference of the new node. Also, the ‘left’ reference for the new node’s right neighbor is also updated to the new node. This re-hooking operation is actually pretty easy to implement with Linked Lists. Let’s toss the coin again. Last toss gave us another Head, so let’s toss the coin again. We got a Tail this time, so no more insertions are required. As you see the probability of a node also getting inserted in the layer above it get reduced by half after every layer. Still, there is a worst case possibility that you would keep getting Heads indefinitely, although the probability of that happening is extremely small. To avoid such cases when you may get a large number of heads sequentially, you could also use a termination condition where you stop inserting if you reach a predefined threshold for the number of layers (height) or a predefined threshold for the number of nodes in a specific layer. Although the expected time complexity would still be O(log(n)) for all operations. ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:6:5","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Deletion in Skip List Deletion operation in Skip List is pretty straightforward. We first perform search operation to find the location of the node. If we find the node, we simply delete it at all levels. Before deleting a node, we simply ensure that no other node is pointing to it. Release all references to this node. And move to the node below it. Remove all references to this node and release it. Move to the node below in the bottom list. Release this node as well. This is how our Skip List looks like after deletion. ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:6:6","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Conclusion In this article we saw how Skip List is a probabilistic data structure that loosely resembles a balanced binary tree. It is also easy to implement and very fast as the search, insert, delete operations have an expected time complexity of O(log(n)), along with O(n) expected space complexity. in next article, I will introduce a practical application of Skip List and explain how Twitter used Skip List to get a huge improvement in their search indexing latency. ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:7:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":["Software Engineering"],"content":"Reference and Further Reading MIT OCW 6.046 Design and Analysis of Algorithms Lecture 7: Randomization: Skip Lists ↩︎ ","date":"2020-07-14","objectID":"/posts/2020/07/skip-list/:8:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/posts/2020/07/skip-list/"},{"categories":null,"content":" Sumit Kumar - UChicago Alum. Research Scientist 2 at Alexa AI, Amazon Prev-SDE2@Amazon, Lead Engineer@Samsung R\u0026D, SSE@Altran ‎‎‎‎‎‏‏‎ ‎ ","date":"2020-07-14","objectID":"/about/:0:0","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"Brief My name is Sumit Kumar. I love learning new things. I have more than 8 years of work exeperience in Software Development, Machine Learning/Deep Learning research. Currently I work as a Research Scientist II for Alexa AI at Amazon, Seattle. And, my area of research focus is Natural Language Understanding. ","date":"2020-07-14","objectID":"/about/:0:1","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"Long Form 😄 Sumit joined the University of Chicago’s Graham School as a full-time, international student in the autumn 2017 cohort. He is currently a Research Scientist 2 at Amazon’s headquarters in Seattle. He has seven years of experience in software engineering and research and has worked extensively with major programming languages including C, C++, JavaScript, Java, MATLAB, R, and Python. After earning his bachelor’s degree in computer science, he started his career at Aricent Group as a software developer where he worked on different telecom projects, writing software for simulating radio network controllers that were used by Nokia-Siemens Networks. In 2012, Sumit joined Samsung R\u0026D in Noida, India and developed the L3 software protocol stack used today in Samsung’s smartphone modems. He has worked on various telecom projects in Canada, Australia, South Korea, and India, where he developed features and algorithms compliant with 3GPP standard for WCDMA/LTE wireless communication. During this period, he also collaborated with various telecom vendors across the world and was instrumental in success of many critical projects. In 2015, Sumit was promoted to lead engineer and started working in the advanced R\u0026D division at Samsung. He mentored engineers in addition to his independent research work. Sumit developed various novel solutions and algorithms during his research at Samsung and is the inventor and co-author for seven algorithms filed for patents by Samsung. Apart from his research work, he also created tools to automate many manual processes at Samsung. He did an extensive amount of machine-learning based research in audio DSP domain to solve problems such as Blind Source Separation (Cocktail Party Problem) and Audio Directionality and Speaker Diarization. During this time, he also volunteered as a teacher in Samsung’s Corporate Social Responsibility (CSR) mission to help under-funded schools. During his time as a student at the Graham School, Sumit also worked as a research assistant at the Research Computing Center (RCC), UChicago. There he worked on web development projects, based on Django frameworks in Python, to help researchers from different universities connect with each other and share large volumes of data. He was the International Student Representative for the Master of Science in Analytics (MScA) program at Graham School to enhance international students' experience and engagement and was also a member of the UChicago cricket team. Sumit has also taught Advanced Analytics and Machine Learning certificate course at The University of Chicago and Python for Internet Programming certificate course at The University of Washington. [Source] [Web Archive backup] ","date":"2020-07-14","objectID":"/about/:0:2","tags":null,"title":"About me","uri":"/about/"}]