[{"categories":["NLP"],"content":"Word vector representations with subword information are great for NLP modeling. But can we make lexical corrections using a trained embeddings space? Can its accuracy be high enough to beat Peter Norvig's spell-corrector? Let's find out!","date":"2020-07-18","objectID":"/spell-checker-fasttext/","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/spell-checker-fasttext/"},{"categories":["NLP"],"content":"Word vector representations with subword information are great for NLP modeling. But can we make lexical corrections using a trained embeddings space? Can its accuracy be high enough to beat Peter Norvig‚Äôs spell-corrector? Let‚Äôs find out! ","date":"2020-07-18","objectID":"/spell-checker-fasttext/:0:0","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/spell-checker-fasttext/"},{"categories":["NLP"],"content":"Introduction Let‚Äôs first define some terminology. ","date":"2020-07-18","objectID":"/spell-checker-fasttext/:1:0","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/spell-checker-fasttext/"},{"categories":["NLP"],"content":"What are word embeddings? Word Embedding techniques have been an important factor behind recent advancements and successes in the field of Natural Language Processing. Word Embeddings provide a way for Machine Learning modelers to represent textual information as the input to ML algorithms. Simply put, they are a hashmap where the key is a language word, and the corresponding value is a vector of real numbers fed to the models in place of that word. There are different kinds of word embeddings available out there that vary in the way they learn and transform a word to a vector. The word vector representations can be as simple as a hot-encoded vector, or they can be more complex (and more successful) representations that are trained on large corpus, take context into account, break the words into subword representations etc ","date":"2020-07-18","objectID":"/spell-checker-fasttext/:1:1","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/spell-checker-fasttext/"},{"categories":["NLP"],"content":"What are subword embeddings? Suppose you have a deep learning NLP model, say a chatbot, running in production. Your customers are directly interacting with the ML model. The model is being fed the vector values, such that each word from the customer is queried against a hashmap and the value corresponding to the word is the input vector. All of the keys in your hashmap represents the vocabulary, i.e. all the words you know. Now, how would you handle a case where the customer uses a word that is not already present in your vocabulary? There are many ways to solve this out-of-vocabulary (OOV) problem. One popular approach is to split the words into ‚Äúsubword‚Äù units, and use those subwords to learn your hashmap during model training stage. At the time of inference, you would again divide each incoming word into smaller subword units, find the word vector corresponding to each subword unit using the hashmap, then aggregate each subword vector to get the vector representation for the complete word. For example, let‚Äôs say we have the word tiktok, the corresponding subwords could be tik, ikt, kto, tok, tikt, ikto, ktok etc. This is the character n-gram division for the input word, where n is the subword sequence length, and is fixed by the modeler. Figure: example subword representations ","date":"2020-07-18","objectID":"/spell-checker-fasttext/:1:2","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/spell-checker-fasttext/"},{"categories":["NLP"],"content":"What is FastText? FastText is an open-source project from Facebook Research. It is a library for fast text-representations and classifications. It is written in C++ and supports multiprocessing. It can be used to train unsupervised word vectors and supervised classification tasks. For learning word-vectors it supports both skipgram and continuous bag-of-words approaches. FastText supports subword representations such that each word can be represented as a bag of character n-grams in addition to the word itself. Incorporating finer (subword level) information is pretty good for handling rare words. You can read more about the FastText approach in their paper here . For an example, let‚Äôs say you have a word ‚Äúsuperman‚Äù in FastText trained word embeddings (‚Äúhashmap‚Äù). Let‚Äôs assume the hyperparameters minimum and maximum length of ngram was set to 4. Corresponding to this word, the hashmap would have the following keys: Original word: superman n-gram size subword 4 \u003c\\sup 4 supe 4 uper 4 perm 4 erma 4 rman 4 man\u003e where ‚Äú\u003c‚Äù and ‚Äú\u003e‚Äù characters mark the start and end of a word respectively ","date":"2020-07-18","objectID":"/spell-checker-fasttext/:1:3","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/spell-checker-fasttext/"},{"categories":["NLP"],"content":"The Task: Spell-Checking In NLP, the learnt word embedding vectors not only have lexical representations for words, but the vector values also have semantically related positioning in the embedding space. We are going to try and build a spell-checker application based on FastText word vectors such that given a misspelled word, our task will be to find the word vector representation closest to the vector representation of that word in trained embedding space. We will work based on this simple heuristic: heuristic\r\rIF word exists in the Vocabulary ‚Äã Do not change the word ELSE ‚Äã Replace the word with the one closest to its sub-word representation \r\r ","date":"2020-07-18","objectID":"/spell-checker-fasttext/:2:0","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/spell-checker-fasttext/"},{"categories":["NLP"],"content":"Dataset \u0026 Benchmark For fun, let‚Äôs build and evaluate our spell-checker on the same training and testing data as this classic article by Peter Norvig, ‚ÄúHow to write a Spelling Corrector‚Äù. In this article, Norvig build a simple spelling corrector based on basic probability theory. Let‚Äôs see how does this FastText based approach hold up against it. ","date":"2020-07-18","objectID":"/spell-checker-fasttext/:2:1","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/spell-checker-fasttext/"},{"categories":["NLP"],"content":"Installing FastText Installation for FastText is straightforward. FastText can be used as a command line tool or via Python client. Click here to access the latest installation instructions for both approaches. ","date":"2020-07-18","objectID":"/spell-checker-fasttext/:2:2","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/spell-checker-fasttext/"},{"categories":["NLP"],"content":"Method 1: Using Pre-trained Word Vectors FastText provides pretrained word vectors based on common-crawl and wikipedia datasets. The details and download instructions for the embeddings can be found here. For a quick experiment, let‚Äôs load the largest pretrained model available from FastText and use that to perform spelling-correction. Download and unzip the trained vectors and binary model file. -\u003e wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip -\u003e unzip crawl-300d-2M-subword.zip There are two files inside the zip: crawl-300d-2M-subword.vec : This file contains the number of words (2M) and the size of each word (vector dimensions; 300) in the first line. All of the following lines start with the word (or the subword) followed by the 300 real number values representing the learnt word vector. crawl-300d-2M-subword.bin: This binary file is the exported model trained on the Common-Crawl dataset. As mentioned in his article, Norvig used spell-testset1.txt and spell-testset2.txt as development and test set respectively, to evaluate the performance of his spelling-corrector. I‚Äôm going to load the pretrained FastText model and make predictions based on the heurisitic defined above. I‚Äôm also going to borrow some of the evaluation code from Norvig. import io import fasttext def load_vectors(fname): fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore') n, d = map(int, fin.readline().split()) data = {} for line in fin: tokens = line.rstrip().split(' ') data[tokens[0]] = map(float, tokens[1:]) return data def spelltest(tests, model, vocab): \"Run correction(wrong) on all (right, wrong) pairs; report results.\" import time start = time.clock() good, unknown = 0, 0 n = len(tests) for right, wrong in tests: w = wrong if w in vocab: print('word: {} exists in the vocabulary. No correction required'.format(w)) else: w_old = w w = model.get_nearest_neighbors(w, k=1)[0][1] print(\"found replacement: {} for word: {}\".format(w, w_old)) good += (w == right) dt = time.clock() - start print('{:.0%} of {} correct at {:.0f} words per second ' .format(good / n, n, n / dt)) def Testset(lines): \"Parse 'right: wrong1 wrong2' lines into [('right', 'wrong1'), ('right', 'wrong2')] pairs.\" return [(right, wrong) for (right, wrongs) in (line.split(':') for line in lines) for wrong in wrongs.split()] if __name__ == \"__main__\": model = fasttext.load_model(\"crawl-300d-2M-subword.bin\") vocab = load_vectors(\"crawl-300d-2M-subword.vec\") spelltest(Testset(open('spell-testset1.txt')), model, vocab) spelltest(Testset(open('spell-testset2.txt')), model, vocab) For the sake of brevity, I‚Äôm reducing the output log to just the metrics trace. output\r\r‚Ä¶ 0% of 270 correct at 3 words per second. ‚Ä¶ 0% of 400 correct at 4 words per second.\r\r That didn‚Äôt go well! üòÖ None of the corrections from the model were right. Let‚Äôs dig into the results. Here are a few snapshots from the output log: output\r\r‚Ä¶ word: sysem exists in the vocabulary. No correction required word: controled exists in the vocabulary. No correction required word: reffered exists in the vocabulary. No correction required word: irrelavent exists in the vocabulary. No correction required word: financialy exists in the vocabulary. No correction required word: whould exists in the vocabulary. No correction required ‚Ä¶ found replacement: reequipped for word: reequired found replacement: putput for word: oputput found replacement: catecholate for word: colate found replacement: yeahNow for word: yesars found replacement: detale for word: segemnt found replacement: \u003cli\u003e\u003cstrong\u003eStyle for word: earlyest ‚Ä¶\r\r Looks like there are a lot of garbage subwords in the pretrained vocabulary that directly matches our misspelled input. Also, the performed lexical corrections show that the model replaced misspelled input words with the closest semantic neighbor. However none of those neighbors are meaningful English words. We can verify this by checking out the neighbors for random m","date":"2020-07-18","objectID":"/spell-checker-fasttext/:2:3","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/spell-checker-fasttext/"},{"categories":["NLP"],"content":"Method 2: Trained Our Own Word Vectors For a fair comparison with Norvig‚Äôs spell-checker, let‚Äôs use the same training data that he used (big.txt). From Norvig's article:\r\r‚Ä¶ by counting the number of times each word appears in a text file of about a million words, big.txt. It is a concatenation of public domain book excerpts from Project Gutenberg and lists of most frequent words from Wiktionary and the British National Corpus.\r\r -\u003e wc big.txt 128457 1095695 6488666 big.txt The training data has around 128K lines, 1M words, 6.5M characters. As discussed above, we should try with keeping the wordNgrams hyperparameter to 1, and use the trained FastText model to perform spell-checking. import io import fasttext def load_vectors(fname): fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore') n, d = map(int, fin.readline().split()) data = {} for line in fin: tokens = line.rstrip().split(' ') data[tokens[0]] = map(float, tokens[1:]) return data def spelltest(tests, model): \"Run correction(wrong) on all (right, wrong) pairs; report results.\" import time start = time.clock() good, unknown = 0, 0 n = len(tests) for right, wrong in tests: w_old = wrong w = wrong if w in model.words: pass else: w = model.get_nearest_neighbors(w, k=1)[0][1] good += (w == right) if not (w == right): if w_old != w: print(\"Edited {} to {}, but the correct word is: {}\".format(w_old, w, right)) dt = time.clock() - start print('{:.0%} of {} correct at {:.0f} words per second ' .format(good / n, n, n / dt)) def Testset(lines): \"Parse 'right: wrong1 wrong2' lines into [('right', 'wrong1'), ('right', 'wrong2')] pairs.\" return [(right, wrong) for (right, wrongs) in (line.split(':') for line in lines) for wrong in wrongs.split()] if __name__ == \"__main__\": model = fasttext.train_unsupervised('big.txt', wordNgrams=1, minn=1, maxn=2, dim=300, ws=8, neg=8, epoch=4, minCount=1, bucket=900000) spelltest(Testset(open('spell-testset1.txt')), model) spelltest(Testset(open('spell-testset2.txt')), model) There are a couple of changes in this code. We are training the model with specific hyper-parameters, and the output traces will inform us what went wrong in our decisions. Note that FastText doesn‚Äôt provide an option to set seed in the above implementation, so the results may vary by 1%-2% on every execution. -\u003e python fasttext_trained.py Read 1M words Number of words: 81398 Number of labels: 0 Progress: 100.0% words/sec/thread: 20348 lr: 0.000000 avg. loss 1.943424 ETA: 0h 0m 0s ... 73% of 270 correct at 46 words per second. ... 69% of 400 correct at 48 words per second. This is a big improvement! üòÑ We have almost the same accuracy as Norvig‚Äôs spell-corrector. Here are Norvig‚Äôs results for comparison. 75% of 270 correct at 41 words per second 68% of 400 correct at 35 words per second Looking at some of the edits in the output traces, I can see that the model output isn‚Äôt essentially incorrect, but the model is biased to certain edit-based operations. output\r\r‚Ä¶ Edited reffered to referred, but the correct word is: refered Edited applogised to apologized, but the correct word is: apologised Edited speeking to seeking, but the correct word is: speaking Edited guidlines to guideline, but the correct word is: guidelines Edited throut to throughout, but the correct word is: through Edited nite to unite, but the correct word is: night Edited oranised to organised, but the correct word is: organized Edited thay to thy, but the correct word is: they Edited stoped to stooped, but the correct word is: stopped Edited upplied to supplied, but the correct word is: applied Edited goegraphicaly to geographical, but the correct word is: geographically ‚Ä¶\r\r As you can see our trained model is producing dictionary based words as edited output. Maybe with more contextual test set and more training data, we can come up with an even better strategy to understand context in a full sentence and produce the correct word as the edit output. Let‚Äôs also compare the model with the pret","date":"2020-07-18","objectID":"/spell-checker-fasttext/:2:4","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/spell-checker-fasttext/"},{"categories":["NLP"],"content":"Conclusion Our model was able to match Peter Norvig‚Äôs spell-corrector. üòä Looking at the failing cases, we realize that the model could do even better with more training data and contextual evaluation during testing. ","date":"2020-07-18","objectID":"/spell-checker-fasttext/:3:0","tags":["embeddings"],"title":"Building a spell-checker with FastText word embeddings","uri":"/spell-checker-fasttext/"},{"categories":["software engineering"],"content":"Analyzing randomness is hard. So why would you want to choose a probabilistic data structure in your system implementation? Allow me to explain!","date":"2020-07-14","objectID":"/skip-list/","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/skip-list/"},{"categories":["software engineering"],"content":"Analyzing randomness is hard. So why would you want to choose a probabilistic data structure in your system implementation? Allow me to explain! ","date":"2020-07-14","objectID":"/skip-list/:0:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/skip-list/"},{"categories":["software engineering"],"content":"Why am I writing about Skip Lists? Skip List is one of those data structures that I have seen briefly mentioned in academic books; but never seen it being used in academic or industrial projects (or in LeetCode questions üôÇ ). However I was pleasantly surprised to see Skip List being one of the major contributors behind Twitter‚Äôs recent success in reducing their search indexing latency by more than 90% (absolute). This encouraged me to brush up my understanding of Skip List and also learn from Twitter‚Äôs solution. ","date":"2020-07-14","objectID":"/skip-list/:1:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/skip-list/"},{"categories":["software engineering"],"content":"Why do we need Skip Lists? I believe it‚Äôs easier to understand the concept of Skip Lists by seeing their application. So let‚Äôs first build a case for Skip List. ","date":"2020-07-14","objectID":"/skip-list/:2:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/skip-list/"},{"categories":["software engineering"],"content":"The Problem: Search Let‚Äôs say you have an array of sorted values as shown below, and your task is to find whether a given value exists in the array or not. Given an array: find whether value 57 exists in this array or not ","date":"2020-07-14","objectID":"/skip-list/:2:1","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/skip-list/"},{"categories":["software engineering"],"content":"Option 1: Linearly Searching an Array As the name suggests, in linear search you can simply traverse the list from one end to the other and compare each item you see with the target value you‚Äôre looking for. You can stop the linear traversal if you find a matching value. You can stop the search if at any point of time you see an item whose value is bigger than the one you‚Äôre looking for. Is this a good solution? [Click here]\r\rThis solution is alright. But as you would notice, it doesn‚Äôt take advantage of the fact that the given sequence of values are in sorted order. The worst case time complexity for this algorithm would be order of the number of values in the sequence, i.e. O(n). We will be comparing our target value with each member of sequence if the element is at the very end of the sequence or does not exist in the sequence.\r\r Can we do better?\r\rYes, with Binary Search!\r\r ","date":"2020-07-14","objectID":"/skip-list/:3:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/skip-list/"},{"categories":["software engineering"],"content":"Option 2: Binary Search on an Array The binary search concept is simple. Think of how you search for a given word in a dictionary. Say if you are trying to find the word ‚Äúquirky‚Äù in the dictionary, you do not need to linearly read each word in the dictionary one-by-one to know if the word exists in the dictionary. Because you have established that the dictionary has the all of the words in alphabetical sorted order. You can simply flip through the book, notice which word you are at, then either go left or right depending on whether the word starts with a character that comes before or after ‚Äòq‚Äô. Going back to our example, let‚Äôs first check the value at the center of the sequence, if the value matches our target. we can successfully end the search. But if it is not the value we are looking for, we will compare the value of the middle position with our target value. If our target value is smaller, we need to look into smaller numbers, so we go towards left and ignore the values on the right. By following this binary search approach have reduced our search space and the required number of comparisons by half. Also, we repeat the same process on the ‚Äúvalid‚Äù half, i.e. we directly go the middle value of the left sub-array and see if we have a match, if not, we again decide whether to go left or right, and hence reduce our candidate search space by half again. What is the time complexity now?\r\rThe time complexity of this algorithm is again defined by the number of comparisons. If you recognize the pattern in this algorithm, you would know that the algorithm belongs to the category for which the size of a variable is continuously divided by a constant (2 in this case). And hence our algorithmic time complexity is O(\\(log_2n\\)). [Read this for an intuitive explanation]\r\r This is excellent. Logarithmic time complexity means that our algorithm will scale really well with large number of values being stored in the sequential array. Is this a perfect solution?\r\rOne big issue, here, is that while sequential storage (for example, arrays) makes in easy and cheap to design binary search algorithms, it makes it costlier for us to implement an insertion and deletion strategy. For example, if you insert an item at a specific index in the array, you will have to shift the values on its right by one place to the right. Similarly if you delete a value at an arbitrary index inside the array, you will have to left shift all the values to the right of that index by one place. (see examples below)\r\r As you can imagine, having extra space on the right of the array would be convenient else you would have to call for dynamic allocation of memory at every insertion and copy over the whole array to newly allocated array memory. List data structure in Python, allocates list in a similar way that leaves capacity at the end of the sequence for future expansion. If we run out of space in the list, Python allocates a new and much larger (double the size, for example) sized list, copies over each value from the old list to new list, and then deletes the old list. (This is why the insertion in a Python list is Amortized O(1) complexity) Could there be a way to have a data structure with better insertion and deletion costs?\r\rYes, we can use Linked Lists!\r\r ","date":"2020-07-14","objectID":"/skip-list/:4:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/skip-list/"},{"categories":["software engineering"],"content":"Option 3: Linear search with Linked Lists Linked Lists are dynamic data structures that store values in non-contiguous memory blocks (called nodes). Each block refers to the other using memory pointers. These pointers are sort of the links in a chain that define the ordering among the non-contiguous nodes. A Singly Linked list represents a sequential structure where each node maintains a pointer to the element that follows it in the sequence. In linked lists we also maintain additional reference to the list nodes. Head is one such reference which is variable that stores the address for the first element in the list (stores null pointer if the list is empty). Insertion and deletion at any arbitrary location is straightforward, as you would simply reconnect the links of the nodes previous and next positions from the target position. We can also have a Doubly Linked List where each node stores the pointer to the previous as well as the next node in the sequence. A tail reference may also be used to store pointer to the last node in the linked list. As you would notice, random access in linked lists are not possible. And to find a value in the list, we would simply have to traverse the list from one end, say from the node referred by Head, towards the other end till we find what we are looking for. So, even though you can insert and delete in O(1), you still need the reference to the target location. Finding the target location is again O(n). With Linked Lists, while we solved the issue of costly insertions and deletions, we haven‚Äôt quite found a good solution to quickly access a target value in the list (or find if the value exists). Wouldn‚Äôt it be great if we could have the random access (or something close to it) in the list to simply our find, insert and delete operations? Well this is where Skip List come handy! ","date":"2020-07-14","objectID":"/skip-list/:5:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/skip-list/"},{"categories":["software engineering"],"content":"Option 4: Pseudo-binary search with Linked Lists [Skip List] Technical advances have made low latency highly desirable for software applications. How well the underlying algorithm scales to demand can make or break a product. On the other hand, the memory/storage costs is getting cheaper by day. This propels many system designers to make decisions that balance compromises that spend extra money on storage costs, while opting for algorithms with low time complexities. Skip Lists is a good example of one such compromise. Skip Lists were proposed by WIlliam Pugh in 1989 (read the original paper ¬†here) as a probabilistic alternative to balanced trees. Skip lists are relatively easy to implement, specially when compared to balanced trees like AVL Trees or Red Black Trees. If you have used the Balanced Trees before, you may remember how they are great data structures to maintain relative order of elements in a sequence, while making search operations easy. However, insertions and deletions in Balanced Trees are quite complex as we have the additional task to strictly ensure the tree is balanced at all times. Skip Lists introduce randomness in by making balancing of the data structure to be probabilistic. While it definitely makes evaluating complexity of the algorithm to be much harder, the insertion, deletion operations become much faster and simpler to implement. The complexities are calculated based on expectation, but they have a high probability, such that we can say with high confidence (but not absolute guarantee) that the run time complexity would be O(log(n)) apart from some specific worst-case scenarios as we would see soon. ","date":"2020-07-14","objectID":"/skip-list/:6:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/skip-list/"},{"categories":["software engineering"],"content":"Skip Lists - Intuition Following is the map of New York Subway‚Äôs ‚Äú7‚Äù train. Each point on the horizontal line represents on train stop on a specific street. The line at the top is the local train that has much more number of stops compared to the express line on the below it. Government decides where to put the stops based on factors like nearby facilities, neighborhood, business centers etc. Let‚Äôs simplify the above route map and display stations by the street numbers. In the simplified map above, the express line is shown at the top and local line is below it. Now if you had this map and you were supposed to travel from stop 14 to stop 59. How would you plan your trip route? For simplicity let‚Äôs assume we can‚Äôt overshoot, meaning that we can‚Äôt go some station to the right and then travel back towards left. You would probably want to take the express line from stop 14 and get down on stop 42. Then take the local train from stop 42 and reach the destination 59 from there, as shown below. As you would notice, by taking the express route you would be able to skip some stations along the way and maybe even reach the destination much faster compared to taking the local train from a original stop and traveling each station one by one to reach a destination stop. This is exactly the core idea behind Skip List. With Skip Lists, we introduce redundant travel paths at the cost of additional space, but the additional travel paths have lesser and lesser number of ‚Äústops‚Äù such that the linear traversal is much faster if we travel in those additional lanes. ","date":"2020-07-14","objectID":"/skip-list/:6:1","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/skip-list/"},{"categories":["software engineering"],"content":"Skip Lists - Basics Let‚Äôs now build the concept of Skips Lists from bottom to top. Assume that we have a Doubly Linked List which is unsorted. What do you think would be the search complexity for this list? Search would be done in O(n), because you would have to traverse the list linearly to find your target node. Let‚Äôs assume that the list is now sorted as shown below. What do you think is the search complexity now? Well, it‚Äôs still O(n). Even if it sorted, the linked list structure doesn‚Äôt provide us random access, hence we can‚Äôt simply calculate the next middle index and access that middle node in O(1). Now, let‚Äôs put our ‚Äúexpress line‚Äù on top of the existing structure. We are going to create a second Doubly Linked List and place the second list above of the first list such that each node has four reference to neighboring nodes: left, right, above, below. which can be visualized as: Now given this Skip Lists structure, you can find the number 73 with lesser number of comparison operations. You can start traversing from the top list first, and at every node see if the next node overshoots the value that you are looking for. If it doesn‚Äôt simply go the next node in the list on the top, else go to the list on the bottom and continue your linear search in the bottom list. We can summarize the searching approach in a simple algorithm. Algorithm\r\r Walk right in the Linked List on top (L1) until going right is going too far Walk down to the bottom list (L0) Walk right in L0 until element is found or we overshoot. \r\r So what is the kind of search cost we are looking at? We would want to travel the length of L1 as much as possible because that gives us skips over unnecessary comparisons. And eventually we may go down to L0 and traverse a portion of it. The idea is to uniformly disperse the nodes in the top lane. Search Cost = \\(|L1| + \\frac{|L0|}{|L1|}\\) How many ‚Äústops‚Äù (nodes) do you need in the top lane? It can be mathematically shown1 that if the cardinality of the bottom list is n (i.e. it has n nodes), we will get an optimal solution if we have \\(\\sqrt{n}\\) nodes in the top layer. So, Search Cost = \\(\\sqrt{n} + \\frac{n}{\\sqrt{n}}\\) = 2\\(\\sqrt{n}\\) = O(\\(\\sqrt{n})\\)) However, our Skip List don‚Äôt have to limited to 2 lists. We can have more lists (‚Äúlanes‚Äù), meaning our search costs would scale as: ‚Äé‚Äè‚Äè‚Äé2 sorted lists ‚Üí \\(2 * \\sqrt[2]{n}\\) 3 sorted lists ‚Üí \\(3 * \\sqrt[3]{n}\\) ‚Äé‚Äè‚Äè‚Äé‚Ä¶ k sorted lists ‚Üí \\(k * \\sqrt[k]{n}\\) (if we have log(n) number of sorted lists:)‚Äé‚Äé‚Äé‚Äé‚Äé‚Äé‚Äé‚Äé‚Äé‚Äé‚Äé‚Äè‚Äè‚Äé ‚Äé‚Äé‚Äé‚Äé‚Äé‚Äé‚Äé‚Äé‚Äé‚Äé‚Äélog(n) sorted lists ‚Üí \\(log(n) * \\sqrt[log(n)]{n}\\) = 2 \\(\\log{n}\\) ‚Äé‚Äé‚Äé‚Äé‚Äé‚Äè‚Äé‚Äé‚Äè‚Äè So, if the nodes are optimally dispersed, theoretically the structure gives us our favorite logarithmic complexity üôÇ (Although in implementation when we use randomization to insert the nodes, things get a little tricky). Now think of the structure we have created. At the bottom we have n nodes, the layer on top has interspersed \\(\\frac{n}{2}\\) nodes and the layer above it has \\(\\frac{n}{4}\\) nodes. Do you how this structure looks quite similar to a tree? We just need to figure out how ‚Äúbalancing‚Äù works in this tree (hint: it‚Äôs probabilistic). ","date":"2020-07-14","objectID":"/skip-list/:6:2","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/skip-list/"},{"categories":["software engineering"],"content":"Skip List - Definition Skip List S consists of a series of lists {\\(S_0,S_1 .. S_h\\)} such that each list \\(S_i\\) stores a subset of the items sorted by increasing keys. Also note:\r\r h represents the height of the Skip List S each list also has two sentinel nodes -\\(\\infty\\) and +\\(\\infty\\) . -\\(\\infty\\) is smaller than any possible key in the list and +\\(\\infty\\) is greater than any possible key in the list. Hence the node with -\\(\\infty\\) key is always on the leftmost position and node with +\\(\\infty\\) key is always the rightmost node in the list For visualization, it is customary to have the \\(S_0\\) list on the bottom and lists \\(S_1, S_2, .. S_h\\) above it Each node in a list has to be ‚Äúsitting‚Äù on another node with same key below it. Meaning that if \\(L_{i+1}\\) has a node with key k, then \\(L_i, L_{i-1} ..\\) all valid lists below it will have the same key in them We may chose to opt for an implementation of Skip List that only uses Single Linked Lists. However it may only improve the asymptotic complexity by a constant factor Skip List also needs to maintain the ‚Äúhead‚Äù pointer, i.e. the reference to the first member (sentinel node -\\(\\infty\\)) in the topmost list On average the Skip List will have O(n) space complexity \r\r Let‚Äôs first simplify the visualization for our nodes and connection in Skip List. A standard, simplified Skip List would look like this: Now, let‚Äôs see how some of the basic operations are performed on a Skip List. ","date":"2020-07-14","objectID":"/skip-list/:6:3","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/skip-list/"},{"categories":["software engineering"],"content":"Searching in Skip List Searching in a Skip List in straightforward. We have already seen an example above with two lists. We can extend the same approach to a Skip List with h lists. We will simply start with the leftmost node in the list on top. Go down if the next value in the same list is greater than the one we are looking for, go right otherwise. The main idea is again to skip comparisons with as many keys as possible, while compromising a little on the extra storage required in the additional lists containing subset of all keys. On average, ‚Äúexpected‚Äù Search complexity for Skip List is O(log(n)) ","date":"2020-07-14","objectID":"/skip-list/:6:4","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/skip-list/"},{"categories":["software engineering"],"content":"Inserting in Skip List To insert a new node, we would first find the location where this new key should be inserted in the list at the very bottom. This can be simply done be re-using the search logic, we start traversing to the right from the list on top and we go one step down if the next item is bigger than the key that we want to insert, else go right. Once we reach at a position in bottom list where we can‚Äôt go more to the right, we insert the new value on the right side of that position. Skip List after inserting 67: Now comes the probabilistic part, after inserting the new node in the list \\(L_i\\) we have to also insert the node in list above it, i.e. list \\(L_{i+1}\\) with probability \\(\\frac{1}{2}\\). We do this with a simple coin toss, such that if we get a head we insert the node in the list \\(L_{i+1}\\) and toss the coin again, we stop when we get a tail. We may repeat the coin toss till we keep getting heads, even if we have to insert a new layer (list) at the top. Note that we also need to ‚Äúre-hook‚Äù the links for the newly inserted node. The new node needs to have it‚Äôs ‚Äòbelow‚Äô reference pointing to the node below, and the node below would have ‚Äòabove‚Äô reference pointer to the new node. To find the ‚Äòleft‚Äô neighbor for the new node, we simply traverse towards left from the node below it and return ‚Äòabove‚Äô pointer from a node for which ‚Äòabove‚Äô pointer is not null. The ‚Äòright‚Äô reference of the ‚Äòleft‚Äô neighbor is used to update the ‚Äòright‚Äô reference of the new node. Also, the ‚Äòleft‚Äô reference for the new node‚Äôs right neighbor is also updated to the new node. This re-hooking operation is actually pretty easy to implement with Linked Lists. Let‚Äôs toss the coin again. Last toss gave us another Head, so let‚Äôs toss the coin again. We got a Tail this time, so no more insertions are required. As you see the probability of a node also getting inserted in the layer above it get reduced by half after every layer. Still, there is a worst case possibility that you would keep getting Heads indefinitely, although the probability of that happening is extremely small. To avoid such cases when you may get a large number of heads sequentially, you could also use a termination condition where you stop inserting if you reach a predefined threshold for the number of layers (height) or a predefined threshold for the number of nodes in a specific layer. Although the expected time complexity would still be O(log(n)) for all operations. ","date":"2020-07-14","objectID":"/skip-list/:6:5","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/skip-list/"},{"categories":["software engineering"],"content":"Deletion in Skip List Deletion operation in Skip List is pretty straightforward. We first perform search operation to find the location of the node. If we find the node, we simply delete it at all levels. Before deleting a node, we simply ensure that no other node is pointing to it. Release all references to this node. And move to the node below it. Remove all references to this node and release it. Move to the node below in the bottom list. Release this node as well. This is how our Skip List looks like after deletion. ","date":"2020-07-14","objectID":"/skip-list/:6:6","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/skip-list/"},{"categories":["software engineering"],"content":"Conclusion In this article we saw how Skip List is a probabilistic data structure that loosely resembles a balanced binary tree. It is also easy to implement and very fast as the search, insert, delete operations have an expected time complexity of O(log(n)), along with O(n) expected space complexity. in next article, I will introduce a practical application of Skip List and explain how Twitter used Skip List to get a huge improvement in their search indexing latency. ","date":"2020-07-14","objectID":"/skip-list/:7:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/skip-list/"},{"categories":["software engineering"],"content":"Reference and Further Reading MIT OCW 6.046 Design and Analysis of Algorithms Lecture 7: Randomization: Skip Lists ‚Ü©Ô∏é ","date":"2020-07-14","objectID":"/skip-list/:8:0","tags":["data structure"],"title":"Skip List Data Structure - Explained!","uri":"/skip-list/"},{"categories":null,"content":" Sumit Kumar - UChicago Alum. Research Scientist 2 at Alexa AI, Amazon Prev-SDE2@Amazon, Lead Engineer@Samsung R\u0026D, SSE@Altran ‚Äé‚Äé‚Äé‚Äé‚Äé‚Äè‚Äè‚Äé ‚Äé ","date":"2020-07-14","objectID":"/about/:0:0","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"Brief My name is Sumit Kumar. I love learning new things. I have more than 8 years of work exeperience in Software Development, Machine Learning/Deep Learning research. Currently I work as a Research Scientist II for Alexa AI at Amazon, Seattle. And, my area of research focus is Natural Language Understanding. ","date":"2020-07-14","objectID":"/about/:0:1","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"Long Form üòÑ Sumit joined the University of Chicago‚Äôs Graham School as a full-time, international student in the autumn 2017 cohort. He is currently a Research Scientist 2 at Amazon‚Äôs headquarters in Seattle. He has seven years of experience in software engineering and research and has worked extensively with major programming languages including C, C++, JavaScript, Java, MATLAB, R, and Python. After earning his bachelor‚Äôs degree in computer science, he started his career at Aricent Group as a software developer where he worked on different telecom projects, writing software for simulating radio network controllers that were used by Nokia-Siemens Networks. In 2012, Sumit joined Samsung R\u0026D in Noida, India and developed the L3 software protocol stack used today in Samsung‚Äôs smartphone modems. He has worked on various telecom projects in Canada, Australia, South Korea, and India, where he developed features and algorithms compliant with 3GPP standard for WCDMA/LTE wireless communication. During this period, he also collaborated with various telecom vendors across the world and was instrumental in success of many critical projects. In 2015, Sumit was promoted to lead engineer and started working in the advanced R\u0026D division at Samsung. He mentored engineers in addition to his independent research work. Sumit developed various novel solutions and algorithms during his research at Samsung and is the inventor and co-author for seven algorithms filed for patents by Samsung. Apart from his research work, he also created tools to automate many manual processes at Samsung. He did an extensive amount of machine-learning based research in audio DSP domain to solve problems such as Blind Source Separation (Cocktail Party Problem) and Audio Directionality and Speaker Diarization. During this time, he also volunteered as a teacher in Samsung‚Äôs Corporate Social Responsibility (CSR) mission to help under-funded schools. During his time as a student at the Graham School, Sumit also worked as a research assistant at the Research Computing Center (RCC), UChicago. There he worked on web development projects, based on Django frameworks in Python, to help researchers from different universities connect with each other and share large volumes of data. He was the International Student Representative for the Master of Science in Analytics (MScA) program at Graham School to enhance international students‚Äô experience and engagement and was also a member of the UChicago cricket team. Sumit has also taught Advanced Analytics and Machine Learning certificate course at The University of Chicago and Python for Internet Programming certificate course at The University of Washington. [Source] [Web Archive backup] ","date":"2020-07-14","objectID":"/about/:0:2","tags":null,"title":"About me","uri":"/about/"}]