<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>NLP - Category - Sumit&#39;s Diary</title>
        <link>https://blog.reachsumit.com/categories/nlp/</link>
        <description>NLP - Category - Sumit&#39;s Diary</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>sam.sumitkumar@gmail.com (Sumit Kumar)</managingEditor>
            <webMaster>sam.sumitkumar@gmail.com (Sumit Kumar)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Mon, 07 Dec 2020 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.reachsumit.com/categories/nlp/" rel="self" type="application/rss+xml" /><item>
    <title>Towards Empathetic NLP Dialogue Systems</title>
    <link>https://blog.reachsumit.com/posts/2020/12/generating-empathetic-responses/</link>
    <pubDate>Mon, 07 Dec 2020 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://blog.reachsumit.com/posts/2020/12/generating-empathetic-responses/</guid>
    <description><![CDATA[<p>Recognizing feelings in the conversation partner and replying empathetically is a trivial skill for humans. But how can we infuse empathy into responses generated by a conversational dialogue agent or Natural Language Processing algorithm in general? In this article, I will describe what empathy means through the lens of various academic disciplines and then do an in-depth review of the prior and current state-of-the-art NLU systems that can simulate empathy.</p>]]></description>
</item><item>
    <title>Building a spell-checker with FastText word embeddings</title>
    <link>https://blog.reachsumit.com/posts/2020/07/spell-checker-fasttext/</link>
    <pubDate>Sat, 18 Jul 2020 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://blog.reachsumit.com/posts/2020/07/spell-checker-fasttext/</guid>
    <description><![CDATA[<p>Word vector representations with subword information are great for NLP modeling. But can we make lexical corrections using a trained embeddings space? Can its accuracy be high enough to beat Peter Norvig&rsquo;s spell-corrector? Let&rsquo;s find out!</p>]]></description>
</item></channel>
</rss>
