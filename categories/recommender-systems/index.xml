<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Recommender Systems - Category - Sumit&#39;s Diary</title>
        <link>https://blog.reachsumit.com/categories/recommender-systems/</link>
        <description>Recommender Systems - Category - Sumit&#39;s Diary</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>hello@reachsumit.com (Sumit Kumar)</managingEditor>
            <webMaster>hello@reachsumit.com (Sumit Kumar)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sat, 22 Jun 2024 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://blog.reachsumit.com/categories/recommender-systems/" rel="self" type="application/rss+xml" /><item>
    <title>The Evolution of Multi-task Learning Based Video Recommender Systems - Part 2</title>
    <link>https://blog.reachsumit.com/posts/2024/06/multi-task-video-recsys-p2/</link>
    <pubDate>Sat, 22 Jun 2024 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2024/06/multi-task-video-recsys-p2/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2024/06/multi-task-video-recsys-p2/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>This article continues the discussion on the evolution of multi-task learning-based large-scale recommender systems. We take a look at strategies from Kuaishou, Tencent, YouTube, Facebook, and Amazon Prime Video to disentangle input space and address systematic biases. The article ends with sharing several tips and learnings for professionals working in this domain.]]></description>
</item><item>
    <title>The Evolution of Multi-task Learning Based Video Recommender Systems - Part 1</title>
    <link>https://blog.reachsumit.com/posts/2024/06/multi-task-video-recsys-p1/</link>
    <pubDate>Sun, 16 Jun 2024 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2024/06/multi-task-video-recsys-p1/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2024/06/multi-task-video-recsys-p1/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>This article introduces the multi-task learning paradigm adopted by various large-scale video recommender systems. It introduces a general setup for such an MTL-based recommender. It highlights several associated challenges and describes solutions adopted by various state-of-the-art recommenders in the industry.]]></description>
</item><item>
    <title>An Introduction to Multi-Task Learning based Recommender Systems</title>
    <link>https://blog.reachsumit.com/posts/2024/01/multi-task-learning-recsys/</link>
    <pubDate>Fri, 26 Jan 2024 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2024/01/multi-task-learning-recsys/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2024/01/multi-task-learning-recsys/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>This article provides an introduction and literature review for multi-task learning based recommender systems. We learn how to discover task relations, design MTL architectures and overcome some of the associated challenges.]]></description>
</item><item>
    <title>A Guide to User Behavior Modeling</title>
    <link>https://blog.reachsumit.com/posts/2024/01/user-behavior-modeling-recsys/</link>
    <pubDate>Sun, 07 Jan 2024 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2024/01/user-behavior-modeling-recsys/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2024/01/user-behavior-modeling-recsys/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>Modeling users&rsquo; past historical interactions or behavior sequences is an essential task for domains like recommender systems, click-through rate prediction, targeted advertisement, and more. This article provides a comprehensive introduction to the user behavior modeling paradigm along with highlighting several relevant and recent research works.]]></description>
</item><item>
    <title>Representing Users and Items in Large Language Models based Recommender Systems</title>
    <link>https://blog.reachsumit.com/posts/2023/06/llms-for-recsys-entity-representation/</link>
    <pubDate>Sun, 18 Jun 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/06/llms-for-recsys-entity-representation/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/06/llms-for-recsys-entity-representation/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>Large Language Models (LLMs) have emerged as viable tools for various recommendation tasks. This article highlights various methods for incorporating users, items, and behavior data into the instructions for LLMs.]]></description>
</item><item>
    <title>Tuning Large Language Models for Recommendation Tasks</title>
    <link>https://blog.reachsumit.com/posts/2023/05/tuning-llm-for-recsys/</link>
    <pubDate>Sun, 21 May 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/05/tuning-llm-for-recsys/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/05/tuning-llm-for-recsys/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>Instruction-tuning methods enable open-source Large Language Models (LLMs) usage for building highly effective recommender systems on private data. This article highlights the latest research work on this paradigm of using LLMs for recommendation tasks.]]></description>
</item><item>
    <title>ChatGPT-based Recommender Systems</title>
    <link>https://blog.reachsumit.com/posts/2023/05/chatgpt-for-recsys/</link>
    <pubDate>Mon, 15 May 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/05/chatgpt-for-recsys/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/05/chatgpt-for-recsys/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div><p>With its outstanding performance, ChatGPT has become a hot topic of discussion in the NLP community and beyond. This article delves into recent efforts to harness the power of ChatGPT for recommendation tasks.</p>]]></description>
</item><item>
    <title>Mixture-of-Experts based Recommender Systems</title>
    <link>https://blog.reachsumit.com/posts/2023/04/moe-for-recsys/</link>
    <pubDate>Sun, 23 Apr 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/04/moe-for-recsys/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/04/moe-for-recsys/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div><p>The Mixture-of-Experts (MoE) is a classical ensemble learning technique originally proposed by Jacobs et. al<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> in 1991. MoEs have the capability to substantially scale up the model capacity and only introduce small computation overhead. This ability combined with recent innovations in the deep learning domain has led to the wide-scale adoption of MoEs in healthcare, finance, pattern recognition, etc. They have been successfully utilized in large-scale applications such as Large Language Modeling (LLM), Machine Translation, and Recommendations. This article gives an introduction to Mixture-of-Experts and some of the most important enhancements made to the original MoE proposal. Then we look at how MoEs have been adapted to compute recommendations by looking at examples of such systems in production.</p>]]></description>
</item><item>
    <title>Diffusion Modeling based Recommender Systems</title>
    <link>https://blog.reachsumit.com/posts/2023/04/diffusion-for-recsys/</link>
    <pubDate>Mon, 17 Apr 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/04/diffusion-for-recsys/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/04/diffusion-for-recsys/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div>Diffusion Models have exhibited state-of-the-art results in image and audio synthesis domains. A recent line of research has started to adopt Diffusion for recommender systems. This article introduces Diffusion and its relevance to the recommendations domain and also highlights some of the most recent proposals on this novel theme.]]></description>
</item><item>
    <title>Zero and Few Shot Recommender Systems based on Large Language Models</title>
    <link>https://blog.reachsumit.com/posts/2023/04/llm-for-recsys/</link>
    <pubDate>Mon, 10 Apr 2023 00:00:00 &#43;0000</pubDate><author>
        <name>Sumit Kumar</name>
    </author><guid>https://blog.reachsumit.com/posts/2023/04/llm-for-recsys/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/2023/04/llm-for-recsys/featured-image-preview.webp" referrerpolicy="no-referrer">
            </div><p>Recent developments in Large Language Models (LLMs) have brought a significant paradigm shift in Natural Language Processing (NLP) domain. These pretrained language models encode an extensive amount of world knowledge, and they can be applied to a multitude of downstream NLP applications with zero or just a handful of demonstrations.</p>
<p>While existing recommender systems mainly focus on behavior data, large language models encode extensive world knowledge mined from large-scale web corpora. Hence these LLMs store knowledge that can complement the behavior data. For example, an LLM-based system, like ChatGPT, can easily recommend buying turkey on Thanksgiving day, in a zero-shot manner, even without having click behavior data related to turkeys or Thanksgiving.</p>
<p>Many researchers have recently proposed different approaches to building recommender systems using LLMs. These methods convert different recommendation tasks into either language understanding or language generation templates. This article highlights the prominent work done on this theme.</p>]]></description>
</item></channel>
</rss>
